version: 2.1
jobs:
  pytest:
    resource_class: xlarge
    docker:
      - image: python:3.10-slim
    steps:
      - checkout
      - run:
          name: Install Git
          command: |
            apt-get update && apt-get install -y git
      - run:
          name: Install uv
          command: |
            pip install uv
      - restore_cache:
          keys:
            - uv-cache-pytest-{{ checksum "uv.lock" }}
            - uv-cache-pytest-
      - run:
          name: Run pytest
          no_output_timeout: 20m
          command: |
            uv sync --group dev --group aws --group spark
            export GIT_PYTHON_REFRESH=quiet
            uv run pytest -k "not test_autonlinear_longer_horizon" --no-cov
      - save_cache:
          key: uv-cache-pytest-{{ checksum "uv.lock" }}
          paths:
            - ~/.cache/uv
            - .venv
  test-model-performance:
    resource_class: xlarge
    docker:
      - image: python:3.10-slim
    steps:
      - checkout
      - run:
          name: Install Git
          command: |
            apt-get update && apt-get install -y git
      - run:
          name: Install uv
          command: |
            pip install uv
      - restore_cache:
          keys:
            - uv-cache-perf-{{ checksum "uv.lock" }}
            - uv-cache-perf-
      - run:
          name: Run model performance tests
          command: |
            uv sync --group dev --group aws --group spark
            uv pip install -r ./action_files/test_models/requirements.txt
            export PYTHONPATH="${PYTHONPATH}:./action_files/test_models"
            uv run python -m src.models
            uv run python -m src.evaluation
      - save_cache:
          key: uv-cache-perf-{{ checksum "uv.lock" }}
          paths:
            - ~/.cache/uv
            - .venv
      - store_artifacts:
          path: ./action_files/test_models/data/evaluation.csv
          destination: evaluation.csv
  test-model-performance2:
    resource_class: xlarge
    docker:
      - image: python:3.10-slim
    steps:
      - checkout
      - run:
          name: Install Git
          command: |
            apt-get update && apt-get install -y git
      - run:
          name: Install uv
          command: |
            pip install uv
      - restore_cache:
          keys:
            - uv-cache-perf-{{ checksum "uv.lock" }}
            - uv-cache-perf-
      - run:
          name: Run model performance tests
          command: |
            uv sync --group dev --group aws --group spark
            uv pip install -r ./action_files/test_models/requirements.txt
            export PYTHONPATH="${PYTHONPATH}:./action_files/test_models"
            uv run python -m src.models2
            uv run python -m src.evaluation2
      - save_cache:
          key: uv-cache-perf-{{ checksum "uv.lock" }}
          paths:
            - ~/.cache/uv
            - .venv
      - store_artifacts:
          path: ./action_files/test_models/data/evaluation.csv
          destination: evaluation.csv
  test-multivariate-model-performance:
    resource_class: xlarge
    docker:
      - image: python:3.10-slim
    steps:
      - checkout
      - run:
          name: Install Git
          command: |
            apt-get update && apt-get install -y git
      - run:
          name: Install uv
          command: |
            pip install uv
      - restore_cache:
          keys:
            - uv-cache-perf-{{ checksum "uv.lock" }}
            - uv-cache-perf-
      - run:
          name: Run model performance tests
          command: |
            uv sync --group dev --group aws --group spark
            uv pip install -r ./action_files/test_models/requirements.txt
            export PYTHONPATH="${PYTHONPATH}:./action_files/test_models"
            uv run python -m src.multivariate_models
            uv run python -m src.multivariate_evaluation
      - save_cache:
          key: uv-cache-perf-{{ checksum "uv.lock" }}
          paths:
            - ~/.cache/uv
            - .venv
      - store_artifacts:
          path: ./action_files/test_models/data/multi_evaluation.csv
          destination: multi_evaluation.csv

workflows:
  sample:
    jobs:
      - pytest
      - test-model-performance
      - test-model-performance2
      - test-multivariate-model-performance
