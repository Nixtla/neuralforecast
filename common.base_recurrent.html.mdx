---
output-file: common.base_recurrent.html
title: BaseRecurrent
---


> The `BaseRecurrent` class contains standard methods shared across
> recurrent neural networks; these models possess the ability to process
> variable-length sequences of inputs through their internal memory
> states. The class is represented by
> [`LSTM`](https://Nixtla.github.io/neuralforecast/models.lstm.html#lstm),
> [`GRU`](https://Nixtla.github.io/neuralforecast/models.gru.html#gru),
> and
> [`RNN`](https://Nixtla.github.io/neuralforecast/models.rnn.html#rnn),
> along with other more sophisticated architectures like `MQCNN`.

The standard methods include `TemporalNorm` preprocessing, optimization
utilities like parameter initialization, `training_step`,
`validation_step`, and shared `fit` and `predict` methods.These shared
methods enable all the `neuralforecast.models` compatibility with the
`core.NeuralForecast` wrapper class.

------------------------------------------------------------------------

### BaseRecurrent

> ``` text
>  BaseRecurrent (h, input_size, inference_input_size, loss, valid_loss,
>                 learning_rate, max_steps, val_check_steps, batch_size,
>                 valid_batch_size, scaler_type='robust', num_lr_decays=0,
>                 early_stop_patience_steps=-1, futr_exog_list=None,
>                 hist_exog_list=None, stat_exog_list=None,
>                 num_workers_loader=0, drop_last_loader=False,
>                 random_seed=1, alias=None, optimizer=None,
>                 optimizer_kwargs=None, **trainer_kwargs)
> ```

Base Recurrent

Base class for all recurrent-based models. The forecasts are produced
sequentially between windows.

This class implements the basic functionality for all windows-based
models, including: - PyTorch Lightning’s methods training_step,
validation_step, predict_step. <br/> - fit and predict methods used by
NeuralForecast.core class. <br/> - sampling and wrangling methods to
sequential windows. <br/>

------------------------------------------------------------------------

### BaseRecurrent.fit

> ``` text
>  BaseRecurrent.fit (dataset, val_size=0, test_size=0, random_seed=None,
>                     distributed_config=None)
> ```

Fit.

The `fit` method, optimizes the neural network’s weights using the
initialization parameters (`learning_rate`, `batch_size`, …) and the
`loss` function as defined during the initialization. Within `fit` we
use a PyTorch Lightning `Trainer` that inherits the initialization’s
`self.trainer_kwargs`, to customize its inputs, see [PL’s trainer
arguments](https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.trainer.trainer.trainer.html?highlight=trainer).

The method is designed to be compatible with SKLearn-like classes and in
particular to be compatible with the StatsForecast library.

By default the `model` is not saving training checkpoints to protect
disk memory, to get them change `enable_checkpointing=True` in
`__init__`.

**Parameters:**<br/> `dataset`: NeuralForecast’s
[`TimeSeriesDataset`](https://Nixtla.github.io/neuralforecast/tsdataset.html#timeseriesdataset),
see
[documentation](https://nixtla.github.io/neuralforecast/tsdataset.html).<br/>
`val_size`: int, validation size for temporal cross-validation.<br/>
`test_size`: int, test size for temporal cross-validation.<br/>
`random_seed`: int=None, random_seed for pytorch initializer and numpy
generators, overwrites model.\_\_init\_\_’s.<br/>

------------------------------------------------------------------------

### BaseRecurrent.predict

> ``` text
>  BaseRecurrent.predict (dataset, step_size=1, random_seed=None,
>                         **data_module_kwargs)
> ```

Predict.

Neural network prediction with PL’s `Trainer` execution of
`predict_step`.

**Parameters:**<br/> `dataset`: NeuralForecast’s
[`TimeSeriesDataset`](https://Nixtla.github.io/neuralforecast/tsdataset.html#timeseriesdataset),
see
[documentation](https://nixtla.github.io/neuralforecast/tsdataset.html).<br/>
`step_size`: int=1, Step size between each window.<br/> `random_seed`:
int=None, random_seed for pytorch initializer and numpy generators,
overwrites model.\_\_init\_\_’s.<br/> `**data_module_kwargs`: PL’s
TimeSeriesDataModule args, see
[documentation](https://pytorch-lightning.readthedocs.io/en/1.6.1/extensions/datamodules.html#using-a-datamodule).

