---
output-file: models.dilated_rnn.html
title: Dilated RNN
---


The Dilated Recurrent Neural Network
([`DilatedRNN`](https://Nixtla.github.io/neuralforecast/models.dilated_rnn.html#dilatedrnn))
addresses common challenges of modeling long sequences like vanishing
gradients, computational efficiency, and improved model flexibility to
model complex relationships while maintaining its parsimony. The
[`DilatedRNN`](https://Nixtla.github.io/neuralforecast/models.dilated_rnn.html#dilatedrnn)
builds a deep stack of RNN layers using skip conditions on the temporal
and the network’s depth dimensions. The temporal dilated recurrent skip
connections offer the capability to focus on multi-resolution inputs.The
predictions are obtained by transforming the hidden states into contexts
$\mathbf{c}_{[t+1:t+H]}$, that are decoded and adapted into
$\mathbf{\hat{y}}_{[t+1:t+H],[q]}$ through MLPs.

where $\mathbf{h}_{t}$, is the hidden state for time $t$,
$\mathbf{y}_{t}$ is the input at time $t$ and $\mathbf{h}_{t-1}$ is the
hidden state of the previous layer at $t-1$, $\mathbf{x}^{(s)}$ are
static exogenous inputs, $\mathbf{x}^{(h)}_{t}$ historic exogenous,
$\mathbf{x}^{(f)}_{[:t+H]}$ are future exogenous available at the time
of the prediction.

**References**<br/>-[Shiyu Chang, et al. “Dilated Recurrent Neural
Networks”.](https://arxiv.org/abs/1710.02224)<br/>-[Yao Qin, et al. “A
Dual-Stage Attention-Based recurrent neural network for time series
prediction”.](https://arxiv.org/abs/1704.02971)<br/>-[Kashif Rasul, et
al. “Zalando Research: PyTorch Dilated Recurrent Neural
Networks”.](https://arxiv.org/abs/1710.02224)<br/>

<figure>
<img src="imgs_models/dilated_rnn.png"
alt="Figure 1. Three layer DilatedRNN with dilation 1, 2, 4." />
<figcaption aria-hidden="true">Figure 1. Three layer DilatedRNN with
dilation 1, 2, 4.</figcaption>
</figure>

------------------------------------------------------------------------

<a
href="https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/models/dilated_rnn.py#L289"
target="_blank" style={{ float: "right", fontSize: "smaller" }}>source</a>

### DilatedRNN

> ``` text
>  DilatedRNN (h:int, input_size:int=-1, inference_input_size:int=-1,
>              cell_type:str='LSTM', dilations:List[List[int]]=[[1, 2], [4,
>              8]], encoder_hidden_size:int=200, context_size:int=10,
>              decoder_hidden_size:int=200, decoder_layers:int=2,
>              futr_exog_list=None, hist_exog_list=None,
>              stat_exog_list=None, loss=MAE(), valid_loss=None,
>              max_steps:int=1000, learning_rate:float=0.001,
>              num_lr_decays:int=3, early_stop_patience_steps:int=-1,
>              val_check_steps:int=100, batch_size=32,
>              valid_batch_size:Optional[int]=None, step_size:int=1,
>              scaler_type:str='robust', random_seed:int=1,
>              num_workers_loader:int=0, drop_last_loader:bool=False,
>              optimizer=None, optimizer_kwargs=None, lr_scheduler=None,
>              lr_scheduler_kwargs=None, **trainer_kwargs)
> ```

\*DilatedRNN

**Parameters:**<br/> `h`: int, forecast horizon.<br/> `input_size`: int,
maximum sequence length for truncated train backpropagation. Default -1
uses all history.<br/> `inference_input_size`: int, maximum sequence
length for truncated inference. Default -1 uses all history.<br/>
`cell_type`: str, type of RNN cell to use. Options: ‘GRU’, ‘RNN’,
‘LSTM’, ‘ResLSTM’, ‘AttentiveLSTM’.<br/> `dilations`: int list, dilations
betweem layers.<br/> `encoder_hidden_size`: int=200, units for the RNN’s
hidden state size.<br/> `context_size`: int=10, size of context vector
for each timestamp on the forecasting window.<br/> `decoder_hidden_size`:
int=200, size of hidden layer for the MLP decoder.<br/> `decoder_layers`:
int=2, number of layers for the MLP decoder.<br/> `futr_exog_list`: str
list, future exogenous columns.<br/> `hist_exog_list`: str list, historic
exogenous columns.<br/> `stat_exog_list`: str list, static exogenous
columns.<br/> `loss`: PyTorch module, instantiated train loss class from
[losses
collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br/>
`valid_loss`: PyTorch module=`loss`, instantiated valid loss class from
[losses
collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br/>
`max_steps`: int, maximum number of training steps.<br/> `learning_rate`:
float, Learning rate between (0, 1).<br/> `num_lr_decays`: int, Number of
learning rate decays, evenly distributed across max_steps.<br/>
`early_stop_patience_steps`: int, Number of validation iterations before
early stopping.<br/> `val_check_steps`: int, Number of training steps
between every validation loss check.<br/> `batch_size`: int=32, number of
different series in each batch.<br/> `valid_batch_size`: int=None, number
of different series in each validation and test batch.<br/> `step_size`:
int=1, step size between each window of temporal data.<br/>
`scaler_type`: str=‘robust’, type of scaler for temporal inputs
normalization see [temporal
scalers](https://nixtla.github.io/neuralforecast/common.scalers.html).<br/>
`random_seed`: int=1, random_seed for pytorch initializer and numpy
generators.<br/> `num_workers_loader`: int=os.cpu_count(), workers to be
used by `TimeSeriesDataLoader`.<br/> `drop_last_loader`: bool=False, if
True `TimeSeriesDataLoader` drops last non-full batch.<br/> `alias`: str,
optional, Custom name of the model.<br/> `optimizer`: Subclass of
‘torch.optim.Optimizer’, optional, user specified optimizer instead of
the default choice (Adam).<br/> `optimizer_kwargs`: dict, optional, list
of parameters used by the user specified `optimizer`.<br/>
`lr_scheduler`: Subclass of ‘torch.optim.lr_scheduler.LRScheduler’,
optional, user specified lr_scheduler instead of the default choice
(StepLR).<br/> `lr_scheduler_kwargs`: dict, optional, list of parameters
used by the user specified `lr_scheduler`.<br/> `**trainer_kwargs`: int,
keyword trainer arguments inherited from [PyTorch Lighning’s
trainer](https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.trainer.trainer.trainer.html?highlight=trainer).<br/>\*

## Usage Example

```python
import numpy as np
import pandas as pd
import pytorch_lightning as pl
import matplotlib.pyplot as plt

from neuralforecast import NeuralForecast
from neuralforecast.models import DilatedRNN
from neuralforecast.losses.pytorch import MQLoss, DistributionLoss
from neuralforecast.utils import AirPassengersPanel, AirPassengersStatic
from neuralforecast.tsdataset import TimeSeriesDataset, TimeSeriesLoader

Y_train_df = AirPassengersPanel[AirPassengersPanel.ds<AirPassengersPanel['ds'].values[-12]] # 132 train
Y_test_df = AirPassengersPanel[AirPassengersPanel.ds>=AirPassengersPanel['ds'].values[-12]].reset_index(drop=True) # 12 test

fcst = NeuralForecast(
    models=[DilatedRNN(h=12,
                       input_size=-1,
                       loss=DistributionLoss(distribution='Normal', level=[80, 90]),
                       scaler_type='robust',
                       encoder_hidden_size=100,
                       max_steps=200,
                       futr_exog_list=['y_[lag12]'],
                       hist_exog_list=None,
                       stat_exog_list=['airline1'],
    )
    ],
    freq='M'
)
fcst.fit(df=Y_train_df, static_df=AirPassengersStatic)
forecasts = fcst.predict(futr_df=Y_test_df)

Y_hat_df = forecasts.reset_index(drop=False).drop(columns=['unique_id','ds'])
plot_df = pd.concat([Y_test_df, Y_hat_df], axis=1)
plot_df = pd.concat([Y_train_df, plot_df])

plot_df = plot_df[plot_df.unique_id=='Airline1'].drop('unique_id', axis=1)
plt.plot(plot_df['ds'], plot_df['y'], c='black', label='True')
plt.plot(plot_df['ds'], plot_df['DilatedRNN-median'], c='blue', label='median')
plt.fill_between(x=plot_df['ds'][-12:], 
                 y1=plot_df['DilatedRNN-lo-90'][-12:].values, 
                 y2=plot_df['DilatedRNN-hi-90'][-12:].values,
                 alpha=0.4, label='level 90')
plt.legend()
plt.grid()
plt.plot()
```

