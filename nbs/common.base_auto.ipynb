{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524620c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp common._base_auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15392f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "12fa25a4",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization\n",
    "\n",
    "> Machine Learning forecasting methods are defined by many hyperparameters that control their behavior, with effects ranging from their speed and memory requirements to their predictive performance. For a long time, manual hyperparameter tuning prevailed. This approach is time-consuming, **automated hyperparameter optimization** methods have been introduced, proving more efficient than manual tuning, grid search, and random search.<br><br> The `BaseAuto` class offers shared API connections to hyperparameter optimization algorithms like [Optuna](https://docs.ray.io/en/latest/tune/examples/bayesopt_example.html), [HyperOpt](https://docs.ray.io/en/latest/tune/examples/hyperopt_example.html), [Dragonfly](https://docs.ray.io/en/latest/tune/examples/dragonfly_example.html) among others through `ray`, which gives you access to grid search, bayesian optimization and other state-of-the-art tools like hyperband.<br><br>Comprehending the impacts of hyperparameters is still a precious skill, as it can help guide the design of informed hyperparameter spaces that are faster to explore automatically."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e37fd67c",
   "metadata": {},
   "source": [
    "![Figure 1. Example of dataset split (left), validation (yellow) and test (orange). The hyperparameter optimization guiding signal is obtained from the validation set.](imgs_models/data_splits.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2508f7a9-1433-4ad8-8f2f-0078c6ed6c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from fastcore.test import test_eq\n",
    "from nbdev.showdoc import show_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44065066-e72a-431f-938f-1528adef9fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from copy import deepcopy\n",
    "from os import cpu_count\n",
    "\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from pytorch_lightning.callbacks import TQDMProgressBar\n",
    "from ray import air, tune\n",
    "from ray.tune.integration.pytorch_lightning import TuneReportCallback\n",
    "from ray.tune.search.basic_variant import BasicVariantGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c253583-8239-4abe-8a04-0c0ba635d8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BaseAuto(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    Class for Automatic Hyperparameter Optimization, it builds on top of `ray` to \n",
    "    give access to a wide variety of hyperparameter optimization tools ranging \n",
    "    from classic grid search, to Bayesian optimization and HyperBand algorithm.\n",
    "\n",
    "    The validation loss to be optimized is defined by the `config['loss']` dictionary\n",
    "    value, the config also contains the rest of the hyperparameter search space.\n",
    "\n",
    "    It is important to note that the success of this hyperparameter optimization\n",
    "    heavily relies on a strong correlation between the validation and test periods.\n",
    "\n",
    "    **Parameters:**<br>\n",
    "    `cls_model`: PyTorch/PyTorchLightning model, see `neuralforecast.models` [collection here](https://nixtla.github.io/neuralforecast/models.html).<br>\n",
    "    `h`: int, forecast horizon.<br>\n",
    "    `loss`: PyTorch module, instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
    "    `valid_loss`: PyTorch module=`loss`, instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
    "    `config`: dict, dictionary with ray.tune defined search space.<br>\n",
    "    `search_alg`: ray.tune.search variant, BasicVariantGenerator, HyperOptSearch, DragonflySearch, TuneBOHB for details\n",
    "        see [tune.search](https://docs.ray.io/en/latest/tune/api_docs/suggestion.html#).<br>\n",
    "    `num_samples`: int, number of hyperparameter optimization steps/samples.<br>\n",
    "    `cpus`: int, number of cpus to use during optimization, default all available.<br>\n",
    "    `gpus`: int, number of gpus to use during optimization, default all available.<br>\n",
    "    `refit_wo_val`: bool, number of gpus to use during optimization, default all available.<br>\n",
    "    `verbose`: bool, wether print partial outputs.<br>\n",
    "    `alias`: str, optional,  Custom name of the model.<br>\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 cls_model,\n",
    "                 h,\n",
    "                 loss,\n",
    "                 valid_loss,\n",
    "                 config, \n",
    "                 search_alg=BasicVariantGenerator(random_state=1),\n",
    "                 num_samples=10,\n",
    "                 cpus=cpu_count(),\n",
    "                 gpus=torch.cuda.device_count(),\n",
    "                 refit_with_val=False,\n",
    "                 verbose=False,\n",
    "                 alias=None):\n",
    "        super(BaseAuto, self).__init__()\n",
    "        self.save_hyperparameters() # Allows instantiation from a checkpoint from class\n",
    "\n",
    "        if config.get('h', None) is not None:\n",
    "            raise Exception(\"Please use `h` init argument instead of `config['h']`.\")\n",
    "        if config.get('loss', None) is not None:\n",
    "            raise Exception(\"Please use `loss` init argument instead of `config['loss']`.\")\n",
    "        if config.get('valid_loss', None) is not None:\n",
    "            raise Exception(\"Please use `valid_loss` init argument instead of `config['valid_loss']`.\")\n",
    "\n",
    "        # Deepcopy to avoid modifying the original config\n",
    "        config_base = deepcopy(config)\n",
    "\n",
    "        # Add losses to config and protect valid_loss default\n",
    "        config_base['h'] = h\n",
    "        config_base['loss'] = loss\n",
    "        if valid_loss is None:\n",
    "            valid_loss = loss\n",
    "        config_base['valid_loss'] = valid_loss\n",
    "        \n",
    "        self.h = h\n",
    "        self.cls_model = cls_model\n",
    "        \n",
    "        self.config = config_base\n",
    "        self.loss = self.config['loss']\n",
    "        self.valid_loss = self.config['valid_loss']\n",
    "\n",
    "        # This attribute helps to protect \n",
    "        # model and datasets interactions protections\n",
    "        if 'early_stop_patience_steps' in config.keys():\n",
    "            self.early_stop_patience_steps = 1\n",
    "        else:\n",
    "            self.early_stop_patience_steps = -1\n",
    "\n",
    "        self.num_samples = num_samples\n",
    "        self.search_alg = search_alg\n",
    "        self.cpus = cpus\n",
    "        self.gpus = gpus\n",
    "        self.refit_with_val = refit_with_val\n",
    "        self.verbose = verbose\n",
    "        self.alias = alias\n",
    "\n",
    "        # Base Class attributes\n",
    "        self.SAMPLING_TYPE = cls_model.SAMPLING_TYPE\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return type(self).__name__ if self.alias is None else self.alias\n",
    "    \n",
    "    def _train_tune(self, config_step, cls_model, dataset, val_size, test_size):\n",
    "        \"\"\" BaseAuto._train_tune\n",
    "\n",
    "        Internal function that instantiates a NF class model, then automatically\n",
    "        explores the validation loss (ptl/val_loss) on which the hyperparameter \n",
    "        exploration is based.\n",
    "\n",
    "        **Parameters:**<br>\n",
    "        `config_step`: Dict, initialization parameters of a NF model.<br>\n",
    "        `cls_model`: NeuralForecast model class, yet to be instantiated.<br>\n",
    "        `dataset`: NeuralForecast dataset, to fit the model.<br>\n",
    "        `val_size`: int, validation size for temporal cross-validation.<br>\n",
    "        `test_size`: int, test size for temporal cross-validation.<br>\n",
    "        \"\"\"\n",
    "        metrics = {\"loss\": \"ptl/val_loss\"}\n",
    "        callbacks = [TQDMProgressBar(), TuneReportCallback(metrics, on=\"validation_end\")]\n",
    "        if 'callbacks' in config_step.keys():\n",
    "            callbacks += config_step['callbacks']\n",
    "        config_step = {**config_step, **{'callbacks': callbacks}}\n",
    "\n",
    "        # Protect dtypes from tune samplers\n",
    "        if 'batch_size' in config_step.keys():\n",
    "            config_step['batch_size'] = int(config_step['batch_size'])\n",
    "        if 'windows_batch_size' in config_step.keys():\n",
    "            config_step['windows_batch_size'] = int(config_step['windows_batch_size'])\n",
    "\n",
    "        # Tune session receives validation signal\n",
    "        # from the specialized PL TuneReportCallback\n",
    "        _ = self._fit_model(cls_model=cls_model,\n",
    "                                config=config_step,\n",
    "                                dataset=dataset,\n",
    "                                val_size=val_size,\n",
    "                                test_size=test_size)\n",
    "\n",
    "    def _tune_model(self, cls_model, dataset, val_size, test_size,\n",
    "                cpus, gpus, verbose, num_samples, search_alg, config):\n",
    "        train_fn_with_parameters = tune.with_parameters(\n",
    "            self._train_tune,\n",
    "            cls_model=cls_model,\n",
    "            dataset=dataset,\n",
    "            val_size=val_size,\n",
    "            test_size=test_size,\n",
    "        )\n",
    "\n",
    "        # Device\n",
    "        if gpus > 0:\n",
    "            device_dict = {'gpu':gpus}\n",
    "        else:\n",
    "            device_dict = {'cpu':cpus}\n",
    "\n",
    "        # on Windows, prevent long trial directory names\n",
    "        import platform\n",
    "        trial_dirname_creator=lambda trial: f\"{trial.trainable_name}_{trial.trial_id}\" if platform.system() == 'Windows' else None\n",
    "\n",
    "        tuner = tune.Tuner(\n",
    "            tune.with_resources(train_fn_with_parameters, device_dict),\n",
    "            run_config=air.RunConfig(\n",
    "                verbose=verbose,\n",
    "                #checkpoint_config=air.CheckpointConfig(\n",
    "                    #num_to_keep=0,\n",
    "                    #keep_checkpoints_num=None\n",
    "                #)\n",
    "            ),\n",
    "            tune_config=tune.TuneConfig(\n",
    "                metric=\"loss\",\n",
    "                mode=\"min\",\n",
    "                num_samples=num_samples, \n",
    "                search_alg=search_alg,\n",
    "                trial_dirname_creator=trial_dirname_creator,\n",
    "            ),\n",
    "            param_space=config,\n",
    "        )\n",
    "        results = tuner.fit()\n",
    "        return results\n",
    "    \n",
    "    def _fit_model(self, cls_model, config,\n",
    "                   dataset, val_size, test_size):\n",
    "        model = cls_model(**config)\n",
    "        model.fit(\n",
    "            dataset,\n",
    "            val_size=val_size, \n",
    "            test_size=test_size\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    def fit(self, dataset, val_size=0, test_size=0, random_seed=None):\n",
    "        \"\"\" BaseAuto.fit\n",
    "\n",
    "        Perform the hyperparameter optimization as specified by the BaseAuto configuration \n",
    "        dictionary `config`.\n",
    "\n",
    "        The optimization is performed on the `TimeSeriesDataset` using temporal cross validation with \n",
    "        the validation set that sequentially precedes the test set.\n",
    "\n",
    "        **Parameters:**<br>\n",
    "        `dataset`: NeuralForecast's `TimeSeriesDataset` see details [here](https://nixtla.github.io/neuralforecast/tsdataset.html)<br>\n",
    "        `val_size`: int, size of temporal validation set (needs to be bigger than 0).<br>\n",
    "        `test_size`: int, size of temporal test set (default 0).<br>\n",
    "        `random_seed`: int=None, random_seed for hyperparameter exploration algorithms, not yet implemented.<br>\n",
    "        **Returns:**<br>\n",
    "        `self`: fitted instance of `BaseAuto` with best hyperparameters and results<br>.\n",
    "        \"\"\"\n",
    "        #we need val_size > 0 to perform\n",
    "        #hyperparameter selection.\n",
    "        search_alg = deepcopy(self.search_alg)\n",
    "        val_size = val_size if val_size > 0 else self.h\n",
    "        results = self._tune_model(\n",
    "            cls_model=self.cls_model,\n",
    "            dataset=dataset,\n",
    "            val_size=val_size, \n",
    "            test_size=test_size, \n",
    "            cpus=self.cpus,\n",
    "            gpus=self.gpus,\n",
    "            verbose=self.verbose,\n",
    "            num_samples=self.num_samples, \n",
    "            search_alg=search_alg, \n",
    "            config=self.config\n",
    "        )\n",
    "        best_config = results.get_best_result().config\n",
    "        #self.model = self.cls_model(**best_config)\n",
    "        #self.model.fit(\n",
    "        #    dataset=dataset, \n",
    "        #    val_size=val_size * (1 - self.refit_with_val), \n",
    "        #    test_size=test_size,\n",
    "        #)\n",
    "        self.model = self._fit_model(cls_model=self.cls_model,\n",
    "                                     config=best_config,\n",
    "                                     dataset=dataset,\n",
    "                                     val_size=val_size * (1 - self.refit_with_val),\n",
    "                                     test_size=test_size)\n",
    "        self.results = results\n",
    "        \n",
    "    def predict(self, dataset, step_size=1, **data_kwargs):\n",
    "        \"\"\" BaseAuto.predict\n",
    "\n",
    "        Predictions of the best performing model on validation.\n",
    "\n",
    "        **Parameters:**<br>\n",
    "        `dataset`: NeuralForecast's `TimeSeriesDataset` see details [here](https://nixtla.github.io/neuralforecast/tsdataset.html)<br>\n",
    "        `step_size`: int, steps between sequential predictions, (default 1).<br>\n",
    "        `**data_kwarg`: additional parameters for the dataset module.<br>\n",
    "        `random_seed`: int=None, random_seed for hyperparameter exploration algorithms (not implemented).<br>\n",
    "        **Returns:**<br>\n",
    "        `y_hat`: numpy predictions of the `NeuralForecast` model.<br>\n",
    "        \"\"\"\n",
    "        return self.model.predict(dataset=dataset, \n",
    "                                  step_size=step_size, **data_kwargs)\n",
    "\n",
    "    def set_test_size(self, test_size):\n",
    "        self.model.set_test_size(test_size)\n",
    "\n",
    "    def get_test_size(self):\n",
    "        return self.model.test_size\n",
    "    \n",
    "    def save(self, path):\n",
    "        \"\"\" BaseAuto.save\n",
    "\n",
    "        Save the fitted model to disk.\n",
    "\n",
    "        **Parameters:**<br>\n",
    "        `path`: str, path to save the model.<br>\n",
    "        \"\"\"\n",
    "        self.model.trainer.save_checkpoint(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2376ed06",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(BaseAuto, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623ebb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(BaseAuto.fit, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d3c1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(BaseAuto.predict, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfd4e8f-2565-4f85-b615-7329a1ae3f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import logging\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "logging.getLogger(\"pytorch_lightning\").setLevel(logging.ERROR)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349627f9-d2c7-40dc-8ebd-99ee1d9e9d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import pandas as pd\n",
    "from neuralforecast.models.mlp import MLP\n",
    "from neuralforecast.utils import AirPassengersDF as Y_df\n",
    "from neuralforecast.tsdataset import TimeSeriesDataset\n",
    "from neuralforecast.losses.pytorch import MAE, MSE\n",
    "\n",
    "Y_train_df = Y_df[Y_df.ds<='1959-12-31'] # 132 train\n",
    "Y_test_df = Y_df[Y_df.ds>'1959-12-31']   # 12 test\n",
    "\n",
    "dataset, *_ = TimeSeriesDataset.from_df(Y_train_df)\n",
    "config = {\n",
    "    \"hidden_size\": tune.choice([512]),\n",
    "    \"num_layers\": tune.choice([3, 4]),\n",
    "    \"input_size\": 12,\n",
    "    \"max_steps\": 10,\n",
    "    \"val_check_steps\": 1\n",
    "}\n",
    "auto = BaseAuto(h=12, loss=MAE(), valid_loss=MSE(), cls_model=MLP, config=config, num_samples=2, cpus=1, gpus=0)\n",
    "auto.fit(dataset=dataset)\n",
    "y_hat = auto.predict(dataset=dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ad2eec-dd93-4bc4-ae19-5df4199577be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "Y_test_df['AutoMLP'] = y_hat\n",
    "\n",
    "pd.concat([Y_train_df, Y_test_df]).drop('unique_id', axis=1).set_index('ds').plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea77450f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# Unit tests to guarantee that losses are correctly instantiated\n",
    "import pandas as pd\n",
    "from neuralforecast.models.mlp import MLP\n",
    "from neuralforecast.utils import AirPassengersDF as Y_df\n",
    "from neuralforecast.tsdataset import TimeSeriesDataset\n",
    "from neuralforecast.losses.pytorch import MAE, MSE\n",
    "\n",
    "Y_train_df = Y_df[Y_df.ds<='1959-12-31'] # 132 train\n",
    "Y_test_df = Y_df[Y_df.ds>'1959-12-31']   # 12 test\n",
    "\n",
    "dataset, *_ = TimeSeriesDataset.from_df(Y_train_df)\n",
    "config = {\n",
    "    \"hidden_size\": tune.choice([512]),\n",
    "    \"num_layers\": tune.choice([3, 4]),\n",
    "    \"input_size\": 12,\n",
    "    \"max_steps\": 1,\n",
    "    \"val_check_steps\": 1\n",
    "}\n",
    "\n",
    "# Test instantiation\n",
    "auto = BaseAuto(h=12, loss=MAE(), valid_loss=MSE(), \n",
    "                cls_model=MLP, config=config, num_samples=2, cpus=1, gpus=0)\n",
    "test_eq(str(type(auto.loss)), \"<class 'neuralforecast.losses.pytorch.MAE'>\")\n",
    "test_eq(str(type(auto.valid_loss)), \"<class 'neuralforecast.losses.pytorch.MSE'>\")\n",
    "\n",
    "# Test validation default\n",
    "auto = BaseAuto(h=12, loss=MSE(), valid_loss=None,\n",
    "                cls_model=MLP, config=config, num_samples=2, cpus=1, gpus=0)\n",
    "test_eq(str(type(auto.loss)), \"<class 'neuralforecast.losses.pytorch.MSE'>\")\n",
    "test_eq(str(type(auto.valid_loss)), \"<class 'neuralforecast.losses.pytorch.MSE'>\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3c8e2d46",
   "metadata": {},
   "source": [
    "### References\n",
    "- [James Bergstra, Remi Bardenet, Yoshua Bengio, and Balazs Kegl (2011). \"Algorithms for Hyper-Parameter Optimization\". In: Advances in Neural Information Processing Systems. url: https://proceedings.neurips.cc/paper/2011/file/86e8f7ab32cfd12577bc2619bc635690-Paper.pdf](https://proceedings.neurips.cc/paper/2011/file/86e8f7ab32cfd12577bc2619bc635690-Paper.pdf)\n",
    "- [Kirthevasan Kandasamy, Karun Raju Vysyaraju, Willie Neiswanger, Biswajit Paria, Christopher R. Collins, Jeff Schneider, Barnabas Poczos, Eric P. Xing (2019). \"Tuning Hyperparameters without Grad Students: Scalable and Robust Bayesian Optimisation with Dragonfly\". Journal of Machine Learning Research. url: https://arxiv.org/abs/1903.06694](https://arxiv.org/abs/1903.06694)\n",
    "- [Lisha Li, Kevin Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, Ameet Talwalkar (2016). \"Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization\". Journal of Machine Learning Research. url: https://arxiv.org/abs/1603.06560](https://arxiv.org/abs/1603.06560)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "267cbf1e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
