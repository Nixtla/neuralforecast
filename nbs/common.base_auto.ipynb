{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "524620c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp common._base_auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15392f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fa25a4",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization\n",
    "\n",
    "> Machine Learning forecasting methods are defined by many hyperparameters that control their behavior, with effects ranging from their speed and memory requirements to their predictive performance. For a long time, manual hyperparameter tuning prevailed. This approach is time-consuming, **automated hyperparameter optimization** methods have been introduced, proving more efficient than manual tuning, grid search, and random search.<br><br> The `BaseAuto` class offers shared API connections to hyperparameter optimization algorithms like [Optuna](https://docs.ray.io/en/latest/tune/examples/bayesopt_example.html), [HyperOpt](https://docs.ray.io/en/latest/tune/examples/hyperopt_example.html), [Dragonfly](https://docs.ray.io/en/latest/tune/examples/dragonfly_example.html) among others through `ray`, which gives you access to grid search, bayesian optimization and other state-of-the-art tools like hyperband.<br><br>Comprehending the impacts of hyperparameters is still a precious skill, as it can help guide the design of informed hyperparameter spaces that are faster to explore automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37fd67c",
   "metadata": {},
   "source": [
    "![Figure 1. Example of dataset split (left), validation (yellow) and test (orange). The hyperparameter optimization guiding signal is obtained from the validation set.](imgs_models/data_splits.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2508f7a9-1433-4ad8-8f2f-0078c6ed6c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from fastcore.test import test_eq\n",
    "from nbdev.showdoc import show_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44065066-e72a-431f-938f-1528adef9fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cchallu/opt/anaconda3/envs/neuralforecast/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "from copy import deepcopy\n",
    "from os import cpu_count\n",
    "\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from pytorch_lightning.callbacks import TQDMProgressBar\n",
    "from ray import air, tune\n",
    "from ray.tune.integration.pytorch_lightning import TuneReportCallback\n",
    "from ray.tune.search.basic_variant import BasicVariantGenerator\n",
    "\n",
    "from neuralforecast.losses.pytorch import MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "693faaae-5429-4046-99ba-ec771b1ce29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def train_tune(config_step, cls_model, dataset, val_size, test_size):\n",
    "    metrics = {\"loss\": \"ptl/val_loss\"}\n",
    "    callbacks = [TQDMProgressBar(), TuneReportCallback(metrics, on=\"validation_end\")]\n",
    "    if 'callbacks' in config_step.keys():\n",
    "        callbacks += config_step['callbacks']\n",
    "    config_step = {**config_step, **{'callbacks': callbacks}}\n",
    "    model = cls_model(**config_step)\n",
    "    model.fit(\n",
    "        dataset,\n",
    "        val_size=val_size, \n",
    "        test_size=test_size\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bdbb29ee-882c-46cc-b919-e3a7661f9744",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def tune_model(\n",
    "        cls_model, \n",
    "        dataset, \n",
    "        val_size, \n",
    "        test_size,\n",
    "        cpus,\n",
    "        gpus,\n",
    "        verbose,\n",
    "        num_samples, \n",
    "        search_alg, \n",
    "        config\n",
    "    ):\n",
    "    train_fn_with_parameters = tune.with_parameters(\n",
    "        train_tune,\n",
    "        cls_model=cls_model,\n",
    "        dataset=dataset,\n",
    "        val_size=val_size,\n",
    "        test_size=test_size,\n",
    "    )\n",
    "\n",
    "    # Device\n",
    "    if gpus > 0:\n",
    "        device_dict = {'gpu':gpus}\n",
    "    else:\n",
    "        device_dict = {'cpu':cpus}\n",
    "\n",
    "    tuner = tune.Tuner(\n",
    "        tune.with_resources(train_fn_with_parameters, device_dict),\n",
    "        run_config=air.RunConfig(\n",
    "            verbose=verbose,\n",
    "            #checkpoint_config=air.CheckpointConfig(\n",
    "                #num_to_keep=0,\n",
    "                #keep_checkpoints_num=None\n",
    "            #)\n",
    "        ),\n",
    "        tune_config=tune.TuneConfig(\n",
    "            metric=\"loss\",\n",
    "            mode=\"min\",\n",
    "            num_samples=num_samples, \n",
    "            search_alg=search_alg\n",
    "        ),\n",
    "        param_space=config,\n",
    "    )\n",
    "    results = tuner.fit()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c253583-8239-4abe-8a04-0c0ba635d8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BaseAuto(pl.LightningModule):\n",
    "    \"\"\" BaseAuto \n",
    "    \n",
    "    Class for Automatic Hyperparameter Optimization, it builds on top of `ray` to \n",
    "    give access to a wide variety of hyperparameter optimization tools ranging \n",
    "    from classic grid search, to Bayesian optimization and HyperBand algorithm.\n",
    "\n",
    "    The validation loss to be optimized is defined by the `config['loss']` dictionary\n",
    "    value, the config also contains the rest of the hyperparameter search space.\n",
    "\n",
    "    It is important to note that the success of this hyperparameter optimization\n",
    "    heavily relies on a strong correlation between the validation and test periods.\n",
    "\n",
    "    **Parameters:**<br>\n",
    "    `cls_model`: PyTorch/PyTorchLightning model, see `neuralforecast.models` [collection here](https://nixtla.github.io/neuralforecast/models.html).<br>\n",
    "    `h`: int, forecast horizon.<br>\n",
    "    `config`: dict, dictionary with ray.tune defined search space.<br>\n",
    "    `search_alg`: ray.tune.search variant, BasicVariantGenerator, HyperOptSearch, DragonflySearch, TuneBOHB for details\n",
    "        see [tune.search](https://docs.ray.io/en/latest/tune/api_docs/suggestion.html#).<br>\n",
    "    `num_samples`: int, number of hyperparameter optimization steps/samples.<br>\n",
    "    `cpus`: int, number of cpus to use during optimization, default all available.<br>\n",
    "    `gpus`: int, number of gpus to use during optimization, default all available.<br>\n",
    "    `refit_wo_val`: bool, number of gpus to use during optimization, default all available.<br>\n",
    "    `verbose`: bool, wether print partial outputs.<br>\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 cls_model,\n",
    "                 h,\n",
    "                 loss,\n",
    "                 config, \n",
    "                 search_alg=BasicVariantGenerator(random_state=1),\n",
    "                 num_samples=10,\n",
    "                 cpus=cpu_count(),\n",
    "                 gpus=torch.cuda.device_count(),\n",
    "                 refit_with_val=False,\n",
    "                 verbose=False):\n",
    "        super(BaseAuto, self).__init__()\n",
    "        self.save_hyperparameters() # Allows instantiation from a checkpoint from class\n",
    "\n",
    "        if config.get('h', None) is not None:\n",
    "            raise Exception(\"Please use `h` argument instead of `config['h']`.\")\n",
    "        if config.get('loss', None) is not None:\n",
    "            raise Exception(\"Please use `loss` argument instead of `config['loss']`.\")\n",
    "\n",
    "        # Deepcopy to avoid modifying the original config\n",
    "        config_base = deepcopy(config)\n",
    "\n",
    "        config_base['h'] = h\n",
    "        config_base['loss'] = loss\n",
    "        self.cls_model = cls_model\n",
    "        self.h = h\n",
    "        self.loss = loss\n",
    "        self.config = config_base\n",
    "        self.num_samples = num_samples\n",
    "        self.search_alg = search_alg\n",
    "        self.cpus = cpus\n",
    "        self.gpus = gpus\n",
    "        self.refit_with_val = refit_with_val\n",
    "        self.verbose = verbose\n",
    "        self.loss = self.config.get('loss', MAE())\n",
    "        \n",
    "    def fit(self, dataset, val_size=0, test_size=0):\n",
    "        \"\"\" BaseAuto.fit\n",
    "\n",
    "        Perform the hyperparameter optimization as specified by the BaseAuto configuration \n",
    "        dictionary `config`.\n",
    "        \n",
    "        The optimization is performed on the `TimeSeriesDataset` using temporal cross validation with \n",
    "        the validation set that sequentially precedes the test set.\n",
    "\n",
    "        **Parameters:**<br>\n",
    "        `dataset`: NeuralForecast's `TimeSeriesDataset` see details [here](https://nixtla.github.io/neuralforecast/tsdataset.html)<br>\n",
    "        `val_size`: int, size of temporal validation set (needs to be bigger than 0).<br>\n",
    "        `test_size`: int, size of temporal test set (default 0).<br>\n",
    "\n",
    "        **Returns:**<br>\n",
    "        `self`: fitted instance of `BaseAuto` with best hyperparameters and results<br>.\n",
    "        \"\"\"\n",
    "        #we need val_size > 0 to perform\n",
    "        #hyperparameter selection.\n",
    "        search_alg = deepcopy(self.search_alg)\n",
    "        val_size = val_size if val_size > 0 else self.h\n",
    "        results = tune_model(\n",
    "            cls_model=self.cls_model,\n",
    "            dataset=dataset,\n",
    "            val_size=val_size, \n",
    "            test_size=test_size, \n",
    "            cpus=self.cpus,\n",
    "            gpus=self.gpus,\n",
    "            verbose=self.verbose,\n",
    "            num_samples=self.num_samples, \n",
    "            search_alg=search_alg, \n",
    "            config=self.config\n",
    "        )\n",
    "        best_config = results.get_best_result().config\n",
    "        self.model = self.cls_model(**best_config)\n",
    "        self.model.fit(\n",
    "            dataset=dataset, \n",
    "            val_size=val_size * (1 - self.refit_with_val), \n",
    "            test_size=test_size,\n",
    "        )\n",
    "        self.results = results\n",
    "        \n",
    "    def predict(self, dataset, step_size=1, **data_kwargs):\n",
    "        \"\"\" BaseAuto.predict\n",
    "\n",
    "        Predictions of the best performing model on validation.\n",
    "\n",
    "        **Parameters:**<br>\n",
    "        `dataset`: NeuralForecast's `TimeSeriesDataset` see details [here](https://nixtla.github.io/neuralforecast/tsdataset.html)<br>\n",
    "        `step_size`: int, steps between sequential predictions, (default 1).<br>\n",
    "        `**data_kwarg`: additional parameters for the dataset module.<br>\n",
    "\n",
    "        **Returns:**<br>\n",
    "        `y_hat`: numpy predictions of the `NeuralForecast` model.<br>\n",
    "        \"\"\"\n",
    "        return self.model.predict(dataset=dataset, \n",
    "                                  step_size=step_size, **data_kwargs)\n",
    "\n",
    "    def set_test_size(self, test_size):\n",
    "        self.model.set_test_size(test_size)\n",
    "\n",
    "    def save(self, path):\n",
    "        \"\"\" BaseAuto.save\n",
    "\n",
    "        Save the fitted model to disk.\n",
    "\n",
    "        **Parameters:**<br>\n",
    "        `path`: str, path to save the model.<br>\n",
    "        \"\"\"\n",
    "        self.model.trainer.save_checkpoint(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2376ed06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### BaseAuto\n",
       "\n",
       ">      BaseAuto (cls_model, h, loss, config,\n",
       ">                search_alg=<ray.tune.search.basic_variant.BasicVariantGenerator\n",
       ">                object at 0x121cefe20>, num_samples=10, cpus=12, gpus=0,\n",
       ">                refit_with_val=False, verbose=False)\n",
       "\n",
       "BaseAuto \n",
       "\n",
       "Class for Automatic Hyperparameter Optimization, it builds on top of `ray` to \n",
       "give access to a wide variety of hyperparameter optimization tools ranging \n",
       "from classic grid search, to Bayesian optimization and HyperBand algorithm.\n",
       "\n",
       "The validation loss to be optimized is defined by the `config['loss']` dictionary\n",
       "value, the config also contains the rest of the hyperparameter search space.\n",
       "\n",
       "It is important to note that the success of this hyperparameter optimization\n",
       "heavily relies on a strong correlation between the validation and test periods.\n",
       "\n",
       "**Parameters:**<br>\n",
       "`cls_model`: PyTorch/PyTorchLightning model, see `neuralforecast.models` [collection here](https://nixtla.github.io/neuralforecast/models.html).<br>\n",
       "`h`: int, forecast horizon.<br>\n",
       "`config`: dict, dictionary with ray.tune defined search space.<br>\n",
       "`search_alg`: ray.tune.search variant, BasicVariantGenerator, HyperOptSearch, DragonflySearch, TuneBOHB for details\n",
       "    see [tune.search](https://docs.ray.io/en/latest/tune/api_docs/suggestion.html#).<br>\n",
       "`num_samples`: int, number of hyperparameter optimization steps/samples.<br>\n",
       "`cpus`: int, number of cpus to use during optimization, default all available.<br>\n",
       "`gpus`: int, number of gpus to use during optimization, default all available.<br>\n",
       "`refit_wo_val`: bool, number of gpus to use during optimization, default all available.<br>\n",
       "`verbose`: bool, wether print partial outputs.<br>"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### BaseAuto\n",
       "\n",
       ">      BaseAuto (cls_model, h, loss, config,\n",
       ">                search_alg=<ray.tune.search.basic_variant.BasicVariantGenerator\n",
       ">                object at 0x121cefe20>, num_samples=10, cpus=12, gpus=0,\n",
       ">                refit_with_val=False, verbose=False)\n",
       "\n",
       "BaseAuto \n",
       "\n",
       "Class for Automatic Hyperparameter Optimization, it builds on top of `ray` to \n",
       "give access to a wide variety of hyperparameter optimization tools ranging \n",
       "from classic grid search, to Bayesian optimization and HyperBand algorithm.\n",
       "\n",
       "The validation loss to be optimized is defined by the `config['loss']` dictionary\n",
       "value, the config also contains the rest of the hyperparameter search space.\n",
       "\n",
       "It is important to note that the success of this hyperparameter optimization\n",
       "heavily relies on a strong correlation between the validation and test periods.\n",
       "\n",
       "**Parameters:**<br>\n",
       "`cls_model`: PyTorch/PyTorchLightning model, see `neuralforecast.models` [collection here](https://nixtla.github.io/neuralforecast/models.html).<br>\n",
       "`h`: int, forecast horizon.<br>\n",
       "`config`: dict, dictionary with ray.tune defined search space.<br>\n",
       "`search_alg`: ray.tune.search variant, BasicVariantGenerator, HyperOptSearch, DragonflySearch, TuneBOHB for details\n",
       "    see [tune.search](https://docs.ray.io/en/latest/tune/api_docs/suggestion.html#).<br>\n",
       "`num_samples`: int, number of hyperparameter optimization steps/samples.<br>\n",
       "`cpus`: int, number of cpus to use during optimization, default all available.<br>\n",
       "`gpus`: int, number of gpus to use during optimization, default all available.<br>\n",
       "`refit_wo_val`: bool, number of gpus to use during optimization, default all available.<br>\n",
       "`verbose`: bool, wether print partial outputs.<br>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(BaseAuto, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "623ebb06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### BaseAuto.fit\n",
       "\n",
       ">      BaseAuto.fit (dataset, val_size=0, test_size=0)\n",
       "\n",
       "BaseAuto.fit\n",
       "\n",
       "Perform the hyperparameter optimization as specified by the BaseAuto configuration \n",
       "dictionary `config`.\n",
       "\n",
       "The optimization is performed on the `TimeSeriesDataset` using temporal cross validation with \n",
       "the validation set that sequentially precedes the test set.\n",
       "\n",
       "**Parameters:**<br>\n",
       "`dataset`: NeuralForecast's `TimeSeriesDataset` see details [here](https://nixtla.github.io/neuralforecast/tsdataset.html)<br>\n",
       "`val_size`: int, size of temporal validation set (needs to be bigger than 0).<br>\n",
       "`test_size`: int, size of temporal test set (default 0).<br>\n",
       "\n",
       "**Returns:**<br>\n",
       "`self`: fitted instance of `BaseAuto` with best hyperparameters and results<br>."
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### BaseAuto.fit\n",
       "\n",
       ">      BaseAuto.fit (dataset, val_size=0, test_size=0)\n",
       "\n",
       "BaseAuto.fit\n",
       "\n",
       "Perform the hyperparameter optimization as specified by the BaseAuto configuration \n",
       "dictionary `config`.\n",
       "\n",
       "The optimization is performed on the `TimeSeriesDataset` using temporal cross validation with \n",
       "the validation set that sequentially precedes the test set.\n",
       "\n",
       "**Parameters:**<br>\n",
       "`dataset`: NeuralForecast's `TimeSeriesDataset` see details [here](https://nixtla.github.io/neuralforecast/tsdataset.html)<br>\n",
       "`val_size`: int, size of temporal validation set (needs to be bigger than 0).<br>\n",
       "`test_size`: int, size of temporal test set (default 0).<br>\n",
       "\n",
       "**Returns:**<br>\n",
       "`self`: fitted instance of `BaseAuto` with best hyperparameters and results<br>."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(BaseAuto.fit, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69d3c1ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### BaseAuto.predict\n",
       "\n",
       ">      BaseAuto.predict (dataset, step_size=1, **data_kwargs)\n",
       "\n",
       "BaseAuto.predict\n",
       "\n",
       "Predictions of the best performing model on validation.\n",
       "\n",
       "**Parameters:**<br>\n",
       "`dataset`: NeuralForecast's `TimeSeriesDataset` see details [here](https://nixtla.github.io/neuralforecast/tsdataset.html)<br>\n",
       "`step_size`: int, steps between sequential predictions, (default 1).<br>\n",
       "`**data_kwarg`: additional parameters for the dataset module.<br>\n",
       "\n",
       "**Returns:**<br>\n",
       "`y_hat`: numpy predictions of the `NeuralForecast` model.<br>"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### BaseAuto.predict\n",
       "\n",
       ">      BaseAuto.predict (dataset, step_size=1, **data_kwargs)\n",
       "\n",
       "BaseAuto.predict\n",
       "\n",
       "Predictions of the best performing model on validation.\n",
       "\n",
       "**Parameters:**<br>\n",
       "`dataset`: NeuralForecast's `TimeSeriesDataset` see details [here](https://nixtla.github.io/neuralforecast/tsdataset.html)<br>\n",
       "`step_size`: int, steps between sequential predictions, (default 1).<br>\n",
       "`**data_kwarg`: additional parameters for the dataset module.<br>\n",
       "\n",
       "**Returns:**<br>\n",
       "`y_hat`: numpy predictions of the `NeuralForecast` model.<br>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(BaseAuto.predict, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bbfd4e8f-2565-4f85-b615-7329a1ae3f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import logging\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "logging.getLogger(\"pytorch_lightning\").setLevel(logging.ERROR)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "349627f9-d2c7-40dc-8ebd-99ee1d9e9d81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_tune pid=20415)\u001b[0m /Users/cchallu/opt/anaconda3/envs/neuralforecast/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n",
      "\u001b[2m\u001b[36m(train_tune pid=20415)\u001b[0m   warnings.warn('resource_tracker: There appear to be %d '\n",
      "\u001b[2m\u001b[36m(train_tune pid=20416)\u001b[0m /Users/cchallu/opt/anaconda3/envs/neuralforecast/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n",
      "\u001b[2m\u001b[36m(train_tune pid=20416)\u001b[0m   warnings.warn('resource_tracker: There appear to be %d '\n",
      "\u001b[2m\u001b[36m(train_tune pid=20439)\u001b[0m Global seed set to 1\n",
      "\u001b[2m\u001b[36m(train_tune pid=20440)\u001b[0m Global seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Epoch 0:  50%|█████     | 1/2 [00:00<00:00, 23.05it/s, loss=250, v_num=0, train_loss_step=250.0]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A0m \n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0: 100%|██████████| 2/2 [00:00<00:00, 36.09it/s, loss=250, v_num=0, train_loss_step=250.0, val_loss=362.0]\n",
      "                                                                       \u001b[A\n",
      "Epoch 0:   0%|          | 0/2 [00:00<?, ?it/s]                             \n",
      "Epoch 1:   0%|          | 0/2 [00:00<?, ?it/s, loss=250, v_num=0, train_loss_step=250.0, val_loss=362.0, train_loss_epoch=250.0]        \n",
      "Epoch 1:  50%|█████     | 1/2 [00:00<00:00, 25.56it/s, loss=232, v_num=0, train_loss_step=214.0, val_loss=362.0, train_loss_epoch=250.0]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A0m \n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|██████████| 2/2 [00:00<00:00, 40.41it/s, loss=232, v_num=0, train_loss_step=214.0, val_loss=284.0, train_loss_epoch=250.0]\n",
      "Epoch 2:  50%|█████     | 1/2 [00:00<00:00, 25.55it/s, loss=211, v_num=0, train_loss_step=169.0, val_loss=284.0, train_loss_epoch=214.0]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A0m \n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|██████████| 2/2 [00:00<00:00, 39.86it/s, loss=211, v_num=0, train_loss_step=169.0, val_loss=169.0, train_loss_epoch=214.0]\n",
      "Epoch 3:   0%|          | 0/2 [00:00<?, ?it/s, loss=211, v_num=0, train_loss_step=169.0, val_loss=169.0, train_loss_epoch=169.0]        \n",
      "Epoch 0:  50%|█████     | 1/2 [00:00<00:00, 20.62it/s, loss=250, v_num=0, train_loss_step=250.0]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A0m \n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0: 100%|██████████| 2/2 [00:00<00:00, 32.95it/s, loss=250, v_num=0, train_loss_step=250.0, val_loss=362.0]\n",
      "Epoch 1:  50%|█████     | 1/2 [00:00<00:00, 24.92it/s, loss=232, v_num=0, train_loss_step=214.0, val_loss=362.0, train_loss_epoch=250.0]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A0m \n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|██████████| 2/2 [00:00<00:00, 38.16it/s, loss=232, v_num=0, train_loss_step=214.0, val_loss=284.0, train_loss_epoch=250.0]\n",
      "Epoch 2:   0%|          | 0/2 [00:00<?, ?it/s, loss=232, v_num=0, train_loss_step=214.0, val_loss=284.0, train_loss_epoch=214.0]        \n",
      "Epoch 3:  50%|█████     | 1/2 [00:00<00:00, 25.34it/s, loss=183, v_num=0, train_loss_step=99.30, val_loss=169.0, train_loss_epoch=169.0]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A0m \n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|██████████| 2/2 [00:00<00:00, 40.71it/s, loss=183, v_num=0, train_loss_step=99.30, val_loss=101.0, train_loss_epoch=169.0]\n",
      "Epoch 4:  50%|█████     | 1/2 [00:00<00:00, 24.24it/s, loss=158, v_num=0, train_loss_step=58.40, val_loss=101.0, train_loss_epoch=99.30]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A0m \n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|██████████| 2/2 [00:00<00:00, 35.99it/s, loss=158, v_num=0, train_loss_step=58.40, val_loss=66.10, train_loss_epoch=99.30]\n",
      "                                                                       \u001b[A\n",
      "Epoch 2:  50%|█████     | 1/2 [00:00<00:00, 24.46it/s, loss=211, v_num=0, train_loss_step=169.0, val_loss=284.0, train_loss_epoch=214.0]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A0m \n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|██████████| 2/2 [00:00<00:00, 38.24it/s, loss=211, v_num=0, train_loss_step=169.0, val_loss=169.0, train_loss_epoch=214.0]\n",
      "Epoch 3:  50%|█████     | 1/2 [00:00<00:00, 28.08it/s, loss=183, v_num=0, train_loss_step=99.30, val_loss=169.0, train_loss_epoch=169.0]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A0m \n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|██████████| 2/2 [00:00<00:00, 39.39it/s, loss=183, v_num=0, train_loss_step=99.30, val_loss=101.0, train_loss_epoch=169.0]\n",
      "Epoch 4:   0%|          | 0/2 [00:00<?, ?it/s, loss=183, v_num=0, train_loss_step=99.30, val_loss=101.0, train_loss_epoch=99.30]        \n",
      "Epoch 4: 100%|██████████| 2/2 [00:00<00:00, 31.21it/s, loss=158, v_num=0, train_loss_step=58.40, val_loss=66.10, train_loss_epoch=58.40]\n",
      "Epoch 5:   0%|          | 0/2 [00:00<?, ?it/s, loss=158, v_num=0, train_loss_step=58.40, val_loss=66.10, train_loss_epoch=58.40]        \n",
      "Epoch 5:  50%|█████     | 1/2 [00:00<00:00, 23.23it/s, loss=138, v_num=0, train_loss_step=38.70, val_loss=66.10, train_loss_epoch=58.40]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A0m \n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|██████████| 2/2 [00:00<00:00, 35.89it/s, loss=138, v_num=0, train_loss_step=38.70, val_loss=92.40, train_loss_epoch=58.40]\n",
      "Epoch 6:  50%|█████     | 1/2 [00:00<00:00, 21.47it/s, loss=126, v_num=0, train_loss_step=55.00, val_loss=92.40, train_loss_epoch=38.70]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A0m \n",
      "Epoch 4:  50%|█████     | 1/2 [00:00<00:00, 20.79it/s, loss=158, v_num=0, train_loss_step=58.40, val_loss=101.0, train_loss_epoch=99.30]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A0m \n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|██████████| 2/2 [00:00<00:00, 33.09it/s, loss=158, v_num=0, train_loss_step=58.40, val_loss=66.10, train_loss_epoch=99.30]\n",
      "Epoch 5:  50%|█████     | 1/2 [00:00<00:00, 21.62it/s, loss=138, v_num=0, train_loss_step=38.70, val_loss=66.10, train_loss_epoch=58.40]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A0m \n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|██████████| 2/2 [00:00<00:00, 33.47it/s, loss=138, v_num=0, train_loss_step=38.70, val_loss=92.40, train_loss_epoch=58.40]\n",
      "                                                                       \u001b[A\n",
      "\u001b[2m\u001b[36m(train_tune pid=20439)\u001b[0m \n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|██████████| 2/2 [00:00<00:00, 35.73it/s, loss=126, v_num=0, train_loss_step=55.00, val_loss=92.40, train_loss_epoch=38.70]\n",
      "Epoch 5: 100%|██████████| 2/2 [00:00<00:00, 29.56it/s, loss=138, v_num=0, train_loss_step=38.70, val_loss=92.40, train_loss_epoch=38.70]\n",
      "Epoch 6: 100%|██████████| 2/2 [00:00<00:00, 33.75it/s, loss=126, v_num=0, train_loss_step=55.00, val_loss=106.0, train_loss_epoch=38.70]\n",
      "                                                                       \u001b[A\n",
      "Epoch 6:   0%|          | 0/2 [00:00<?, ?it/s, loss=138, v_num=0, train_loss_step=38.70, val_loss=92.40, train_loss_epoch=38.70]        \n",
      "Epoch 6: 100%|██████████| 2/2 [00:00<00:00, 30.08it/s, loss=126, v_num=0, train_loss_step=55.00, val_loss=106.0, train_loss_epoch=55.00]\n",
      "Epoch 7:   0%|          | 0/2 [00:00<?, ?it/s, loss=126, v_num=0, train_loss_step=55.00, val_loss=106.0, train_loss_epoch=55.00]        \n",
      "Epoch 7:  50%|█████     | 1/2 [00:00<00:00, 21.46it/s, loss=118, v_num=0, train_loss_step=63.20, val_loss=106.0, train_loss_epoch=55.00]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A0m \n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|██████████| 2/2 [00:00<00:00, 34.76it/s, loss=118, v_num=0, train_loss_step=63.20, val_loss=103.0, train_loss_epoch=55.00]\n",
      "Epoch 8:   0%|          | 0/2 [00:00<?, ?it/s, loss=118, v_num=0, train_loss_step=63.20, val_loss=103.0, train_loss_epoch=63.20]        \n",
      "Epoch 6:  50%|█████     | 1/2 [00:00<00:00, 21.09it/s, loss=126, v_num=0, train_loss_step=55.00, val_loss=92.40, train_loss_epoch=38.70]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A0m \n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|██████████| 2/2 [00:00<00:00, 33.76it/s, loss=126, v_num=0, train_loss_step=55.00, val_loss=106.0, train_loss_epoch=38.70]\n",
      "Epoch 7:  50%|█████     | 1/2 [00:00<00:00, 22.23it/s, loss=118, v_num=0, train_loss_step=63.20, val_loss=106.0, train_loss_epoch=55.00]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A0m \n",
      "\u001b[2m\u001b[36m(train_tune pid=20440)\u001b[0m \n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 8:  50%|█████     | 1/2 [00:00<00:00, 15.82it/s, loss=112, v_num=0, train_loss_step=58.40, val_loss=103.0, train_loss_epoch=63.20]\n",
      "\u001b[2m\u001b[36m(train_tune pid=20440)\u001b[0m \n",
      "Epoch 7: 100%|██████████| 2/2 [00:00<00:00, 26.61it/s, loss=118, v_num=0, train_loss_step=63.20, val_loss=106.0, train_loss_epoch=55.00]\n",
      "\u001b[2m\u001b[36m(train_tune pid=20439)\u001b[0m \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A0m \n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|██████████| 2/2 [00:00<00:00, 25.29it/s, loss=118, v_num=0, train_loss_step=63.20, val_loss=103.0, train_loss_epoch=55.00]\n",
      "                                                                       \u001b[A\n",
      "\u001b[2m\u001b[36m(train_tune pid=20439)\u001b[0m \n",
      "Epoch 8: 100%|██████████| 2/2 [00:00<00:00, 25.94it/s, loss=112, v_num=0, train_loss_step=58.40, val_loss=91.30, train_loss_epoch=63.20]\n",
      "                                                                       \u001b[A\n",
      "Epoch 8:   0%|          | 0/2 [00:00<?, ?it/s, loss=118, v_num=0, train_loss_step=63.20, val_loss=103.0, train_loss_epoch=63.20]        \n",
      "Epoch 9:  50%|█████     | 1/2 [00:00<00:00, 23.82it/s, loss=106, v_num=0, train_loss_step=51.90, val_loss=91.30, train_loss_epoch=58.40]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A0m \n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|██████████| 2/2 [00:00<00:00, 37.24it/s, loss=106, v_num=0, train_loss_step=51.90, val_loss=83.90, train_loss_epoch=58.40]\n",
      "Epoch 9: 100%|██████████| 2/2 [00:00<00:00, 32.37it/s, loss=106, v_num=0, train_loss_step=51.90, val_loss=83.90, train_loss_epoch=51.90]\n",
      "Epoch 8:  50%|█████     | 1/2 [00:00<00:00, 22.13it/s, loss=112, v_num=0, train_loss_step=58.40, val_loss=103.0, train_loss_epoch=63.20]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A0m \n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|██████████| 2/2 [00:00<00:00, 34.76it/s, loss=112, v_num=0, train_loss_step=58.40, val_loss=91.30, train_loss_epoch=63.20]\n",
      "Epoch 9:  50%|█████     | 1/2 [00:00<00:00, 26.30it/s, loss=106, v_num=0, train_loss_step=51.90, val_loss=91.30, train_loss_epoch=58.40]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A0m \n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|██████████| 2/2 [00:00<00:00, 41.93it/s, loss=106, v_num=0, train_loss_step=51.90, val_loss=83.90, train_loss_epoch=58.40]\n",
      "                                                                       \u001b[A\n",
      "Epoch 9: 100%|██████████| 2/2 [00:00<00:00, 35.89it/s, loss=106, v_num=0, train_loss_step=51.90, val_loss=83.90, train_loss_epoch=51.90]\n",
      "\u001b[2m\u001b[36m(train_tune pid=20440)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_tune pid=20440)\u001b[0m /Users/cchallu/opt/anaconda3/envs/neuralforecast/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n",
      "\u001b[2m\u001b[36m(train_tune pid=20440)\u001b[0m   warnings.warn('resource_tracker: There appear to be %d '\n",
      "\u001b[2m\u001b[36m(train_tune pid=20439)\u001b[0m /Users/cchallu/opt/anaconda3/envs/neuralforecast/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n",
      "\u001b[2m\u001b[36m(train_tune pid=20439)\u001b[0m   warnings.warn('resource_tracker: There appear to be %d '\n",
      "Global seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 2/2 [00:00<00:00, 20.82it/s, loss=106, v_num=238, train_loss_step=51.90, val_loss=83.90, train_loss_epoch=51.90]\n",
      "Predicting DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 87.50it/s] \n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "import pandas as pd\n",
    "from neuralforecast.models.mlp import MLP\n",
    "from neuralforecast.utils import AirPassengersDF as Y_df\n",
    "from neuralforecast.tsdataset import TimeSeriesDataset\n",
    "\n",
    "Y_train_df = Y_df[Y_df.ds<='1959-12-31'] # 132 train\n",
    "Y_test_df = Y_df[Y_df.ds>'1959-12-31']   # 12 test\n",
    "\n",
    "dataset, *_ = TimeSeriesDataset.from_df(Y_train_df)\n",
    "config = {\n",
    "    \"hidden_size\": tune.choice([512]),\n",
    "    \"num_layers\": tune.choice([3, 4]),\n",
    "    \"input_size\": 12,\n",
    "    \"max_steps\": 10,\n",
    "    \"val_check_steps\": 1\n",
    "}\n",
    "auto = BaseAuto(h=12, loss=MAE(), cls_model=MLP, config=config, num_samples=2, cpus=1, gpus=0)\n",
    "auto.fit(dataset=dataset)\n",
    "y_hat = auto.predict(dataset=dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ad2eec-dd93-4bc4-ae19-5df4199577be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "Y_test_df['AutoMLP'] = y_hat\n",
    "\n",
    "pd.concat([Y_train_df, Y_test_df]).drop('unique_id', axis=1).set_index('ds').plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8e2d46",
   "metadata": {},
   "source": [
    "### References\n",
    "- [James Bergstra, Remi Bardenet, Yoshua Bengio, and Balazs Kegl (2011). \"Algorithms for Hyper-Parameter Optimization\". In: Advances in Neural Information Processing Systems. url: https://proceedings.neurips.cc/paper/2011/file/86e8f7ab32cfd12577bc2619bc635690-Paper.pdf](https://proceedings.neurips.cc/paper/2011/file/86e8f7ab32cfd12577bc2619bc635690-Paper.pdf)\n",
    "- [Kirthevasan Kandasamy, Karun Raju Vysyaraju, Willie Neiswanger, Biswajit Paria, Christopher R. Collins, Jeff Schneider, Barnabas Poczos, Eric P. Xing (2019). \"Tuning Hyperparameters without Grad Students: Scalable and Robust Bayesian Optimisation with Dragonfly\". Journal of Machine Learning Research. url: https://arxiv.org/abs/1903.06694](https://arxiv.org/abs/1903.06694)\n",
    "- [Lisha Li, Kevin Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, Ameet Talwalkar (2016). \"Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization\". Journal of Machine Learning Research. url: https://arxiv.org/abs/1603.06560](https://arxiv.org/abs/1603.06560)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267cbf1e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuralforecast",
   "language": "python",
   "name": "neuralforecast"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
