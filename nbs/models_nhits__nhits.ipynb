{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp models.nhits.nhits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import math\n",
    "import random\n",
    "from functools import partial\n",
    "from typing import Tuple, List\n",
    "from fastcore.foundation import patch\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from neuralforecast.models.components.tcn import _TemporalConvNet\n",
    "from neuralforecast.models.components.common import Chomp1d, RepeatVector\n",
    "from neuralforecast.losses.utils import LossFunction\n",
    "from neuralforecast.data.tsdataset import WindowsDataset\n",
    "from neuralforecast.data.tsloader import TimeSeriesLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class _StaticFeaturesEncoder(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(_StaticFeaturesEncoder, self).__init__()\n",
    "        layers = [nn.Dropout(p=0.5),\n",
    "                  nn.Linear(in_features=in_features, out_features=out_features),\n",
    "                  nn.ReLU()]\n",
    "        self.encoder = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        return x\n",
    "\n",
    "class _sEncoder(nn.Module):\n",
    "    def __init__(self, in_features, out_features, n_time_in):\n",
    "        super(_sEncoder, self).__init__()\n",
    "        layers = [nn.Dropout(p=0.5),\n",
    "                  nn.Linear(in_features=in_features, out_features=out_features),\n",
    "                  nn.ReLU()]\n",
    "        self.encoder = nn.Sequential(*layers)\n",
    "        self.repeat = RepeatVector(repeats=n_time_in)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Encode and repeat values to match time\n",
    "        x = self.encoder(x)\n",
    "        x = self.repeat(x) # [N,S_out] -> [N,S_out,T]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class _IdentityBasis(nn.Module):\n",
    "    def __init__(self, backcast_size: int, forecast_size: int, interpolation_mode: str):\n",
    "        super().__init__()\n",
    "        assert (interpolation_mode in ['linear','nearest']) or ('cubic' in interpolation_mode)\n",
    "        self.forecast_size = forecast_size\n",
    "        self.backcast_size = backcast_size\n",
    "        self.interpolation_mode = interpolation_mode\n",
    " \n",
    "    def forward(self, theta: t.Tensor, insample_x_t: t.Tensor, outsample_x_t: t.Tensor) -> Tuple[t.Tensor, t.Tensor]:\n",
    "\n",
    "        backcast = theta[:, :self.backcast_size]\n",
    "        knots = theta[:, self.backcast_size:]\n",
    "\n",
    "        if self.interpolation_mode=='nearest':\n",
    "            knots = knots[:,None,:]\n",
    "            forecast = F.interpolate(knots, size=self.forecast_size, mode=self.interpolation_mode)\n",
    "            forecast = forecast[:,0,:]\n",
    "        elif self.interpolation_mode=='linear':\n",
    "            knots = knots[:,None,:]\n",
    "            forecast = F.interpolate(knots, size=self.forecast_size, mode=self.interpolation_mode)\n",
    "            forecast = forecast[:,0,:]\n",
    "        elif 'cubic' in self.interpolation_mode:\n",
    "            batch_size = len(backcast)\n",
    "            knots = knots[:,None,None,:]\n",
    "            forecast = t.zeros((len(knots), self.forecast_size)).to(knots.device)\n",
    "            n_batches = int(np.ceil(len(knots)/batch_size))\n",
    "            for i in range(n_batches):\n",
    "                forecast_i = F.interpolate(knots[i*batch_size:(i+1)*batch_size], size=self.forecast_size, mode='bicubic')\n",
    "                forecast[i*batch_size:(i+1)*batch_size] += forecast_i[:,0,0,:]\n",
    "\n",
    "        return backcast, forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class _ExogenousBasisInterpretable(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, theta: t.Tensor, insample_x_t: t.Tensor, outsample_x_t: t.Tensor) -> Tuple[t.Tensor, t.Tensor]:\n",
    "        backcast_basis = insample_x_t\n",
    "        forecast_basis = outsample_x_t\n",
    "\n",
    "        cut_point = forecast_basis.shape[1]\n",
    "        backcast = t.einsum('bp,bpt->bt', theta[:, cut_point:], backcast_basis)\n",
    "        forecast = t.einsum('bp,bpt->bt', theta[:, :cut_point], forecast_basis)\n",
    "        return backcast, forecast\n",
    "\n",
    "class _ExogenousBasisWavenet(nn.Module):\n",
    "    def __init__(self, out_features, in_features, num_levels=4, kernel_size=3, dropout_prob=0):\n",
    "        super().__init__()\n",
    "        # Shape of (1, in_features, 1) to broadcast over b and t\n",
    "        self.weight = nn.Parameter(t.Tensor(1, in_features, 1), requires_grad=True)\n",
    "        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(0.5))\n",
    "\n",
    "        padding = (kernel_size - 1) * (2**0)\n",
    "        input_layer = [nn.Conv1d(in_channels=in_features, out_channels=out_features,\n",
    "                                 kernel_size=kernel_size, padding=padding, dilation=2**0),\n",
    "                                 Chomp1d(padding),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Dropout(dropout_prob)]\n",
    "        conv_layers = []\n",
    "        for i in range(1, num_levels):\n",
    "            dilation = 2**i\n",
    "            padding = (kernel_size - 1) * dilation\n",
    "            conv_layers.append(nn.Conv1d(in_channels=out_features, out_channels=out_features,\n",
    "                                         padding=padding, kernel_size=3, dilation=dilation))\n",
    "            conv_layers.append(Chomp1d(padding))\n",
    "            conv_layers.append(nn.ReLU())\n",
    "        conv_layers = input_layer + conv_layers\n",
    "\n",
    "        self.wavenet = nn.Sequential(*conv_layers)\n",
    "        \n",
    "    def transform(self, insample_x_t, outsample_x_t):\n",
    "        n_time_in = insample_x_t.shape[2]\n",
    "        \n",
    "        x_t = t.cat([insample_x_t, outsample_x_t], dim=2)\n",
    "        \n",
    "        x_t = x_t * self.weight # Element-wise multiplication, broadcasted on b and t. Weights used in L1 regularization\n",
    "        x_t = self.wavenet(x_t)[:]\n",
    "\n",
    "        backcast_basis = x_t[:,:, :n_time_in]\n",
    "        forecast_basis = x_t[:,:, n_time_in:]\n",
    "\n",
    "        return backcast_basis, forecast_basis\n",
    "\n",
    "    def forward(self, theta: t.Tensor, insample_x_t: t.Tensor, outsample_x_t: t.Tensor) -> Tuple[t.Tensor, t.Tensor]:\n",
    "        backcast_basis, forecast_basis = self.transform(insample_x_t, outsample_x_t)\n",
    "\n",
    "        cut_point = forecast_basis.shape[1]\n",
    "        backcast = t.einsum('bp,bpt->bt', theta[:, cut_point:], backcast_basis)\n",
    "        forecast = t.einsum('bp,bpt->bt', theta[:, :cut_point], forecast_basis)\n",
    "        return backcast, forecast\n",
    "\n",
    "class _ExogenousBasisTCN(nn.Module):\n",
    "    def __init__(self, out_features, in_features, num_levels = 4, kernel_size=2, dropout_prob=0):\n",
    "        super().__init__()\n",
    "        n_channels = num_levels * [out_features]\n",
    "        self.tcn = _TemporalConvNet(num_inputs=in_features, num_channels=n_channels, kernel_size=kernel_size, dropout=dropout_prob)\n",
    "        \n",
    "    def transform(self, insample_x_t, outsample_x_t):\n",
    "        n_time_in = insample_x_t.shape[2]\n",
    "        \n",
    "        x_t = t.cat([insample_x_t, outsample_x_t], dim=2)\n",
    "        \n",
    "        x_t = self.tcn(x_t)[:]\n",
    "        backcast_basis = x_t[:,:, :n_time_in]\n",
    "        forecast_basis = x_t[:,:, n_time_in:]\n",
    "\n",
    "        return backcast_basis, forecast_basis\n",
    "\n",
    "    def forward(self, theta: t.Tensor, insample_x_t: t.Tensor, outsample_x_t: t.Tensor) -> Tuple[t.Tensor, t.Tensor]:\n",
    "        backcast_basis, forecast_basis = self.transform(insample_x_t, outsample_x_t)\n",
    "\n",
    "        cut_point = forecast_basis.shape[1]\n",
    "        backcast = t.einsum('bp,bpt->bt', theta[:, cut_point:], backcast_basis)\n",
    "        forecast = t.einsum('bp,bpt->bt', theta[:, :cut_point], forecast_basis)\n",
    "        return backcast, forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _init_weights(module, initialization):\n",
    "    if type(module) == t.nn.Linear:\n",
    "        if initialization == 'orthogonal':\n",
    "            t.nn.init.orthogonal_(module.weight)\n",
    "        elif initialization == 'he_uniform':\n",
    "            t.nn.init.kaiming_uniform_(module.weight)\n",
    "        elif initialization == 'he_normal':\n",
    "            t.nn.init.kaiming_normal_(module.weight)\n",
    "        elif initialization == 'glorot_uniform':\n",
    "            t.nn.init.xavier_uniform_(module.weight)\n",
    "        elif initialization == 'glorot_normal':\n",
    "            t.nn.init.xavier_normal_(module.weight)\n",
    "        elif initialization == 'lecun_normal':\n",
    "            pass #t.nn.init.normal_(module.weight, 0.0, std=1/np.sqrt(module.weight.numel()))\n",
    "        else:\n",
    "            assert 1<0, f'Initialization {initialization} not found'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "ACTIVATIONS = ['ReLU',\n",
    "               'Softplus',\n",
    "               'Tanh',\n",
    "               'SELU',\n",
    "               'LeakyReLU',\n",
    "               'PReLU',\n",
    "               'Sigmoid']\n",
    "\n",
    "class _NHITSBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    N-BEATS block which takes a basis function as an argument.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_time_in: int, n_time_out: int, n_x: int,\n",
    "                 n_s: int, n_s_hidden: int, n_theta: int, n_mlp_units: list,\n",
    "                 n_pool_kernel_size: int, pooling_mode: str, basis: nn.Module,\n",
    "                 n_layers: int,  batch_normalization: bool, dropout_prob: float, activation: str):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        assert (pooling_mode in ['max','average'])\n",
    "\n",
    "        n_time_in_pooled = int(np.ceil(n_time_in/n_pool_kernel_size))\n",
    "\n",
    "        if n_s == 0:\n",
    "            n_s_hidden = 0\n",
    "        n_mlp_units = [n_time_in_pooled + (n_time_in+n_time_out)*n_x + n_s_hidden] + n_mlp_units\n",
    "        \n",
    "        self.n_time_in = n_time_in\n",
    "        self.n_time_out = n_time_out\n",
    "        self.n_s = n_s\n",
    "        self.n_s_hidden = n_s_hidden\n",
    "        self.n_x = n_x\n",
    "        self.n_pool_kernel_size = n_pool_kernel_size\n",
    "        self.batch_normalization = batch_normalization\n",
    "        self.dropout_prob = dropout_prob\n",
    "        \n",
    "        assert activation in ACTIVATIONS, f'{activation} is not in {ACTIVATIONS}'\n",
    "        activ = getattr(nn, activation)()\n",
    "\n",
    "        if pooling_mode == 'max':\n",
    "            self.pooling_layer = nn.MaxPool1d(kernel_size=self.n_pool_kernel_size,\n",
    "                                              stride=self.n_pool_kernel_size, ceil_mode=True)\n",
    "        elif pooling_mode == 'average':\n",
    "            self.pooling_layer = nn.AvgPool1d(kernel_size=self.n_pool_kernel_size,\n",
    "                                              stride=self.n_pool_kernel_size, ceil_mode=True)\n",
    "\n",
    "        hidden_layers = []\n",
    "        for i in range(n_layers):\n",
    "            hidden_layers.append(nn.Linear(in_features=n_mlp_units[i], out_features=n_mlp_units[i+1]))\n",
    "            hidden_layers.append(activ)\n",
    "\n",
    "            if self.batch_normalization:\n",
    "                hidden_layers.append(nn.BatchNorm1d(num_features=n_mlp_units[i+1]))\n",
    "\n",
    "            if self.dropout_prob>0:\n",
    "                hidden_layers.append(nn.Dropout(p=self.dropout_prob))\n",
    "\n",
    "        output_layer = [nn.Linear(in_features=n_mlp_units[-1], out_features=n_theta)]\n",
    "        layers = hidden_layers + output_layer\n",
    "\n",
    "        # n_s is computed with data, n_s_hidden is provided by user, if 0 no statics are used\n",
    "        if (self.n_s > 0) and (self.n_s_hidden > 0):\n",
    "            self.static_encoder = _StaticFeaturesEncoder(in_features=n_s, out_features=n_s_hidden)\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        self.basis = basis\n",
    "\n",
    "    def forward(self, insample_y: t.Tensor, insample_x_t: t.Tensor,\n",
    "                outsample_x_t: t.Tensor, x_s: t.Tensor) -> Tuple[t.Tensor, t.Tensor]:\n",
    "\n",
    "        insample_y = insample_y.unsqueeze(1)\n",
    "        insample_y = self.pooling_layer(insample_y)\n",
    "        insample_y = insample_y.squeeze(1)\n",
    "\n",
    "        batch_size = len(insample_y)\n",
    "        if self.n_x > 0:\n",
    "            insample_y = t.cat(( insample_y, insample_x_t.reshape(batch_size, -1) ), 1)\n",
    "            insample_y = t.cat(( insample_y, outsample_x_t.reshape(batch_size, -1) ), 1)\n",
    "        \n",
    "        # Static exogenous\n",
    "        if (self.n_s > 0) and (self.n_s_hidden > 0):\n",
    "            x_s = self.static_encoder(x_s)\n",
    "            insample_y = t.cat((insample_y, x_s), 1)\n",
    "\n",
    "        # Compute local projection weights and projection\n",
    "        theta = self.layers(insample_y)\n",
    "        backcast, forecast = self.basis(theta, insample_x_t, outsample_x_t)\n",
    "\n",
    "        return backcast, forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class _NHITS(nn.Module):\n",
    "    \"\"\"\n",
    "    NHITS Model.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 n_time_in,\n",
    "                 n_time_out,\n",
    "                 n_s,\n",
    "                 n_x,\n",
    "                 n_s_hidden,\n",
    "                 n_x_hidden,\n",
    "                 stack_types: list,\n",
    "                 n_blocks: list,\n",
    "                 n_layers: list,\n",
    "                 n_mlp_units: list,\n",
    "                 n_pool_kernel_size: list,\n",
    "                 n_freq_downsample: list,\n",
    "                 pooling_mode,\n",
    "                 interpolation_mode,\n",
    "                 dropout_prob_theta,\n",
    "                 activation,\n",
    "                 initialization,\n",
    "                 batch_normalization,\n",
    "                 shared_weights):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_time_out = n_time_out\n",
    "\n",
    "        blocks = self.create_stack(stack_types=stack_types, \n",
    "                                   n_blocks=n_blocks,\n",
    "                                   n_time_in=n_time_in,\n",
    "                                   n_time_out=n_time_out,\n",
    "                                   n_x=n_x,\n",
    "                                   n_x_hidden=n_x_hidden,\n",
    "                                   n_s=n_s,\n",
    "                                   n_s_hidden=n_s_hidden,\n",
    "                                   n_layers=n_layers,\n",
    "                                   n_mlp_units=n_mlp_units,\n",
    "                                   n_pool_kernel_size=n_pool_kernel_size,\n",
    "                                   n_freq_downsample=n_freq_downsample,\n",
    "                                   pooling_mode=pooling_mode,\n",
    "                                   interpolation_mode=interpolation_mode,\n",
    "                                   batch_normalization=batch_normalization,\n",
    "                                   dropout_prob_theta=dropout_prob_theta,\n",
    "                                   activation=activation,\n",
    "                                   shared_weights=shared_weights,\n",
    "                                   initialization=initialization)\n",
    "        self.blocks = t.nn.ModuleList(blocks)\n",
    "\n",
    "    def create_stack(self, stack_types, n_blocks, \n",
    "                     n_time_in, n_time_out, \n",
    "                     n_x, n_x_hidden, n_s, n_s_hidden, \n",
    "                     n_layers, n_mlp_units, \n",
    "                     n_pool_kernel_size, n_freq_downsample, pooling_mode, interpolation_mode,\n",
    "                     batch_normalization, dropout_prob_theta, \n",
    "                     activation, shared_weights, initialization):                     \n",
    "        block_list = []\n",
    "        for i in range(len(stack_types)):\n",
    "            assert stack_types[i] in ['identity', 'exogenous', 'exogenous_tcn', 'exogenous_wavenet'], 'f Invalid stack type {stack_types[i]}'\n",
    "            for block_id in range(n_blocks[i]):\n",
    "                \n",
    "                # Batch norm only on first block\n",
    "                if (len(block_list)==0) and (batch_normalization):\n",
    "                    batch_normalization_block = True\n",
    "                else:\n",
    "                    batch_normalization_block = False\n",
    "\n",
    "                # Shared weights\n",
    "                if shared_weights and block_id>0:\n",
    "                    nbeats_block = block_list[-1]\n",
    "                else:\n",
    "                    if stack_types[i] == 'identity':\n",
    "                        n_theta = (n_time_in + max(n_time_out//n_freq_downsample[i], 1) )\n",
    "                        basis = _IdentityBasis(backcast_size=n_time_in,\n",
    "                                              forecast_size=n_time_out,\n",
    "                                              interpolation_mode=interpolation_mode)                        \n",
    "\n",
    "                    elif stack_types[i] == 'exogenous':\n",
    "                        n_theta = 2 * n_x\n",
    "                        basis = _ExogenousBasisInterpretable()\n",
    "\n",
    "                    elif stack_types[i] == 'exogenous_tcn':\n",
    "                        n_theta = 2 * n_x_hidden\n",
    "                        basis = _ExogenousBasisTCN(n_x_hidden, n_x)\n",
    "\n",
    "                    elif stack_types[i] == 'exogenous_wavenet':\n",
    "                        n_theta = 2 * n_x_hidden\n",
    "                        basis = _ExogenousBasisWavenet(n_x_hidden, n_x)\n",
    "\n",
    "                    nbeats_block = _NHITSBlock(n_time_in=n_time_in,\n",
    "                                               n_time_out=n_time_out,\n",
    "                                               n_x=n_x,\n",
    "                                               n_s=n_s,\n",
    "                                               n_s_hidden=n_s_hidden,\n",
    "                                               n_theta=n_theta,\n",
    "                                               n_mlp_units=n_mlp_units[i],\n",
    "                                               n_pool_kernel_size=n_pool_kernel_size[i],\n",
    "                                               pooling_mode=pooling_mode,\n",
    "                                               basis=basis,\n",
    "                                               n_layers=n_layers[i],\n",
    "                                               batch_normalization=batch_normalization_block,\n",
    "                                               dropout_prob=dropout_prob_theta,\n",
    "                                               activation=activation)\n",
    "\n",
    "                # Select type of evaluation and apply it to all layers of block\n",
    "                init_function = partial(_init_weights, initialization=initialization)                                             \n",
    "                nbeats_block.layers.apply(init_function)\n",
    "                block_list.append(nbeats_block)\n",
    "        return block_list\n",
    "\n",
    "    def forward(self, S: t.Tensor, Y: t.Tensor, X: t.Tensor, \n",
    "                insample_mask: t.Tensor, outsample_mask: t.Tensor,\n",
    "                return_decomposition: bool=False):\n",
    "        \n",
    "        # insample\n",
    "        insample_y    = Y[:, :-self.n_time_out]\n",
    "        insample_x_t  = X[:, :, :-self.n_time_out]\n",
    "        insample_mask = insample_mask[:, :-self.n_time_out]\n",
    "        \n",
    "        # outsample\n",
    "        outsample_y   = Y[:, -self.n_time_out:]\n",
    "        outsample_x_t = X[:, :, -self.n_time_out:]\n",
    "        outsample_mask = outsample_mask[:, -self.n_time_out:]\n",
    "\n",
    "        if return_decomposition:\n",
    "            forecast, block_forecasts = self.forecast_decomposition(insample_y=insample_y, \n",
    "                                                                    insample_x_t=insample_x_t, \n",
    "                                                                    insample_mask=insample_mask,\n",
    "                                                                    outsample_x_t=outsample_x_t,\n",
    "                                                                    x_s=S)\n",
    "            return outsample_y, forecast, block_forecasts, outsample_mask\n",
    "        \n",
    "        else:\n",
    "            forecast = self.forecast(insample_y=insample_y,\n",
    "                                     insample_x_t=insample_x_t, \n",
    "                                     insample_mask=insample_mask,\n",
    "                                     outsample_x_t=outsample_x_t,\n",
    "                                     x_s=S)\n",
    "            return outsample_y, forecast, outsample_mask\n",
    "\n",
    "    def forecast(self, insample_y: t.Tensor, insample_x_t: t.Tensor, insample_mask: t.Tensor,\n",
    "                 outsample_x_t: t.Tensor, x_s: t.Tensor):\n",
    "\n",
    "        residuals = insample_y.flip(dims=(-1,))\n",
    "        insample_x_t = insample_x_t.flip(dims=(-1,))\n",
    "        insample_mask = insample_mask.flip(dims=(-1,))\n",
    "\n",
    "        forecast = insample_y[:, -1:] # Level with Naive1\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            backcast, block_forecast = block(insample_y=residuals, insample_x_t=insample_x_t,\n",
    "                                             outsample_x_t=outsample_x_t, x_s=x_s)\n",
    "            residuals = (residuals - backcast) * insample_mask\n",
    "            forecast = forecast + block_forecast\n",
    "\n",
    "        return forecast\n",
    "\n",
    "    def forecast_decomposition(self, insample_y: t.Tensor, insample_x_t: t.Tensor, insample_mask: t.Tensor,\n",
    "                               outsample_x_t: t.Tensor, x_s: t.Tensor):\n",
    "\n",
    "        residuals = insample_y.flip(dims=(-1,))\n",
    "        insample_x_t = insample_x_t.flip(dims=(-1,))\n",
    "        insample_mask = insample_mask.flip(dims=(-1,))\n",
    "        \n",
    "        n_batch, n_channels, n_t = outsample_x_t.size(0), outsample_x_t.size(1), outsample_x_t.size(2)\n",
    "        \n",
    "        level = insample_y[:, -1:] # Level with Naive1\n",
    "        block_forecasts = [ level.repeat(1, n_t) ]\n",
    "                \n",
    "        forecast = level\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            backcast, block_forecast = block(insample_y=residuals, insample_x_t=insample_x_t,\n",
    "                                             outsample_x_t=outsample_x_t, x_s=x_s)\n",
    "            residuals = (residuals - backcast) * insample_mask\n",
    "            forecast = forecast + block_forecast\n",
    "            block_forecasts.append(block_forecast)\n",
    "            \n",
    "        # (n_batch, n_blocks, n_t)\n",
    "        block_forecasts = t.stack(block_forecasts)\n",
    "        block_forecasts = block_forecasts.permute(1,0,2)\n",
    "\n",
    "        return forecast, block_forecasts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-HiTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A new model for long-horizon forecasting which incorporates novel hierarchical interpolation and multi-rate data sampling techniques to specialize blocks of its architecture to different frequency band of the time-series signal. It achieves SoTA performance on several benchmark datasets, outperforming current Transformer-based models by more than 25%. Paper available at https://arxiv.org/abs/2201.12886"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class NHITS(pl.LightningModule):\n",
    "    def __init__(self,\n",
    "                 n_time_in: int,\n",
    "                 n_time_out: int,\n",
    "                 n_x: int,\n",
    "                 n_s: int,\n",
    "                 shared_weights: bool,\n",
    "                 activation: str,\n",
    "                 initialization: str,\n",
    "                 stack_types: List[str],\n",
    "                 n_blocks: List[int],\n",
    "                 n_layers: List[int],\n",
    "                 n_mlp_units: List[List[int]],\n",
    "                 n_x_hidden: int,\n",
    "                 n_s_hidden: int,\n",
    "                 n_pool_kernel_size: List[int],\n",
    "                 n_freq_downsample: List[int],\n",
    "                 pooling_mode: str,\n",
    "                 interpolation_mode: str,\n",
    "                 batch_normalization: bool,\n",
    "                 dropout_prob_theta: float,\n",
    "                 learning_rate: float,\n",
    "                 lr_decay: float,\n",
    "                 lr_decay_step_size: int,\n",
    "                 weight_decay: float,\n",
    "                 loss_train: str,\n",
    "                 loss_hypar: float,\n",
    "                 loss_valid: str,\n",
    "                 frequency: str,\n",
    "                 random_seed: int):\n",
    "        \"\"\"\n",
    "        N-HiTS model.\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            n_time_in: int\n",
    "                Multiplier to get insample size.\n",
    "                Insample size = n_time_in * output_size\n",
    "            n_time_out: int\n",
    "                Forecast horizon.\n",
    "            n_x: int\n",
    "                Number of exogenous variables.\n",
    "            n_s: int\n",
    "                Number of static variables.\n",
    "            shared_weights: bool\n",
    "                If True, all blocks within each stack will share parameters.\n",
    "            activation: str\n",
    "                Activation function.\n",
    "                An item from ['ReLU', 'Softplus', 'Tanh', 'SELU', 'LeakyReLU', 'PReLU', 'Sigmoid'].\n",
    "            initialization: str\n",
    "                Initialization function.\n",
    "                An item from ['orthogonal', 'he_uniform', 'glorot_uniform', 'glorot_normal', 'lecun_normal'].\n",
    "            stack_types: List[str]\n",
    "                List of stack types.\n",
    "                Subset from ['seasonality', 'trend', 'identity', 'exogenous', 'exogenous_tcn', 'exogenous_wavenet'].\n",
    "            n_blocks: List[int]\n",
    "                Number of blocks for each stack.\n",
    "                Note that len(n_blocks) = len(stack_types).\n",
    "            n_layers: List[int]\n",
    "                Number of layers for each stack type.\n",
    "                Note that len(n_layers) = len(stack_types).\n",
    "            n_mlp_units: List[List[int]]\n",
    "                Structure of hidden layers for each stack type.\n",
    "                Each internal list should contain the number of units of each hidden layer.\n",
    "                Note that len(n_hidden) = len(stack_types).\n",
    "            n_x_hidden: int\n",
    "                Number of hidden output channels of exogenous_tcn and exogenous_wavenet stacks. \n",
    "            n_s_hidden: int\n",
    "                Number of encoded static features, output dim of _StaticFeaturesEncoder. \n",
    "            n_pool_kernel_size List[int]:\n",
    "                Pooling size for input for each stack.\n",
    "                Note that len(n_pool_kernel_size) = len(stack_types).\n",
    "            n_freq_downsample List[int]:\n",
    "                Downsample multiplier of output for each stack. Expressivity ratio (r) = 1/n_freq_downsample\n",
    "                Note that len(n_freq_downsample) = len(stack_types).\n",
    "            pooling_mode: str\n",
    "                Pooling type.\n",
    "                An item from ['average', 'max']\n",
    "            interpolation_mode: str\n",
    "                Interpolation function.\n",
    "                An item from ['linear', 'nearest', 'cubic']\n",
    "            batch_normalization: bool\n",
    "                Whether perform batch normalization.\n",
    "            dropout_prob_theta: float\n",
    "                Float between (0, 1).\n",
    "                Dropout for Nbeats basis.\n",
    "            learning_rate: float\n",
    "                Learning rate between (0, 1).\n",
    "            lr_decay: float\n",
    "                Decreasing multiplier for the learning rate.\n",
    "            lr_decay_step_size: int\n",
    "                Steps between each learning rate decay.\n",
    "            weight_decay: float\n",
    "                L2 penalty for optimizer.\n",
    "            loss_train: str\n",
    "                Loss to optimize.\n",
    "                An item from ['MAPE', 'MASE', 'SMAPE', 'MSE', 'MAE', 'QUANTILE', 'QUANTILE2'].\n",
    "            loss_hypar: float\n",
    "                Hyperparameter for chosen loss.\n",
    "            loss_valid: str\n",
    "                Validation loss.\n",
    "                An item from ['MAPE', 'MASE', 'SMAPE', 'RMSE', 'MAE', 'QUANTILE'].\n",
    "            frequency: str\n",
    "                Time series frequency.\n",
    "            random_seed: int\n",
    "                random_seed for pseudo random pytorch initializer and\n",
    "                numpy random generator.\n",
    "        \"\"\"\n",
    "        \n",
    "        super(NHITS, self).__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        if activation == 'SELU': initialization = 'lecun_normal'\n",
    "\n",
    "        #------------------------ Model Attributes ------------------------#\n",
    "        # Architecture parameters\n",
    "        self.n_time_in = n_time_in\n",
    "        self.n_time_out = n_time_out\n",
    "        self.n_x = n_x\n",
    "        self.n_x_hidden = n_x_hidden\n",
    "        self.n_s = n_s\n",
    "        self.n_s_hidden = n_s_hidden\n",
    "        self.shared_weights = shared_weights\n",
    "        self.activation = activation\n",
    "        self.initialization = initialization\n",
    "        self.stack_types = stack_types\n",
    "        self.n_blocks = n_blocks\n",
    "        self.n_layers = n_layers\n",
    "        self.n_mlp_units = n_mlp_units\n",
    "        self.n_pool_kernel_size = n_pool_kernel_size\n",
    "        self.n_freq_downsample = n_freq_downsample\n",
    "        self.pooling_mode = pooling_mode\n",
    "        self.interpolation_mode = interpolation_mode\n",
    "\n",
    "        # Loss functions\n",
    "        self.loss_train = loss_train\n",
    "        self.loss_hypar = loss_hypar\n",
    "        self.loss_valid = loss_valid\n",
    "        self.loss_fn_train = LossFunction(loss_train,\n",
    "                                          percentile=self.loss_hypar,\n",
    "                                          seasonality=self.loss_hypar)\n",
    "        self.loss_fn_valid = LossFunction(loss_valid,\n",
    "                                          percentile=self.loss_hypar,\n",
    "                                          seasonality=self.loss_hypar)\n",
    "        \n",
    "        # Regularization and optimization parameters\n",
    "        self.batch_normalization = batch_normalization\n",
    "        self.dropout_prob_theta = dropout_prob_theta        \n",
    "        self.learning_rate = learning_rate\n",
    "        self.lr_decay = lr_decay\n",
    "        self.weight_decay = weight_decay\n",
    "        self.lr_decay_step_size = lr_decay_step_size\n",
    "        self.random_seed = random_seed\n",
    "\n",
    "        # Data parameters\n",
    "        self.frequency = frequency\n",
    "        self.return_decomposition = False\n",
    "\n",
    "        self.model = _NHITS(n_time_in=self.n_time_in,\n",
    "                            n_time_out=self.n_time_out,\n",
    "                            n_s=self.n_s,\n",
    "                            n_x=self.n_x,\n",
    "                            n_s_hidden=self.n_s_hidden,\n",
    "                            n_x_hidden=self.n_x_hidden,\n",
    "                            stack_types=self.stack_types,\n",
    "                            n_blocks=self.n_blocks,\n",
    "                            n_layers=self.n_layers,\n",
    "                            n_mlp_units=self.n_mlp_units,\n",
    "                            n_pool_kernel_size=self.n_pool_kernel_size,\n",
    "                            n_freq_downsample=self.n_freq_downsample,\n",
    "                            pooling_mode=self.pooling_mode,\n",
    "                            interpolation_mode=self.interpolation_mode,\n",
    "                            dropout_prob_theta=self.dropout_prob_theta,\n",
    "                            activation=self.activation,\n",
    "                            initialization=self.initialization,\n",
    "                            batch_normalization=self.batch_normalization,\n",
    "                            shared_weights=self.shared_weights)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        S = batch['S']\n",
    "        Y = batch['Y']\n",
    "        X = batch['X']\n",
    "        sample_mask = batch['sample_mask']\n",
    "        available_mask = batch['available_mask']\n",
    "\n",
    "        outsample_y, forecast, outsample_mask = self.model(S=S, Y=Y, X=X,\n",
    "                                                           insample_mask=available_mask,\n",
    "                                                           outsample_mask=sample_mask,\n",
    "                                                           return_decomposition=False)\n",
    "\n",
    "        loss = self.loss_fn_train(y=outsample_y,\n",
    "                                  y_hat=forecast,\n",
    "                                  mask=outsample_mask,\n",
    "                                  y_insample=Y)\n",
    "\n",
    "        self.log('train_loss', loss, prog_bar=True, on_epoch=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, idx):\n",
    "        S = batch['S']\n",
    "        Y = batch['Y']\n",
    "        X = batch['X']\n",
    "        sample_mask = batch['sample_mask']\n",
    "        available_mask = batch['available_mask']\n",
    "\n",
    "        outsample_y, forecast, outsample_mask = self.model(S=S, Y=Y, X=X,\n",
    "                                                           insample_mask=available_mask,\n",
    "                                                           outsample_mask=sample_mask,\n",
    "                                                           return_decomposition=False)\n",
    "\n",
    "        loss = self.loss_fn_valid(y=outsample_y,\n",
    "                                  y_hat=forecast,\n",
    "                                  mask=outsample_mask,\n",
    "                                  y_insample=Y)\n",
    "\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def on_fit_start(self):\n",
    "        t.manual_seed(self.random_seed)\n",
    "        np.random.seed(self.random_seed)\n",
    "        random.seed(self.random_seed) #TODO: interaccion rara con window_sampling de validacion\n",
    "\n",
    "    def forward(self, batch):\n",
    "        S = batch['S']\n",
    "        Y = batch['Y']\n",
    "        X = batch['X']\n",
    "        sample_mask = batch['sample_mask']\n",
    "        available_mask = batch['available_mask']\n",
    "\n",
    "        if self.return_decomposition:\n",
    "            outsample_y, forecast, block_forecast, outsample_mask = self.model(S=S, Y=Y, X=X,\n",
    "                                                                     insample_mask=available_mask,\n",
    "                                                                     outsample_mask=sample_mask,\n",
    "                                                                     return_decomposition=True)\n",
    "            return outsample_y, forecast, block_forecast, outsample_mask\n",
    "\n",
    "        outsample_y, forecast, outsample_mask = self.model(S=S, Y=Y, X=X,\n",
    "                                                           insample_mask=available_mask,\n",
    "                                                           outsample_mask=sample_mask,\n",
    "                                                           return_decomposition=False)\n",
    "        return outsample_y, forecast, outsample_mask\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.model.parameters(),\n",
    "                               lr=self.learning_rate, \n",
    "                               weight_decay=self.weight_decay)\n",
    "        \n",
    "        lr_scheduler = optim.lr_scheduler.StepLR(optimizer, \n",
    "                                                 step_size=self.lr_decay_step_size, \n",
    "                                                 gamma=self.lr_decay)\n",
    "\n",
    "        return {'optimizer': optimizer, 'lr_scheduler': lr_scheduler}\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def predict(self: NHITS, Y_df: pd.DataFrame, X_df: pd.DataFrame = None, S_df: pd.DataFrame = None, \n",
    "            batch_size: int=1, trainer: pl.Trainer =None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Method for forecasting self.n_time_out periods after last timestamp of Y_df.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        Y_df: pd.DataFrame\n",
    "            Dataframe with target time-series data, needs 'unique_id','ds' and 'y' columns.\n",
    "        X_df: pd.DataFrame\n",
    "            Dataframe with exogenous time-series data, needs 'unique_id' and 'ds' columns.\n",
    "            Note that 'unique_id' and 'ds' must match Y_df.\n",
    "        S_df: pd.DataFrame\n",
    "            Dataframe with static data, needs 'unique_id' column.\n",
    "        bath_size: int\n",
    "            Batch size for forecasting.\n",
    "        trainer: pl.Trainer\n",
    "            Trainer object for model training and evaluation.\n",
    "\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "        y_true: np.array\n",
    "            Array with true values, of shape (n_series, n_windows, n_time_out)\n",
    "        y_hat: np.array\n",
    "            Array with forecasts, of shape (n_series, n_windows, n_time_out)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add forecast dates to Y_df\n",
    "    Y_df['ds'] = pd.to_datetime(Y_df['ds'])\n",
    "    if X_df is not None:\n",
    "        X_df['ds'] = pd.to_datetime(X_df['ds'])\n",
    "\n",
    "    n_series = Y_df['unique_id'].nunique()\n",
    "\n",
    "    # Dataset, loader and trainer\n",
    "    dataset = WindowsDataset(S_df=S_df, Y_df=Y_df, X_df=X_df,\n",
    "                             mask_df=None, f_cols=[],\n",
    "                             input_size=self.n_time_in,\n",
    "                             output_size=self.n_time_out,\n",
    "                             sample_freq=1,\n",
    "                             complete_windows=True,\n",
    "                             ds_in_test=0,\n",
    "                             is_test=False,\n",
    "                             verbose=True)\n",
    "\n",
    "    loader = TimeSeriesLoader(dataset=dataset,\n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=False)\n",
    "\n",
    "    if trainer is None:\n",
    "        gpus = -1 if t.cuda.is_available() else 0\n",
    "        trainer = pl.Trainer(progress_bar_refresh_rate=1,\n",
    "                             gpus=gpus,\n",
    "                             logger=False)\n",
    "\n",
    "    # Forecast\n",
    "    outputs = trainer.predict(self, loader)\n",
    "\n",
    "    y_true, y_hat, _ = [t.cat(output).cpu().numpy() for output in zip(*outputs)]\n",
    "\n",
    "    # Y_hat dataframe wrangling\n",
    "    unique_ids = Y_df['unique_id'].unique()\n",
    "\n",
    "    # Extend ds vector to match padded ts_tensor\n",
    "    timedelta = dataset.ds[1] -dataset.ds[0]\n",
    "    start_date = dataset.ds[0] - dataset.input_size*timedelta\n",
    "    end_date = dataset.ds[-1] + dataset.output_size*timedelta\n",
    "    ds = pd.date_range(start_date, end_date, freq=dataset.frequency)\n",
    "\n",
    "    # Tensor with sample_mask\n",
    "    tensor = dataset.ts_tensor[:,dataset.t_cols.index('sample_mask'),:]\n",
    "    padding = (model.n_time_in, model.n_time_out)\n",
    "    padder = t.nn.ConstantPad1d(padding=padding, value=0)\n",
    "    tensor = padder(tensor)\n",
    "\n",
    "    # Add dates indexes to tensor\n",
    "    ds_idxs = t.tile(t.range(start=0, end=len(ds)-1),dims=[len(tensor), 1])\n",
    "    tensor = t.cat((tensor[:,None,:],ds_idxs[:,None,:]),dim=1)\n",
    "\n",
    "    # Rolling window of sampling mask and dates indexes\n",
    "    tensor = tensor.unfold(dimension=-1, \n",
    "                            size=model.n_time_in+model.n_time_out,\n",
    "                            step=dataset.sample_freq)\n",
    "\n",
    "    # Split rolled tensor in sample_mask and ds\n",
    "    ds_rolled = ds[tensor[:,1,:,-model.n_time_out-1:].numpy().astype(int)] # -model.n_time_out-1 to add forecast creation date\n",
    "    sample_mask_rolled = tensor[:,0,:,-model.n_time_out:]\n",
    "\n",
    "    # Sampling condition, ALL ds in the forecast window should be sampleaple\n",
    "    sample_condition = (sample_mask_rolled.sum(dim=2)==model.n_time_out)\n",
    "\n",
    "    # Filter rolled ds with sample_condition\n",
    "    ds_rolled = ds_rolled.reshape(-1, model.n_time_out+1)\n",
    "    ds_rolled = ds_rolled[sample_condition.flatten()]\n",
    "\n",
    "    # Split forecast creation date and ds\n",
    "    fcds = ds_rolled[:,0]\n",
    "    ds_rolled = ds_rolled[:,1:]\n",
    "\n",
    "    # Unique_id column\n",
    "    unique_id_counts = model.n_time_out*sample_condition.sum(dim=1)\n",
    "    unique_ids_column = [[u_id]*count for u_id, count in zip(unique_ids, unique_id_counts)]\n",
    "    unique_ids_column = np.concatenate(unique_ids_column)\n",
    "\n",
    "    # Forecast creation date column\n",
    "    fcds_column = fcds.repeat(model.n_time_out)\n",
    "\n",
    "    # Output DataFrame\n",
    "    Y_hat_df = pd.DataFrame({'unique_id':unique_ids_column,\n",
    "                             'forecast_creation_ds': fcds_column,\n",
    "                             'ds':ds_rolled.flatten(),\n",
    "                             'y': y_true.flatten(),\n",
    "                             'y_hat':y_hat.flatten()})\n",
    "\n",
    "    return Y_hat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def forecast(self: NHITS, Y_df: pd.DataFrame, X_df: pd.DataFrame = None, S_df: pd.DataFrame = None, \n",
    "                batch_size: int =1, trainer: pl.Trainer =None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Method for forecasting self.n_time_out periods after last timestamp of Y_df.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    Y_df: pd.DataFrame\n",
    "        Dataframe with target time-series data, needs 'unique_id','ds' and 'y' columns.\n",
    "    X_df: pd.DataFrame\n",
    "        Dataframe with exogenous time-series data, needs 'unique_id' and 'ds' columns.\n",
    "        Note that 'unique_id' and 'ds' must match Y_df plus the forecasting horizon.\n",
    "    S_df: pd.DataFrame\n",
    "        Dataframe with static data, needs 'unique_id' column.\n",
    "    bath_size: int\n",
    "        Batch size for forecasting.\n",
    "    trainer: pl.Trainer\n",
    "        Trainer object for model training and evaluation.\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    forecast_df: pd.DataFrame\n",
    "        Dataframe with forecasts.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add forecast dates to Y_df\n",
    "    Y_df = Y_df.reset_index(drop=True)\n",
    "    Y_df['ds'] = pd.to_datetime(Y_df['ds'])\n",
    "    if X_df is not None:\n",
    "        X_df = X_df.reset_index(drop=True)\n",
    "        X_df['ds'] = pd.to_datetime(X_df['ds'])\n",
    "    forecast_dates = pd.date_range(Y_df['ds'].max(), periods=self.n_time_out+1, freq=self.frequency)[1:]\n",
    "    index = pd.MultiIndex.from_product([Y_df['unique_id'].unique(), forecast_dates], names=['unique_id', 'ds'])\n",
    "    forecast_df = pd.DataFrame({'y':[0]}, index=index).reset_index()\n",
    "\n",
    "    Y_df = Y_df.append(forecast_df).sort_values(['unique_id','ds']).reset_index(drop=True)\n",
    "\n",
    "    # Dataset, loader and trainer\n",
    "    dataset = WindowsDataset(S_df=S_df, Y_df=Y_df, X_df=X_df,\n",
    "                                mask_df=None, f_cols=[],\n",
    "                                input_size=self.n_time_in,\n",
    "                                output_size=self.n_time_out,\n",
    "                                sample_freq=1,\n",
    "                                complete_windows=True,\n",
    "                                ds_in_test=self.n_time_out,\n",
    "                                is_test=True,\n",
    "                                verbose=True)\n",
    "\n",
    "    loader = TimeSeriesLoader(dataset=dataset,\n",
    "                                batch_size=batch_size,\n",
    "                                shuffle=False)\n",
    "\n",
    "    if trainer is None:\n",
    "        gpus = -1 if t.cuda.is_available() else 0\n",
    "        trainer = pl.Trainer(progress_bar_refresh_rate=1,\n",
    "                             gpus=gpus,\n",
    "                             logger=False)\n",
    "\n",
    "    # Forecast\n",
    "    outputs = trainer.predict(self, loader)\n",
    "\n",
    "    # Process forecast and include in forecast_df\n",
    "    _, forecast, _ = [t.cat(output).cpu().numpy() for output in zip(*outputs)]\n",
    "    forecast_df['y'] = forecast.flatten()\n",
    "\n",
    "    return forecast_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-HITS Usage Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from neuralforecast.data.datasets.long_horizon import LongHorizon\n",
    "\n",
    "from neuralforecast.data.tsloader import TimeSeriesLoader\n",
    "\n",
    "import pylab as plt\n",
    "from pylab import rcParams\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "\n",
    "FONTSIZE = 19\n",
    "\n",
    "Y_df, X_df, S_df = LongHorizon.load(directory='./data', group='ETTm2')\n",
    "Y_df = Y_df[Y_df['unique_id'].isin(['HUFL','OT'])]\n",
    "Y_df = Y_df[1000:]\n",
    "Y_df = Y_df.reset_index(drop=True)\n",
    "Y_df['y'] = range(len(Y_df)) \n",
    "Y_df['ds'] = pd.to_datetime(Y_df['ds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(15, 6))\n",
    "plt.plot(Y_df[Y_df['unique_id']=='OT'].ds, Y_df[Y_df['unique_id']=='OT'].y.values, color='#628793', linewidth=0.4)\n",
    "plt.ylabel('Y', fontsize=19)\n",
    "plt.xlabel('ds', fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declare Model and Data Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "model                                             n-hits\n",
      "mode                                              simple\n",
      "activation                                          ReLU\n",
      "n_time_in                                            192\n",
      "n_time_out                                            96\n",
      "n_x_hidden                                             8\n",
      "n_s_hidden                                             0\n",
      "stack_types               [identity, identity, identity]\n",
      "constant_n_blocks                                      1\n",
      "constant_n_layers                                      2\n",
      "constant_n_mlp_units                                 256\n",
      "n_pool_kernel_size                             [4, 2, 1]\n",
      "n_freq_downsample                            [24, 12, 1]\n",
      "pooling_mode                                         max\n",
      "interpolation_mode                                linear\n",
      "shared_weights                                     False\n",
      "initialization                              lecun_normal\n",
      "learning_rate                                      0.001\n",
      "batch_size                                             1\n",
      "n_windows                                             32\n",
      "lr_decay                                             0.5\n",
      "lr_decay_step_size                                     2\n",
      "max_epochs                                             1\n",
      "max_steps                                           None\n",
      "early_stop_patience                                   20\n",
      "eval_freq                                            500\n",
      "batch_normalization                                False\n",
      "dropout_prob_theta                                   0.0\n",
      "dropout_prob_exogenous                               0.0\n",
      "weight_decay                                           0\n",
      "loss_train                                           MAE\n",
      "loss_hypar                                           0.5\n",
      "loss_valid                                           MAE\n",
      "random_seed                                            1\n",
      "idx_to_sample_freq                                     1\n",
      "val_idx_to_sample_freq                                 1\n",
      "n_val_weeks                                           52\n",
      "normalizer_y                                        None\n",
      "normalizer_x                                        None\n",
      "complete_windows                                   False\n",
      "frequency                                              H\n",
      "dtype: object\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "# Architecture parameters\n",
    "mc = {}\n",
    "mc['model'] = 'n-hits'\n",
    "mc['mode'] = 'simple'\n",
    "mc['activation'] = 'ReLU'\n",
    "\n",
    "mc['n_time_in'] = 96*2\n",
    "mc['n_time_out'] = 96\n",
    "mc['n_x_hidden'] = 8\n",
    "mc['n_s_hidden'] = 0\n",
    "\n",
    "mc['stack_types'] = ['identity', 'identity', 'identity']\n",
    "mc['constant_n_blocks'] = 1\n",
    "mc['constant_n_layers'] = 2\n",
    "mc['constant_n_mlp_units'] = 256\n",
    "mc['n_pool_kernel_size'] = [4, 2, 1]\n",
    "mc['n_freq_downsample'] = [24, 12, 1]\n",
    "mc['pooling_mode'] = 'max'\n",
    "mc['interpolation_mode'] = 'linear'\n",
    "mc['shared_weights'] = False\n",
    "\n",
    "# Optimization and regularization parameters\n",
    "mc['initialization'] = 'lecun_normal'\n",
    "mc['learning_rate'] = 0.001\n",
    "mc['batch_size'] = 1\n",
    "mc['n_windows'] = 32\n",
    "mc['lr_decay'] = 0.5\n",
    "mc['lr_decay_step_size'] = 2\n",
    "mc['max_epochs'] = 1\n",
    "mc['max_steps'] = None\n",
    "mc['early_stop_patience'] = 20\n",
    "mc['eval_freq'] = 500\n",
    "mc['batch_normalization'] = False\n",
    "mc['dropout_prob_theta'] = 0.0\n",
    "mc['dropout_prob_exogenous'] = 0.0\n",
    "mc['weight_decay'] = 0\n",
    "mc['loss_train'] = 'MAE'\n",
    "mc['loss_hypar'] = 0.5\n",
    "mc['loss_valid'] = mc['loss_train']\n",
    "mc['random_seed'] = 1\n",
    "\n",
    "# Data Parameters\n",
    "mc['idx_to_sample_freq'] = 1\n",
    "mc['val_idx_to_sample_freq'] = 1\n",
    "mc['n_val_weeks'] = 52\n",
    "mc['normalizer_y'] = None\n",
    "mc['normalizer_x'] = None\n",
    "mc['complete_windows'] = False\n",
    "mc['frequency'] = 'H'\n",
    "\n",
    "print(65*'=')\n",
    "print(pd.Series(mc))\n",
    "print(65*'=')\n",
    "\n",
    "mc['n_mlp_units'] = len(mc['stack_types']) * [ mc['constant_n_layers'] * [int(mc['constant_n_mlp_units'])] ]\n",
    "mc['n_blocks'] =  len(mc['stack_types']) * [ mc['constant_n_blocks'] ]\n",
    "mc['n_layers'] =  len(mc['stack_types']) * [ mc['constant_n_layers'] ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate Loaders and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralforecast.experiments.utils import create_datasets\n",
    "\n",
    "train_dataset, val_dataset, test_dataset, scaler_y = create_datasets(mc=mc,\n",
    "                                                                     S_df=None, Y_df=Y_df, X_df=None,\n",
    "                                                                     f_cols=[],\n",
    "                                                                     ds_in_val=11520,\n",
    "                                                                     ds_in_test=11520)\n",
    "\n",
    "train_loader = TimeSeriesLoader(dataset=train_dataset,\n",
    "                                batch_size=int(mc['batch_size']),\n",
    "                                n_windows=mc['n_windows'],\n",
    "                                shuffle=True)\n",
    "\n",
    "val_loader = TimeSeriesLoader(dataset=val_dataset,\n",
    "                              batch_size=int(mc['batch_size']),\n",
    "                              shuffle=False)\n",
    "\n",
    "test_loader = TimeSeriesLoader(dataset=test_dataset,\n",
    "                               batch_size=int(mc['batch_size']),\n",
    "                               shuffle=False)\n",
    "\n",
    "mc['n_x'], mc['n_s'] = train_dataset.get_n_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NHITS(n_time_in=int(mc['n_time_in']),\n",
    "              n_time_out=int(mc['n_time_out']),\n",
    "              n_x=mc['n_x'],\n",
    "              n_s=mc['n_s'],\n",
    "              n_s_hidden=int(mc['n_s_hidden']),\n",
    "              n_x_hidden=int(mc['n_x_hidden']),\n",
    "              shared_weights=mc['shared_weights'],\n",
    "              initialization=mc['initialization'],\n",
    "              activation=mc['activation'],\n",
    "              stack_types=mc['stack_types'],\n",
    "              n_blocks=mc['n_blocks'],\n",
    "              n_layers=mc['n_layers'],\n",
    "              n_mlp_units=mc['n_mlp_units'],\n",
    "              n_pool_kernel_size=mc['n_pool_kernel_size'],\n",
    "              n_freq_downsample=mc['n_freq_downsample'],\n",
    "              pooling_mode=mc['pooling_mode'],\n",
    "              interpolation_mode=mc['interpolation_mode'],\n",
    "              batch_normalization = mc['batch_normalization'],\n",
    "              dropout_prob_theta=mc['dropout_prob_theta'],\n",
    "              learning_rate=float(mc['learning_rate']),\n",
    "              lr_decay=float(mc['lr_decay']),\n",
    "              lr_decay_step_size=float(mc['lr_decay_step_size']),\n",
    "              weight_decay=mc['weight_decay'],\n",
    "              loss_train=mc['loss_train'],\n",
    "              loss_hypar=float(mc['loss_hypar']),\n",
    "              loss_valid=mc['loss_valid'],\n",
    "              frequency=mc['frequency'],\n",
    "              random_seed=int(mc['random_seed']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cchallu/opt/anaconda3/envs/neuralforecast/lib/python3.7/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:49: LightningDeprecationWarning: Setting `max_steps = None` is deprecated in v1.5 and will no longer be supported in v1.7. Use `max_steps = -1` instead.\n",
      "  \"Setting `max_steps = None` is deprecated in v1.5 and will no longer be supported in v1.7.\"\n",
      "/Users/cchallu/opt/anaconda3/envs/neuralforecast/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:91: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=10)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "  f\"Setting `Trainer(progress_bar_refresh_rate={progress_bar_refresh_rate})` is deprecated in v1.5 and\"\n",
      "\n",
      "  | Name  | Type   | Params\n",
      "---------------------------------\n",
      "0 | model | _NHITS | 459 K \n",
      "---------------------------------\n",
      "459 K     Trainable params\n",
      "0         Non-trainable params\n",
      "459 K     Total params\n",
      "1.840     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cchallu/opt/anaconda3/envs/neuralforecast/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "/Users/cchallu/opt/anaconda3/envs/neuralforecast/lib/python3.7/site-packages/torch/nn/functional.py:3635: UserWarning: Default upsampling behavior when mode=linear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cchallu/opt/anaconda3/envs/neuralforecast/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "/Users/cchallu/opt/anaconda3/envs/neuralforecast/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:433: UserWarning: The number of training samples (2) is smaller than the logging interval Trainer(log_every_n_steps=500). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 4/4 [00:00<00:00,  4.86it/s, loss=4.8e+03, v_num=323, train_loss_step=3.13e+3, val_loss=1.02e+4, train_loss_epoch=4.8e+3]\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", \n",
    "                               min_delta=1e-4, \n",
    "                               patience=mc['early_stop_patience'],\n",
    "                               verbose=False,\n",
    "                               mode=\"min\")\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=mc['max_epochs'], \n",
    "                     max_steps=mc['max_steps'],\n",
    "                     gradient_clip_val=1.0,\n",
    "                     progress_bar_refresh_rate=10, \n",
    "                     log_every_n_steps=500, \n",
    "                     check_val_every_n_epoch=1,\n",
    "                     callbacks=[early_stopping])\n",
    "\n",
    "trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cchallu/opt/anaconda3/envs/neuralforecast/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, predict_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "outputs[0][0].shape torch.Size([11425, 96])\n",
      "outputs[0][1].shape torch.Size([11425, 96])\n",
      "outputs[0][2].shape torch.Size([11425, 96])\n"
     ]
    }
   ],
   "source": [
    "model.return_decomposition = False\n",
    "outputs = trainer.predict(model, val_loader)\n",
    "\n",
    "print(\"outputs[0][0].shape\", outputs[0][0].shape)\n",
    "print(\"outputs[0][1].shape\", outputs[0][1].shape)\n",
    "print(\"outputs[0][2].shape\", outputs[0][2].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>73779</th>\n",
       "      <td>OT</td>\n",
       "      <td>2016-12-26 22:45:00</td>\n",
       "      <td>73779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73780</th>\n",
       "      <td>OT</td>\n",
       "      <td>2016-12-26 23:00:00</td>\n",
       "      <td>73780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73781</th>\n",
       "      <td>OT</td>\n",
       "      <td>2016-12-26 23:15:00</td>\n",
       "      <td>73781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73782</th>\n",
       "      <td>OT</td>\n",
       "      <td>2016-12-26 23:30:00</td>\n",
       "      <td>73782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73783</th>\n",
       "      <td>OT</td>\n",
       "      <td>2016-12-26 23:45:00</td>\n",
       "      <td>73783</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      unique_id                  ds      y\n",
       "73779        OT 2016-12-26 22:45:00  73779\n",
       "73780        OT 2016-12-26 23:00:00  73780\n",
       "73781        OT 2016-12-26 23:15:00  73781\n",
       "73782        OT 2016-12-26 23:30:00  73782\n",
       "73783        OT 2016-12-26 23:45:00  73783"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_forecast_df = Y_df[Y_df['ds']<'2016-12-27']\n",
    "Y_forecast_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_forecast_df = X_df[X_df['ds']<'2016-12-28']\n",
    "# X_forecast_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Train Validation splits\n",
      "\n",
      "INFO:root:                                       ds                    \n",
      "                                      min                 max\n",
      "unique_id sample_mask                                        \n",
      "HUFL      0           2016-07-11 10:00:00 2016-12-26 23:45:00\n",
      "          1           2016-12-27 00:45:00 2016-12-30 23:45:00\n",
      "OT        0           2016-07-01 00:00:00 2016-12-26 23:45:00\n",
      "          1           2016-12-27 00:45:00 2016-12-30 23:45:00\n",
      "INFO:root:\n",
      "Total data \t\t\t33560 time stamps \n",
      "Available percentage=100.0, \t33560 time stamps \n",
      "Insample  percentage=0.57, \t192 time stamps \n",
      "Outsample percentage=99.43, \t33368 time stamps \n",
      "\n",
      "/Users/cchallu/opt/anaconda3/envs/neuralforecast/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:91: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=1)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "  f\"Setting `Trainer(progress_bar_refresh_rate={progress_bar_refresh_rate})` is deprecated in v1.5 and\"\n",
      "/Users/cchallu/opt/anaconda3/envs/neuralforecast/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, predict_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 1/1 [00:00<00:00, 35.44it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cchallu/opt/anaconda3/envs/neuralforecast/lib/python3.7/site-packages/torch/nn/functional.py:3635: UserWarning: Default upsampling behavior when mode=linear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode)\n"
     ]
    }
   ],
   "source": [
    "model.return_decomposition = False\n",
    "forecast_df = model.forecast(Y_df=Y_forecast_df, X_df=None, S_df=None, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HUFL</td>\n",
       "      <td>2016-12-27 00:45:00</td>\n",
       "      <td>12993.560547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HUFL</td>\n",
       "      <td>2016-12-27 01:45:00</td>\n",
       "      <td>13086.474609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HUFL</td>\n",
       "      <td>2016-12-27 02:45:00</td>\n",
       "      <td>12819.617188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HUFL</td>\n",
       "      <td>2016-12-27 03:45:00</td>\n",
       "      <td>13744.444336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HUFL</td>\n",
       "      <td>2016-12-27 04:45:00</td>\n",
       "      <td>13641.042969</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  unique_id                  ds             y\n",
       "0      HUFL 2016-12-27 00:45:00  12993.560547\n",
       "1      HUFL 2016-12-27 01:45:00  13086.474609\n",
       "2      HUFL 2016-12-27 02:45:00  12819.617188\n",
       "3      HUFL 2016-12-27 03:45:00  13744.444336\n",
       "4      HUFL 2016-12-27 04:45:00  13641.042969"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Train Validation splits\n",
      "\n",
      "INFO:root:                                       ds                    \n",
      "                                      min                 max\n",
      "unique_id sample_mask                                        \n",
      "HUFL      1           2016-07-11 10:00:00 2018-02-20 23:45:00\n",
      "OT        1           2016-07-01 00:00:00 2018-02-20 23:45:00\n",
      "INFO:root:\n",
      "Total data \t\t\t114200 time stamps \n",
      "Available percentage=100.0, \t114200 time stamps \n",
      "Insample  percentage=100.0, \t114200 time stamps \n",
      "Outsample percentage=0.0, \t0 time stamps \n",
      "\n",
      "/Users/cchallu/opt/anaconda3/envs/neuralforecast/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:91: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=1)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "  f\"Setting `Trainer(progress_bar_refresh_rate={progress_bar_refresh_rate})` is deprecated in v1.5 and\"\n",
      "/Users/cchallu/opt/anaconda3/envs/neuralforecast/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, predict_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cchallu/opt/anaconda3/envs/neuralforecast/lib/python3.7/site-packages/torch/nn/functional.py:3635: UserWarning: Default upsampling behavior when mode=linear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 1/1 [00:02<00:00,  2.12s/it]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cchallu/opt/anaconda3/envs/neuralforecast/lib/python3.7/site-packages/ipykernel_launcher.py:91: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n"
     ]
    }
   ],
   "source": [
    "Y_hat_df = model.predict(Y_df=Y_df, X_df=None, S_df=None, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>forecast_creation_ds</th>\n",
       "      <th>ds</th>\n",
       "      <th>y</th>\n",
       "      <th>y_hat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HUFL</td>\n",
       "      <td>2016-07-11 09:45:00</td>\n",
       "      <td>2016-07-11 10:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.088515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HUFL</td>\n",
       "      <td>2016-07-11 09:45:00</td>\n",
       "      <td>2016-07-11 10:15:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.093949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HUFL</td>\n",
       "      <td>2016-07-11 09:45:00</td>\n",
       "      <td>2016-07-11 10:30:00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-0.056966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HUFL</td>\n",
       "      <td>2016-07-11 09:45:00</td>\n",
       "      <td>2016-07-11 10:45:00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-0.051535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HUFL</td>\n",
       "      <td>2016-07-11 09:45:00</td>\n",
       "      <td>2016-07-11 11:00:00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-0.032547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10944955</th>\n",
       "      <td>OT</td>\n",
       "      <td>2018-02-19 23:45:00</td>\n",
       "      <td>2018-02-20 22:45:00</td>\n",
       "      <td>114195.0</td>\n",
       "      <td>132567.171875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10944956</th>\n",
       "      <td>OT</td>\n",
       "      <td>2018-02-19 23:45:00</td>\n",
       "      <td>2018-02-20 23:00:00</td>\n",
       "      <td>114196.0</td>\n",
       "      <td>140063.437500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10944957</th>\n",
       "      <td>OT</td>\n",
       "      <td>2018-02-19 23:45:00</td>\n",
       "      <td>2018-02-20 23:15:00</td>\n",
       "      <td>114197.0</td>\n",
       "      <td>147368.515625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10944958</th>\n",
       "      <td>OT</td>\n",
       "      <td>2018-02-19 23:45:00</td>\n",
       "      <td>2018-02-20 23:30:00</td>\n",
       "      <td>114198.0</td>\n",
       "      <td>147895.359375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10944959</th>\n",
       "      <td>OT</td>\n",
       "      <td>2018-02-19 23:45:00</td>\n",
       "      <td>2018-02-20 23:45:00</td>\n",
       "      <td>114199.0</td>\n",
       "      <td>147214.468750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10944960 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         unique_id forecast_creation_ds                  ds         y  \\\n",
       "0             HUFL  2016-07-11 09:45:00 2016-07-11 10:00:00       0.0   \n",
       "1             HUFL  2016-07-11 09:45:00 2016-07-11 10:15:00       1.0   \n",
       "2             HUFL  2016-07-11 09:45:00 2016-07-11 10:30:00       2.0   \n",
       "3             HUFL  2016-07-11 09:45:00 2016-07-11 10:45:00       3.0   \n",
       "4             HUFL  2016-07-11 09:45:00 2016-07-11 11:00:00       4.0   \n",
       "...            ...                  ...                 ...       ...   \n",
       "10944955        OT  2018-02-19 23:45:00 2018-02-20 22:45:00  114195.0   \n",
       "10944956        OT  2018-02-19 23:45:00 2018-02-20 23:00:00  114196.0   \n",
       "10944957        OT  2018-02-19 23:45:00 2018-02-20 23:15:00  114197.0   \n",
       "10944958        OT  2018-02-19 23:45:00 2018-02-20 23:30:00  114198.0   \n",
       "10944959        OT  2018-02-19 23:45:00 2018-02-20 23:45:00  114199.0   \n",
       "\n",
       "                  y_hat  \n",
       "0             -0.088515  \n",
       "1             -0.093949  \n",
       "2             -0.056966  \n",
       "3             -0.051535  \n",
       "4             -0.032547  \n",
       "...                 ...  \n",
       "10944955  132567.171875  \n",
       "10944956  140063.437500  \n",
       "10944957  147368.515625  \n",
       "10944958  147895.359375  \n",
       "10944959  147214.468750  \n",
       "\n",
       "[10944960 rows x 5 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_hat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>forecast_creation_ds</th>\n",
       "      <th>ds</th>\n",
       "      <th>y_x</th>\n",
       "      <th>y_hat</th>\n",
       "      <th>y_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HUFL</td>\n",
       "      <td>2016-07-11 09:45:00</td>\n",
       "      <td>2016-07-11 10:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.088515</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HUFL</td>\n",
       "      <td>2016-07-11 09:45:00</td>\n",
       "      <td>2016-07-11 10:15:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.093949</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HUFL</td>\n",
       "      <td>2016-07-11 10:00:00</td>\n",
       "      <td>2016-07-11 10:15:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.087869</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HUFL</td>\n",
       "      <td>2016-07-11 09:45:00</td>\n",
       "      <td>2016-07-11 10:30:00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-0.056966</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HUFL</td>\n",
       "      <td>2016-07-11 10:00:00</td>\n",
       "      <td>2016-07-11 10:30:00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-0.093476</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10944955</th>\n",
       "      <td>OT</td>\n",
       "      <td>2018-02-19 23:30:00</td>\n",
       "      <td>2018-02-20 23:15:00</td>\n",
       "      <td>114197.0</td>\n",
       "      <td>147894.062500</td>\n",
       "      <td>114197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10944956</th>\n",
       "      <td>OT</td>\n",
       "      <td>2018-02-19 23:45:00</td>\n",
       "      <td>2018-02-20 23:15:00</td>\n",
       "      <td>114197.0</td>\n",
       "      <td>147368.515625</td>\n",
       "      <td>114197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10944957</th>\n",
       "      <td>OT</td>\n",
       "      <td>2018-02-19 23:30:00</td>\n",
       "      <td>2018-02-20 23:30:00</td>\n",
       "      <td>114198.0</td>\n",
       "      <td>147213.171875</td>\n",
       "      <td>114198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10944958</th>\n",
       "      <td>OT</td>\n",
       "      <td>2018-02-19 23:45:00</td>\n",
       "      <td>2018-02-20 23:30:00</td>\n",
       "      <td>114198.0</td>\n",
       "      <td>147895.359375</td>\n",
       "      <td>114198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10944959</th>\n",
       "      <td>OT</td>\n",
       "      <td>2018-02-19 23:45:00</td>\n",
       "      <td>2018-02-20 23:45:00</td>\n",
       "      <td>114199.0</td>\n",
       "      <td>147214.468750</td>\n",
       "      <td>114199</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10944960 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         unique_id forecast_creation_ds                  ds       y_x  \\\n",
       "0             HUFL  2016-07-11 09:45:00 2016-07-11 10:00:00       0.0   \n",
       "1             HUFL  2016-07-11 09:45:00 2016-07-11 10:15:00       1.0   \n",
       "2             HUFL  2016-07-11 10:00:00 2016-07-11 10:15:00       1.0   \n",
       "3             HUFL  2016-07-11 09:45:00 2016-07-11 10:30:00       2.0   \n",
       "4             HUFL  2016-07-11 10:00:00 2016-07-11 10:30:00       2.0   \n",
       "...            ...                  ...                 ...       ...   \n",
       "10944955        OT  2018-02-19 23:30:00 2018-02-20 23:15:00  114197.0   \n",
       "10944956        OT  2018-02-19 23:45:00 2018-02-20 23:15:00  114197.0   \n",
       "10944957        OT  2018-02-19 23:30:00 2018-02-20 23:30:00  114198.0   \n",
       "10944958        OT  2018-02-19 23:45:00 2018-02-20 23:30:00  114198.0   \n",
       "10944959        OT  2018-02-19 23:45:00 2018-02-20 23:45:00  114199.0   \n",
       "\n",
       "                  y_hat     y_y  \n",
       "0             -0.088515       0  \n",
       "1             -0.093949       1  \n",
       "2             -0.087869       1  \n",
       "3             -0.056966       2  \n",
       "4             -0.093476       2  \n",
       "...                 ...     ...  \n",
       "10944955  147894.062500  114197  \n",
       "10944956  147368.515625  114197  \n",
       "10944957  147213.171875  114198  \n",
       "10944958  147895.359375  114198  \n",
       "10944959  147214.468750  114199  \n",
       "\n",
       "[10944960 rows x 6 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "debugging = Y_hat_df.merge(Y_df,on=['unique_id','ds'])\n",
    "debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert all(debugging['y_x'].astype(float) == debugging['y_y'].astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
