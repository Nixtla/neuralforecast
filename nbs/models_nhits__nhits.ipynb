{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp models.nhits.nhits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import math\n",
    "import random\n",
    "from functools import partial\n",
    "from typing import Tuple, List\n",
    "from fastcore.foundation import patch\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from neuralforecast.models.components.tcn import _TemporalConvNet\n",
    "from neuralforecast.models.components.common import Chomp1d, RepeatVector\n",
    "from neuralforecast.losses.utils import LossFunction\n",
    "from neuralforecast.data.tsdataset import WindowsDataset\n",
    "from neuralforecast.data.tsloader import TimeSeriesLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class _StaticFeaturesEncoder(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(_StaticFeaturesEncoder, self).__init__()\n",
    "        layers = [nn.Dropout(p=0.5),\n",
    "                  nn.Linear(in_features=in_features, out_features=out_features),\n",
    "                  nn.ReLU()]\n",
    "        self.encoder = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        return x\n",
    "\n",
    "class _sEncoder(nn.Module):\n",
    "    def __init__(self, in_features, out_features, n_time_in):\n",
    "        super(_sEncoder, self).__init__()\n",
    "        layers = [nn.Dropout(p=0.5),\n",
    "                  nn.Linear(in_features=in_features, out_features=out_features),\n",
    "                  nn.ReLU()]\n",
    "        self.encoder = nn.Sequential(*layers)\n",
    "        self.repeat = RepeatVector(repeats=n_time_in)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Encode and repeat values to match time\n",
    "        x = self.encoder(x)\n",
    "        x = self.repeat(x) # [N,S_out] -> [N,S_out,T]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class _IdentityBasis(nn.Module):\n",
    "    def __init__(self, backcast_size: int, forecast_size: int, interpolation_mode: str):\n",
    "        super().__init__()\n",
    "        assert (interpolation_mode in ['linear','nearest']) or ('cubic' in interpolation_mode)\n",
    "        self.forecast_size = forecast_size\n",
    "        self.backcast_size = backcast_size\n",
    "        self.interpolation_mode = interpolation_mode\n",
    " \n",
    "    def forward(self, theta: t.Tensor, insample_x_t: t.Tensor, outsample_x_t: t.Tensor) -> Tuple[t.Tensor, t.Tensor]:\n",
    "\n",
    "        backcast = theta[:, :self.backcast_size]\n",
    "        knots = theta[:, self.backcast_size:]\n",
    "\n",
    "        if self.interpolation_mode=='nearest':\n",
    "            knots = knots[:,None,:]\n",
    "            forecast = F.interpolate(knots, size=self.forecast_size, mode=self.interpolation_mode)\n",
    "            forecast = forecast[:,0,:]\n",
    "        elif self.interpolation_mode=='linear':\n",
    "            knots = knots[:,None,:]\n",
    "            forecast = F.interpolate(knots, size=self.forecast_size, mode=self.interpolation_mode)\n",
    "            forecast = forecast[:,0,:]\n",
    "        elif 'cubic' in self.interpolation_mode:\n",
    "            batch_size = len(backcast)\n",
    "            knots = knots[:,None,None,:]\n",
    "            forecast = t.zeros((len(knots), self.forecast_size)).to(knots.device)\n",
    "            n_batches = int(np.ceil(len(knots)/batch_size))\n",
    "            for i in range(n_batches):\n",
    "                forecast_i = F.interpolate(knots[i*batch_size:(i+1)*batch_size], size=self.forecast_size, mode='bicubic')\n",
    "                forecast[i*batch_size:(i+1)*batch_size] += forecast_i[:,0,0,:]\n",
    "\n",
    "        return backcast, forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class _ExogenousBasisInterpretable(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, theta: t.Tensor, insample_x_t: t.Tensor, outsample_x_t: t.Tensor) -> Tuple[t.Tensor, t.Tensor]:\n",
    "        backcast_basis = insample_x_t\n",
    "        forecast_basis = outsample_x_t\n",
    "\n",
    "        cut_point = forecast_basis.shape[1]\n",
    "        backcast = t.einsum('bp,bpt->bt', theta[:, cut_point:], backcast_basis)\n",
    "        forecast = t.einsum('bp,bpt->bt', theta[:, :cut_point], forecast_basis)\n",
    "        return backcast, forecast\n",
    "\n",
    "class _ExogenousBasisWavenet(nn.Module):\n",
    "    def __init__(self, out_features, in_features, num_levels=4, kernel_size=3, dropout_prob=0):\n",
    "        super().__init__()\n",
    "        # Shape of (1, in_features, 1) to broadcast over b and t\n",
    "        self.weight = nn.Parameter(t.Tensor(1, in_features, 1), requires_grad=True)\n",
    "        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(0.5))\n",
    "\n",
    "        padding = (kernel_size - 1) * (2**0)\n",
    "        input_layer = [nn.Conv1d(in_channels=in_features, out_channels=out_features,\n",
    "                                 kernel_size=kernel_size, padding=padding, dilation=2**0),\n",
    "                                 Chomp1d(padding),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Dropout(dropout_prob)]\n",
    "        conv_layers = []\n",
    "        for i in range(1, num_levels):\n",
    "            dilation = 2**i\n",
    "            padding = (kernel_size - 1) * dilation\n",
    "            conv_layers.append(nn.Conv1d(in_channels=out_features, out_channels=out_features,\n",
    "                                         padding=padding, kernel_size=3, dilation=dilation))\n",
    "            conv_layers.append(Chomp1d(padding))\n",
    "            conv_layers.append(nn.ReLU())\n",
    "        conv_layers = input_layer + conv_layers\n",
    "\n",
    "        self.wavenet = nn.Sequential(*conv_layers)\n",
    "        \n",
    "    def transform(self, insample_x_t, outsample_x_t):\n",
    "        n_time_in = insample_x_t.shape[2]\n",
    "        \n",
    "        x_t = t.cat([insample_x_t, outsample_x_t], dim=2)\n",
    "        \n",
    "        x_t = x_t * self.weight # Element-wise multiplication, broadcasted on b and t. Weights used in L1 regularization\n",
    "        x_t = self.wavenet(x_t)[:]\n",
    "\n",
    "        backcast_basis = x_t[:,:, :n_time_in]\n",
    "        forecast_basis = x_t[:,:, n_time_in:]\n",
    "\n",
    "        return backcast_basis, forecast_basis\n",
    "\n",
    "    def forward(self, theta: t.Tensor, insample_x_t: t.Tensor, outsample_x_t: t.Tensor) -> Tuple[t.Tensor, t.Tensor]:\n",
    "        backcast_basis, forecast_basis = self.transform(insample_x_t, outsample_x_t)\n",
    "\n",
    "        cut_point = forecast_basis.shape[1]\n",
    "        backcast = t.einsum('bp,bpt->bt', theta[:, cut_point:], backcast_basis)\n",
    "        forecast = t.einsum('bp,bpt->bt', theta[:, :cut_point], forecast_basis)\n",
    "        return backcast, forecast\n",
    "\n",
    "class _ExogenousBasisTCN(nn.Module):\n",
    "    def __init__(self, out_features, in_features, num_levels = 4, kernel_size=2, dropout_prob=0):\n",
    "        super().__init__()\n",
    "        n_channels = num_levels * [out_features]\n",
    "        self.tcn = _TemporalConvNet(num_inputs=in_features, num_channels=n_channels, kernel_size=kernel_size, dropout=dropout_prob)\n",
    "        \n",
    "    def transform(self, insample_x_t, outsample_x_t):\n",
    "        n_time_in = insample_x_t.shape[2]\n",
    "        \n",
    "        x_t = t.cat([insample_x_t, outsample_x_t], dim=2)\n",
    "        \n",
    "        x_t = self.tcn(x_t)[:]\n",
    "        backcast_basis = x_t[:,:, :n_time_in]\n",
    "        forecast_basis = x_t[:,:, n_time_in:]\n",
    "\n",
    "        return backcast_basis, forecast_basis\n",
    "\n",
    "    def forward(self, theta: t.Tensor, insample_x_t: t.Tensor, outsample_x_t: t.Tensor) -> Tuple[t.Tensor, t.Tensor]:\n",
    "        backcast_basis, forecast_basis = self.transform(insample_x_t, outsample_x_t)\n",
    "\n",
    "        cut_point = forecast_basis.shape[1]\n",
    "        backcast = t.einsum('bp,bpt->bt', theta[:, cut_point:], backcast_basis)\n",
    "        forecast = t.einsum('bp,bpt->bt', theta[:, :cut_point], forecast_basis)\n",
    "        return backcast, forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _init_weights(module, initialization):\n",
    "    if type(module) == t.nn.Linear:\n",
    "        if initialization == 'orthogonal':\n",
    "            t.nn.init.orthogonal_(module.weight)\n",
    "        elif initialization == 'he_uniform':\n",
    "            t.nn.init.kaiming_uniform_(module.weight)\n",
    "        elif initialization == 'he_normal':\n",
    "            t.nn.init.kaiming_normal_(module.weight)\n",
    "        elif initialization == 'glorot_uniform':\n",
    "            t.nn.init.xavier_uniform_(module.weight)\n",
    "        elif initialization == 'glorot_normal':\n",
    "            t.nn.init.xavier_normal_(module.weight)\n",
    "        elif initialization == 'lecun_normal':\n",
    "            pass #t.nn.init.normal_(module.weight, 0.0, std=1/np.sqrt(module.weight.numel()))\n",
    "        else:\n",
    "            assert 1<0, f'Initialization {initialization} not found'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "ACTIVATIONS = ['ReLU',\n",
    "               'Softplus',\n",
    "               'Tanh',\n",
    "               'SELU',\n",
    "               'LeakyReLU',\n",
    "               'PReLU',\n",
    "               'Sigmoid']\n",
    "\n",
    "class _NHITSBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    N-BEATS block which takes a basis function as an argument.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_time_in: int, n_time_out: int, n_x: int,\n",
    "                 n_s: int, n_s_hidden: int, n_theta: int, n_mlp_units: list,\n",
    "                 n_pool_kernel_size: int, pooling_mode: str, basis: nn.Module,\n",
    "                 n_layers: int,  batch_normalization: bool, dropout_prob: float, activation: str):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        assert (pooling_mode in ['max','average'])\n",
    "\n",
    "        n_time_in_pooled = int(np.ceil(n_time_in/n_pool_kernel_size))\n",
    "\n",
    "        if n_s == 0:\n",
    "            n_s_hidden = 0\n",
    "        n_mlp_units = [n_time_in_pooled + (n_time_in+n_time_out)*n_x + n_s_hidden] + n_mlp_units\n",
    "        \n",
    "        self.n_time_in = n_time_in\n",
    "        self.n_time_out = n_time_out\n",
    "        self.n_s = n_s\n",
    "        self.n_s_hidden = n_s_hidden\n",
    "        self.n_x = n_x\n",
    "        self.n_pool_kernel_size = n_pool_kernel_size\n",
    "        self.batch_normalization = batch_normalization\n",
    "        self.dropout_prob = dropout_prob\n",
    "        \n",
    "        assert activation in ACTIVATIONS, f'{activation} is not in {ACTIVATIONS}'\n",
    "        activ = getattr(nn, activation)()\n",
    "\n",
    "        if pooling_mode == 'max':\n",
    "            self.pooling_layer = nn.MaxPool1d(kernel_size=self.n_pool_kernel_size,\n",
    "                                              stride=self.n_pool_kernel_size, ceil_mode=True)\n",
    "        elif pooling_mode == 'average':\n",
    "            self.pooling_layer = nn.AvgPool1d(kernel_size=self.n_pool_kernel_size,\n",
    "                                              stride=self.n_pool_kernel_size, ceil_mode=True)\n",
    "\n",
    "        hidden_layers = []\n",
    "        for i in range(n_layers):\n",
    "            hidden_layers.append(nn.Linear(in_features=n_mlp_units[i], out_features=n_mlp_units[i+1]))\n",
    "            hidden_layers.append(activ)\n",
    "\n",
    "            if self.batch_normalization:\n",
    "                hidden_layers.append(nn.BatchNorm1d(num_features=n_mlp_units[i+1]))\n",
    "\n",
    "            if self.dropout_prob>0:\n",
    "                hidden_layers.append(nn.Dropout(p=self.dropout_prob))\n",
    "\n",
    "        output_layer = [nn.Linear(in_features=n_mlp_units[-1], out_features=n_theta)]\n",
    "        layers = hidden_layers + output_layer\n",
    "\n",
    "        # n_s is computed with data, n_s_hidden is provided by user, if 0 no statics are used\n",
    "        if (self.n_s > 0) and (self.n_s_hidden > 0):\n",
    "            self.static_encoder = _StaticFeaturesEncoder(in_features=n_s, out_features=n_s_hidden)\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        self.basis = basis\n",
    "\n",
    "    def forward(self, insample_y: t.Tensor, insample_x_t: t.Tensor,\n",
    "                outsample_x_t: t.Tensor, x_s: t.Tensor) -> Tuple[t.Tensor, t.Tensor]:\n",
    "\n",
    "        insample_y = insample_y.unsqueeze(1)\n",
    "        insample_y = self.pooling_layer(insample_y)\n",
    "        insample_y = insample_y.squeeze(1)\n",
    "\n",
    "        batch_size = len(insample_y)\n",
    "        if self.n_x > 0:\n",
    "            insample_y = t.cat(( insample_y, insample_x_t.reshape(batch_size, -1) ), 1)\n",
    "            insample_y = t.cat(( insample_y, outsample_x_t.reshape(batch_size, -1) ), 1)\n",
    "        \n",
    "        # Static exogenous\n",
    "        if (self.n_s > 0) and (self.n_s_hidden > 0):\n",
    "            x_s = self.static_encoder(x_s)\n",
    "            insample_y = t.cat((insample_y, x_s), 1)\n",
    "\n",
    "        # Compute local projection weights and projection\n",
    "        theta = self.layers(insample_y)\n",
    "        backcast, forecast = self.basis(theta, insample_x_t, outsample_x_t)\n",
    "\n",
    "        return backcast, forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class _NHITS(nn.Module):\n",
    "    \"\"\"\n",
    "    NHITS Model.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 n_time_in,\n",
    "                 n_time_out,\n",
    "                 n_s,\n",
    "                 n_x,\n",
    "                 n_s_hidden,\n",
    "                 n_x_hidden,\n",
    "                 stack_types: list,\n",
    "                 n_blocks: list,\n",
    "                 n_layers: list,\n",
    "                 n_mlp_units: list,\n",
    "                 n_pool_kernel_size: list,\n",
    "                 n_freq_downsample: list,\n",
    "                 pooling_mode,\n",
    "                 interpolation_mode,\n",
    "                 dropout_prob_theta,\n",
    "                 activation,\n",
    "                 initialization,\n",
    "                 batch_normalization,\n",
    "                 shared_weights):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_time_out = n_time_out\n",
    "\n",
    "        blocks = self.create_stack(stack_types=stack_types, \n",
    "                                   n_blocks=n_blocks,\n",
    "                                   n_time_in=n_time_in,\n",
    "                                   n_time_out=n_time_out,\n",
    "                                   n_x=n_x,\n",
    "                                   n_x_hidden=n_x_hidden,\n",
    "                                   n_s=n_s,\n",
    "                                   n_s_hidden=n_s_hidden,\n",
    "                                   n_layers=n_layers,\n",
    "                                   n_mlp_units=n_mlp_units,\n",
    "                                   n_pool_kernel_size=n_pool_kernel_size,\n",
    "                                   n_freq_downsample=n_freq_downsample,\n",
    "                                   pooling_mode=pooling_mode,\n",
    "                                   interpolation_mode=interpolation_mode,\n",
    "                                   batch_normalization=batch_normalization,\n",
    "                                   dropout_prob_theta=dropout_prob_theta,\n",
    "                                   activation=activation,\n",
    "                                   shared_weights=shared_weights,\n",
    "                                   initialization=initialization)\n",
    "        self.blocks = t.nn.ModuleList(blocks)\n",
    "\n",
    "    def create_stack(self, stack_types, n_blocks, \n",
    "                     n_time_in, n_time_out, \n",
    "                     n_x, n_x_hidden, n_s, n_s_hidden, \n",
    "                     n_layers, n_mlp_units, \n",
    "                     n_pool_kernel_size, n_freq_downsample, pooling_mode, interpolation_mode,\n",
    "                     batch_normalization, dropout_prob_theta, \n",
    "                     activation, shared_weights, initialization):                     \n",
    "        block_list = []\n",
    "        for i in range(len(stack_types)):\n",
    "            assert stack_types[i] in ['identity', 'exogenous', 'exogenous_tcn', 'exogenous_wavenet'], 'f Invalid stack type {stack_types[i]}'\n",
    "            for block_id in range(n_blocks[i]):\n",
    "                \n",
    "                # Batch norm only on first block\n",
    "                if (len(block_list)==0) and (batch_normalization):\n",
    "                    batch_normalization_block = True\n",
    "                else:\n",
    "                    batch_normalization_block = False\n",
    "\n",
    "                # Shared weights\n",
    "                if shared_weights and block_id>0:\n",
    "                    nbeats_block = block_list[-1]\n",
    "                else:\n",
    "                    if stack_types[i] == 'identity':\n",
    "                        n_theta = (n_time_in + max(n_time_out//n_freq_downsample[i], 1) )\n",
    "                        basis = _IdentityBasis(backcast_size=n_time_in,\n",
    "                                              forecast_size=n_time_out,\n",
    "                                              interpolation_mode=interpolation_mode)                        \n",
    "\n",
    "                    elif stack_types[i] == 'exogenous':\n",
    "                        n_theta = 2 * n_x\n",
    "                        basis = _ExogenousBasisInterpretable()\n",
    "\n",
    "                    elif stack_types[i] == 'exogenous_tcn':\n",
    "                        n_theta = 2 * n_x_hidden\n",
    "                        basis = _ExogenousBasisTCN(n_x_hidden, n_x)\n",
    "\n",
    "                    elif stack_types[i] == 'exogenous_wavenet':\n",
    "                        n_theta = 2 * n_x_hidden\n",
    "                        basis = _ExogenousBasisWavenet(n_x_hidden, n_x)\n",
    "\n",
    "                    nbeats_block = _NHITSBlock(n_time_in=n_time_in,\n",
    "                                               n_time_out=n_time_out,\n",
    "                                               n_x=n_x,\n",
    "                                               n_s=n_s,\n",
    "                                               n_s_hidden=n_s_hidden,\n",
    "                                               n_theta=n_theta,\n",
    "                                               n_mlp_units=n_mlp_units[i],\n",
    "                                               n_pool_kernel_size=n_pool_kernel_size[i],\n",
    "                                               pooling_mode=pooling_mode,\n",
    "                                               basis=basis,\n",
    "                                               n_layers=n_layers[i],\n",
    "                                               batch_normalization=batch_normalization_block,\n",
    "                                               dropout_prob=dropout_prob_theta,\n",
    "                                               activation=activation)\n",
    "\n",
    "                # Select type of evaluation and apply it to all layers of block\n",
    "                init_function = partial(_init_weights, initialization=initialization)                                             \n",
    "                nbeats_block.layers.apply(init_function)\n",
    "                block_list.append(nbeats_block)\n",
    "        return block_list\n",
    "\n",
    "    def forward(self, S: t.Tensor, Y: t.Tensor, X: t.Tensor, \n",
    "                insample_mask: t.Tensor, outsample_mask: t.Tensor,\n",
    "                return_decomposition: bool=False):\n",
    "        \n",
    "        # insample\n",
    "        insample_y    = Y[:, :-self.n_time_out]\n",
    "        insample_x_t  = X[:, :, :-self.n_time_out]\n",
    "        insample_mask = insample_mask[:, :-self.n_time_out]\n",
    "        \n",
    "        # outsample\n",
    "        outsample_y   = Y[:, -self.n_time_out:]\n",
    "        outsample_x_t = X[:, :, -self.n_time_out:]\n",
    "        outsample_mask = outsample_mask[:, -self.n_time_out:]\n",
    "\n",
    "        if return_decomposition:\n",
    "            forecast, block_forecasts = self.forecast_decomposition(insample_y=insample_y, \n",
    "                                                                    insample_x_t=insample_x_t, \n",
    "                                                                    insample_mask=insample_mask,\n",
    "                                                                    outsample_x_t=outsample_x_t,\n",
    "                                                                    x_s=S)\n",
    "            return outsample_y, forecast, block_forecasts, outsample_mask\n",
    "        \n",
    "        else:\n",
    "            forecast = self.forecast(insample_y=insample_y,\n",
    "                                     insample_x_t=insample_x_t, \n",
    "                                     insample_mask=insample_mask,\n",
    "                                     outsample_x_t=outsample_x_t,\n",
    "                                     x_s=S)\n",
    "            return outsample_y, forecast, outsample_mask\n",
    "\n",
    "    def forecast(self, insample_y: t.Tensor, insample_x_t: t.Tensor, insample_mask: t.Tensor,\n",
    "                 outsample_x_t: t.Tensor, x_s: t.Tensor):\n",
    "\n",
    "        residuals = insample_y.flip(dims=(-1,))\n",
    "        insample_x_t = insample_x_t.flip(dims=(-1,))\n",
    "        insample_mask = insample_mask.flip(dims=(-1,))\n",
    "\n",
    "        forecast = insample_y[:, -1:] # Level with Naive1\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            backcast, block_forecast = block(insample_y=residuals, insample_x_t=insample_x_t,\n",
    "                                             outsample_x_t=outsample_x_t, x_s=x_s)\n",
    "            residuals = (residuals - backcast) * insample_mask\n",
    "            forecast = forecast + block_forecast\n",
    "\n",
    "        return forecast\n",
    "\n",
    "    def forecast_decomposition(self, insample_y: t.Tensor, insample_x_t: t.Tensor, insample_mask: t.Tensor,\n",
    "                               outsample_x_t: t.Tensor, x_s: t.Tensor):\n",
    "\n",
    "        residuals = insample_y.flip(dims=(-1,))\n",
    "        insample_x_t = insample_x_t.flip(dims=(-1,))\n",
    "        insample_mask = insample_mask.flip(dims=(-1,))\n",
    "        \n",
    "        n_batch, n_channels, n_t = outsample_x_t.size(0), outsample_x_t.size(1), outsample_x_t.size(2)\n",
    "        \n",
    "        level = insample_y[:, -1:] # Level with Naive1\n",
    "        block_forecasts = [ level.repeat(1, n_t) ]\n",
    "                \n",
    "        forecast = level\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            backcast, block_forecast = block(insample_y=residuals, insample_x_t=insample_x_t,\n",
    "                                             outsample_x_t=outsample_x_t, x_s=x_s)\n",
    "            residuals = (residuals - backcast) * insample_mask\n",
    "            forecast = forecast + block_forecast\n",
    "            block_forecasts.append(block_forecast)\n",
    "            \n",
    "        # (n_batch, n_blocks, n_t)\n",
    "        block_forecasts = t.stack(block_forecasts)\n",
    "        block_forecasts = block_forecasts.permute(1,0,2)\n",
    "\n",
    "        return forecast, block_forecasts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-HiTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A new model for long-horizon forecasting which incorporates novel hierarchical interpolation and multi-rate data sampling techniques to specialize blocks of its architecture to different frequency band of the time-series signal. It achieves SoTA performance on several benchmark datasets, outperforming current Transformer-based models by more than 25%. Paper available at https://arxiv.org/abs/2201.12886"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class NHITS(pl.LightningModule):\n",
    "    def __init__(self,\n",
    "                 n_time_in: int,\n",
    "                 n_time_out: int,\n",
    "                 n_x: int,\n",
    "                 n_s: int,\n",
    "                 shared_weights: bool,\n",
    "                 activation: str,\n",
    "                 initialization: str,\n",
    "                 stack_types: List[str],\n",
    "                 n_blocks: List[int],\n",
    "                 n_layers: List[int],\n",
    "                 n_mlp_units: List[List[int]],\n",
    "                 n_x_hidden: int,\n",
    "                 n_s_hidden: int,\n",
    "                 n_pool_kernel_size: List[int],\n",
    "                 n_freq_downsample: List[int],\n",
    "                 pooling_mode: str,\n",
    "                 interpolation_mode: str,\n",
    "                 batch_normalization: bool,\n",
    "                 dropout_prob_theta: float,\n",
    "                 learning_rate: float,\n",
    "                 lr_decay: float,\n",
    "                 lr_decay_step_size: int,\n",
    "                 weight_decay: float,\n",
    "                 loss_train: str,\n",
    "                 loss_hypar: float,\n",
    "                 loss_valid: str,\n",
    "                 frequency: str,\n",
    "                 random_seed: int):\n",
    "        \"\"\"\n",
    "        N-HiTS model.\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            n_time_in: int\n",
    "                Multiplier to get insample size.\n",
    "                Insample size = n_time_in * output_size\n",
    "            n_time_out: int\n",
    "                Forecast horizon.\n",
    "            n_x: int\n",
    "                Number of exogenous variables.\n",
    "            n_s: int\n",
    "                Number of static variables.\n",
    "            shared_weights: bool\n",
    "                If True, all blocks within each stack will share parameters.\n",
    "            activation: str\n",
    "                Activation function.\n",
    "                An item from ['ReLU', 'Softplus', 'Tanh', 'SELU', 'LeakyReLU', 'PReLU', 'Sigmoid'].\n",
    "            initialization: str\n",
    "                Initialization function.\n",
    "                An item from ['orthogonal', 'he_uniform', 'glorot_uniform', 'glorot_normal', 'lecun_normal'].\n",
    "            stack_types: List[str]\n",
    "                List of stack types.\n",
    "                Subset from ['seasonality', 'trend', 'identity', 'exogenous', 'exogenous_tcn', 'exogenous_wavenet'].\n",
    "            n_blocks: List[int]\n",
    "                Number of blocks for each stack.\n",
    "                Note that len(n_blocks) = len(stack_types).\n",
    "            n_layers: List[int]\n",
    "                Number of layers for each stack type.\n",
    "                Note that len(n_layers) = len(stack_types).\n",
    "            n_mlp_units: List[List[int]]\n",
    "                Structure of hidden layers for each stack type.\n",
    "                Each internal list should contain the number of units of each hidden layer.\n",
    "                Note that len(n_hidden) = len(stack_types).\n",
    "            n_x_hidden: int\n",
    "                Number of hidden output channels of exogenous_tcn and exogenous_wavenet stacks. \n",
    "            n_s_hidden: int\n",
    "                Number of encoded static features, output dim of _StaticFeaturesEncoder. \n",
    "            n_pool_kernel_size List[int]:\n",
    "                Pooling size for input for each stack.\n",
    "                Note that len(n_pool_kernel_size) = len(stack_types).\n",
    "            n_freq_downsample List[int]:\n",
    "                Downsample multiplier of output for each stack. Expressivity ratio (r) = 1/n_freq_downsample\n",
    "                Note that len(n_freq_downsample) = len(stack_types).\n",
    "            pooling_mode: str\n",
    "                Pooling type.\n",
    "                An item from ['average', 'max']\n",
    "            interpolation_mode: str\n",
    "                Interpolation function.\n",
    "                An item from ['linear', 'nearest', 'cubic']\n",
    "            batch_normalization: bool\n",
    "                Whether perform batch normalization.\n",
    "            dropout_prob_theta: float\n",
    "                Float between (0, 1).\n",
    "                Dropout for Nbeats basis.\n",
    "            learning_rate: float\n",
    "                Learning rate between (0, 1).\n",
    "            lr_decay: float\n",
    "                Decreasing multiplier for the learning rate.\n",
    "            lr_decay_step_size: int\n",
    "                Steps between each learning rate decay.\n",
    "            weight_decay: float\n",
    "                L2 penalty for optimizer.\n",
    "            loss_train: str\n",
    "                Loss to optimize.\n",
    "                An item from ['MAPE', 'MASE', 'SMAPE', 'MSE', 'MAE', 'QUANTILE', 'QUANTILE2'].\n",
    "            loss_hypar: float\n",
    "                Hyperparameter for chosen loss.\n",
    "            loss_valid: str\n",
    "                Validation loss.\n",
    "                An item from ['MAPE', 'MASE', 'SMAPE', 'RMSE', 'MAE', 'QUANTILE'].\n",
    "            frequency: str\n",
    "                Time series frequency.\n",
    "            random_seed: int\n",
    "                random_seed for pseudo random pytorch initializer and\n",
    "                numpy random generator.\n",
    "        \"\"\"\n",
    "        \n",
    "        super(NHITS, self).__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        if activation == 'SELU': initialization = 'lecun_normal'\n",
    "\n",
    "        #------------------------ Model Attributes ------------------------#\n",
    "        # Architecture parameters\n",
    "        self.n_time_in = n_time_in\n",
    "        self.n_time_out = n_time_out\n",
    "        self.n_x = n_x\n",
    "        self.n_x_hidden = n_x_hidden\n",
    "        self.n_s = n_s\n",
    "        self.n_s_hidden = n_s_hidden\n",
    "        self.shared_weights = shared_weights\n",
    "        self.activation = activation\n",
    "        self.initialization = initialization\n",
    "        self.stack_types = stack_types\n",
    "        self.n_blocks = n_blocks\n",
    "        self.n_layers = n_layers\n",
    "        self.n_mlp_units = n_mlp_units\n",
    "        self.n_pool_kernel_size = n_pool_kernel_size\n",
    "        self.n_freq_downsample = n_freq_downsample\n",
    "        self.pooling_mode = pooling_mode\n",
    "        self.interpolation_mode = interpolation_mode\n",
    "\n",
    "        # Loss functions\n",
    "        self.loss_train = loss_train\n",
    "        self.loss_hypar = loss_hypar\n",
    "        self.loss_valid = loss_valid\n",
    "        self.loss_fn_train = LossFunction(loss_train,\n",
    "                                          percentile=self.loss_hypar,\n",
    "                                          seasonality=self.loss_hypar)\n",
    "        self.loss_fn_valid = LossFunction(loss_valid,\n",
    "                                          percentile=self.loss_hypar,\n",
    "                                          seasonality=self.loss_hypar)\n",
    "        \n",
    "        # Regularization and optimization parameters\n",
    "        self.batch_normalization = batch_normalization\n",
    "        self.dropout_prob_theta = dropout_prob_theta        \n",
    "        self.learning_rate = learning_rate\n",
    "        self.lr_decay = lr_decay\n",
    "        self.weight_decay = weight_decay\n",
    "        self.lr_decay_step_size = lr_decay_step_size\n",
    "        self.random_seed = random_seed\n",
    "\n",
    "        # Data parameters\n",
    "        self.frequency = frequency\n",
    "        self.return_decomposition = False\n",
    "\n",
    "        self.model = _NHITS(n_time_in=self.n_time_in,\n",
    "                            n_time_out=self.n_time_out,\n",
    "                            n_s=self.n_s,\n",
    "                            n_x=self.n_x,\n",
    "                            n_s_hidden=self.n_s_hidden,\n",
    "                            n_x_hidden=self.n_x_hidden,\n",
    "                            stack_types=self.stack_types,\n",
    "                            n_blocks=self.n_blocks,\n",
    "                            n_layers=self.n_layers,\n",
    "                            n_mlp_units=self.n_mlp_units,\n",
    "                            n_pool_kernel_size=self.n_pool_kernel_size,\n",
    "                            n_freq_downsample=self.n_freq_downsample,\n",
    "                            pooling_mode=self.pooling_mode,\n",
    "                            interpolation_mode=self.interpolation_mode,\n",
    "                            dropout_prob_theta=self.dropout_prob_theta,\n",
    "                            activation=self.activation,\n",
    "                            initialization=self.initialization,\n",
    "                            batch_normalization=self.batch_normalization,\n",
    "                            shared_weights=self.shared_weights)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        S = batch['S']\n",
    "        Y = batch['Y']\n",
    "        X = batch['X']\n",
    "        sample_mask = batch['sample_mask']\n",
    "        available_mask = batch['available_mask']\n",
    "\n",
    "        outsample_y, forecast, outsample_mask = self.model(S=S, Y=Y, X=X,\n",
    "                                                           insample_mask=available_mask,\n",
    "                                                           outsample_mask=sample_mask,\n",
    "                                                           return_decomposition=False)\n",
    "\n",
    "        loss = self.loss_fn_train(y=outsample_y,\n",
    "                                  y_hat=forecast,\n",
    "                                  mask=outsample_mask,\n",
    "                                  y_insample=Y)\n",
    "\n",
    "        self.log('train_loss', loss, prog_bar=True, on_epoch=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, idx):\n",
    "        S = batch['S']\n",
    "        Y = batch['Y']\n",
    "        X = batch['X']\n",
    "        sample_mask = batch['sample_mask']\n",
    "        available_mask = batch['available_mask']\n",
    "\n",
    "        outsample_y, forecast, outsample_mask = self.model(S=S, Y=Y, X=X,\n",
    "                                                           insample_mask=available_mask,\n",
    "                                                           outsample_mask=sample_mask,\n",
    "                                                           return_decomposition=False)\n",
    "\n",
    "        loss = self.loss_fn_valid(y=outsample_y,\n",
    "                                  y_hat=forecast,\n",
    "                                  mask=outsample_mask,\n",
    "                                  y_insample=Y)\n",
    "\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def on_fit_start(self):\n",
    "        t.manual_seed(self.random_seed)\n",
    "        np.random.seed(self.random_seed)\n",
    "        random.seed(self.random_seed) #TODO: interaccion rara con window_sampling de validacion\n",
    "\n",
    "    def forward(self, batch):\n",
    "        S = batch['S']\n",
    "        Y = batch['Y']\n",
    "        X = batch['X']\n",
    "        ts_idxs = batch['ts_idxs']\n",
    "        sample_mask = batch['sample_mask']\n",
    "        available_mask = batch['available_mask']\n",
    "\n",
    "        if self.return_decomposition:\n",
    "            outsample_y, forecast, block_forecast, outsample_mask = self.model(S=S, Y=Y, X=X,\n",
    "                                                                     insample_mask=available_mask,\n",
    "                                                                     outsample_mask=sample_mask,\n",
    "                                                                     return_decomposition=True)\n",
    "            return outsample_y, forecast, block_forecast, outsample_mask\n",
    "\n",
    "        outsample_y, forecast, outsample_mask = self.model(S=S, Y=Y, X=X,\n",
    "                                                           insample_mask=available_mask,\n",
    "                                                           outsample_mask=sample_mask,\n",
    "                                                           return_decomposition=False)\n",
    "        return outsample_y, forecast, outsample_mask, ts_idxs\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.model.parameters(),\n",
    "                               lr=self.learning_rate, \n",
    "                               weight_decay=self.weight_decay)\n",
    "        \n",
    "        lr_scheduler = optim.lr_scheduler.StepLR(optimizer, \n",
    "                                                 step_size=self.lr_decay_step_size, \n",
    "                                                 gamma=self.lr_decay)\n",
    "\n",
    "        return {'optimizer': optimizer, 'lr_scheduler': lr_scheduler}\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def forecast(self: NHITS, Y_df: pd.DataFrame, X_df: pd.DataFrame = None, S_df: pd.DataFrame = None, \n",
    "             batch_size: int =1, trainer: pl.Trainer = None,\n",
    "             verbose: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Method for forecasting self.n_time_out periods after last timestamp of Y_df.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    Y_df: pd.DataFrame\n",
    "        Dataframe with target time-series data, needs 'unique_id','ds' and 'y' columns.\n",
    "    X_df: pd.DataFrame\n",
    "        Dataframe with exogenous time-series data, needs 'unique_id' and 'ds' columns.\n",
    "        Note that 'unique_id' and 'ds' must match Y_df plus the forecasting horizon.\n",
    "    S_df: pd.DataFrame\n",
    "        Dataframe with static data, needs 'unique_id' column.\n",
    "    batch_size: int\n",
    "        Batch size for forecasting.\n",
    "    trainer: pl.Trainer\n",
    "        Trainer object for model training and evaluation.\n",
    "    verbose: bool\n",
    "        Print dataset information.\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    forecast_df: pd.DataFrame\n",
    "        Dataframe with forecasts.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add forecast dates to Y_df\n",
    "    Y_df = Y_df.reset_index(drop=True)\n",
    "    Y_df['ds'] = pd.to_datetime(Y_df['ds'])\n",
    "    if X_df is not None:\n",
    "        X_df = X_df.reset_index(drop=True)\n",
    "        X_df['ds'] = pd.to_datetime(X_df['ds'])\n",
    "    forecast_dates = pd.date_range(Y_df['ds'].max(), periods=self.n_time_out+1, freq=self.frequency)[1:]\n",
    "    index = pd.MultiIndex.from_product([Y_df['unique_id'].unique(), forecast_dates], names=['unique_id', 'ds'])\n",
    "    forecast_df = pd.DataFrame({'y':[0]}, index=index).reset_index()\n",
    "\n",
    "    Y_df = Y_df.append(forecast_df).sort_values(['unique_id','ds']).reset_index(drop=True)\n",
    "\n",
    "    # Dataset, loader and trainer\n",
    "    dataset = WindowsDataset(S_df=S_df, Y_df=Y_df, X_df=X_df,\n",
    "                                mask_df=None, f_cols=[],\n",
    "                                input_size=self.n_time_in,\n",
    "                                output_size=self.n_time_out,\n",
    "                                sample_freq=1,\n",
    "                                complete_windows=True,\n",
    "                                ds_in_test=self.n_time_out,\n",
    "                                is_test=True,\n",
    "                                verbose=verbose)\n",
    "\n",
    "    loader = TimeSeriesLoader(dataset=dataset,\n",
    "                                batch_size=batch_size,\n",
    "                                shuffle=False)\n",
    "\n",
    "    if trainer is None:\n",
    "        gpus = -1 if t.cuda.is_available() else 0\n",
    "        trainer = pl.Trainer(progress_bar_refresh_rate=1,\n",
    "                             gpus=gpus,\n",
    "                             logger=False)\n",
    "\n",
    "    # Forecast\n",
    "    outputs = trainer.predict(self, loader)\n",
    "\n",
    "    # Process forecast and include in forecast_df\n",
    "    _, forecast, _, _ = [t.cat(output).cpu().numpy() for output in zip(*outputs)]\n",
    "    forecast_df['y'] = forecast.flatten()\n",
    "\n",
    "    return forecast_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def predict(self: NHITS, Y_df: pd.DataFrame, X_df: pd.DataFrame = None, S_df: pd.DataFrame = None, \n",
    "            batch_size: int=1, trainer: pl.Trainer = None,\n",
    "            verbose: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "\n",
    "    This function creates rolled predictions for historic `Y_df`, parses and outputs them\n",
    "    in a long format `Y_hat_df` dataframe.\n",
    "    \n",
    "    This function does not returns forecasts outside Y_df dates.\n",
    "    To get forecasts for future ds use the `forecast` method.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        Y_df: pd.DataFrame\n",
    "            Dataframe with target time-series data, needs 'unique_id','ds' and 'y' columns.\n",
    "        X_df: pd.DataFrame\n",
    "            Dataframe with exogenous time-series data, needs 'unique_id' and 'ds' columns.\n",
    "            Note that 'unique_id' and 'ds' must match Y_df.\n",
    "        S_df: pd.DataFrame\n",
    "            Dataframe with static data, needs 'unique_id' column.\n",
    "        bath_size: int\n",
    "            Batch size for forecasting.\n",
    "        trainer: pl.Trainer\n",
    "            Trainer object for model training and evaluation.\n",
    "        verbose: bool\n",
    "            Print dataset information.\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "        Y_hat_df: pd.DataFrame\n",
    "            Predictions DataFrame with columns:\n",
    "            'unique_id': string/object series identifier\n",
    "            'fcds': date stamps marking the forecast creation.\n",
    "            'ds': prediction date stamps.\n",
    "            'y': historic true values.\n",
    "            'y_hat': forecasted values.\n",
    "    \"\"\"\n",
    "    \n",
    "    #------------------------------------- Model preds -------------------------------------#\n",
    "\n",
    "    # Add forecast dates to Y_df\n",
    "    Y_df['ds'] = pd.to_datetime(Y_df['ds'])\n",
    "    if X_df is not None:\n",
    "        X_df['ds'] = pd.to_datetime(X_df['ds'])\n",
    "\n",
    "    # Dataset, loader and trainer\n",
    "    dataset = WindowsDataset(S_df=S_df, Y_df=Y_df, X_df=X_df,\n",
    "                             mask_df=None, f_cols=[],\n",
    "                             input_size=self.n_time_in,\n",
    "                             output_size=self.n_time_out,\n",
    "                             sample_freq=1,\n",
    "                             complete_windows=True,\n",
    "                             ds_in_test=0,\n",
    "                             is_test=False,\n",
    "                             verbose=True)\n",
    "\n",
    "    loader = TimeSeriesLoader(dataset=dataset,\n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=False)\n",
    "\n",
    "    if trainer is None:\n",
    "        gpus = -1 if t.cuda.is_available() else 0\n",
    "        trainer = pl.Trainer(progress_bar_refresh_rate=1,\n",
    "                             gpus=gpus,\n",
    "                             logger=False)\n",
    "\n",
    "    # Forecast all rolled windows\n",
    "    outputs = trainer.predict(self, loader)\n",
    "\n",
    "    y_true, y_hat, _, _ = [t.cat(output).cpu().numpy() for output in zip(*outputs)]\n",
    "\n",
    "    #------------------------------------ ds Wrangling -------------------------------------#\n",
    "\n",
    "    # Y_hat dataframe wrangling\n",
    "    unique_ids = Y_df['unique_id'].unique()\n",
    "\n",
    "    # Extend ds vector to match padded ts_tensor\n",
    "    timedelta = dataset.ds[1] -dataset.ds[0]\n",
    "    start_date = dataset.ds[0] - dataset.input_size*timedelta\n",
    "    end_date = dataset.ds[-1] + dataset.output_size*timedelta\n",
    "    ds = pd.date_range(start_date, end_date, freq=dataset.frequency)\n",
    "\n",
    "    # Padded sample_mask to filter forecasted datestamps\n",
    "    sample_mask = dataset.ts_tensor[:,dataset.t_cols.index('sample_mask'),:]\n",
    "    padding = (self.n_time_in, self.n_time_out)\n",
    "    padder = t.nn.ConstantPad1d(padding=padding, value=0)\n",
    "    sample_mask = padder(sample_mask)\n",
    "\n",
    "    # Create tensor with rolled date indexes and sample_mask\n",
    "    # [n_series,n_channel,n_time_padded]->[n_series,n_channel,n_windows,L+H]\n",
    "    ds_idxs = t.tile(t.arange(start=0, end=len(ds)), dims=[len(sample_mask), 1])\n",
    "    windows_tensor = t.cat((sample_mask[:,None,:], ds_idxs[:,None,:]), dim=1)\n",
    "    windows_tensor = windows_tensor.unfold(dimension=-1, \n",
    "                            size=self.n_time_in+self.n_time_out,\n",
    "                            step=dataset.sample_freq) # For the moment sample_freq=1\n",
    "    \n",
    "    # Sampling condition, ALL ds in the forecast window should be sampleaple\n",
    "    # [n_series,n_windows,L+H]->[n_series,n_windows]\n",
    "    sample_mask_rolled = windows_tensor[:,0,:,-self.n_time_out:]\n",
    "    sample_condition = (sample_mask_rolled.sum(dim=2)==self.n_time_out)\n",
    "\n",
    "    # Use date indexes to create windows of date stamps\n",
    "    # And filter the dates of the forecast and the forecast creation date \n",
    "    # with -self.n_time_out-1\n",
    "    ds_rolled = ds[windows_tensor[:,1,:,-self.n_time_out-1:].numpy().astype(int)]     \n",
    "\n",
    "    # Filter rolled ds with sample_condition\n",
    "    ds_rolled = ds_rolled.reshape(-1, self.n_time_out+1)\n",
    "    ds_rolled = ds_rolled[sample_condition.flatten()]\n",
    "\n",
    "    # Split forecast creation date from the prediction ds\n",
    "    fcds = ds_rolled[:,0]\n",
    "    ds_rolled = ds_rolled[:,1:]\n",
    "\n",
    "    #------------------------------------- Forecast df -------------------------------------#\n",
    "\n",
    "    # Compute per series' observations = horizon * n_windows\n",
    "    # and create unique_id column in long format\n",
    "    unique_id_counts = self.n_time_out*sample_condition.sum(dim=1) # vectorized\n",
    "    unique_ids_column = [[u_id]*count for u_id,count in zip(unique_ids, unique_id_counts)]\n",
    "    unique_ids_column = np.concatenate(unique_ids_column)\n",
    "\n",
    "    # Forecast creation date column in long format, shared across horizon\n",
    "    fcds_column = fcds.repeat(self.n_time_out)\n",
    "\n",
    "    # Output DataFrame\n",
    "    Y_hat_df = pd.DataFrame({'unique_id':unique_ids_column,\n",
    "                             'fcds': fcds_column,\n",
    "                             'ds':ds_rolled.flatten(),\n",
    "                             'y': y_true.flatten(),\n",
    "                             'y_hat':y_hat.flatten()})\n",
    "\n",
    "    return Y_hat_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-HITS Usage Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from neuralforecast.data.datasets.long_horizon import LongHorizon\n",
    "\n",
    "from neuralforecast.data.tsloader import TimeSeriesLoader\n",
    "\n",
    "import pylab as plt\n",
    "from pylab import rcParams\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "\n",
    "FONTSIZE = 19\n",
    "\n",
    "Y_df, X_df, S_df = LongHorizon.load(directory='./data', group='ETTm2')\n",
    "Y_df = Y_df[Y_df['unique_id'].isin(['HUFL','OT'])]\n",
    "Y_df = Y_df[1000:]\n",
    "Y_df = Y_df.reset_index(drop=True)\n",
    "Y_df['y'] = range(len(Y_df)) \n",
    "Y_df['ds'] = pd.to_datetime(Y_df['ds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 6))\n",
    "plt.plot(Y_df[Y_df['unique_id']=='OT'].ds, Y_df[Y_df['unique_id']=='OT'].y.values, color='#628793', linewidth=0.4)\n",
    "plt.ylabel('Y', fontsize=19)\n",
    "plt.xlabel('ds', fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declare Model and Data Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Architecture parameters\n",
    "mc = {}\n",
    "mc['model'] = 'n-hits'\n",
    "mc['mode'] = 'simple'\n",
    "mc['activation'] = 'ReLU'\n",
    "\n",
    "mc['n_time_in'] = 96*2\n",
    "mc['n_time_out'] = 96\n",
    "mc['n_x_hidden'] = 8\n",
    "mc['n_s_hidden'] = 0\n",
    "\n",
    "mc['stack_types'] = ['identity', 'identity', 'identity']\n",
    "mc['constant_n_blocks'] = 1\n",
    "mc['constant_n_layers'] = 2\n",
    "mc['constant_n_mlp_units'] = 256\n",
    "mc['n_pool_kernel_size'] = [4, 2, 1]\n",
    "mc['n_freq_downsample'] = [24, 12, 1]\n",
    "mc['pooling_mode'] = 'max'\n",
    "mc['interpolation_mode'] = 'linear'\n",
    "mc['shared_weights'] = False\n",
    "\n",
    "# Optimization and regularization parameters\n",
    "mc['initialization'] = 'lecun_normal'\n",
    "mc['learning_rate'] = 0.001\n",
    "mc['batch_size'] = 1\n",
    "mc['n_windows'] = 32\n",
    "mc['lr_decay'] = 0.5\n",
    "mc['lr_decay_step_size'] = 2\n",
    "mc['max_epochs'] = 1\n",
    "mc['max_steps'] = None\n",
    "mc['early_stop_patience'] = 20\n",
    "mc['eval_freq'] = 500\n",
    "mc['batch_normalization'] = False\n",
    "mc['dropout_prob_theta'] = 0.0\n",
    "mc['dropout_prob_exogenous'] = 0.0\n",
    "mc['weight_decay'] = 0\n",
    "mc['loss_train'] = 'MAE'\n",
    "mc['loss_hypar'] = 0.5\n",
    "mc['loss_valid'] = mc['loss_train']\n",
    "mc['random_seed'] = 1\n",
    "\n",
    "# Data Parameters\n",
    "mc['idx_to_sample_freq'] = 1\n",
    "mc['val_idx_to_sample_freq'] = 1\n",
    "mc['n_val_weeks'] = 52\n",
    "mc['scaler'] = None\n",
    "mc['complete_windows'] = False\n",
    "mc['frequency'] = 'H'\n",
    "\n",
    "print(65*'=')\n",
    "print(pd.Series(mc))\n",
    "print(65*'=')\n",
    "\n",
    "mc['n_mlp_units'] = len(mc['stack_types']) * [ mc['constant_n_layers'] * [int(mc['constant_n_mlp_units'])] ]\n",
    "mc['n_blocks'] =  len(mc['stack_types']) * [ mc['constant_n_blocks'] ]\n",
    "mc['n_layers'] =  len(mc['stack_types']) * [ mc['constant_n_layers'] ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate Loaders and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralforecast.experiments.utils import create_datasets\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = create_datasets(mc=mc,\n",
    "                                                                     S_df=None, Y_df=Y_df, X_df=None,\n",
    "                                                                     f_cols=[],\n",
    "                                                                     ds_in_val=11520,\n",
    "                                                                     ds_in_test=11520)\n",
    "\n",
    "train_loader = TimeSeriesLoader(dataset=train_dataset,\n",
    "                                batch_size=int(mc['batch_size']),\n",
    "                                n_windows=mc['n_windows'],\n",
    "                                shuffle=True)\n",
    "\n",
    "val_loader = TimeSeriesLoader(dataset=val_dataset,\n",
    "                              batch_size=int(mc['batch_size']),\n",
    "                              shuffle=False)\n",
    "\n",
    "test_loader = TimeSeriesLoader(dataset=test_dataset,\n",
    "                               batch_size=int(mc['batch_size']),\n",
    "                               shuffle=False)\n",
    "\n",
    "mc['n_x'], mc['n_s'] = train_dataset.get_n_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NHITS(n_time_in=int(mc['n_time_in']),\n",
    "              n_time_out=int(mc['n_time_out']),\n",
    "              n_x=mc['n_x'],\n",
    "              n_s=mc['n_s'],\n",
    "              n_s_hidden=int(mc['n_s_hidden']),\n",
    "              n_x_hidden=int(mc['n_x_hidden']),\n",
    "              shared_weights=mc['shared_weights'],\n",
    "              initialization=mc['initialization'],\n",
    "              activation=mc['activation'],\n",
    "              stack_types=mc['stack_types'],\n",
    "              n_blocks=mc['n_blocks'],\n",
    "              n_layers=mc['n_layers'],\n",
    "              n_mlp_units=mc['n_mlp_units'],\n",
    "              n_pool_kernel_size=mc['n_pool_kernel_size'],\n",
    "              n_freq_downsample=mc['n_freq_downsample'],\n",
    "              pooling_mode=mc['pooling_mode'],\n",
    "              interpolation_mode=mc['interpolation_mode'],\n",
    "              batch_normalization = mc['batch_normalization'],\n",
    "              dropout_prob_theta=mc['dropout_prob_theta'],\n",
    "              learning_rate=float(mc['learning_rate']),\n",
    "              lr_decay=float(mc['lr_decay']),\n",
    "              lr_decay_step_size=float(mc['lr_decay_step_size']),\n",
    "              weight_decay=mc['weight_decay'],\n",
    "              loss_train=mc['loss_train'],\n",
    "              loss_hypar=float(mc['loss_hypar']),\n",
    "              loss_valid=mc['loss_valid'],\n",
    "              frequency=mc['frequency'],\n",
    "              random_seed=int(mc['random_seed']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", \n",
    "                               min_delta=1e-4, \n",
    "                               patience=mc['early_stop_patience'],\n",
    "                               verbose=False,\n",
    "                               mode=\"min\")\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=mc['max_epochs'], \n",
    "                     max_steps=mc['max_steps'],\n",
    "                     gradient_clip_val=1.0,\n",
    "                     progress_bar_refresh_rate=10, \n",
    "                     log_every_n_steps=500, \n",
    "                     check_val_every_n_epoch=1,\n",
    "                     callbacks=[early_stopping])\n",
    "\n",
    "trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.return_decomposition = False\n",
    "outputs = trainer.predict(model, val_loader)\n",
    "\n",
    "print(\"outputs[0][0].shape\", outputs[0][0].shape)\n",
    "print(\"outputs[0][1].shape\", outputs[0][1].shape)\n",
    "print(\"outputs[0][2].shape\", outputs[0][2].shape)\n",
    "print(\"outputs[0][3].shape\", outputs[0][3].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_forecast_df = Y_df[Y_df['ds']<'2016-12-27']\n",
    "Y_forecast_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.return_decomposition = False\n",
    "forecast_df = model.forecast(Y_df=Y_forecast_df, X_df=None, S_df=None, batch_size=2)\n",
    "forecast_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_hat_df = model.predict(Y_df=Y_df, X_df=None, S_df=None, batch_size=64)\n",
    "Y_hat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.12 ('neuralforecast')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
