{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp models.nhits.nhits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import math\n",
    "import random\n",
    "from functools import partial\n",
    "from typing import Tuple, List\n",
    "from fastcore.foundation import patch\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from neuralforecast.models.components.tcn import _TemporalConvNet\n",
    "from neuralforecast.models.components.common import Chomp1d, RepeatVector\n",
    "from neuralforecast.losses.utils import LossFunction\n",
    "from neuralforecast.data.tsdataset import WindowsDataset\n",
    "from neuralforecast.data.tsloader import TimeSeriesLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class _StaticFeaturesEncoder(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(_StaticFeaturesEncoder, self).__init__()\n",
    "        layers = [nn.Dropout(p=0.5),\n",
    "                  nn.Linear(in_features=in_features, out_features=out_features),\n",
    "                  nn.ReLU()]\n",
    "        self.encoder = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        return x\n",
    "\n",
    "class _sEncoder(nn.Module):\n",
    "    def __init__(self, in_features, out_features, n_time_in):\n",
    "        super(_sEncoder, self).__init__()\n",
    "        layers = [nn.Dropout(p=0.5),\n",
    "                  nn.Linear(in_features=in_features, out_features=out_features),\n",
    "                  nn.ReLU()]\n",
    "        self.encoder = nn.Sequential(*layers)\n",
    "        self.repeat = RepeatVector(repeats=n_time_in)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Encode and repeat values to match time\n",
    "        x = self.encoder(x)\n",
    "        x = self.repeat(x) # [N,S_out] -> [N,S_out,T]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class _IdentityBasis(nn.Module):\n",
    "    def __init__(self, backcast_size: int, forecast_size: int, interpolation_mode: str):\n",
    "        super().__init__()\n",
    "        assert (interpolation_mode in ['linear','nearest']) or ('cubic' in interpolation_mode)\n",
    "        self.forecast_size = forecast_size\n",
    "        self.backcast_size = backcast_size\n",
    "        self.interpolation_mode = interpolation_mode\n",
    " \n",
    "    def forward(self, theta: t.Tensor, insample_x_t: t.Tensor, outsample_x_t: t.Tensor) -> Tuple[t.Tensor, t.Tensor]:\n",
    "\n",
    "        backcast = theta[:, :self.backcast_size]\n",
    "        knots = theta[:, self.backcast_size:]\n",
    "\n",
    "        if self.interpolation_mode=='nearest':\n",
    "            knots = knots[:,None,:]\n",
    "            forecast = F.interpolate(knots, size=self.forecast_size, mode=self.interpolation_mode)\n",
    "            forecast = forecast[:,0,:]\n",
    "        elif self.interpolation_mode=='linear':\n",
    "            knots = knots[:,None,:]\n",
    "            forecast = F.interpolate(knots, size=self.forecast_size, mode=self.interpolation_mode)\n",
    "            forecast = forecast[:,0,:]\n",
    "        elif 'cubic' in self.interpolation_mode:\n",
    "            batch_size = len(backcast)\n",
    "            knots = knots[:,None,None,:]\n",
    "            forecast = t.zeros((len(knots), self.forecast_size)).to(knots.device)\n",
    "            n_batches = int(np.ceil(len(knots)/batch_size))\n",
    "            for i in range(n_batches):\n",
    "                forecast_i = F.interpolate(knots[i*batch_size:(i+1)*batch_size], size=self.forecast_size, mode='bicubic')\n",
    "                forecast[i*batch_size:(i+1)*batch_size] += forecast_i[:,0,0,:]\n",
    "\n",
    "        return backcast, forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class _ExogenousBasisInterpretable(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, theta: t.Tensor, insample_x_t: t.Tensor, outsample_x_t: t.Tensor) -> Tuple[t.Tensor, t.Tensor]:\n",
    "        backcast_basis = insample_x_t\n",
    "        forecast_basis = outsample_x_t\n",
    "\n",
    "        cut_point = forecast_basis.shape[1]\n",
    "        backcast = t.einsum('bp,bpt->bt', theta[:, cut_point:], backcast_basis)\n",
    "        forecast = t.einsum('bp,bpt->bt', theta[:, :cut_point], forecast_basis)\n",
    "        return backcast, forecast\n",
    "\n",
    "class _ExogenousBasisWavenet(nn.Module):\n",
    "    def __init__(self, out_features, in_features, num_levels=4, kernel_size=3, dropout_prob=0):\n",
    "        super().__init__()\n",
    "        # Shape of (1, in_features, 1) to broadcast over b and t\n",
    "        self.weight = nn.Parameter(t.Tensor(1, in_features, 1), requires_grad=True)\n",
    "        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(0.5))\n",
    "\n",
    "        padding = (kernel_size - 1) * (2**0)\n",
    "        input_layer = [nn.Conv1d(in_channels=in_features, out_channels=out_features,\n",
    "                                 kernel_size=kernel_size, padding=padding, dilation=2**0),\n",
    "                                 Chomp1d(padding),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Dropout(dropout_prob)]\n",
    "        conv_layers = []\n",
    "        for i in range(1, num_levels):\n",
    "            dilation = 2**i\n",
    "            padding = (kernel_size - 1) * dilation\n",
    "            conv_layers.append(nn.Conv1d(in_channels=out_features, out_channels=out_features,\n",
    "                                         padding=padding, kernel_size=3, dilation=dilation))\n",
    "            conv_layers.append(Chomp1d(padding))\n",
    "            conv_layers.append(nn.ReLU())\n",
    "        conv_layers = input_layer + conv_layers\n",
    "\n",
    "        self.wavenet = nn.Sequential(*conv_layers)\n",
    "        \n",
    "    def transform(self, insample_x_t, outsample_x_t):\n",
    "        n_time_in = insample_x_t.shape[2]\n",
    "        \n",
    "        x_t = t.cat([insample_x_t, outsample_x_t], dim=2)\n",
    "        \n",
    "        x_t = x_t * self.weight # Element-wise multiplication, broadcasted on b and t. Weights used in L1 regularization\n",
    "        x_t = self.wavenet(x_t)[:]\n",
    "\n",
    "        backcast_basis = x_t[:,:, :n_time_in]\n",
    "        forecast_basis = x_t[:,:, n_time_in:]\n",
    "\n",
    "        return backcast_basis, forecast_basis\n",
    "\n",
    "    def forward(self, theta: t.Tensor, insample_x_t: t.Tensor, outsample_x_t: t.Tensor) -> Tuple[t.Tensor, t.Tensor]:\n",
    "        backcast_basis, forecast_basis = self.transform(insample_x_t, outsample_x_t)\n",
    "\n",
    "        cut_point = forecast_basis.shape[1]\n",
    "        backcast = t.einsum('bp,bpt->bt', theta[:, cut_point:], backcast_basis)\n",
    "        forecast = t.einsum('bp,bpt->bt', theta[:, :cut_point], forecast_basis)\n",
    "        return backcast, forecast\n",
    "\n",
    "class _ExogenousBasisTCN(nn.Module):\n",
    "    def __init__(self, out_features, in_features, num_levels = 4, kernel_size=2, dropout_prob=0):\n",
    "        super().__init__()\n",
    "        n_channels = num_levels * [out_features]\n",
    "        self.tcn = _TemporalConvNet(num_inputs=in_features, num_channels=n_channels, kernel_size=kernel_size, dropout=dropout_prob)\n",
    "        \n",
    "    def transform(self, insample_x_t, outsample_x_t):\n",
    "        n_time_in = insample_x_t.shape[2]\n",
    "        \n",
    "        x_t = t.cat([insample_x_t, outsample_x_t], dim=2)\n",
    "        \n",
    "        x_t = self.tcn(x_t)[:]\n",
    "        backcast_basis = x_t[:,:, :n_time_in]\n",
    "        forecast_basis = x_t[:,:, n_time_in:]\n",
    "\n",
    "        return backcast_basis, forecast_basis\n",
    "\n",
    "    def forward(self, theta: t.Tensor, insample_x_t: t.Tensor, outsample_x_t: t.Tensor) -> Tuple[t.Tensor, t.Tensor]:\n",
    "        backcast_basis, forecast_basis = self.transform(insample_x_t, outsample_x_t)\n",
    "\n",
    "        cut_point = forecast_basis.shape[1]\n",
    "        backcast = t.einsum('bp,bpt->bt', theta[:, cut_point:], backcast_basis)\n",
    "        forecast = t.einsum('bp,bpt->bt', theta[:, :cut_point], forecast_basis)\n",
    "        return backcast, forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _init_weights(module, initialization):\n",
    "    if type(module) == t.nn.Linear:\n",
    "        if initialization == 'orthogonal':\n",
    "            t.nn.init.orthogonal_(module.weight)\n",
    "        elif initialization == 'he_uniform':\n",
    "            t.nn.init.kaiming_uniform_(module.weight)\n",
    "        elif initialization == 'he_normal':\n",
    "            t.nn.init.kaiming_normal_(module.weight)\n",
    "        elif initialization == 'glorot_uniform':\n",
    "            t.nn.init.xavier_uniform_(module.weight)\n",
    "        elif initialization == 'glorot_normal':\n",
    "            t.nn.init.xavier_normal_(module.weight)\n",
    "        elif initialization == 'lecun_normal':\n",
    "            pass #t.nn.init.normal_(module.weight, 0.0, std=1/np.sqrt(module.weight.numel()))\n",
    "        else:\n",
    "            assert 1<0, f'Initialization {initialization} not found'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "ACTIVATIONS = ['ReLU',\n",
    "               'Softplus',\n",
    "               'Tanh',\n",
    "               'SELU',\n",
    "               'LeakyReLU',\n",
    "               'PReLU',\n",
    "               'Sigmoid']\n",
    "\n",
    "class _NHITSBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    N-BEATS block which takes a basis function as an argument.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_time_in: int, n_time_out: int, n_x: int,\n",
    "                 n_s: int, n_s_hidden: int, n_theta: int, n_mlp_units: list,\n",
    "                 n_pool_kernel_size: int, pooling_mode: str, basis: nn.Module,\n",
    "                 n_layers: int,  batch_normalization: bool, dropout_prob: float, activation: str):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        assert (pooling_mode in ['max','average'])\n",
    "\n",
    "        n_time_in_pooled = int(np.ceil(n_time_in/n_pool_kernel_size))\n",
    "\n",
    "        if n_s == 0:\n",
    "            n_s_hidden = 0\n",
    "        n_mlp_units = [n_time_in_pooled + (n_time_in+n_time_out)*n_x + n_s_hidden] + n_mlp_units\n",
    "        \n",
    "        self.n_time_in = n_time_in\n",
    "        self.n_time_out = n_time_out\n",
    "        self.n_s = n_s\n",
    "        self.n_s_hidden = n_s_hidden\n",
    "        self.n_x = n_x\n",
    "        self.n_pool_kernel_size = n_pool_kernel_size\n",
    "        self.batch_normalization = batch_normalization\n",
    "        self.dropout_prob = dropout_prob\n",
    "        \n",
    "        assert activation in ACTIVATIONS, f'{activation} is not in {ACTIVATIONS}'\n",
    "        activ = getattr(nn, activation)()\n",
    "\n",
    "        if pooling_mode == 'max':\n",
    "            self.pooling_layer = nn.MaxPool1d(kernel_size=self.n_pool_kernel_size,\n",
    "                                              stride=self.n_pool_kernel_size, ceil_mode=True)\n",
    "        elif pooling_mode == 'average':\n",
    "            self.pooling_layer = nn.AvgPool1d(kernel_size=self.n_pool_kernel_size,\n",
    "                                              stride=self.n_pool_kernel_size, ceil_mode=True)\n",
    "\n",
    "        hidden_layers = []\n",
    "        for i in range(n_layers):\n",
    "            hidden_layers.append(nn.Linear(in_features=n_mlp_units[i], out_features=n_mlp_units[i+1]))\n",
    "            hidden_layers.append(activ)\n",
    "\n",
    "            if self.batch_normalization:\n",
    "                hidden_layers.append(nn.BatchNorm1d(num_features=n_mlp_units[i+1]))\n",
    "\n",
    "            if self.dropout_prob>0:\n",
    "                hidden_layers.append(nn.Dropout(p=self.dropout_prob))\n",
    "\n",
    "        output_layer = [nn.Linear(in_features=n_mlp_units[-1], out_features=n_theta)]\n",
    "        layers = hidden_layers + output_layer\n",
    "\n",
    "        # n_s is computed with data, n_s_hidden is provided by user, if 0 no statics are used\n",
    "        if (self.n_s > 0) and (self.n_s_hidden > 0):\n",
    "            self.static_encoder = _StaticFeaturesEncoder(in_features=n_s, out_features=n_s_hidden)\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        self.basis = basis\n",
    "\n",
    "    def forward(self, insample_y: t.Tensor, insample_x_t: t.Tensor,\n",
    "                outsample_x_t: t.Tensor, x_s: t.Tensor) -> Tuple[t.Tensor, t.Tensor]:\n",
    "\n",
    "        insample_y = insample_y.unsqueeze(1)\n",
    "        insample_y = self.pooling_layer(insample_y)\n",
    "        insample_y = insample_y.squeeze(1)\n",
    "\n",
    "        batch_size = len(insample_y)\n",
    "        if self.n_x > 0:\n",
    "            insample_y = t.cat(( insample_y, insample_x_t.reshape(batch_size, -1) ), 1)\n",
    "            insample_y = t.cat(( insample_y, outsample_x_t.reshape(batch_size, -1) ), 1)\n",
    "        \n",
    "        # Static exogenous\n",
    "        if (self.n_s > 0) and (self.n_s_hidden > 0):\n",
    "            x_s = self.static_encoder(x_s)\n",
    "            insample_y = t.cat((insample_y, x_s), 1)\n",
    "\n",
    "        # Compute local projection weights and projection\n",
    "        theta = self.layers(insample_y)\n",
    "        backcast, forecast = self.basis(theta, insample_x_t, outsample_x_t)\n",
    "\n",
    "        return backcast, forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class _NHITS(nn.Module):\n",
    "    \"\"\"\n",
    "    NHITS Model.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 n_time_in,\n",
    "                 n_time_out,\n",
    "                 n_s,\n",
    "                 n_x,\n",
    "                 n_s_hidden,\n",
    "                 n_x_hidden,\n",
    "                 stack_types: list,\n",
    "                 n_blocks: list,\n",
    "                 n_layers: list,\n",
    "                 n_mlp_units: list,\n",
    "                 n_pool_kernel_size: list,\n",
    "                 n_freq_downsample: list,\n",
    "                 pooling_mode,\n",
    "                 interpolation_mode,\n",
    "                 dropout_prob_theta,\n",
    "                 activation,\n",
    "                 initialization,\n",
    "                 batch_normalization,\n",
    "                 shared_weights):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_time_out = n_time_out\n",
    "\n",
    "        blocks = self.create_stack(stack_types=stack_types, \n",
    "                                   n_blocks=n_blocks,\n",
    "                                   n_time_in=n_time_in,\n",
    "                                   n_time_out=n_time_out,\n",
    "                                   n_x=n_x,\n",
    "                                   n_x_hidden=n_x_hidden,\n",
    "                                   n_s=n_s,\n",
    "                                   n_s_hidden=n_s_hidden,\n",
    "                                   n_layers=n_layers,\n",
    "                                   n_mlp_units=n_mlp_units,\n",
    "                                   n_pool_kernel_size=n_pool_kernel_size,\n",
    "                                   n_freq_downsample=n_freq_downsample,\n",
    "                                   pooling_mode=pooling_mode,\n",
    "                                   interpolation_mode=interpolation_mode,\n",
    "                                   batch_normalization=batch_normalization,\n",
    "                                   dropout_prob_theta=dropout_prob_theta,\n",
    "                                   activation=activation,\n",
    "                                   shared_weights=shared_weights,\n",
    "                                   initialization=initialization)\n",
    "        self.blocks = t.nn.ModuleList(blocks)\n",
    "\n",
    "    def create_stack(self, stack_types, n_blocks, \n",
    "                     n_time_in, n_time_out, \n",
    "                     n_x, n_x_hidden, n_s, n_s_hidden, \n",
    "                     n_layers, n_mlp_units, \n",
    "                     n_pool_kernel_size, n_freq_downsample, pooling_mode, interpolation_mode,\n",
    "                     batch_normalization, dropout_prob_theta, \n",
    "                     activation, shared_weights, initialization):                     \n",
    "        block_list = []\n",
    "        for i in range(len(stack_types)):\n",
    "            assert stack_types[i] in ['identity', 'exogenous', 'exogenous_tcn', 'exogenous_wavenet'], 'f Invalid stack type {stack_types[i]}'\n",
    "            for block_id in range(n_blocks[i]):\n",
    "                \n",
    "                # Batch norm only on first block\n",
    "                if (len(block_list)==0) and (batch_normalization):\n",
    "                    batch_normalization_block = True\n",
    "                else:\n",
    "                    batch_normalization_block = False\n",
    "\n",
    "                # Shared weights\n",
    "                if shared_weights and block_id>0:\n",
    "                    nbeats_block = block_list[-1]\n",
    "                else:\n",
    "                    if stack_types[i] == 'identity':\n",
    "                        n_theta = (n_time_in + max(n_time_out//n_freq_downsample[i], 1) )\n",
    "                        basis = _IdentityBasis(backcast_size=n_time_in,\n",
    "                                              forecast_size=n_time_out,\n",
    "                                              interpolation_mode=interpolation_mode)                        \n",
    "\n",
    "                    elif stack_types[i] == 'exogenous':\n",
    "                        n_theta = 2 * n_x\n",
    "                        basis = _ExogenousBasisInterpretable()\n",
    "\n",
    "                    elif stack_types[i] == 'exogenous_tcn':\n",
    "                        n_theta = 2 * n_x_hidden\n",
    "                        basis = _ExogenousBasisTCN(n_x_hidden, n_x)\n",
    "\n",
    "                    elif stack_types[i] == 'exogenous_wavenet':\n",
    "                        n_theta = 2 * n_x_hidden\n",
    "                        basis = _ExogenousBasisWavenet(n_x_hidden, n_x)\n",
    "\n",
    "                    nbeats_block = _NHITSBlock(n_time_in=n_time_in,\n",
    "                                               n_time_out=n_time_out,\n",
    "                                               n_x=n_x,\n",
    "                                               n_s=n_s,\n",
    "                                               n_s_hidden=n_s_hidden,\n",
    "                                               n_theta=n_theta,\n",
    "                                               n_mlp_units=n_mlp_units[i],\n",
    "                                               n_pool_kernel_size=n_pool_kernel_size[i],\n",
    "                                               pooling_mode=pooling_mode,\n",
    "                                               basis=basis,\n",
    "                                               n_layers=n_layers[i],\n",
    "                                               batch_normalization=batch_normalization_block,\n",
    "                                               dropout_prob=dropout_prob_theta,\n",
    "                                               activation=activation)\n",
    "\n",
    "                # Select type of evaluation and apply it to all layers of block\n",
    "                init_function = partial(_init_weights, initialization=initialization)                                             \n",
    "                nbeats_block.layers.apply(init_function)\n",
    "                block_list.append(nbeats_block)\n",
    "        return block_list\n",
    "\n",
    "    def forward(self, S: t.Tensor, Y: t.Tensor, X: t.Tensor, \n",
    "                insample_mask: t.Tensor, outsample_mask: t.Tensor,\n",
    "                return_decomposition: bool=False):\n",
    "        \n",
    "        # insample\n",
    "        insample_y    = Y[:, :-self.n_time_out]\n",
    "        insample_x_t  = X[:, :, :-self.n_time_out]\n",
    "        insample_mask = insample_mask[:, :-self.n_time_out]\n",
    "        \n",
    "        # outsample\n",
    "        outsample_y   = Y[:, -self.n_time_out:]\n",
    "        outsample_x_t = X[:, :, -self.n_time_out:]\n",
    "        outsample_mask = outsample_mask[:, -self.n_time_out:]\n",
    "\n",
    "        if return_decomposition:\n",
    "            forecast, block_forecasts = self.forecast_decomposition(insample_y=insample_y, \n",
    "                                                                    insample_x_t=insample_x_t, \n",
    "                                                                    insample_mask=insample_mask,\n",
    "                                                                    outsample_x_t=outsample_x_t,\n",
    "                                                                    x_s=S)\n",
    "            return outsample_y, forecast, block_forecasts, outsample_mask\n",
    "        \n",
    "        else:\n",
    "            forecast = self.forecast(insample_y=insample_y,\n",
    "                                     insample_x_t=insample_x_t, \n",
    "                                     insample_mask=insample_mask,\n",
    "                                     outsample_x_t=outsample_x_t,\n",
    "                                     x_s=S)\n",
    "            return outsample_y, forecast, outsample_mask\n",
    "\n",
    "    def forecast(self, insample_y: t.Tensor, insample_x_t: t.Tensor, insample_mask: t.Tensor,\n",
    "                 outsample_x_t: t.Tensor, x_s: t.Tensor):\n",
    "\n",
    "        residuals = insample_y.flip(dims=(-1,))\n",
    "        insample_x_t = insample_x_t.flip(dims=(-1,))\n",
    "        insample_mask = insample_mask.flip(dims=(-1,))\n",
    "\n",
    "        forecast = insample_y[:, -1:] # Level with Naive1\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            backcast, block_forecast = block(insample_y=residuals, insample_x_t=insample_x_t,\n",
    "                                             outsample_x_t=outsample_x_t, x_s=x_s)\n",
    "            residuals = (residuals - backcast) * insample_mask\n",
    "            forecast = forecast + block_forecast\n",
    "\n",
    "        return forecast\n",
    "\n",
    "    def forecast_decomposition(self, insample_y: t.Tensor, insample_x_t: t.Tensor, insample_mask: t.Tensor,\n",
    "                               outsample_x_t: t.Tensor, x_s: t.Tensor):\n",
    "\n",
    "        residuals = insample_y.flip(dims=(-1,))\n",
    "        insample_x_t = insample_x_t.flip(dims=(-1,))\n",
    "        insample_mask = insample_mask.flip(dims=(-1,))\n",
    "        \n",
    "        n_batch, n_channels, n_t = outsample_x_t.size(0), outsample_x_t.size(1), outsample_x_t.size(2)\n",
    "        \n",
    "        level = insample_y[:, -1:] # Level with Naive1\n",
    "        block_forecasts = [ level.repeat(1, n_t) ]\n",
    "                \n",
    "        forecast = level\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            backcast, block_forecast = block(insample_y=residuals, insample_x_t=insample_x_t,\n",
    "                                             outsample_x_t=outsample_x_t, x_s=x_s)\n",
    "            residuals = (residuals - backcast) * insample_mask\n",
    "            forecast = forecast + block_forecast\n",
    "            block_forecasts.append(block_forecast)\n",
    "            \n",
    "        # (n_batch, n_blocks, n_t)\n",
    "        block_forecasts = t.stack(block_forecasts)\n",
    "        block_forecasts = block_forecasts.permute(1,0,2)\n",
    "\n",
    "        return forecast, block_forecasts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-HiTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A new model for long-horizon forecasting which incorporates novel hierarchical interpolation and multi-rate data sampling techniques to specialize blocks of its architecture to different frequency band of the time-series signal. It achieves SoTA performance on several benchmark datasets, outperforming current Transformer-based models by more than 25%. Paper available at https://arxiv.org/abs/2201.12886"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class NHITS(pl.LightningModule):\n",
    "    def __init__(self,\n",
    "                 n_time_in: int,\n",
    "                 n_time_out: int,\n",
    "                 n_x: int,\n",
    "                 n_s: int,\n",
    "                 shared_weights: bool,\n",
    "                 activation: str,\n",
    "                 initialization: str,\n",
    "                 stack_types: List[str],\n",
    "                 n_blocks: List[int],\n",
    "                 n_layers: List[int],\n",
    "                 n_mlp_units: List[List[int]],\n",
    "                 n_x_hidden: int,\n",
    "                 n_s_hidden: int,\n",
    "                 n_pool_kernel_size: List[int],\n",
    "                 n_freq_downsample: List[int],\n",
    "                 pooling_mode: str,\n",
    "                 interpolation_mode: str,\n",
    "                 batch_normalization: bool,\n",
    "                 dropout_prob_theta: float,\n",
    "                 learning_rate: float,\n",
    "                 lr_decay: float,\n",
    "                 lr_decay_step_size: int,\n",
    "                 weight_decay: float,\n",
    "                 loss_train: str,\n",
    "                 loss_hypar: float,\n",
    "                 loss_valid: str,\n",
    "                 frequency: str,\n",
    "                 random_seed: int):\n",
    "        \"\"\"\n",
    "        N-HiTS model.\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            n_time_in: int\n",
    "                Multiplier to get insample size.\n",
    "                Insample size = n_time_in * output_size\n",
    "            n_time_out: int\n",
    "                Forecast horizon.\n",
    "            n_x: int\n",
    "                Number of exogenous variables.\n",
    "            n_s: int\n",
    "                Number of static variables.\n",
    "            shared_weights: bool\n",
    "                If True, all blocks within each stack will share parameters.\n",
    "            activation: str\n",
    "                Activation function.\n",
    "                An item from ['ReLU', 'Softplus', 'Tanh', 'SELU', 'LeakyReLU', 'PReLU', 'Sigmoid'].\n",
    "            initialization: str\n",
    "                Initialization function.\n",
    "                An item from ['orthogonal', 'he_uniform', 'glorot_uniform', 'glorot_normal', 'lecun_normal'].\n",
    "            stack_types: List[str]\n",
    "                List of stack types.\n",
    "                Subset from ['seasonality', 'trend', 'identity', 'exogenous', 'exogenous_tcn', 'exogenous_wavenet'].\n",
    "            n_blocks: List[int]\n",
    "                Number of blocks for each stack.\n",
    "                Note that len(n_blocks) = len(stack_types).\n",
    "            n_layers: List[int]\n",
    "                Number of layers for each stack type.\n",
    "                Note that len(n_layers) = len(stack_types).\n",
    "            n_mlp_units: List[List[int]]\n",
    "                Structure of hidden layers for each stack type.\n",
    "                Each internal list should contain the number of units of each hidden layer.\n",
    "                Note that len(n_hidden) = len(stack_types).\n",
    "            n_x_hidden: int\n",
    "                Number of hidden output channels of exogenous_tcn and exogenous_wavenet stacks. \n",
    "            n_s_hidden: int\n",
    "                Number of encoded static features, output dim of _StaticFeaturesEncoder. \n",
    "            n_pool_kernel_size List[int]:\n",
    "                Pooling size for input for each stack.\n",
    "                Note that len(n_pool_kernel_size) = len(stack_types).\n",
    "            n_freq_downsample List[int]:\n",
    "                Downsample multiplier of output for each stack. Expressivity ratio (r) = 1/n_freq_downsample\n",
    "                Note that len(n_freq_downsample) = len(stack_types).\n",
    "            pooling_mode: str\n",
    "                Pooling type.\n",
    "                An item from ['average', 'max']\n",
    "            interpolation_mode: str\n",
    "                Interpolation function.\n",
    "                An item from ['linear', 'nearest', 'cubic']\n",
    "            batch_normalization: bool\n",
    "                Whether perform batch normalization.\n",
    "            dropout_prob_theta: float\n",
    "                Float between (0, 1).\n",
    "                Dropout for Nbeats basis.\n",
    "            learning_rate: float\n",
    "                Learning rate between (0, 1).\n",
    "            lr_decay: float\n",
    "                Decreasing multiplier for the learning rate.\n",
    "            lr_decay_step_size: int\n",
    "                Steps between each learning rate decay.\n",
    "            weight_decay: float\n",
    "                L2 penalty for optimizer.\n",
    "            loss_train: str\n",
    "                Loss to optimize.\n",
    "                An item from ['MAPE', 'MASE', 'SMAPE', 'MSE', 'MAE', 'QUANTILE', 'QUANTILE2'].\n",
    "            loss_hypar: float\n",
    "                Hyperparameter for chosen loss.\n",
    "            loss_valid: str\n",
    "                Validation loss.\n",
    "                An item from ['MAPE', 'MASE', 'SMAPE', 'RMSE', 'MAE', 'QUANTILE'].\n",
    "            frequency: str\n",
    "                Time series frequency.\n",
    "            random_seed: int\n",
    "                random_seed for pseudo random pytorch initializer and\n",
    "                numpy random generator.\n",
    "        \"\"\"\n",
    "        \n",
    "        super(NHITS, self).__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        if activation == 'SELU': initialization = 'lecun_normal'\n",
    "\n",
    "        #------------------------ Model Attributes ------------------------#\n",
    "        # Architecture parameters\n",
    "        self.n_time_in = n_time_in\n",
    "        self.n_time_out = n_time_out\n",
    "        self.n_x = n_x\n",
    "        self.n_x_hidden = n_x_hidden\n",
    "        self.n_s = n_s\n",
    "        self.n_s_hidden = n_s_hidden\n",
    "        self.shared_weights = shared_weights\n",
    "        self.activation = activation\n",
    "        self.initialization = initialization\n",
    "        self.stack_types = stack_types\n",
    "        self.n_blocks = n_blocks\n",
    "        self.n_layers = n_layers\n",
    "        self.n_mlp_units = n_mlp_units\n",
    "        self.n_pool_kernel_size = n_pool_kernel_size\n",
    "        self.n_freq_downsample = n_freq_downsample\n",
    "        self.pooling_mode = pooling_mode\n",
    "        self.interpolation_mode = interpolation_mode\n",
    "\n",
    "        # Loss functions\n",
    "        self.loss_train = loss_train\n",
    "        self.loss_hypar = loss_hypar\n",
    "        self.loss_valid = loss_valid\n",
    "        self.loss_fn_train = LossFunction(loss_train,\n",
    "                                          percentile=self.loss_hypar,\n",
    "                                          seasonality=self.loss_hypar)\n",
    "        self.loss_fn_valid = LossFunction(loss_valid,\n",
    "                                          percentile=self.loss_hypar,\n",
    "                                          seasonality=self.loss_hypar)\n",
    "        \n",
    "        # Regularization and optimization parameters\n",
    "        self.batch_normalization = batch_normalization\n",
    "        self.dropout_prob_theta = dropout_prob_theta        \n",
    "        self.learning_rate = learning_rate\n",
    "        self.lr_decay = lr_decay\n",
    "        self.weight_decay = weight_decay\n",
    "        self.lr_decay_step_size = lr_decay_step_size\n",
    "        self.random_seed = random_seed\n",
    "\n",
    "        # Data parameters\n",
    "        self.frequency = frequency\n",
    "        self.return_decomposition = False\n",
    "\n",
    "        self.model = _NHITS(n_time_in=self.n_time_in,\n",
    "                            n_time_out=self.n_time_out,\n",
    "                            n_s=self.n_s,\n",
    "                            n_x=self.n_x,\n",
    "                            n_s_hidden=self.n_s_hidden,\n",
    "                            n_x_hidden=self.n_x_hidden,\n",
    "                            stack_types=self.stack_types,\n",
    "                            n_blocks=self.n_blocks,\n",
    "                            n_layers=self.n_layers,\n",
    "                            n_mlp_units=self.n_mlp_units,\n",
    "                            n_pool_kernel_size=self.n_pool_kernel_size,\n",
    "                            n_freq_downsample=self.n_freq_downsample,\n",
    "                            pooling_mode=self.pooling_mode,\n",
    "                            interpolation_mode=self.interpolation_mode,\n",
    "                            dropout_prob_theta=self.dropout_prob_theta,\n",
    "                            activation=self.activation,\n",
    "                            initialization=self.initialization,\n",
    "                            batch_normalization=self.batch_normalization,\n",
    "                            shared_weights=self.shared_weights)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        S = batch['S']\n",
    "        Y = batch['Y']\n",
    "        X = batch['X']\n",
    "        sample_mask = batch['sample_mask']\n",
    "        available_mask = batch['available_mask']\n",
    "\n",
    "        outsample_y, forecast, outsample_mask = self.model(S=S, Y=Y, X=X,\n",
    "                                                           insample_mask=available_mask,\n",
    "                                                           outsample_mask=sample_mask,\n",
    "                                                           return_decomposition=False)\n",
    "\n",
    "        loss = self.loss_fn_train(y=outsample_y,\n",
    "                                  y_hat=forecast,\n",
    "                                  mask=outsample_mask,\n",
    "                                  y_insample=Y)\n",
    "\n",
    "        self.log('train_loss', loss, prog_bar=True, on_epoch=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, idx):\n",
    "        S = batch['S']\n",
    "        Y = batch['Y']\n",
    "        X = batch['X']\n",
    "        sample_mask = batch['sample_mask']\n",
    "        available_mask = batch['available_mask']\n",
    "\n",
    "        outsample_y, forecast, outsample_mask = self.model(S=S, Y=Y, X=X,\n",
    "                                                           insample_mask=available_mask,\n",
    "                                                           outsample_mask=sample_mask,\n",
    "                                                           return_decomposition=False)\n",
    "\n",
    "        loss = self.loss_fn_valid(y=outsample_y,\n",
    "                                  y_hat=forecast,\n",
    "                                  mask=outsample_mask,\n",
    "                                  y_insample=Y)\n",
    "\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def on_fit_start(self):\n",
    "        t.manual_seed(self.random_seed)\n",
    "        np.random.seed(self.random_seed)\n",
    "        random.seed(self.random_seed) #TODO: interaccion rara con window_sampling de validacion\n",
    "\n",
    "    def forward(self, batch):\n",
    "        S = batch['S']\n",
    "        Y = batch['Y']\n",
    "        X = batch['X']\n",
    "        sample_mask = batch['sample_mask']\n",
    "        available_mask = batch['available_mask']\n",
    "\n",
    "        if self.return_decomposition:\n",
    "            outsample_y, forecast, block_forecast, outsample_mask = self.model(S=S, Y=Y, X=X,\n",
    "                                                                     insample_mask=available_mask,\n",
    "                                                                     outsample_mask=sample_mask,\n",
    "                                                                     return_decomposition=True)\n",
    "            return outsample_y, forecast, block_forecast, outsample_mask\n",
    "\n",
    "        outsample_y, forecast, outsample_mask = self.model(S=S, Y=Y, X=X,\n",
    "                                                           insample_mask=available_mask,\n",
    "                                                           outsample_mask=sample_mask,\n",
    "                                                           return_decomposition=False)\n",
    "        return outsample_y, forecast, outsample_mask\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.model.parameters(),\n",
    "                               lr=self.learning_rate, \n",
    "                               weight_decay=self.weight_decay)\n",
    "        \n",
    "        lr_scheduler = optim.lr_scheduler.StepLR(optimizer, \n",
    "                                                 step_size=self.lr_decay_step_size, \n",
    "                                                 gamma=self.lr_decay)\n",
    "\n",
    "        return {'optimizer': optimizer, 'lr_scheduler': lr_scheduler}\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def forecast(self: NHITS, Y_df: pd.DataFrame, X_df: pd.DataFrame = None, S_df: pd.DataFrame = None, \n",
    "                batch_size: int =1, trainer: pl.Trainer =None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Method for forecasting self.n_time_out periods after last timestamp of Y_df.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    Y_df: pd.DataFrame\n",
    "        Dataframe with target time-series data, needs 'unique_id','ds' and 'y' columns.\n",
    "    X_df: pd.DataFrame\n",
    "        Dataframe with exogenous time-series data, needs 'unique_id' and 'ds' columns.\n",
    "        Note that 'unique_id' and 'ds' must match Y_df plus the forecasting horizon.\n",
    "    S_df: pd.DataFrame\n",
    "        Dataframe with static data, needs 'unique_id' column.\n",
    "    bath_size: int\n",
    "        Batch size for forecasting.\n",
    "    trainer: pl.Trainer\n",
    "        Trainer object for model training and evaluation.\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    forecast_df: pd.DataFrame\n",
    "        Dataframe with forecasts.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add forecast dates to Y_df\n",
    "    Y_df['ds'] = pd.to_datetime(Y_df['ds'])\n",
    "    if X_df is not None:\n",
    "        X_df['ds'] = pd.to_datetime(X_df['ds'])\n",
    "    forecast_dates = pd.date_range(Y_df['ds'].max(), periods=self.n_time_out+1, freq=self.frequency)[1:]\n",
    "    index = pd.MultiIndex.from_product([Y_df['unique_id'].unique(), forecast_dates], names=['unique_id', 'ds'])\n",
    "    forecast_df = pd.DataFrame({'y':[0]}, index=index).reset_index()\n",
    "\n",
    "    Y_df = Y_df.append(forecast_df).sort_values(['unique_id','ds']).reset_index(drop=True)\n",
    "\n",
    "    # Dataset, loader and trainer\n",
    "    dataset = WindowsDataset(S_df=S_df, Y_df=Y_df, X_df=X_df,\n",
    "                                mask_df=None, f_cols=[],\n",
    "                                input_size=self.n_time_in,\n",
    "                                output_size=self.n_time_out,\n",
    "                                sample_freq=1,\n",
    "                                complete_windows=True,\n",
    "                                ds_in_test=self.n_time_out,\n",
    "                                is_test=True,\n",
    "                                verbose=True)\n",
    "\n",
    "    loader = TimeSeriesLoader(dataset=dataset,\n",
    "                                batch_size=batch_size,\n",
    "                                shuffle=False)\n",
    "\n",
    "    if trainer is None:\n",
    "        gpus = -1 if t.cuda.is_available() else 0\n",
    "        trainer = pl.Trainer(progress_bar_refresh_rate=1,\n",
    "                             gpus=gpus,\n",
    "                             logger=False)\n",
    "\n",
    "    # Forecast\n",
    "    outputs = trainer.predict(self, loader)\n",
    "\n",
    "    # Process forecast and include in forecast_df\n",
    "    _, forecast, _ = [t.cat(output).cpu().numpy() for output in zip(*outputs)]\n",
    "    forecast_df['y'] = forecast.flatten()\n",
    "\n",
    "    return forecast_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-HITS Usage Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from neuralforecast.data.datasets.epf import EPF\n",
    "from neuralforecast.data.tsloader import TimeSeriesLoader\n",
    "\n",
    "import pylab as plt\n",
    "from pylab import rcParams\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "\n",
    "FONTSIZE = 19\n",
    "\n",
    "# Load and plot data\n",
    "Y_df, X_df, S_df = EPF.load_groups(directory='./data', groups=['NP','FR'])\n",
    "\n",
    "fig = plt.figure(figsize=(15, 6))\n",
    "plt.plot(Y_df[Y_df['unique_id']=='NP'].ds, Y_df[Y_df['unique_id']=='NP'].y.values, color='#628793', linewidth=0.4)\n",
    "plt.ylabel('Price [EUR/MWh]', fontsize=19)\n",
    "plt.xlabel('Date', fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declare Model and Data Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Architecture parameters\n",
    "mc = {}\n",
    "mc['model'] = 'n-hits'\n",
    "mc['mode'] = 'simple'\n",
    "mc['activation'] = 'SELU'\n",
    "\n",
    "mc['n_time_in'] = 24*3\n",
    "mc['n_time_out'] = 24\n",
    "mc['n_x_hidden'] = 8\n",
    "mc['n_s_hidden'] = 0\n",
    "\n",
    "mc['stack_types'] = ['identity', 'identity', 'identity']\n",
    "mc['constant_n_blocks'] = 1\n",
    "mc['constant_n_layers'] = 2\n",
    "mc['constant_n_mlp_units'] = 256\n",
    "mc['n_pool_kernel_size'] = [4, 2, 1]\n",
    "mc['n_freq_downsample'] = [24, 12, 1]\n",
    "mc['pooling_mode'] = 'max'\n",
    "mc['interpolation_mode'] = 'linear'\n",
    "mc['shared_weights'] = False\n",
    "\n",
    "# Optimization and regularization parameters\n",
    "mc['initialization'] = 'lecun_normal'\n",
    "mc['learning_rate'] = 0.001\n",
    "mc['batch_size'] = 1\n",
    "mc['n_windows'] = 32\n",
    "mc['lr_decay'] = 0.5\n",
    "mc['lr_decay_step_size'] = 2\n",
    "mc['max_epochs'] = 1\n",
    "mc['max_steps'] = None\n",
    "mc['early_stop_patience'] = 20\n",
    "mc['eval_freq'] = 500\n",
    "mc['batch_normalization'] = False\n",
    "mc['dropout_prob_theta'] = 0.0\n",
    "mc['dropout_prob_exogenous'] = 0.0\n",
    "mc['weight_decay'] = 0\n",
    "mc['loss_train'] = 'MAE'\n",
    "mc['loss_hypar'] = 0.5\n",
    "mc['loss_valid'] = mc['loss_train']\n",
    "mc['random_seed'] = 1\n",
    "\n",
    "# Data Parameters\n",
    "mc['idx_to_sample_freq'] = 1\n",
    "mc['val_idx_to_sample_freq'] = 1\n",
    "mc['n_val_weeks'] = 52\n",
    "mc['normalizer_y'] = None\n",
    "mc['normalizer_x'] = 'median'\n",
    "mc['complete_windows'] = False\n",
    "mc['frequency'] = 'H'\n",
    "\n",
    "print(65*'=')\n",
    "print(pd.Series(mc))\n",
    "print(65*'=')\n",
    "\n",
    "mc['n_mlp_units'] = len(mc['stack_types']) * [ mc['constant_n_layers'] * [int(mc['constant_n_mlp_units'])] ]\n",
    "mc['n_blocks'] =  len(mc['stack_types']) * [ mc['constant_n_blocks'] ]\n",
    "mc['n_layers'] =  len(mc['stack_types']) * [ mc['constant_n_layers'] ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate Loaders and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralforecast.experiments.utils import create_datasets\n",
    "\n",
    "train_dataset, val_dataset, test_dataset, scaler_y = create_datasets(mc=mc,\n",
    "                                                                     S_df=S_df, Y_df=Y_df, X_df=X_df,\n",
    "                                                                     f_cols=['Exogenous1', 'Exogenous2'],\n",
    "                                                                     ds_in_val=294*24,\n",
    "                                                                     ds_in_test=728*24)\n",
    "\n",
    "train_loader = TimeSeriesLoader(dataset=train_dataset,\n",
    "                                batch_size=int(mc['batch_size']),\n",
    "                                n_windows=mc['n_windows'],\n",
    "                                shuffle=True)\n",
    "\n",
    "val_loader = TimeSeriesLoader(dataset=val_dataset,\n",
    "                              batch_size=int(mc['batch_size']),\n",
    "                              shuffle=False)\n",
    "\n",
    "test_loader = TimeSeriesLoader(dataset=test_dataset,\n",
    "                               batch_size=int(mc['batch_size']),\n",
    "                               shuffle=False)\n",
    "\n",
    "mc['n_x'], mc['n_s'] = train_dataset.get_n_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NHITS(n_time_in=int(mc['n_time_in']),\n",
    "              n_time_out=int(mc['n_time_out']),\n",
    "              n_x=mc['n_x'],\n",
    "              n_s=mc['n_s'],\n",
    "              n_s_hidden=int(mc['n_s_hidden']),\n",
    "              n_x_hidden=int(mc['n_x_hidden']),\n",
    "              shared_weights=mc['shared_weights'],\n",
    "              initialization=mc['initialization'],\n",
    "              activation=mc['activation'],\n",
    "              stack_types=mc['stack_types'],\n",
    "              n_blocks=mc['n_blocks'],\n",
    "              n_layers=mc['n_layers'],\n",
    "              n_mlp_units=mc['n_mlp_units'],\n",
    "              n_pool_kernel_size=mc['n_pool_kernel_size'],\n",
    "              n_freq_downsample=mc['n_freq_downsample'],\n",
    "              pooling_mode=mc['pooling_mode'],\n",
    "              interpolation_mode=mc['interpolation_mode'],\n",
    "              batch_normalization = mc['batch_normalization'],\n",
    "              dropout_prob_theta=mc['dropout_prob_theta'],\n",
    "              learning_rate=float(mc['learning_rate']),\n",
    "              lr_decay=float(mc['lr_decay']),\n",
    "              lr_decay_step_size=float(mc['lr_decay_step_size']),\n",
    "              weight_decay=mc['weight_decay'],\n",
    "              loss_train=mc['loss_train'],\n",
    "              loss_hypar=float(mc['loss_hypar']),\n",
    "              loss_valid=mc['loss_valid'],\n",
    "              frequency=mc['frequency'],\n",
    "              random_seed=int(mc['random_seed']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", \n",
    "                               min_delta=1e-4, \n",
    "                               patience=mc['early_stop_patience'],\n",
    "                               verbose=False,\n",
    "                               mode=\"min\")\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=mc['max_epochs'], \n",
    "                     max_steps=mc['max_steps'],\n",
    "                     gradient_clip_val=1.0,\n",
    "                     progress_bar_refresh_rate=10, \n",
    "                     log_every_n_steps=500, \n",
    "                     check_val_every_n_epoch=1,\n",
    "                     callbacks=[early_stopping])\n",
    "\n",
    "trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.return_decomposition = False\n",
    "outputs = trainer.predict(model, val_loader)\n",
    "\n",
    "print(\"outputs[0][0].shape\", outputs[0][0].shape)\n",
    "print(\"outputs[0][1].shape\", outputs[0][1].shape)\n",
    "print(\"outputs[0][2].shape\", outputs[0][2].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_forecast_df = Y_df[Y_df['ds']<'2016-12-27']\n",
    "Y_forecast_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_forecast_df = X_df[X_df['ds']<'2016-12-28']\n",
    "X_forecast_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.return_decomposition = False\n",
    "forecast_df = model.forecast(Y_df=Y_forecast_df, X_df=X_forecast_df, S_df=S_df, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
