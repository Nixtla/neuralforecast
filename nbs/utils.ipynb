{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Data\n",
    "\n",
    "> The `core.NeuralForecast` class allows you to efficiently fit multiple `NeuralForecast` models for large sets of time series. It operates with pandas DataFrame `df` that identifies individual series and datestamps with the `unique_id` and `ds` columns, and the `y` column denotes the target time series variable. To assist development, we declare useful datasets that we use throughout all `NeuralForecast`'s unit tests.<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import random\n",
    "from itertools import chain\n",
    "from typing import List, Union, Optional, Tuple\n",
    "from utilsforecast.compat import DFType\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nbdev.showdoc import add_docs, show_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Synthetic Panel Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def generate_series(n_series: int,\n",
    "                    freq: str = 'D',\n",
    "                    min_length: int = 50,\n",
    "                    max_length: int = 500,\n",
    "                    n_temporal_features: int = 0,\n",
    "                    n_static_features: int = 0,\n",
    "                    equal_ends: bool = False,\n",
    "                    seed: int = 0) -> pd.DataFrame:\n",
    "    \"\"\"Generate Synthetic Panel Series.\n",
    "\n",
    "    Generates `n_series` of frequency `freq` of different lengths in the interval [`min_length`, `max_length`].\n",
    "    If `n_temporal_features > 0`, then each serie gets temporal features with random values.\n",
    "    If `n_static_features > 0`, then a static dataframe is returned along the temporal dataframe.\n",
    "    If `equal_ends == True` then all series end at the same date.\n",
    "\n",
    "    **Parameters:**<br>\n",
    "    `n_series`: int, number of series for synthetic panel.<br>\n",
    "    `min_length`: int, minimal length of synthetic panel's series.<br>\n",
    "    `max_length`: int, minimal length of synthetic panel's series.<br>\n",
    "    `n_temporal_features`: int, default=0, number of temporal exogenous variables for synthetic panel's series.<br>\n",
    "    `n_static_features`: int, default=0, number of static exogenous variables for synthetic panel's series.<br>\n",
    "    `equal_ends`: bool, if True, series finish in the same date stamp `ds`.<br>\n",
    "    `freq`: str, frequency of the data, [panda's available frequencies](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases).<br>\n",
    "\n",
    "    **Returns:**<br>\n",
    "    `freq`: pandas.DataFrame, synthetic panel with columns [`unique_id`, `ds`, `y`] and exogenous.\n",
    "    \"\"\"\n",
    "    seasonalities = {'D': 7, 'M': 12}\n",
    "    season = seasonalities[freq]\n",
    "\n",
    "    rng = np.random.RandomState(seed)\n",
    "    series_lengths = rng.randint(min_length, max_length + 1, n_series)\n",
    "    total_length = series_lengths.sum()\n",
    "\n",
    "    dates = pd.date_range('2000-01-01', periods=max_length, freq=freq).values\n",
    "    uids = [\n",
    "        np.repeat(i, serie_length) for i, serie_length in enumerate(series_lengths)\n",
    "    ]\n",
    "    if equal_ends:\n",
    "        ds = [dates[-serie_length:] for serie_length in series_lengths]\n",
    "    else:\n",
    "        ds = [dates[:serie_length] for serie_length in series_lengths]\n",
    "\n",
    "    y = np.arange(total_length) % season + rng.rand(total_length) * 0.5\n",
    "    temporal_df = pd.DataFrame(dict(unique_id=chain.from_iterable(uids),\n",
    "                                    ds=chain.from_iterable(ds),\n",
    "                                    y=y))\n",
    "\n",
    "    random.seed(seed)\n",
    "    for i in range(n_temporal_features):\n",
    "        random.seed(seed)\n",
    "        temporal_values = [\n",
    "            [random.randint(0, 100)] * serie_length for serie_length in series_lengths\n",
    "        ]\n",
    "        temporal_df[f'temporal_{i}'] = np.hstack(temporal_values)\n",
    "        temporal_df[f'temporal_{i}'] = temporal_df[f'temporal_{i}'].astype('category')\n",
    "        if i == 0:\n",
    "            temporal_df['y'] = temporal_df['y'] * \\\n",
    "                                  (1 + temporal_df[f'temporal_{i}'].cat.codes)\n",
    "\n",
    "    temporal_df['unique_id'] = temporal_df['unique_id'].astype('category')\n",
    "    temporal_df['unique_id'] = temporal_df['unique_id'].cat.as_ordered()\n",
    "\n",
    "    if n_static_features > 0:\n",
    "        static_features = np.random.uniform(low=0.0, high=1.0, \n",
    "                        size=(n_series, n_static_features))\n",
    "        static_df = pd.DataFrame.from_records(static_features, \n",
    "                           columns = [f'static_{i}'for i in  range(n_static_features)])\n",
    "        \n",
    "        static_df['unique_id'] = np.arange(n_series)\n",
    "        static_df['unique_id'] = static_df['unique_id'].astype('category')\n",
    "        static_df['unique_id'] = static_df['unique_id'].cat.as_ordered()\n",
    "\n",
    "        return temporal_df, static_df\n",
    "\n",
    "    return temporal_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(generate_series, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_panel = generate_series(n_series=2)\n",
    "synthetic_panel.groupby('unique_id').head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_df, static_df = generate_series(n_series=1000, n_static_features=2,\n",
    "                                         n_temporal_features=4, equal_ends=False)\n",
    "static_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. AirPassengers Data\n",
    "\n",
    "The classic Box & Jenkins airline data. Monthly totals of international airline passengers, 1949 to 1960.\n",
    "\n",
    "It has been used as a reference on several forecasting libraries, since it is a series that shows clear trends and seasonalities it offers a nice opportunity to quickly showcase a model's predictions performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "AirPassengers = np.array([112., 118., 132., 129., 121., 135., 148., 148., 136., 119., 104.,\n",
    "                          118., 115., 126., 141., 135., 125., 149., 170., 170., 158., 133.,\n",
    "                          114., 140., 145., 150., 178., 163., 172., 178., 199., 199., 184.,\n",
    "                          162., 146., 166., 171., 180., 193., 181., 183., 218., 230., 242.,\n",
    "                          209., 191., 172., 194., 196., 196., 236., 235., 229., 243., 264.,\n",
    "                          272., 237., 211., 180., 201., 204., 188., 235., 227., 234., 264.,\n",
    "                          302., 293., 259., 229., 203., 229., 242., 233., 267., 269., 270.,\n",
    "                          315., 364., 347., 312., 274., 237., 278., 284., 277., 317., 313.,\n",
    "                          318., 374., 413., 405., 355., 306., 271., 306., 315., 301., 356.,\n",
    "                          348., 355., 422., 465., 467., 404., 347., 305., 336., 340., 318.,\n",
    "                          362., 348., 363., 435., 491., 505., 404., 359., 310., 337., 360.,\n",
    "                          342., 406., 396., 420., 472., 548., 559., 463., 407., 362., 405.,\n",
    "                          417., 391., 419., 461., 472., 535., 622., 606., 508., 461., 390.,\n",
    "                          432.], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "AirPassengersDF = pd.DataFrame({'unique_id': np.ones(len(AirPassengers)),\n",
    "                                'ds': pd.date_range(start='1949-01-01',\n",
    "                                                    periods=len(AirPassengers), freq=pd.offsets.MonthEnd()),\n",
    "                                'y': AirPassengers})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AirPassengersDF.head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We are going to plot the ARIMA predictions, and the prediction intervals.\n",
    "fig, ax = plt.subplots(1, 1, figsize = (20, 7))\n",
    "plot_df = AirPassengersDF.set_index('ds')\n",
    "\n",
    "plot_df[['y']].plot(ax=ax, linewidth=2)\n",
    "ax.set_title('AirPassengers Forecast', fontsize=22)\n",
    "ax.set_ylabel('Monthly Passengers', fontsize=20)\n",
    "ax.set_xlabel('Timestamp [t]', fontsize=20)\n",
    "ax.legend(prop={'size': 15})\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_static_features = 3\n",
    "n_series = 5\n",
    "\n",
    "static_features = np.random.uniform(low=0.0, high=1.0, \n",
    "                        size=(n_series, n_static_features))\n",
    "static_df = pd.DataFrame.from_records(static_features, \n",
    "                   columns = [f'static_{i}'for i in  range(n_static_features)])\n",
    "static_df['unique_id'] = np.arange(n_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Panel AirPassengers Data\n",
    "\n",
    "Extension to classic Box & Jenkins airline data. Monthly totals of international airline passengers, 1949 to 1960.\n",
    "\n",
    "It includes two series with static, temporal and future exogenous variables, that can help to explore the performance of models like `NBEATSx` and `TFT`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# Declare Panel Data\n",
    "unique_id = np.concatenate([['Airline1']*len(AirPassengers), ['Airline2']*len(AirPassengers)])\n",
    "ds = np.tile(\n",
    "    pd.date_range(\n",
    "        start='1949-01-01', periods=len(AirPassengers), freq=pd.offsets.MonthEnd()\n",
    "    ).to_numpy(), \n",
    "    2,\n",
    ")\n",
    "y = np.concatenate([AirPassengers, AirPassengers+300])\n",
    "\n",
    "AirPassengersPanel = pd.DataFrame({'unique_id': unique_id, 'ds': ds, 'y': y})\n",
    "\n",
    "# For future exogenous variables\n",
    "# Declare SeasonalNaive12 and fill first 12 values with y\n",
    "snaive = AirPassengersPanel.groupby('unique_id')['y'].shift(periods=12).reset_index(drop=True)\n",
    "AirPassengersPanel['trend'] = range(len(AirPassengersPanel))\n",
    "AirPassengersPanel['y_[lag12]'] = snaive.fillna(AirPassengersPanel['y'])\n",
    "\n",
    "# Declare Static Data\n",
    "unique_id = np.array(['Airline1', 'Airline2'])\n",
    "airline1_dummy = [0, 1]\n",
    "airline2_dummy = [1, 0]\n",
    "AirPassengersStatic = pd.DataFrame({'unique_id': unique_id,\n",
    "                                    'airline1': airline1_dummy,\n",
    "                                    'airline2': airline2_dummy})\n",
    "\n",
    "AirPassengersPanel.groupby('unique_id').tail(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize = (20, 7))\n",
    "plot_df = AirPassengersPanel.set_index('ds')\n",
    "\n",
    "plot_df.groupby('unique_id')['y'].plot(legend=True)\n",
    "ax.set_title('AirPassengers Panel Data', fontsize=22)\n",
    "ax.set_ylabel('Monthly Passengers', fontsize=20)\n",
    "ax.set_xlabel('Timestamp [t]', fontsize=20)\n",
    "ax.legend(title='unique_id', prop={'size': 15})\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize = (20, 7))\n",
    "plot_df = AirPassengersPanel[AirPassengersPanel.unique_id=='Airline1'].set_index('ds')\n",
    "\n",
    "plot_df[['y', 'trend', 'y_[lag12]']].plot(ax=ax, linewidth=2)\n",
    "ax.set_title('Box-Cox AirPassengers Data', fontsize=22)\n",
    "ax.set_ylabel('Monthly Passengers', fontsize=20)\n",
    "ax.set_xlabel('Timestamp [t]', fontsize=20)\n",
    "ax.legend(prop={'size': 15})\n",
    "ax.grid()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Time Features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have developed a utility that generates normalized calendar features for use as absolute positional embeddings in Transformer-based models. These embeddings capture seasonal patterns in time series data and can be easily incorporated into the model architecture. Additionally, the features can be used as exogenous variables in other models to inform them of calendar patterns in the data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**References**<br>\n",
    "- [Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, Wancai Zhang. \"Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting\"](https://arxiv.org/abs/2012.07436)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TimeFeature:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, index: pd.DatetimeIndex):\n",
    "        return print('Overwrite with corresponding feature')\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + \"()\"\n",
    "\n",
    "class SecondOfMinute(TimeFeature):\n",
    "    \"\"\"Minute of hour encoded as value between [-0.5, 0.5]\"\"\"\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return index.second / 59.0 - 0.5\n",
    "\n",
    "class MinuteOfHour(TimeFeature):\n",
    "    \"\"\"Minute of hour encoded as value between [-0.5, 0.5]\"\"\"\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return index.minute / 59.0 - 0.5\n",
    "\n",
    "class HourOfDay(TimeFeature):\n",
    "    \"\"\"Hour of day encoded as value between [-0.5, 0.5]\"\"\"\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return index.hour / 23.0 - 0.5\n",
    "\n",
    "class DayOfWeek(TimeFeature):\n",
    "    \"\"\"Hour of day encoded as value between [-0.5, 0.5]\"\"\"\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return index.dayofweek / 6.0 - 0.5\n",
    "\n",
    "class DayOfMonth(TimeFeature):\n",
    "    \"\"\"Day of month encoded as value between [-0.5, 0.5]\"\"\"\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return (index.day - 1) / 30.0 - 0.5\n",
    "\n",
    "class DayOfYear(TimeFeature):\n",
    "    \"\"\"Day of year encoded as value between [-0.5, 0.5]\"\"\"\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return (index.dayofyear - 1) / 365.0 - 0.5\n",
    "\n",
    "class MonthOfYear(TimeFeature):\n",
    "    \"\"\"Month of year encoded as value between [-0.5, 0.5]\"\"\"\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return (index.month - 1) / 11.0 - 0.5\n",
    "\n",
    "class WeekOfYear(TimeFeature):\n",
    "    \"\"\"Week of year encoded as value between [-0.5, 0.5]\"\"\"\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return (index.week - 1) / 52.0 - 0.5\n",
    "\n",
    "def time_features_from_frequency_str(freq_str: str) -> List[TimeFeature]:\n",
    "    \"\"\"\n",
    "    Returns a list of time features that will be appropriate for the given frequency string.\n",
    "    Parameters\n",
    "    ----------\n",
    "    freq_str\n",
    "        Frequency string of the form [multiple][granularity] such as \"12H\", \"5min\", \"1D\" etc.\n",
    "    \"\"\"\n",
    "\n",
    "    if freq_str not in ['Q', 'M', 'MS', 'W', 'D', 'B', 'H', 'T', 'S']:\n",
    "        raise Exception('Frequency not supported')\n",
    "    \n",
    "    if freq_str in ['Q','M', 'MS']:\n",
    "        return [cls() for cls in [MonthOfYear]]\n",
    "    elif freq_str == 'W':\n",
    "        return [cls() for cls in [DayOfMonth, WeekOfYear]]\n",
    "    elif freq_str in ['D','B']:\n",
    "        return [cls() for cls in [DayOfWeek, DayOfMonth, DayOfYear]]\n",
    "    elif freq_str == 'H':\n",
    "        return [cls() for cls in [HourOfDay, DayOfWeek, DayOfMonth, DayOfYear]]\n",
    "    elif freq_str == 'T':\n",
    "        return [cls() for cls in [MinuteOfHour, HourOfDay, DayOfWeek, DayOfMonth, DayOfYear]]\n",
    "    else:\n",
    "        return [cls() for cls in [SecondOfMinute, MinuteOfHour, HourOfDay, DayOfWeek, DayOfMonth, DayOfYear]]\n",
    "\n",
    "def augment_calendar_df(df, freq='H'):\n",
    "    \"\"\"\n",
    "    > * Q - [month]\n",
    "    > * M - [month]\n",
    "    > * W - [Day of month, week of year]\n",
    "    > * D - [Day of week, day of month, day of year]\n",
    "    > * B - [Day of week, day of month, day of year]\n",
    "    > * H - [Hour of day, day of week, day of month, day of year]\n",
    "    > * T - [Minute of hour*, hour of day, day of week, day of month, day of year]\n",
    "    > * S - [Second of minute, minute of hour, hour of day, day of week, day of month, day of year]\n",
    "    *minute returns a number from 0-3 corresponding to the 15 minute period it falls into.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    freq_map = {\n",
    "        'Q':['month'],\n",
    "        'M':['month'],\n",
    "        'MS':['month'],\n",
    "        'W':['monthday', 'yearweek'],\n",
    "        'D':['weekday','monthday','yearday'],\n",
    "        'B':['weekday','monthday','yearday'],\n",
    "        'H':['dayhour','weekday','monthday','yearday'],\n",
    "        'T':['hourminute','dayhour','weekday','monthday','yearday'],\n",
    "        'S':['minutesecond','hourminute','dayhour','weekday','monthday','yearday']\n",
    "    }\n",
    "\n",
    "    ds_col = pd.to_datetime(df.ds.values)\n",
    "    ds_data = np.vstack([feat(ds_col) for feat in time_features_from_frequency_str(freq)]).transpose(1,0)\n",
    "    ds_data = pd.DataFrame(ds_data, columns=freq_map[freq])\n",
    "    \n",
    "    return pd.concat([df, ds_data], axis=1), freq_map[freq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AirPassengerPanelCalendar, calendar_cols = augment_calendar_df(df=AirPassengersPanel, freq='M')\n",
    "AirPassengerPanelCalendar.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df = AirPassengerPanelCalendar[AirPassengerPanelCalendar.unique_id=='Airline1'].set_index('ds')\n",
    "plt.plot(plot_df['month'])\n",
    "plt.grid()\n",
    "plt.xlabel('Datestamp')\n",
    "plt.ylabel('Normalized Month')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_indexer_raise_missing(idx: pd.Index, vals: List[str]) -> List[int]:\n",
    "    idxs = idx.get_indexer(vals)\n",
    "    missing = [v for i, v in zip(idxs, vals) if i == -1]\n",
    "    if missing:\n",
    "        raise ValueError(f'The following values are missing from the index: {missing}')\n",
    "    return idxs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Prediction Intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class PredictionIntervals:\n",
    "    \"\"\"Class for storing prediction intervals metadata information.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_windows: int = 2,\n",
    "        method: str = \"conformal_distribution\",\n",
    "    ):\n",
    "        \"\"\" \n",
    "        n_windows : int\n",
    "            Number of windows to evaluate.\n",
    "        method : str, default is conformal_distribution\n",
    "            One of the supported methods for the computation of prediction intervals:\n",
    "            conformal_error or conformal_distribution\n",
    "        \"\"\"\n",
    "        if n_windows < 2:\n",
    "            raise ValueError(\n",
    "                \"You need at least two windows to compute conformal intervals\"\n",
    "            )\n",
    "        allowed_methods = [\"conformal_error\", \"conformal_distribution\"]\n",
    "        if method not in allowed_methods:\n",
    "            raise ValueError(f\"method must be one of {allowed_methods}\")\n",
    "        self.n_windows = n_windows\n",
    "        self.method = method\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"PredictionIntervals(n_windows={self.n_windows}, method='{self.method}')\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def add_conformal_distribution_intervals(\n",
    "    model_fcsts: np.array, \n",
    "    cs_df: DFType,\n",
    "    model: str,\n",
    "    cs_n_windows: int,\n",
    "    n_series: int,\n",
    "    horizon: int,\n",
    "    level: Optional[List[Union[int, float]]] = None,\n",
    "    quantiles: Optional[List[float]] = None,\n",
    ") -> Tuple[np.array, List[str]]:\n",
    "    \"\"\"\n",
    "    Adds conformal intervals to a `fcst_df` based on conformal scores `cs_df`.\n",
    "    `level` should be already sorted. This strategy creates forecasts paths\n",
    "    based on errors and calculate quantiles using those paths.\n",
    "    \"\"\"\n",
    "    assert level is not None or quantiles is not None, \"Either level or quantiles must be provided\"\n",
    "    \n",
    "    if quantiles is None and level is not None:\n",
    "        alphas = [100 - lv for lv in level]\n",
    "        cuts = [alpha / 200 for alpha in reversed(alphas)]\n",
    "        cuts.extend(1 - alpha / 200 for alpha in alphas)\n",
    "    elif quantiles is not None:\n",
    "        cuts = quantiles\n",
    "    \n",
    "    scores = cs_df[model].to_numpy().reshape(n_series, cs_n_windows, horizon)\n",
    "    scores = scores.transpose(1, 0, 2)\n",
    "    # restrict scores to horizon\n",
    "    scores = scores[:,:,:horizon]\n",
    "    mean = model_fcsts.reshape(1, n_series, -1)\n",
    "    scores = np.vstack([mean - scores, mean + scores])\n",
    "    scores_quantiles = np.quantile(\n",
    "        scores,\n",
    "        cuts,\n",
    "        axis=0,\n",
    "    )\n",
    "    scores_quantiles = scores_quantiles.reshape(len(cuts), -1).T\n",
    "    if quantiles is None and level is not None:\n",
    "        lo_cols = [f\"{model}-lo-{lv}\" for lv in reversed(level)]\n",
    "        hi_cols = [f\"{model}-hi-{lv}\" for lv in level]\n",
    "        out_cols = lo_cols + hi_cols\n",
    "    elif quantiles is not None:\n",
    "        out_cols = [f\"{model}-ql{q}\" for q in quantiles]\n",
    "\n",
    "    fcsts_with_intervals = np.hstack([model_fcsts, scores_quantiles])\n",
    "\n",
    "    return fcsts_with_intervals, out_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def add_conformal_error_intervals(\n",
    "    model_fcsts: np.array, \n",
    "    cs_df: DFType, \n",
    "    model: str,\n",
    "    cs_n_windows: int,\n",
    "    n_series: int,\n",
    "    horizon: int,\n",
    "    level: Optional[List[Union[int, float]]] = None,\n",
    "    quantiles: Optional[List[float]] = None,\n",
    ") -> Tuple[np.array, List[str]]:\n",
    "    \"\"\"\n",
    "    Adds conformal intervals to a `fcst_df` based on conformal scores `cs_df`.\n",
    "    `level` should be already sorted. This startegy creates prediction intervals\n",
    "    based on the absolute errors.\n",
    "    \"\"\"\n",
    "    assert level is not None or quantiles is not None, \"Either level or quantiles must be provided\"\n",
    "\n",
    "    if quantiles is None and level is not None:\n",
    "        alphas = [100 - lv for lv in level]\n",
    "        cuts = [alpha / 200 for alpha in reversed(alphas)]\n",
    "        cuts.extend(1 - alpha / 200 for alpha in alphas)\n",
    "    elif quantiles is not None:\n",
    "        cuts = quantiles\n",
    "\n",
    "    mean = model_fcsts.ravel()\n",
    "    scores = cs_df[model].to_numpy().reshape(n_series, cs_n_windows, horizon)\n",
    "    scores = scores.transpose(1, 0, 2)\n",
    "    # restrict scores to horizon\n",
    "    scores = scores[:,:,:horizon]\n",
    "    scores_quantiles = np.quantile(\n",
    "        scores,\n",
    "        cuts,\n",
    "        axis=0,\n",
    "    )\n",
    "    scores_quantiles = scores_quantiles.reshape(len(cuts), -1)          \n",
    "\n",
    "    if quantiles is None and level is not None:\n",
    "        lo_cols = [f\"{model}-lo-{lv}\" for lv in reversed(level)]\n",
    "        hi_cols = [f\"{model}-hi-{lv}\" for lv in level]\n",
    "        out_cols = lo_cols + hi_cols\n",
    "    else:\n",
    "        out_cols = [f\"{model}-ql{q}\" for q in cuts]\n",
    "    \n",
    "    scores_quantiles_ls = []\n",
    "    for i, q in enumerate(cuts):\n",
    "        if q < 0.5:\n",
    "            scores_quantiles_ls.append(mean - scores_quantiles[::-1][i])\n",
    "        elif q > 0.5:\n",
    "            scores_quantiles_ls.append(mean + scores_quantiles[i])\n",
    "        else:\n",
    "            scores_quantiles_ls.append(mean)\n",
    "    scores_quantiles = np.vstack(scores_quantiles_ls).T    \n",
    "\n",
    "    fcsts_with_intervals = np.hstack([model_fcsts, scores_quantiles])\n",
    "\n",
    "    return fcsts_with_intervals, out_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_prediction_interval_method(method: str):\n",
    "    available_methods = {\n",
    "        \"conformal_distribution\": add_conformal_distribution_intervals,\n",
    "        \"conformal_error\": add_conformal_error_intervals,\n",
    "    }\n",
    "    if method not in available_methods.keys():\n",
    "        raise ValueError(\n",
    "            f\"prediction intervals method {method} not supported \"\n",
    "            f'please choose one of {\", \".join(available_methods.keys())}'\n",
    "        )\n",
    "    return available_methods[method]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def level_to_quantiles(level: List[Union[int, float]]) -> List[float]:\n",
    "    \"\"\"\n",
    "    Converts a list of levels to a list of quantiles.\n",
    "    \"\"\"\n",
    "    level_set = set(level)\n",
    "    return sorted(list(set(sum([[(50 - l / 2) / 100, (50 + l / 2) / 100] for l in level_set], []))))\n",
    "\n",
    "def quantiles_to_level(quantiles: List[float]) -> List[Union[int, float]]:\n",
    "    \"\"\"\n",
    "    Converts a list of quantiles to a list of levels.\n",
    "    \"\"\"\n",
    "    quantiles_set = set(quantiles)\n",
    "    return sorted(set([int(round(100 - 200 * (q * (q < 0.5) + (1 - q) * (q >= 0.5)), 2)) for q in quantiles_set]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# Test level_to_quantiles\n",
    "level_base = [80, 90]\n",
    "quantiles_base = [0.05, 0.1, 0.9, 0.95]\n",
    "quantiles = level_to_quantiles(level_base)\n",
    "level = quantiles_to_level(quantiles_base)\n",
    "\n",
    "assert quantiles == quantiles_base\n",
    "assert level == level_base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Explainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# class ShapModelWrapper(torch.nn.Module):\n",
    "#     \"\"\"\n",
    "#     SHAP wrapper model that converts flattened tensor input to dictionary format\n",
    "#     and handles multiple series dynamically.\n",
    "#     \"\"\"\n",
    "#     def __init__(self, original_model, train_df, static_df):\n",
    "#         super().__init__()\n",
    "#         self.original_model = original_model\n",
    "#         self.train_df = train_df\n",
    "#         self.static_df = static_df\n",
    "#         self.futr_exog_cols = original_model.futr_exog_list\n",
    "#         self.hist_exog_cols = original_model.hist_exog_list\n",
    "#         self.stat_exog_cols = original_model.stat_exog_list\n",
    "#         self.input_size = original_model.input_size\n",
    "#         self.h = original_model.h\n",
    "        \n",
    "#         # Calculate input dimensions\n",
    "#         self.n_futr_features = len(self.futr_exog_cols) * (self.input_size + self.h) if self.futr_exog_cols else 0\n",
    "#         self.n_hist_exog_features = len(self.hist_exog_cols) * self.input_size if self.hist_exog_cols else 0\n",
    "#         self.n_hist_target_features = self.input_size\n",
    "#         self.n_series_features = 1  # unique_id encoded as integer\n",
    "        \n",
    "#         # Create mapping for unique_ids to static features (only if static features exist)\n",
    "#         self.static_mapping = {}\n",
    "#         if self.stat_exog_cols and static_df is not None:\n",
    "#             for unique_id in static_df['unique_id'].unique():\n",
    "#                 static_values = static_df[static_df['unique_id'] == unique_id][self.stat_exog_cols].values[0]\n",
    "#                 self.static_mapping[unique_id] = torch.tensor(static_values, dtype=torch.float32)\n",
    "        \n",
    "#         # Create unique_id to integer mapping using training data\n",
    "#         available_unique_ids = sorted(self.train_df['unique_id'].unique())\n",
    "#         self.unique_id_to_int = {uid: i for i, uid in enumerate(available_unique_ids)}\n",
    "#         self.int_to_unique_id = {i: uid for uid, i in self.unique_id_to_int.items()}\n",
    "\n",
    "#     def forward(self, X_flat):\n",
    "#         \"\"\"\n",
    "#         Convert flattened tensor input to dictionary format and call original model\n",
    "#         X_flat: [batch_size, n_total_features] where features include series_id + futr_exog + hist_exog + hist_target\n",
    "#         \"\"\"\n",
    "#         batch_size = X_flat.shape[0]\n",
    "        \n",
    "#         # Split the input tensor\n",
    "#         idx = 0\n",
    "        \n",
    "#         # Series identifier (first feature)\n",
    "#         series_ids = X_flat[:, idx:idx + self.n_series_features].long().flatten()\n",
    "#         idx += self.n_series_features\n",
    "        \n",
    "#         # Future exogenous features (varying or None)\n",
    "#         if self.futr_exog_cols:\n",
    "#             futr_flat = X_flat[:, idx:idx + self.n_futr_features]\n",
    "#             idx += self.n_futr_features\n",
    "#             futr_exog = futr_flat.reshape(batch_size, self.input_size + self.h, len(self.futr_exog_cols))\n",
    "#         else:\n",
    "#             futr_exog = None\n",
    "        \n",
    "#         # Historical exogenous features (varying or None)\n",
    "#         if self.hist_exog_cols:\n",
    "#             hist_exog_flat = X_flat[:, idx:idx + self.n_hist_exog_features]\n",
    "#             hist_exog = hist_exog_flat.reshape(batch_size, self.input_size, len(self.hist_exog_cols))\n",
    "#             idx += self.n_hist_exog_features\n",
    "#         else:\n",
    "#             hist_exog = None\n",
    "        \n",
    "#         # Historical target values (always present)\n",
    "#         hist_target_flat = X_flat[:, idx:idx + self.n_hist_target_features]\n",
    "#         insample_y = hist_target_flat.reshape(batch_size, self.input_size)\n",
    "        \n",
    "#         # Static exogenous features (varies by series, None if no static features)\n",
    "#         if self.stat_exog_cols and self.static_mapping:\n",
    "#             stat_exog_batch = []\n",
    "#             for i in range(batch_size):\n",
    "#                 series_int = series_ids[i].item()\n",
    "#                 unique_id = self.int_to_unique_id[series_int]\n",
    "#                 stat_exog_batch.append(self.static_mapping[unique_id])\n",
    "#             stat_exog = torch.stack(stat_exog_batch)\n",
    "#         else:\n",
    "#             stat_exog = None\n",
    "        \n",
    "#         # Create windows_batch dictionary\n",
    "#         windows_batch = {\n",
    "#             'insample_y': insample_y.unsqueeze(-1),\n",
    "#             'futr_exog': futr_exog,\n",
    "#             'hist_exog': hist_exog,\n",
    "#             'stat_exog': stat_exog,\n",
    "#             'insample_mask': torch.ones(batch_size, self.input_size, dtype=torch.bool)\n",
    "#         }\n",
    "        \n",
    "#         # Call original model\n",
    "#         return self.original_model(windows_batch)\n",
    "\n",
    "class ShapModelWrapper:\n",
    "    \"\"\"\n",
    "    SHAP wrapper that uses nf.predict() to properly handle scaling\n",
    "    \"\"\"\n",
    "    def __init__(self, nf_object, model, train_df, static_df, futr_df=None):\n",
    "        self.nf = nf_object  # Already fitted NeuralForecast object\n",
    "        self.model = model\n",
    "        self.train_df = train_df\n",
    "        self.static_df = static_df\n",
    "        self.futr_df = futr_df\n",
    "        self.freq = nf_object.freq\n",
    "        \n",
    "        self.futr_exog_cols = model.futr_exog_list\n",
    "        self.hist_exog_cols = model.hist_exog_list\n",
    "        self.stat_exog_cols = model.stat_exog_list\n",
    "        self.input_size = model.input_size\n",
    "        self.h = model.h\n",
    "        self.model_alias = model.alias or type(model).__name__\n",
    "        self.model.trainer_kwargs['logger'] = False\n",
    "        \n",
    "        # Calculate input dimensions\n",
    "        self.n_futr_features = len(self.futr_exog_cols) * (self.input_size + self.h) if self.futr_exog_cols else 0\n",
    "        self.n_hist_exog_features = len(self.hist_exog_cols) * self.input_size if self.hist_exog_cols else 0\n",
    "        self.n_hist_target_features = self.input_size\n",
    "        self.n_series_features = 1  # unique_id encoded as integer\n",
    "        \n",
    "        # Create unique_id to integer mapping\n",
    "        available_unique_ids = sorted(self.train_df['unique_id'].unique())\n",
    "        self.unique_id_to_int = {uid: i for i, uid in enumerate(available_unique_ids)}\n",
    "        self.int_to_unique_id = {i: uid for uid, i in self.unique_id_to_int.items()}\n",
    "        \n",
    "        # Store static features mapping if needed\n",
    "        self.static_mapping = {}\n",
    "        if self.stat_exog_cols and static_df is not None:\n",
    "            for unique_id in static_df['unique_id'].unique():\n",
    "                static_values = static_df[static_df['unique_id'] == unique_id][self.stat_exog_cols].values[0]\n",
    "                self.static_mapping[unique_id] = static_values\n",
    "\n",
    "    def _predict_single_sample(self, x_flat_single, horizon_idx=None):\n",
    "        \"\"\"Process a single sample - extracted from predict_batch for parallelization\"\"\"\n",
    "        # Parse the flattened input\n",
    "        idx = 0\n",
    "        \n",
    "        # Series identifier\n",
    "        series_int = int(x_flat_single[idx])\n",
    "        unique_id = self.int_to_unique_id[series_int]\n",
    "        idx += self.n_series_features\n",
    "        \n",
    "        # Future exogenous features (if present)\n",
    "        temp_futr_df = None\n",
    "        if self.futr_exog_cols and self.futr_df is not None:\n",
    "            futr_flat = x_flat_single[idx:idx + self.n_futr_features]\n",
    "            idx += self.n_futr_features\n",
    "            \n",
    "            # Reshape to (input_size + h, n_futr_features)\n",
    "            futr_reshaped = futr_flat.reshape(self.input_size + self.h, len(self.futr_exog_cols))\n",
    "            \n",
    "            # Get the future part only (last h timesteps)\n",
    "            futr_future = futr_reshaped[-self.h:, :]\n",
    "            \n",
    "            # Create future DataFrame with dummy consecutive dates\n",
    "            series_train_data = self.train_df[self.train_df['unique_id'] == unique_id]\n",
    "            last_train_date = series_train_data['ds'].iloc[-1]\n",
    "\n",
    "            if pd.api.types.is_datetime64_any_dtype(series_train_data['ds']):\n",
    "                future_dates = pd.date_range(start=last_train_date, periods=self.h + 1, freq=self.freq)[1:]\n",
    "            else:\n",
    "                future_dates = np.arange(last_train_date + 1, last_train_date + self.h + 1)\n",
    "            \n",
    "            temp_futr_df = pd.DataFrame(futr_future, columns=self.futr_exog_cols)\n",
    "            temp_futr_df['ds'] = future_dates[:self.h]\n",
    "            temp_futr_df['unique_id'] = unique_id\n",
    "        \n",
    "        # Create a copy of ALL training data\n",
    "        temp_train_df = self.train_df.copy()\n",
    "        series_mask = temp_train_df['unique_id'] == unique_id\n",
    "        series_indices = temp_train_df[series_mask].index\n",
    "        \n",
    "        # Historical exogenous features (if present)\n",
    "        if self.hist_exog_cols:\n",
    "            hist_exog_flat = x_flat_single[idx:idx + self.n_hist_exog_features]\n",
    "            hist_exog_reshaped = hist_exog_flat.reshape(self.input_size, len(self.hist_exog_cols))\n",
    "            idx += self.n_hist_exog_features\n",
    "            \n",
    "            last_indices = series_indices[-self.input_size:]\n",
    "            for i, col in enumerate(self.hist_exog_cols):\n",
    "                temp_train_df.loc[last_indices, col] = hist_exog_reshaped[:, i]\n",
    "        \n",
    "        # Historical target values\n",
    "        hist_target = x_flat_single[idx:idx + self.n_hist_target_features]\n",
    "        \n",
    "        last_indices = series_indices[-self.input_size:]\n",
    "        temp_train_df.loc[last_indices, 'y'] = hist_target\n",
    "        \n",
    "        # Prepare futr_df for ALL series\n",
    "        if temp_futr_df is not None:\n",
    "            all_futr_df = []\n",
    "            for uid in temp_train_df['unique_id'].unique():\n",
    "                if uid == unique_id:\n",
    "                    all_futr_df.append(temp_futr_df)\n",
    "                else:\n",
    "                    if self.futr_df is not None:\n",
    "                        other_series_futr = self.futr_df[self.futr_df['unique_id'] == uid].copy()\n",
    "                        if len(other_series_futr) > 0:\n",
    "                            other_series_futr = other_series_futr.iloc[:self.h]\n",
    "                            all_futr_df.append(other_series_futr)\n",
    "                        else:\n",
    "                            other_train_data = temp_train_df[temp_train_df['unique_id'] == uid]\n",
    "                            last_date = other_train_data['ds'].iloc[-1]\n",
    "\n",
    "                            if pd.api.types.is_datetime64_any_dtype(other_train_data['ds']):\n",
    "                                future_dates = pd.date_range(start=last_date, periods=self.h + 1, freq=self.freq)[1:]\n",
    "                            else:\n",
    "                                future_dates = np.arange(last_date + 1, last_date + self.h + 1)\n",
    "                            \n",
    "                            dummy_futr = pd.DataFrame()\n",
    "                            for col in self.futr_exog_cols:\n",
    "                                dummy_futr[col] = np.zeros(self.h)\n",
    "                            dummy_futr['ds'] = future_dates[:self.h]\n",
    "                            dummy_futr['unique_id'] = uid\n",
    "                            all_futr_df.append(dummy_futr)\n",
    "            \n",
    "            combined_futr_df = pd.concat(all_futr_df, ignore_index=True) if all_futr_df else None\n",
    "        else:\n",
    "            combined_futr_df = None\n",
    "        \n",
    "        # Use the already fitted NeuralForecast object to predict\n",
    "        forecast = self.nf.predict(df=temp_train_df, static_df=self.static_df, futr_df=combined_futr_df)\n",
    "        \n",
    "        # Extract predictions for this series\n",
    "        series_forecast = forecast[forecast['unique_id'] == unique_id].reset_index(drop=True)\n",
    "        \n",
    "        if len(series_forecast) == 0:\n",
    "            raise ValueError(f\"No predictions found for series {unique_id}\")\n",
    "        \n",
    "        if self.model_alias not in series_forecast.columns:\n",
    "            model_cols = [col for col in series_forecast.columns if col not in ['unique_id', 'ds']]\n",
    "            if model_cols:\n",
    "                actual_model_col = model_cols[0]\n",
    "            else:\n",
    "                raise ValueError(f\"Model column {self.model_alias} not found in forecast columns: {series_forecast.columns.tolist()}\")\n",
    "        else:\n",
    "            actual_model_col = self.model_alias\n",
    "        \n",
    "        if horizon_idx is not None:\n",
    "            if horizon_idx >= len(series_forecast):\n",
    "                raise IndexError(f\"horizon_idx {horizon_idx} is out of bounds. Only {len(series_forecast)} horizons available.\")\n",
    "            pred = series_forecast[actual_model_col].iloc[horizon_idx]\n",
    "        else:\n",
    "            pred = series_forecast[actual_model_col].mean()\n",
    "        \n",
    "        return pred\n",
    "\n",
    "\n",
    "    def predict_batch(self, X_flat, horizon_idx=None):\n",
    "        \"\"\"\n",
    "        Convert flattened input to DataFrames and use nf.predict()\n",
    "        Now with parallel processing for better performance\n",
    "        \"\"\"\n",
    "        batch_size = X_flat.shape[0]\n",
    "        \n",
    "        # Use ThreadPoolExecutor for parallel processing\n",
    "        # Adjust max_workers based on your system (2-8 typically works well)\n",
    "        max_workers = min(4, batch_size)  # Don't create more threads than samples\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            # Submit all tasks\n",
    "            futures = [\n",
    "                executor.submit(self._predict_single_sample, X_flat[b], horizon_idx)\n",
    "                for b in range(batch_size)\n",
    "            ]\n",
    "            \n",
    "            # Collect results in order\n",
    "            predictions = [future.result() for future in futures]\n",
    "        \n",
    "        return np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def create_input_tensor_for_series(train_df, unique_id, wrapper_model, futr_df=None):\n",
    "    \"\"\"Create input tensor for a specific series\"\"\"\n",
    "    train_series = train_df[train_df['unique_id'] == unique_id]\n",
    "    \n",
    "    input_components = []\n",
    "    futr_exog_cols = wrapper_model.futr_exog_cols\n",
    "    hist_exog_cols = wrapper_model.hist_exog_cols\n",
    "    input_size = wrapper_model.input_size\n",
    "    \n",
    "    # Series identifier (encoded as integer)\n",
    "    series_int = wrapper_model.unique_id_to_int[unique_id]\n",
    "    input_components.append(np.array([series_int]))\n",
    "    \n",
    "    # Future exogenous features (if they exist) - FULL LENGTH (historical + future)\n",
    "    if futr_exog_cols:\n",
    "        if futr_df is None:\n",
    "            raise ValueError(\"You must pass a futr_df if futr_exog_list is specified.\")\n",
    "        futr_exog_series = futr_df[futr_df['unique_id'] == unique_id].reset_index(drop=True)\n",
    "        # Get historical part\n",
    "        futr_hist_data = train_series[futr_exog_cols].values[-input_size:]\n",
    "        # Get future part  \n",
    "        futr_pred_data = futr_exog_series[futr_exog_cols].values\n",
    "        # Combine and flatten\n",
    "        full_futr_data = np.vstack([futr_hist_data, futr_pred_data]).flatten()\n",
    "        input_components.append(full_futr_data)\n",
    "    \n",
    "    # Historical exogenous features (if they exist)\n",
    "    if hist_exog_cols:\n",
    "        hist_exog_data = train_series[hist_exog_cols].values[-input_size:].flatten()\n",
    "        input_components.append(hist_exog_data)\n",
    "    \n",
    "    # Historical target values (always present)\n",
    "    hist_target_data = train_series['y'].values[-input_size:]\n",
    "    input_components.append(hist_target_data)\n",
    "    \n",
    "    # Combine all features\n",
    "    complete_input = np.concatenate(input_components)\n",
    "    return complete_input.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def create_multi_series_background_data(train_df, wrapper_model, n_samples_per_series=10):\n",
    "    \"\"\"Create background data including samples from all series\"\"\"\n",
    "    background_samples = []\n",
    "    \n",
    "    futr_exog_cols = wrapper_model.futr_exog_cols\n",
    "    hist_exog_cols = wrapper_model.hist_exog_cols\n",
    "    input_size = wrapper_model.input_size\n",
    "    horizon = wrapper_model.h\n",
    "    \n",
    "    for unique_id in train_df['unique_id'].unique():\n",
    "        train_series = train_df[train_df['unique_id'] == unique_id]\n",
    "        \n",
    "        # Determine the range of valid indices\n",
    "        start_idx = input_size\n",
    "        if futr_exog_cols:\n",
    "            end_idx = len(train_series) - horizon + 1\n",
    "        else:\n",
    "            end_idx = len(train_series)\n",
    "        \n",
    "        # Sample points from this series\n",
    "        if end_idx > start_idx:\n",
    "            sample_indices = np.linspace(start_idx, end_idx - 1, \n",
    "                                       min(n_samples_per_series, end_idx - start_idx), \n",
    "                                       dtype=int)\n",
    "            \n",
    "            for i in sample_indices:\n",
    "                sample_components = []\n",
    "                \n",
    "                # Series identifier\n",
    "                series_int = wrapper_model.unique_id_to_int[unique_id]\n",
    "                sample_components.append(np.array([series_int]))\n",
    "                \n",
    "                # Future exogenous features (if they exist) - FULL LENGTH\n",
    "                if futr_exog_cols:\n",
    "                    futr_hist_data = train_series[futr_exog_cols].iloc[i-input_size:i].to_numpy().flatten()\n",
    "                    futr_pred_data = train_series[futr_exog_cols].iloc[i:i + horizon].to_numpy().flatten()\n",
    "                    full_futr_data = np.concatenate([futr_hist_data, futr_pred_data])\n",
    "                    sample_components.append(full_futr_data)\n",
    "                \n",
    "                # Historical exogenous features (if they exist)\n",
    "                if hist_exog_cols:\n",
    "                    hist_exog_window = train_series[hist_exog_cols].iloc[i-input_size:i].to_numpy().flatten()\n",
    "                    sample_components.append(hist_exog_window)\n",
    "                \n",
    "                # Historical target values\n",
    "                hist_target_window = train_series['y'].iloc[i-input_size:i].to_numpy()\n",
    "                sample_components.append(hist_target_window)\n",
    "                \n",
    "                # Combine all features\n",
    "                complete_sample = np.concatenate(sample_components)\n",
    "                background_samples.append(complete_sample)\n",
    "    \n",
    "    return np.array(background_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def model_predict(X, wrapper_model):\n",
    "    \"\"\"Prediction function that takes numpy array and returns predictions\"\"\"\n",
    "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "    with torch.no_grad():\n",
    "        predictions = wrapper_model(X_tensor)\n",
    "    return predictions.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def create_multi_series_feature_names(wrapper_model):\n",
    "    \"\"\"Create feature names for multi-series input\"\"\"\n",
    "    feature_names = ['series_id']\n",
    "    futr_exog_cols = wrapper_model.futr_exog_cols\n",
    "    hist_exog_cols = wrapper_model.hist_exog_cols\n",
    "    input_size = wrapper_model.input_size\n",
    "    horizon = wrapper_model.h\n",
    "    \n",
    "    # Future exogenous features (if they exist) - FULL LENGTH\n",
    "    if futr_exog_cols:\n",
    "        for i in range(input_size):\n",
    "            for col in futr_exog_cols:\n",
    "                feature_names.append(f'{col}_hist_lag{i+1}')\n",
    "        for i in range(horizon):\n",
    "            for col in futr_exog_cols:\n",
    "                feature_names.append(f'{col}_h{i+1}')\n",
    "    \n",
    "    # Historical exogenous features (if they exist)\n",
    "    if hist_exog_cols:\n",
    "        for i in range(input_size):\n",
    "            for col in hist_exog_cols:\n",
    "                feature_names.append(f'{col}_lag{i+1}')\n",
    "    \n",
    "    # Historical target values (always present)\n",
    "    for i in range(input_size):\n",
    "        feature_names.append(f'y_lag{i+1}')\n",
    "    \n",
    "    return feature_names\n",
    "\n",
    "# ======================= NOT USED ========================================== #\n",
    "\n",
    "def group_features_data(X, feature_groups):\n",
    "    \"\"\"\n",
    "    Group features by averaging within each group\n",
    "    X: [n_samples, n_features] including series_id\n",
    "    feature_groups: list of arrays with feature indices\n",
    "    Returns: [n_samples, n_groups + 1] where +1 is for series_id\n",
    "    \"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "    n_groups = len(feature_groups) + 1  # +1 for series_id\n",
    "    # Use float32 for consistency\n",
    "    X_grouped = np.zeros((n_samples, n_groups), dtype=np.float32)\n",
    "    \n",
    "    # Keep series_id as is\n",
    "    X_grouped[:, 0] = X[:, 0].astype(np.float32)\n",
    "    \n",
    "    # Group other features by averaging\n",
    "    for group_idx, feature_indices in enumerate(feature_groups):\n",
    "        X_grouped[:, group_idx + 1] = X[:, feature_indices].mean(axis=1).astype(np.float32)\n",
    "    \n",
    "    return X_grouped\n",
    "\n",
    "def expand_grouped_features(X_grouped, feature_groups, original_dim):\n",
    "    \"\"\"\n",
    "    Expand grouped features back to original dimension\n",
    "    X_grouped: [n_samples, n_groups] including series_id\n",
    "    Returns: [n_samples, original_dim]\n",
    "    \"\"\"\n",
    "    n_samples = X_grouped.shape[0]\n",
    "    # Use float32 to match typical PyTorch/neural network dtypes\n",
    "    X_expanded = np.zeros((n_samples, original_dim), dtype=np.float32)\n",
    "    \n",
    "    # Copy series_id\n",
    "    X_expanded[:, 0] = X_grouped[:, 0].astype(np.float32)\n",
    "    \n",
    "    # Expand each group by replicating the averaged value\n",
    "    for group_idx, feature_indices in enumerate(feature_groups[1:]):  # Skip series_id group\n",
    "        # Replicate the grouped value to all features in the group\n",
    "        # Cast to float32 to avoid dtype warnings\n",
    "        X_expanded[:, feature_indices] = X_grouped[:, group_idx + 1:group_idx + 2].astype(np.float32)\n",
    "    \n",
    "    return X_expanded\n",
    "\n",
    "def create_feature_groups(wrapper_model):\n",
    "    \"\"\"Create feature groups for SHAP\"\"\"\n",
    "    groups = []\n",
    "    idx = 0\n",
    "    \n",
    "    # Group 0: Series ID (we'll exclude this later)\n",
    "    groups.append(np.array([idx]))\n",
    "    idx += 1\n",
    "    \n",
    "    # Group 1: Future exogenous (if exists)\n",
    "    futr_group = []\n",
    "    if wrapper_model.futr_exog_cols:\n",
    "        n_futr = wrapper_model.n_futr_features\n",
    "        futr_group = np.arange(idx, idx + n_futr)\n",
    "        groups.append(futr_group)\n",
    "        idx += n_futr\n",
    "    \n",
    "    # Group 2: Historical exogenous (if exists)\n",
    "    hist_exog_group = []\n",
    "    if wrapper_model.hist_exog_cols:\n",
    "        n_hist_exog = wrapper_model.n_hist_exog_features\n",
    "        hist_exog_group = np.arange(idx, idx + n_hist_exog)\n",
    "        groups.append(hist_exog_group)\n",
    "        idx += n_hist_exog\n",
    "    \n",
    "    # Group 3: Historical target (lags)\n",
    "    lags_group = np.arange(idx, idx + wrapper_model.n_hist_target_features)\n",
    "    groups.append(lags_group)\n",
    "    \n",
    "    # Create group names\n",
    "    group_names = ['series_id']\n",
    "    if wrapper_model.futr_exog_cols:\n",
    "        group_names.append('futr_exog')\n",
    "    if wrapper_model.hist_exog_cols:\n",
    "        group_names.append('hist_exog')\n",
    "    group_names.append('lags')\n",
    "    \n",
    "    return groups, group_names"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
