{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp models.lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Long Short-Term Memory Recurrent Neural Network (`LSTM`), uses a multilayer `LSTM` encoder and an `MLP` decoder. It builds upon the LSTM-cell that improves the exploding and vanishing gradients of classic `RNN`'s. This network has been extensively used in sequential prediction tasks like language modeling, phonetic labeling, and forecasting. The predictions are obtained by transforming the hidden states into contexts $\\mathbf{c}_{[t+1:t+H]}$, that are decoded and adapted into $\\mathbf{\\hat{y}}_{[t+1:t+H],[q]}$ through MLPs.\n",
    "\n",
    "\\begin{align}\n",
    " \\mathbf{h}_{t} &= \\textrm{LSTM}([\\mathbf{y}_{t},\\mathbf{x}^{(h)}_{t},\\mathbf{x}^{(s)}], \\mathbf{h}_{t-1})\\\\\n",
    "\\mathbf{c}_{[t+1:t+H]}&=\\textrm{Linear}([\\mathbf{h}_{t}, \\mathbf{x}^{(f)}_{[:t+H]}]) \\\\ \n",
    "\\hat{y}_{\\tau,[q]}&=\\textrm{MLP}([\\mathbf{c}_{\\tau},\\mathbf{x}^{(f)}_{\\tau}])\n",
    "\\end{align}\n",
    "\n",
    "where $\\mathbf{h}_{t}$, is the hidden state for time $t$, $\\mathbf{y}_{t}$ is the input at time $t$ and $\\mathbf{h}_{t-1}$ is the hidden state of the previous layer at $t-1$, $\\mathbf{x}^{(s)}$ are static exogenous inputs, $\\mathbf{x}^{(h)}_{t}$ historic exogenous, $\\mathbf{x}^{(f)}_{[:t+H]}$ are future exogenous available at the time of the prediction.\n",
    "\n",
    "**References**<br>-[Jeffrey L. Elman (1990). \"Finding Structure in Time\".](https://onlinelibrary.wiley.com/doi/abs/10.1207/s15516709cog1402_1)<br>-[Haşim Sak, Andrew Senior, Françoise Beaufays (2014). \"Long Short-Term Memory Based Recurrent Neural Network Architectures for Large Vocabulary Speech Recognition.\"](https://arxiv.org/abs/1402.1128)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Figure 1. Long Short-Term Memory Cell.](imgs_models/lstm.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import show_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from neuralforecast.losses.pytorch import MAE\n",
    "from neuralforecast.common._base_model import BaseModel\n",
    "from neuralforecast.common._modules import MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class LSTM(BaseModel):\n",
    "    \"\"\" LSTM\n",
    "\n",
    "    LSTM encoder, with MLP decoder.\n",
    "    The network has `tanh` or `relu` non-linearities, it is trained using \n",
    "    ADAM stochastic gradient descent. The network accepts static, historic \n",
    "    and future exogenous data.\n",
    "\n",
    "    **Parameters:**<br>\n",
    "    `h`: int, forecast horizon.<br>\n",
    "    `input_size`: int, maximum sequence length for truncated train backpropagation. Default -1 uses all history.<br>\n",
    "    `inference_input_size`: int, maximum sequence length for truncated inference. Default -1 uses all history.<br>\n",
    "    `encoder_n_layers`: int=2, number of layers for the LSTM.<br>\n",
    "    `encoder_hidden_size`: int=200, units for the LSTM's hidden state size.<br>\n",
    "    `encoder_bias`: bool=True, whether or not to use biases b_ih, b_hh within LSTM units.<br>\n",
    "    `encoder_dropout`: float=0., dropout regularization applied to LSTM outputs.<br>\n",
    "    `context_size`: int=10, size of context vector for each timestamp on the forecasting window.<br>\n",
    "    `decoder_hidden_size`: int=200, size of hidden layer for the MLP decoder.<br>\n",
    "    `decoder_layers`: int=2, number of layers for the MLP decoder.<br>\n",
    "    `futr_exog_list`: str list, future exogenous columns.<br>\n",
    "    `hist_exog_list`: str list, historic exogenous columns.<br>\n",
    "    `stat_exog_list`: str list, static exogenous columns.<br>\n",
    "    `loss`: PyTorch module, instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
    "    `valid_loss`: PyTorch module=`loss`, instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
    "    `max_steps`: int=1000, maximum number of training steps.<br>\n",
    "    `learning_rate`: float=1e-3, Learning rate between (0, 1).<br>\n",
    "    `num_lr_decays`: int=-1, Number of learning rate decays, evenly distributed across max_steps.<br>\n",
    "    `early_stop_patience_steps`: int=-1, Number of validation iterations before early stopping.<br>\n",
    "    `val_check_steps`: int=100, Number of training steps between every validation loss check.<br>\n",
    "    `batch_size`: int=32, number of differentseries in each batch.<br>\n",
    "    `valid_batch_size`: int=None, number of different series in each validation and test batch.<br>\n",
    "    `scaler_type`: str='robust', type of scaler for temporal inputs normalization see [temporal scalers](https://nixtla.github.io/neuralforecast/common.scalers.html).<br>\n",
    "    `random_seed`: int=1, random_seed for pytorch initializer and numpy generators.<br>\n",
    "    `num_workers_loader`: int=os.cpu_count(), workers to be used by `TimeSeriesDataLoader`.<br>\n",
    "    `drop_last_loader`: bool=False, if True `TimeSeriesDataLoader` drops last non-full batch.<br>\n",
    "    `alias`: str, optional,  Custom name of the model.<br>\n",
    "    `optimizer`: Subclass of 'torch.optim.Optimizer', optional, user specified optimizer instead of the default choice (Adam).<br>\n",
    "    `optimizer_kwargs`: dict, optional, list of parameters used by the user specified `optimizer`.<br>\n",
    "    `lr_scheduler`: Subclass of 'torch.optim.lr_scheduler.LRScheduler', optional, user specified lr_scheduler instead of the default choice (StepLR).<br>\n",
    "    `lr_scheduler_kwargs`: dict, optional, list of parameters used by the user specified `lr_scheduler`.<br>    \n",
    "    `**trainer_kwargs`: int,  keyword trainer arguments inherited from [PyTorch Lighning's trainer](https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.trainer.trainer.Trainer.html?highlight=trainer).<br>    \n",
    "    \"\"\"\n",
    "    # Class attributes\n",
    "    SAMPLING_TYPE = 'recurrent'\n",
    "    EXOGENOUS_FUTR = True\n",
    "    EXOGENOUS_HIST = True\n",
    "    EXOGENOUS_STAT = True\n",
    "    MULTIVARIATE = False    # If the model produces multivariate forecasts (True) or univariate (False)\n",
    "    RECURRENT = True        # If the model produces forecasts recursively (True) or direct (False)\n",
    "\n",
    "    def __init__(self,\n",
    "                 h: int,\n",
    "                 input_size: int,\n",
    "                 encoder_n_layers: int = 2,\n",
    "                 encoder_hidden_size: int = 200,\n",
    "                 encoder_bias: bool = True,\n",
    "                 encoder_dropout: float = 0.,\n",
    "                 context_size: int = 10,\n",
    "                 decoder_hidden_size: int = 200,\n",
    "                 decoder_layers: int = 2,\n",
    "                 futr_exog_list = None,\n",
    "                 hist_exog_list = None,\n",
    "                 stat_exog_list = None,\n",
    "                 exclude_insample_y = False,\n",
    "                 loss = MAE(),\n",
    "                 valid_loss = None,\n",
    "                 max_steps: int = 1000,\n",
    "                 learning_rate: float = 1e-3,\n",
    "                 num_lr_decays: int = -1,\n",
    "                 early_stop_patience_steps: int =-1,\n",
    "                 val_check_steps: int = 100,\n",
    "                 batch_size = 32,\n",
    "                 valid_batch_size: Optional[int] = None,\n",
    "                 windows_batch_size = 1024,\n",
    "                 inference_windows_batch_size = 1024,\n",
    "                 start_padding_enabled = False,\n",
    "                 step_size: int = 1,\n",
    "                 scaler_type: str = 'robust',\n",
    "                 random_seed = 1,\n",
    "                 num_workers_loader = 0,\n",
    "                 drop_last_loader = False,\n",
    "                 optimizer = None,\n",
    "                 optimizer_kwargs = None,\n",
    "                 lr_scheduler = None,\n",
    "                 lr_scheduler_kwargs = None,\n",
    "                 **trainer_kwargs):\n",
    "        super(LSTM, self).__init__(\n",
    "            h=h,\n",
    "            input_size=input_size,\n",
    "            futr_exog_list=futr_exog_list,\n",
    "            hist_exog_list=hist_exog_list,\n",
    "            stat_exog_list=stat_exog_list,\n",
    "            exclude_insample_y = exclude_insample_y,\n",
    "            loss=loss,\n",
    "            valid_loss=valid_loss,\n",
    "            max_steps=max_steps,\n",
    "            learning_rate=learning_rate,\n",
    "            num_lr_decays=num_lr_decays,\n",
    "            early_stop_patience_steps=early_stop_patience_steps,\n",
    "            val_check_steps=val_check_steps,\n",
    "            batch_size=batch_size,\n",
    "            valid_batch_size=valid_batch_size,\n",
    "            windows_batch_size=windows_batch_size,\n",
    "            inference_windows_batch_size=inference_windows_batch_size,\n",
    "            start_padding_enabled=start_padding_enabled,\n",
    "            step_size=step_size,\n",
    "            scaler_type=scaler_type,\n",
    "            random_seed=random_seed,\n",
    "            num_workers_loader=num_workers_loader,\n",
    "            drop_last_loader=drop_last_loader,\n",
    "            optimizer=optimizer,\n",
    "            optimizer_kwargs=optimizer_kwargs,\n",
    "            lr_scheduler=lr_scheduler,\n",
    "            lr_scheduler_kwargs=lr_scheduler_kwargs,\n",
    "            **trainer_kwargs\n",
    "        )\n",
    "\n",
    "        # LSTM\n",
    "        self.encoder_n_layers = encoder_n_layers\n",
    "        self.encoder_hidden_size = encoder_hidden_size\n",
    "        self.encoder_bias = encoder_bias\n",
    "        self.encoder_dropout = encoder_dropout\n",
    "        \n",
    "        # Context adapter\n",
    "        self.context_size = context_size\n",
    "\n",
    "        # MLP decoder\n",
    "        self.decoder_hidden_size = decoder_hidden_size\n",
    "        self.decoder_layers = decoder_layers\n",
    "\n",
    "        # LSTM input size (1 for target variable y)\n",
    "        input_encoder = 1 + self.hist_exog_size + self.stat_exog_size + self.futr_exog_size\n",
    "\n",
    "        # Instantiate model\n",
    "        self.rnn_state = None\n",
    "        self.hist_encoder = nn.LSTM(input_size=input_encoder,\n",
    "                                    hidden_size=self.encoder_hidden_size,\n",
    "                                    num_layers=self.encoder_n_layers,\n",
    "                                    bias=self.encoder_bias,\n",
    "                                    dropout=self.encoder_dropout,\n",
    "                                    batch_first=True)\n",
    "\n",
    "        # Context adapter\n",
    "        self.context_adapter = nn.Linear(in_features=self.encoder_hidden_size,\n",
    "                                         out_features=self.context_size * h)\n",
    "\n",
    "        # Decoder MLP\n",
    "        self.mlp_decoder = MLP(in_features=self.context_size * h + self.futr_exog_size,\n",
    "                               out_features=self.loss.outputsize_multiplier,\n",
    "                               hidden_size=self.decoder_hidden_size,\n",
    "                               num_layers=self.decoder_layers,\n",
    "                               activation='ReLU',\n",
    "                               dropout=0.0)\n",
    "\n",
    "    def forward(self, windows_batch):\n",
    "        \n",
    "        # Parse windows_batch\n",
    "        encoder_input = windows_batch['insample_y']                         # [B, seq_len, 1]\n",
    "        futr_exog     = windows_batch['futr_exog']                          # [B, seq_len, F]\n",
    "        hist_exog     = windows_batch['hist_exog']                          # [B, seq_len, X]\n",
    "        stat_exog     = windows_batch['stat_exog']                          # [B, S]\n",
    "\n",
    "        # Concatenate y, historic and static inputs              \n",
    "        batch_size, seq_len = encoder_input.shape[:2]\n",
    "        if self.hist_exog_size > 0:\n",
    "            encoder_input = torch.cat((encoder_input, hist_exog), dim=2)    # [B, seq_len, 1] + [B, seq_len, X] -> [B, seq_len, 1 + X]\n",
    "\n",
    "        if self.stat_exog_size > 0:\n",
    "            # print(encoder_input.shape)\n",
    "            stat_exog = stat_exog.unsqueeze(1).repeat(1, seq_len, 1)        # [B, S] -> [B, seq_len, S]\n",
    "            encoder_input = torch.cat((encoder_input, stat_exog), dim=2)    # [B, seq_len, 1 + X] + [B, seq_len, S] -> [B, seq_len, 1 + X + S]\n",
    "\n",
    "        if self.futr_exog_size > 0:\n",
    "            encoder_input = torch.cat((encoder_input, futr_exog), dim=2)    # [B, seq_len, 1 + X + S] + [B, seq_len, F] -> [B, seq_len, 1 + X + S + F]\n",
    "\n",
    "        # RNN forward\n",
    "        if self.maintain_state:\n",
    "            rnn_state = self.rnn_state\n",
    "        else:\n",
    "            rnn_state = None\n",
    "        \n",
    "        hidden_state, rnn_state = self.hist_encoder(encoder_input, \n",
    "                                                         rnn_state)    # [B, seq_len, rnn_hidden_state]\n",
    "        if self.maintain_state:\n",
    "            self.rnn_state = rnn_state\n",
    "\n",
    "        # Context adapter\n",
    "        context = self.context_adapter(hidden_state)                        # [B, seq_len, rnn_hidden_state] -> [B, seq_len, context_size * h]\n",
    "\n",
    "        # Residual connection with futr_exog\n",
    "        if self.futr_exog_size > 0:\n",
    "            context = torch.cat((context, futr_exog), dim=-1)               # [B, seq_len, context_size * h] + [B, seq_len, F] = [B, seq_len, context_size * h + F]\n",
    "\n",
    "        # Final forecast\n",
    "        output = self.mlp_decoder(context)                                  # [B, seq_len, context_size * h + F] -> [B, seq_len, n_output]\n",
    "        \n",
    "        return output[:, -self.h:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/models/lstm.py#L17){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### LSTM\n",
       "\n",
       ">      LSTM (h:int, input_size:int, encoder_n_layers:int=2,\n",
       ">            encoder_hidden_size:int=200, encoder_bias:bool=True,\n",
       ">            encoder_dropout:float=0.0, context_size:int=10,\n",
       ">            decoder_hidden_size:int=200, decoder_layers:int=2,\n",
       ">            futr_exog_list=None, hist_exog_list=None, stat_exog_list=None,\n",
       ">            exclude_insample_y=False, loss=MAE(), valid_loss=None,\n",
       ">            max_steps:int=1000, learning_rate:float=0.001,\n",
       ">            num_lr_decays:int=-1, early_stop_patience_steps:int=-1,\n",
       ">            val_check_steps:int=100, batch_size=32,\n",
       ">            valid_batch_size:Optional[int]=None, windows_batch_size=1024,\n",
       ">            inference_windows_batch_size=1024, start_padding_enabled=False,\n",
       ">            step_size:int=1, scaler_type:str='robust', random_seed=1,\n",
       ">            num_workers_loader=0, drop_last_loader=False, optimizer=None,\n",
       ">            optimizer_kwargs=None, lr_scheduler=None, lr_scheduler_kwargs=None,\n",
       ">            **trainer_kwargs)\n",
       "\n",
       "*LSTM\n",
       "\n",
       "LSTM encoder, with MLP decoder.\n",
       "The network has `tanh` or `relu` non-linearities, it is trained using \n",
       "ADAM stochastic gradient descent. The network accepts static, historic \n",
       "and future exogenous data.\n",
       "\n",
       "**Parameters:**<br>\n",
       "`h`: int, forecast horizon.<br>\n",
       "`input_size`: int, maximum sequence length for truncated train backpropagation. Default -1 uses all history.<br>\n",
       "`inference_input_size`: int, maximum sequence length for truncated inference. Default -1 uses all history.<br>\n",
       "`encoder_n_layers`: int=2, number of layers for the LSTM.<br>\n",
       "`encoder_hidden_size`: int=200, units for the LSTM's hidden state size.<br>\n",
       "`encoder_bias`: bool=True, whether or not to use biases b_ih, b_hh within LSTM units.<br>\n",
       "`encoder_dropout`: float=0., dropout regularization applied to LSTM outputs.<br>\n",
       "`context_size`: int=10, size of context vector for each timestamp on the forecasting window.<br>\n",
       "`decoder_hidden_size`: int=200, size of hidden layer for the MLP decoder.<br>\n",
       "`decoder_layers`: int=2, number of layers for the MLP decoder.<br>\n",
       "`futr_exog_list`: str list, future exogenous columns.<br>\n",
       "`hist_exog_list`: str list, historic exogenous columns.<br>\n",
       "`stat_exog_list`: str list, static exogenous columns.<br>\n",
       "`loss`: PyTorch module, instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
       "`valid_loss`: PyTorch module=`loss`, instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
       "`max_steps`: int=1000, maximum number of training steps.<br>\n",
       "`learning_rate`: float=1e-3, Learning rate between (0, 1).<br>\n",
       "`num_lr_decays`: int=-1, Number of learning rate decays, evenly distributed across max_steps.<br>\n",
       "`early_stop_patience_steps`: int=-1, Number of validation iterations before early stopping.<br>\n",
       "`val_check_steps`: int=100, Number of training steps between every validation loss check.<br>\n",
       "`batch_size`: int=32, number of differentseries in each batch.<br>\n",
       "`valid_batch_size`: int=None, number of different series in each validation and test batch.<br>\n",
       "`scaler_type`: str='robust', type of scaler for temporal inputs normalization see [temporal scalers](https://nixtla.github.io/neuralforecast/common.scalers.html).<br>\n",
       "`random_seed`: int=1, random_seed for pytorch initializer and numpy generators.<br>\n",
       "`num_workers_loader`: int=os.cpu_count(), workers to be used by `TimeSeriesDataLoader`.<br>\n",
       "`drop_last_loader`: bool=False, if True `TimeSeriesDataLoader` drops last non-full batch.<br>\n",
       "`alias`: str, optional,  Custom name of the model.<br>\n",
       "`optimizer`: Subclass of 'torch.optim.Optimizer', optional, user specified optimizer instead of the default choice (Adam).<br>\n",
       "`optimizer_kwargs`: dict, optional, list of parameters used by the user specified `optimizer`.<br>\n",
       "`lr_scheduler`: Subclass of 'torch.optim.lr_scheduler.LRScheduler', optional, user specified lr_scheduler instead of the default choice (StepLR).<br>\n",
       "`lr_scheduler_kwargs`: dict, optional, list of parameters used by the user specified `lr_scheduler`.<br>    \n",
       "`**trainer_kwargs`: int,  keyword trainer arguments inherited from [PyTorch Lighning's trainer](https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.trainer.trainer.Trainer.html?highlight=trainer).<br>*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/models/lstm.py#L17){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### LSTM\n",
       "\n",
       ">      LSTM (h:int, input_size:int, encoder_n_layers:int=2,\n",
       ">            encoder_hidden_size:int=200, encoder_bias:bool=True,\n",
       ">            encoder_dropout:float=0.0, context_size:int=10,\n",
       ">            decoder_hidden_size:int=200, decoder_layers:int=2,\n",
       ">            futr_exog_list=None, hist_exog_list=None, stat_exog_list=None,\n",
       ">            exclude_insample_y=False, loss=MAE(), valid_loss=None,\n",
       ">            max_steps:int=1000, learning_rate:float=0.001,\n",
       ">            num_lr_decays:int=-1, early_stop_patience_steps:int=-1,\n",
       ">            val_check_steps:int=100, batch_size=32,\n",
       ">            valid_batch_size:Optional[int]=None, windows_batch_size=1024,\n",
       ">            inference_windows_batch_size=1024, start_padding_enabled=False,\n",
       ">            step_size:int=1, scaler_type:str='robust', random_seed=1,\n",
       ">            num_workers_loader=0, drop_last_loader=False, optimizer=None,\n",
       ">            optimizer_kwargs=None, lr_scheduler=None, lr_scheduler_kwargs=None,\n",
       ">            **trainer_kwargs)\n",
       "\n",
       "*LSTM\n",
       "\n",
       "LSTM encoder, with MLP decoder.\n",
       "The network has `tanh` or `relu` non-linearities, it is trained using \n",
       "ADAM stochastic gradient descent. The network accepts static, historic \n",
       "and future exogenous data.\n",
       "\n",
       "**Parameters:**<br>\n",
       "`h`: int, forecast horizon.<br>\n",
       "`input_size`: int, maximum sequence length for truncated train backpropagation. Default -1 uses all history.<br>\n",
       "`inference_input_size`: int, maximum sequence length for truncated inference. Default -1 uses all history.<br>\n",
       "`encoder_n_layers`: int=2, number of layers for the LSTM.<br>\n",
       "`encoder_hidden_size`: int=200, units for the LSTM's hidden state size.<br>\n",
       "`encoder_bias`: bool=True, whether or not to use biases b_ih, b_hh within LSTM units.<br>\n",
       "`encoder_dropout`: float=0., dropout regularization applied to LSTM outputs.<br>\n",
       "`context_size`: int=10, size of context vector for each timestamp on the forecasting window.<br>\n",
       "`decoder_hidden_size`: int=200, size of hidden layer for the MLP decoder.<br>\n",
       "`decoder_layers`: int=2, number of layers for the MLP decoder.<br>\n",
       "`futr_exog_list`: str list, future exogenous columns.<br>\n",
       "`hist_exog_list`: str list, historic exogenous columns.<br>\n",
       "`stat_exog_list`: str list, static exogenous columns.<br>\n",
       "`loss`: PyTorch module, instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
       "`valid_loss`: PyTorch module=`loss`, instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
       "`max_steps`: int=1000, maximum number of training steps.<br>\n",
       "`learning_rate`: float=1e-3, Learning rate between (0, 1).<br>\n",
       "`num_lr_decays`: int=-1, Number of learning rate decays, evenly distributed across max_steps.<br>\n",
       "`early_stop_patience_steps`: int=-1, Number of validation iterations before early stopping.<br>\n",
       "`val_check_steps`: int=100, Number of training steps between every validation loss check.<br>\n",
       "`batch_size`: int=32, number of differentseries in each batch.<br>\n",
       "`valid_batch_size`: int=None, number of different series in each validation and test batch.<br>\n",
       "`scaler_type`: str='robust', type of scaler for temporal inputs normalization see [temporal scalers](https://nixtla.github.io/neuralforecast/common.scalers.html).<br>\n",
       "`random_seed`: int=1, random_seed for pytorch initializer and numpy generators.<br>\n",
       "`num_workers_loader`: int=os.cpu_count(), workers to be used by `TimeSeriesDataLoader`.<br>\n",
       "`drop_last_loader`: bool=False, if True `TimeSeriesDataLoader` drops last non-full batch.<br>\n",
       "`alias`: str, optional,  Custom name of the model.<br>\n",
       "`optimizer`: Subclass of 'torch.optim.Optimizer', optional, user specified optimizer instead of the default choice (Adam).<br>\n",
       "`optimizer_kwargs`: dict, optional, list of parameters used by the user specified `optimizer`.<br>\n",
       "`lr_scheduler`: Subclass of 'torch.optim.lr_scheduler.LRScheduler', optional, user specified lr_scheduler instead of the default choice (StepLR).<br>\n",
       "`lr_scheduler_kwargs`: dict, optional, list of parameters used by the user specified `lr_scheduler`.<br>    \n",
       "`**trainer_kwargs`: int,  keyword trainer arguments inherited from [PyTorch Lighning's trainer](https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.trainer.trainer.Trainer.html?highlight=trainer).<br>*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### LSTM.fit\n",
       "\n",
       ">      LSTM.fit (dataset, val_size=0, test_size=0, random_seed=None,\n",
       ">                distributed_config=None)\n",
       "\n",
       "*Fit.\n",
       "\n",
       "The `fit` method, optimizes the neural network's weights using the\n",
       "initialization parameters (`learning_rate`, `windows_batch_size`, ...)\n",
       "and the `loss` function as defined during the initialization.\n",
       "Within `fit` we use a PyTorch Lightning `Trainer` that\n",
       "inherits the initialization's `self.trainer_kwargs`, to customize\n",
       "its inputs, see [PL's trainer arguments](https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.trainer.trainer.Trainer.html?highlight=trainer).\n",
       "\n",
       "The method is designed to be compatible with SKLearn-like classes\n",
       "and in particular to be compatible with the StatsForecast library.\n",
       "\n",
       "By default the `model` is not saving training checkpoints to protect\n",
       "disk memory, to get them change `enable_checkpointing=True` in `__init__`.\n",
       "\n",
       "**Parameters:**<br>\n",
       "`dataset`: NeuralForecast's `TimeSeriesDataset`, see [documentation](https://nixtla.github.io/neuralforecast/tsdataset.html).<br>\n",
       "`val_size`: int, validation size for temporal cross-validation.<br>\n",
       "`random_seed`: int=None, random_seed for pytorch initializer and numpy generators, overwrites model.__init__'s.<br>\n",
       "`test_size`: int, test size for temporal cross-validation.<br>*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### LSTM.fit\n",
       "\n",
       ">      LSTM.fit (dataset, val_size=0, test_size=0, random_seed=None,\n",
       ">                distributed_config=None)\n",
       "\n",
       "*Fit.\n",
       "\n",
       "The `fit` method, optimizes the neural network's weights using the\n",
       "initialization parameters (`learning_rate`, `windows_batch_size`, ...)\n",
       "and the `loss` function as defined during the initialization.\n",
       "Within `fit` we use a PyTorch Lightning `Trainer` that\n",
       "inherits the initialization's `self.trainer_kwargs`, to customize\n",
       "its inputs, see [PL's trainer arguments](https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.trainer.trainer.Trainer.html?highlight=trainer).\n",
       "\n",
       "The method is designed to be compatible with SKLearn-like classes\n",
       "and in particular to be compatible with the StatsForecast library.\n",
       "\n",
       "By default the `model` is not saving training checkpoints to protect\n",
       "disk memory, to get them change `enable_checkpointing=True` in `__init__`.\n",
       "\n",
       "**Parameters:**<br>\n",
       "`dataset`: NeuralForecast's `TimeSeriesDataset`, see [documentation](https://nixtla.github.io/neuralforecast/tsdataset.html).<br>\n",
       "`val_size`: int, validation size for temporal cross-validation.<br>\n",
       "`random_seed`: int=None, random_seed for pytorch initializer and numpy generators, overwrites model.__init__'s.<br>\n",
       "`test_size`: int, test size for temporal cross-validation.<br>*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(LSTM.fit, name='LSTM.fit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### LSTM.predict\n",
       "\n",
       ">      LSTM.predict (dataset, test_size=None, step_size=1, random_seed=None,\n",
       ">                    **data_module_kwargs)\n",
       "\n",
       "*Predict.\n",
       "\n",
       "Neural network prediction with PL's `Trainer` execution of `predict_step`.\n",
       "\n",
       "**Parameters:**<br>\n",
       "`dataset`: NeuralForecast's `TimeSeriesDataset`, see [documentation](https://nixtla.github.io/neuralforecast/tsdataset.html).<br>\n",
       "`test_size`: int=None, test size for temporal cross-validation.<br>\n",
       "`step_size`: int=1, Step size between each window.<br>\n",
       "`random_seed`: int=None, random_seed for pytorch initializer and numpy generators, overwrites model.__init__'s.<br>\n",
       "`**data_module_kwargs`: PL's TimeSeriesDataModule args, see [documentation](https://pytorch-lightning.readthedocs.io/en/1.6.1/extensions/datamodules.html#using-a-datamodule).*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### LSTM.predict\n",
       "\n",
       ">      LSTM.predict (dataset, test_size=None, step_size=1, random_seed=None,\n",
       ">                    **data_module_kwargs)\n",
       "\n",
       "*Predict.\n",
       "\n",
       "Neural network prediction with PL's `Trainer` execution of `predict_step`.\n",
       "\n",
       "**Parameters:**<br>\n",
       "`dataset`: NeuralForecast's `TimeSeriesDataset`, see [documentation](https://nixtla.github.io/neuralforecast/tsdataset.html).<br>\n",
       "`test_size`: int=None, test size for temporal cross-validation.<br>\n",
       "`step_size`: int=1, Step size between each window.<br>\n",
       "`random_seed`: int=None, random_seed for pytorch initializer and numpy generators, overwrites model.__init__'s.<br>\n",
       "`**data_module_kwargs`: PL's TimeSeriesDataModule args, see [documentation](https://pytorch-lightning.readthedocs.io/en/1.6.1/extensions/datamodules.html#using-a-datamodule).*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(LSTM.predict, name='LSTM.predict')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from neuralforecast import NeuralForecast\n",
    "from neuralforecast.models import LSTM\n",
    "from neuralforecast.losses.pytorch import DistributionLoss\n",
    "from neuralforecast.utils import AirPassengersPanel, AirPassengersStatic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 1\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name            | Type          | Params\n",
      "--------------------------------------------------\n",
      "0 | loss            | MAE           | 0     \n",
      "1 | padder_train    | ConstantPad1d | 0     \n",
      "2 | scaler          | TemporalNorm  | 0     \n",
      "3 | hist_encoder    | LSTM          | 200 K \n",
      "4 | context_adapter | Linear        | 15.5 K\n",
      "5 | mlp_decoder     | MLP           | 15.7 K\n",
      "--------------------------------------------------\n",
      "231 K     Trainable params\n",
      "0         Non-trainable params\n",
      "231 K     Total params\n",
      "0.926     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/1 [00:00<?, ?it/s]                              torch.Size([2, 4, 119, 26])\n",
      "Epoch 1:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=1.780, train_loss_epoch=1.780]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 2:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=1.790, train_loss_epoch=1.790]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 3:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=1.740, train_loss_epoch=1.740]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 4:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=1.590, train_loss_epoch=1.590]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 5:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=1.610, train_loss_epoch=1.610]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 6:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=1.650, train_loss_epoch=1.650]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 7:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=1.550, train_loss_epoch=1.550]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 8:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=1.510, train_loss_epoch=1.510]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 9:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=1.400, train_loss_epoch=1.400]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 10:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=1.340, train_loss_epoch=1.340]       torch.Size([2, 4, 119, 26])\n",
      "Epoch 11:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=1.130, train_loss_epoch=1.130]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 12:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=1.240, train_loss_epoch=1.240]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 13:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=1.230, train_loss_epoch=1.230]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 14:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=1.200, train_loss_epoch=1.200]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 15:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=1.150, train_loss_epoch=1.150]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 16:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=1.110, train_loss_epoch=1.110]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 17:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=1.140, train_loss_epoch=1.140]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 18:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=1.170, train_loss_epoch=1.170]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 19:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=1.100, train_loss_epoch=1.100]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 20:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=1.140, train_loss_epoch=1.140]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 21:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=1.110, train_loss_epoch=1.110]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 22:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=1.080, train_loss_epoch=1.080]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 23:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=1.060, train_loss_epoch=1.060]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 24:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=1.080, train_loss_epoch=1.080]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 25:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=1.100, train_loss_epoch=1.100]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 26:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=1.080, train_loss_epoch=1.080]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 27:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=1.080, train_loss_epoch=1.080]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 28:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=1.040, train_loss_epoch=1.040]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 29:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=1.000, train_loss_epoch=1.000]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 30:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=1.060, train_loss_epoch=1.060]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 31:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=1.030, train_loss_epoch=1.030]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 32:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=1.050, train_loss_epoch=1.050]         torch.Size([2, 4, 119, 26])\n",
      "Epoch 33:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=1.000, train_loss_epoch=1.000]         torch.Size([2, 4, 119, 26])\n",
      "Epoch 34:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=1.010, train_loss_epoch=1.010]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 35:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.974, train_loss_epoch=0.974]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 36:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.953, train_loss_epoch=0.953]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 37:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=1.020, train_loss_epoch=1.020]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 38:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.967, train_loss_epoch=0.967]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 39:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.909, train_loss_epoch=0.909]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 40:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.912, train_loss_epoch=0.912]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 41:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.907, train_loss_epoch=0.907]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 42:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.850, train_loss_epoch=0.850]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 43:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.862, train_loss_epoch=0.862]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 44:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.824, train_loss_epoch=0.824]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 45:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.819, train_loss_epoch=0.819]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 46:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.777, train_loss_epoch=0.777]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 47:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.736, train_loss_epoch=0.736]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 48:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.665, train_loss_epoch=0.665]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 49:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.610, train_loss_epoch=0.610]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 50:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.642, train_loss_epoch=0.642]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 51:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.613, train_loss_epoch=0.613]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 52:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.647, train_loss_epoch=0.647]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 53:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.627, train_loss_epoch=0.627]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 54:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.640, train_loss_epoch=0.640]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 55:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.589, train_loss_epoch=0.589]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 56:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.550, train_loss_epoch=0.550]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 57:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.588, train_loss_epoch=0.588]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 58:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.593, train_loss_epoch=0.593]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 59:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.572, train_loss_epoch=0.572]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 60:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.559, train_loss_epoch=0.559]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 61:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.541, train_loss_epoch=0.541]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 62:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.568, train_loss_epoch=0.568]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 63:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.565, train_loss_epoch=0.565]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 64:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.532, train_loss_epoch=0.532]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 65:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.514, train_loss_epoch=0.514]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 66:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.532, train_loss_epoch=0.532]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 67:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.517, train_loss_epoch=0.517]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 68:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.539, train_loss_epoch=0.539]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 69:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.521, train_loss_epoch=0.521]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 70:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.490, train_loss_epoch=0.490]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 71:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.517, train_loss_epoch=0.517]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 72:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.510, train_loss_epoch=0.510]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 73:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.494, train_loss_epoch=0.494]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 74:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.501, train_loss_epoch=0.501]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 75:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.488, train_loss_epoch=0.488]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 76:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.519, train_loss_epoch=0.519]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 77:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.503, train_loss_epoch=0.503]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 78:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.503, train_loss_epoch=0.503]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 79:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.492, train_loss_epoch=0.492]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 80:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.486, train_loss_epoch=0.486]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 81:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.481, train_loss_epoch=0.481]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 82:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.452, train_loss_epoch=0.452]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 83:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.454, train_loss_epoch=0.454]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 84:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.484, train_loss_epoch=0.484]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 85:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.459, train_loss_epoch=0.459]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 86:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.446, train_loss_epoch=0.446]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 87:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.443, train_loss_epoch=0.443]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 88:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.479, train_loss_epoch=0.479]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 89:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.467, train_loss_epoch=0.467]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 90:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.433, train_loss_epoch=0.433]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 91:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.429, train_loss_epoch=0.429]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 92:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.453, train_loss_epoch=0.453]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 93:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.469, train_loss_epoch=0.469]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 94:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.450, train_loss_epoch=0.450]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 95:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.444, train_loss_epoch=0.444]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 96:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.422, train_loss_epoch=0.422]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 97:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.445, train_loss_epoch=0.445]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 98:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.455, train_loss_epoch=0.455]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 99:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.438, train_loss_epoch=0.438]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 100:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.444, train_loss_epoch=0.444]       torch.Size([2, 4, 119, 26])\n",
      "Epoch 101:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.421, train_loss_epoch=0.421]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 102:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.422, train_loss_epoch=0.422]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 103:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.422, train_loss_epoch=0.422]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 104:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.416, train_loss_epoch=0.416]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 105:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.416, train_loss_epoch=0.416]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 106:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.403, train_loss_epoch=0.403]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 107:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.422, train_loss_epoch=0.422]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 108:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.388, train_loss_epoch=0.388]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 109:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.422, train_loss_epoch=0.422]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 110:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.422, train_loss_epoch=0.422]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 111:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.419, train_loss_epoch=0.419]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 112:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.392, train_loss_epoch=0.392]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 113:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.423, train_loss_epoch=0.423]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 114:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.405, train_loss_epoch=0.405]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 115:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.419, train_loss_epoch=0.419]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 116:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.395, train_loss_epoch=0.395]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 117:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.392, train_loss_epoch=0.392]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 118:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.411, train_loss_epoch=0.411]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 119:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.420, train_loss_epoch=0.420]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 120:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.397, train_loss_epoch=0.397]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 121:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.392, train_loss_epoch=0.392]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 122:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.386, train_loss_epoch=0.386]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 123:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.384, train_loss_epoch=0.384]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 124:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.384, train_loss_epoch=0.384]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 125:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.344, train_loss_epoch=0.344]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 126:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.368, train_loss_epoch=0.368]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 127:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.371, train_loss_epoch=0.371]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 128:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.385, train_loss_epoch=0.385]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 129:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.370, train_loss_epoch=0.370]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 130:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.369, train_loss_epoch=0.369]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 131:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.330, train_loss_epoch=0.330]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 132:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.371, train_loss_epoch=0.371]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 133:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.357, train_loss_epoch=0.357]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 134:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.386, train_loss_epoch=0.386]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 135:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.364, train_loss_epoch=0.364]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 136:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.359, train_loss_epoch=0.359]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 137:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.369, train_loss_epoch=0.369]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 138:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.322, train_loss_epoch=0.322]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 139:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.362, train_loss_epoch=0.362]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 140:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.389, train_loss_epoch=0.389]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 141:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.366, train_loss_epoch=0.366]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 142:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.349, train_loss_epoch=0.349]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 143:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.361, train_loss_epoch=0.361]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 144:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.347, train_loss_epoch=0.347]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 145:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.333, train_loss_epoch=0.333]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 146:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.346, train_loss_epoch=0.346]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 147:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.376, train_loss_epoch=0.376]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 148:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.343, train_loss_epoch=0.343]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 149:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.324, train_loss_epoch=0.324]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 150:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.342, train_loss_epoch=0.342]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 151:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.360, train_loss_epoch=0.360]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 152:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.330, train_loss_epoch=0.330]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 153:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.348, train_loss_epoch=0.348]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 154:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.347, train_loss_epoch=0.347]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 155:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.323, train_loss_epoch=0.323]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 156:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.329, train_loss_epoch=0.329]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 157:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.341, train_loss_epoch=0.341]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 158:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.339, train_loss_epoch=0.339]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 159:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.323, train_loss_epoch=0.323]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 160:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.358, train_loss_epoch=0.358]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 161:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.328, train_loss_epoch=0.328]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 162:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.347, train_loss_epoch=0.347]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 163:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.351, train_loss_epoch=0.351]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 164:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.344, train_loss_epoch=0.344]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 165:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.378, train_loss_epoch=0.378]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 166:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.314, train_loss_epoch=0.314]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 167:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.372, train_loss_epoch=0.372]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 168:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.348, train_loss_epoch=0.348]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 169:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.335, train_loss_epoch=0.335]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 170:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.323, train_loss_epoch=0.323]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 171:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.352, train_loss_epoch=0.352]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 172:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.346, train_loss_epoch=0.346]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 173:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.337, train_loss_epoch=0.337]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 174:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.342, train_loss_epoch=0.342]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 175:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.324, train_loss_epoch=0.324]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 176:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.343, train_loss_epoch=0.343]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 177:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.309, train_loss_epoch=0.309]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 178:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.329, train_loss_epoch=0.329]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 179:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.294, train_loss_epoch=0.294]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 180:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.306, train_loss_epoch=0.306]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 181:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.321, train_loss_epoch=0.321]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 182:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.324, train_loss_epoch=0.324]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 183:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.314, train_loss_epoch=0.314]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 184:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.299, train_loss_epoch=0.299]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 185:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.306, train_loss_epoch=0.306]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 186:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.303, train_loss_epoch=0.303]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 187:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.311, train_loss_epoch=0.311]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 188:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.309, train_loss_epoch=0.309]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 189:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.283, train_loss_epoch=0.283]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 190:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.320, train_loss_epoch=0.320]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 191:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.307, train_loss_epoch=0.307]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 192:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.290, train_loss_epoch=0.290]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 193:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.316, train_loss_epoch=0.316]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 194:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.326, train_loss_epoch=0.326]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 195:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.326, train_loss_epoch=0.326]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 196:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.287, train_loss_epoch=0.287]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 197:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.299, train_loss_epoch=0.299]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 198:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.293, train_loss_epoch=0.293]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 199:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3876, train_loss_step=0.297, train_loss_epoch=0.297]        torch.Size([2, 4, 119, 26])\n",
      "Epoch 199: 100%|██████████| 1/1 [00:00<00:00, 39.09it/s, v_num=3876, train_loss_step=0.285, train_loss_epoch=0.285]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=200` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 199: 100%|██████████| 1/1 [00:00<00:00, 36.13it/s, v_num=3876, train_loss_step=0.285, train_loss_epoch=0.285]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ospra\\miniconda3\\envs\\neuralforecast\\lib\\site-packages\\utilsforecast\\processing.py:374: FutureWarning: 'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n",
      "  freq = pd.tseries.frequencies.to_offset(freq)\n",
      "c:\\Users\\ospra\\miniconda3\\envs\\neuralforecast\\lib\\site-packages\\utilsforecast\\processing.py:428: FutureWarning: 'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n",
      "  freq = pd.tseries.frequencies.to_offset(freq)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 87.52it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ospra\\OneDrive\\Phd\\Repositories\\neuralforecast\\neuralforecast\\core.py:196: FutureWarning: In a future version the predictions will have the id as a column. You can set the `NIXTLA_ID_AS_COL` environment variable to adopt the new behavior and to suppress this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "Y_train_df = AirPassengersPanel[AirPassengersPanel.ds<AirPassengersPanel['ds'].values[-12]] # 132 train\n",
    "Y_test_df = AirPassengersPanel[AirPassengersPanel.ds>=AirPassengersPanel['ds'].values[-12]].reset_index(drop=True) # 12 test\n",
    "\n",
    "nf = NeuralForecast(\n",
    "    models=[LSTM(h=12, \n",
    "                 input_size=24,\n",
    "                #  loss=DistributionLoss(distribution='Normal', level=[80, 90]),\n",
    "                 loss=MAE(),\n",
    "                 scaler_type='robust',\n",
    "                 encoder_n_layers=2,\n",
    "                 encoder_hidden_size=128,\n",
    "                 context_size=10,\n",
    "                 decoder_hidden_size=128,\n",
    "                 decoder_layers=2,\n",
    "                 max_steps=200,\n",
    "                 futr_exog_list=['y_[lag12]'],\n",
    "                #  hist_exog_list=['y_[lag12]'],\n",
    "                 stat_exog_list=['airline1'],\n",
    "                 )\n",
    "    ],\n",
    "    freq='M'\n",
    ")\n",
    "nf.fit(df=Y_train_df, static_df=AirPassengersStatic)\n",
    "Y_hat_df = nf.predict(futr_df=Y_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plots\n",
    "Y_hat_df = Y_hat_df.reset_index(drop=False).drop(columns=['unique_id','ds'])\n",
    "plot_df = pd.concat([Y_test_df, Y_hat_df], axis=1)\n",
    "plot_df = pd.concat([Y_train_df, plot_df])\n",
    "\n",
    "plot_df = plot_df[plot_df.unique_id=='Airline1'].drop('unique_id', axis=1)\n",
    "plt.plot(plot_df['ds'], plot_df['y'], c='black', label='True')\n",
    "plt.plot(plot_df['ds'], plot_df['LSTM'], c='purple', label='mean')\n",
    "# plt.plot(plot_df['ds'], plot_df['LSTM-median'], c='blue', label='median')\n",
    "# plt.fill_between(x=plot_df['ds'][-12:], \n",
    "#                  y1=plot_df['LSTM-lo-90'][-12:].values, \n",
    "#                  y2=plot_df['LSTM-hi-90'][-12:].values,\n",
    "#                  alpha=0.4, label='level 90')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
