{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp models.deepar"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepAR"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DeepAR methodology produces conditional probabilistic forecasts based on an Seq2Seq autoregressive neural network optimized on panel data using cross-learning.\n",
    "$$\\mathbb{P}(\\mathbf{y}_{[t+1:t+H]}|\\;\\mathbf{y}_{[:t]},\\; \\mathbf{x}^{(h)}_{[:t]},\\; \\mathbf{x}^{(f)}_{[:t+H]},\\; \\mathbf{x}^{(s)})$$\n",
    "\n",
    "where $\\mathbf{x}^{(s)}$ are static exogenous inputs, $\\mathbf{x}^{(h)}_{t}$ historic exogenous, $\\mathbf{x}^{(f)}_{[:t+H]}$ are future exogenous available at the time of the prediction.\n",
    "\n",
    "\n",
    "**References**<br>\n",
    "- [David Salinas, Valentin Flunkert, Jan Gasthaus, Tim Januschowski (2020). \"DeepAR: Probabilistic forecasting with autoregressive recurrent networks\". International Journal of Forecasting.](https://www.sciencedirect.com/science/article/pii/S0169207019301888)<br>\n",
    "- [Alexander Alexandrov et. al (2020). \"GluonTS: Probabilistic and Neural Time Series Modeling in Python\". Journal of Machine Learning Research.](https://www.jmlr.org/papers/v21/19-820.html)<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ![Figure 1. DeepAR model, during training the optimization signal comes from likelihood of observations, during inference a recurrent multi-step strategy is used to generate predictive distributions.](imgs_models/deepar.png) -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import logging\n",
    "import warnings\n",
    "logging.getLogger(\"pytorch_lightning\").setLevel(logging.ERROR)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from typing import Optional\n",
    "\n",
    "from neuralforecast.losses.pytorch import MAE\n",
    "from neuralforecast.common._base_windows import BaseWindows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from fastcore.test import test_eq\n",
    "from nbdev.showdoc import show_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import logging\n",
    "import warnings\n",
    "logging.getLogger(\"pytorch_lightning\").setLevel(logging.ERROR)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"Multi-Layer Perceptron Decoder\n",
    "\n",
    "    **Parameters:**<br>\n",
    "    `in_features`: int, dimension of input.<br>\n",
    "    `out_features`: int, dimension of output.<br>\n",
    "    `hidden_size`: int, dimension of hidden layers.<br>\n",
    "    `num_layers`: int, number of hidden layers.<br>\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, hidden_size, hidden_layers):\n",
    "        super().__init__()\n",
    "\n",
    "        if hidden_layers == 0:\n",
    "            # Input layer\n",
    "            layers = [nn.Linear(in_features=in_features, out_features=out_features)]\n",
    "        else:\n",
    "            # Input layer\n",
    "            layers = [nn.Linear(in_features=in_features, out_features=hidden_size), nn.ReLU()]\n",
    "            # Hidden layers\n",
    "            for i in range(hidden_layers - 2):\n",
    "                layers += [nn.Linear(in_features=hidden_size, out_features=hidden_size), nn.ReLU()]\n",
    "            # Output layer\n",
    "            layers += [nn.Linear(in_features=hidden_size, out_features=out_features)]\n",
    "\n",
    "        # Store in layers as ModuleList\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class DeepAR(BaseWindows):\n",
    "    \"\"\" DeepAR\n",
    "\n",
    "    DeepAR is a method for producing probabilistic forecasts. It uses two recurrent neural networks (RNN) \n",
    "    to encode temporal and static variables and generate forecast distribution parameters.\n",
    "\n",
    "    **Parameters:**<br>\n",
    "\n",
    "    **References**<br>\n",
    "    - [David Salinas, Valentin Flunkert, Jan Gasthaus, Tim Januschowski (2020). \"DeepAR: Probabilistic forecasting with autoregressive recurrent networks\". International Journal of Forecasting.](https://www.sciencedirect.com/science/article/pii/S0169207019301888)<br>\n",
    "    - [Alexander Alexandrov et. al (2020). \"GluonTS: Probabilistic and Neural Time Series Modeling in Python\". Journal of Machine Learning Research.](https://www.jmlr.org/papers/v21/19-820.html)<br>\n",
    "    \"\"\"\n",
    "    # Class attributes\n",
    "    SAMPLING_TYPE = 'windows'\n",
    "    \n",
    "    def __init__(self,\n",
    "                 h,\n",
    "                 input_size: int = -1,\n",
    "                 lstm_n_layers: int=2,\n",
    "                 lstm_hidden_size: int = 128,\n",
    "                 lstm_dropout: float = 0.1,\n",
    "                 decoder_hidden_layers: int = 0,\n",
    "                 decoder_hidden_size: int = 0,\n",
    "                 trayectory_samples: int = 100,\n",
    "                 futr_exog_list = None,\n",
    "                 stat_exog_list = None,\n",
    "                 exclude_insample_y = False,\n",
    "                 loss = MAE(),\n",
    "                 valid_loss = None,\n",
    "                 max_steps: int = 1000,\n",
    "                 learning_rate: float = 1e-3,\n",
    "                 num_lr_decays: int = 3,\n",
    "                 early_stop_patience_steps: int =-1,\n",
    "                 val_check_steps: int = 100,\n",
    "                 batch_size: int = 32,\n",
    "                 valid_batch_size: Optional[int] = None,\n",
    "                 windows_batch_size: int = 1024,\n",
    "                 inference_windows_batch_size: int = -1,\n",
    "                 step_size: int = 1,\n",
    "                 scaler_type: str = 'identity',\n",
    "                 random_seed: int = 1,\n",
    "                 num_workers_loader = 0,\n",
    "                 drop_last_loader = False,\n",
    "                 **trainer_kwargs):\n",
    "\n",
    "        # DeepAR doues not support historic exogenous variables \n",
    "        hist_exog_list = None,\n",
    "\n",
    "        # Inherit BaseWindows class\n",
    "        super(DeepAR, self).__init__(h=h,\n",
    "                                    input_size=input_size,\n",
    "                                    futr_exog_list=futr_exog_list,\n",
    "                                    hist_exog_list=hist_exog_list,\n",
    "                                    stat_exog_list=stat_exog_list,\n",
    "                                    exclude_insample_y = exclude_insample_y,\n",
    "                                    loss=loss,\n",
    "                                    valid_loss=valid_loss,\n",
    "                                    max_steps=max_steps,\n",
    "                                    learning_rate=learning_rate,\n",
    "                                    num_lr_decays=num_lr_decays,\n",
    "                                    early_stop_patience_steps=early_stop_patience_steps,\n",
    "                                    val_check_steps=val_check_steps,\n",
    "                                    batch_size=batch_size,\n",
    "                                    windows_batch_size=windows_batch_size,\n",
    "                                    valid_batch_size=valid_batch_size,\n",
    "                                    inference_windows_batch_size=inference_windows_batch_size,\n",
    "                                    step_size=step_size,\n",
    "                                    scaler_type=scaler_type,\n",
    "                                    num_workers_loader=num_workers_loader,\n",
    "                                    drop_last_loader=drop_last_loader,\n",
    "                                    random_seed=random_seed,\n",
    "                                    **trainer_kwargs)\n",
    "\n",
    "        self.horizon_backup = self.h # Used because h=0 during training\n",
    "        self.trayectory_samples = trayectory_samples\n",
    "\n",
    "        # LSTM\n",
    "        self.encoder_n_layers = lstm_n_layers\n",
    "        self.encoder_hidden_size = lstm_hidden_size\n",
    "        self.encoder_dropout = lstm_dropout\n",
    "\n",
    "        self.futr_exog_size = len(self.futr_exog_list)\n",
    "        self.hist_exog_size = 0\n",
    "        self.stat_exog_size = len(self.stat_exog_list)\n",
    "        \n",
    "        # LSTM input size (1 for target variable y)\n",
    "        input_encoder = 1 + self.futr_exog_size + self.stat_exog_size\n",
    "\n",
    "        # Instantiate model\n",
    "        self.hist_encoder = nn.LSTM(input_size=input_encoder,\n",
    "                                    hidden_size=self.encoder_hidden_size,\n",
    "                                    num_layers=self.encoder_n_layers,\n",
    "                                    dropout=self.encoder_dropout,\n",
    "                                    batch_first=True)\n",
    "\n",
    "        # Decoder MLP\n",
    "        self.decoder = Decoder(in_features=lstm_hidden_size,\n",
    "                               out_features=self.loss.outputsize_multiplier,\n",
    "                               hidden_size=decoder_hidden_size,\n",
    "                               hidden_layers=decoder_hidden_layers)\n",
    "\n",
    "    # Override BaseWindows method\n",
    "    def training_step(self, batch, batch_idx):\n",
    "\n",
    "        # During training h=0  \n",
    "        self.h = 0     \n",
    "\n",
    "        # Create and normalize windows [Ws, L+H, C]\n",
    "        windows = self._create_windows(batch, step='train')\n",
    "        original_insample_y = windows['temporal'][:, :, 0].clone() # windows: [B, L, Feature] -> [B, L]\n",
    "        original_insample_y = original_insample_y[:,1:] # Remove first (shift in DeepAr, cell at t outputs t+1)\n",
    "        windows = self._normalization(windows=windows)\n",
    "\n",
    "        # Parse windows\n",
    "        insample_y, insample_mask, _, _, _, futr_exog, stat_exog = self._parse_windows(batch, windows)\n",
    "\n",
    "        windows_batch = dict(insample_y=insample_y, # [Ws, L]\n",
    "                             insample_mask=insample_mask, # [Ws, L]\n",
    "                             futr_exog=futr_exog, # [Ws, L+H]\n",
    "                             hist_exog=None, # None\n",
    "                             stat_exog=stat_exog) # [Ws, 1]\n",
    "\n",
    "        # Model Predictions\n",
    "        output = self.train_forward(windows_batch)\n",
    "\n",
    "        if self.loss.is_distribution_output:\n",
    "            _, y_loc, y_scale = self._inv_normalization(y_hat=original_insample_y,\n",
    "                                            temporal_cols=batch['temporal_cols'])\n",
    "            outsample_y = original_insample_y\n",
    "            distr_args = self.loss.scale_decouple(output=output, loc=y_loc, scale=y_scale)\n",
    "            mask = insample_mask[:,1:].clone() # Remove first (shift in DeepAr, cell at t outputs t+1)\n",
    "            loss = self.loss(y=outsample_y, distr_args=distr_args, mask=mask)\n",
    "        else:\n",
    "            raise Exception('DeepAR only supports distributional outputs.')\n",
    "\n",
    "        if torch.isnan(loss):\n",
    "            print('Model Parameters', self.hparams)\n",
    "            print('insample_y', torch.isnan(insample_y).sum())\n",
    "            print('outsample_y', torch.isnan(outsample_y).sum())\n",
    "            print('output', torch.isnan(output).sum())\n",
    "            raise Exception('Loss is NaN, training stopped.')\n",
    "\n",
    "        self.log('train_loss', loss, prog_bar=True, on_epoch=True)\n",
    "        self.train_trajectories.append((self.global_step, float(loss)))\n",
    "\n",
    "        self.h = self.horizon_backup # Restore horizon\n",
    "        return loss\n",
    "\n",
    "    def predict_step(self, batch, batch_idx):\n",
    "\n",
    "        # TODO: Hack to compute number of windows\n",
    "        windows = self._create_windows(batch, step='predict')\n",
    "        n_windows = len(windows['temporal'])\n",
    "\n",
    "        # Number of windows in batch\n",
    "        windows_batch_size = self.inference_windows_batch_size\n",
    "        if windows_batch_size < 0:\n",
    "            windows_batch_size = n_windows\n",
    "        n_batches = int(np.ceil(n_windows/windows_batch_size))\n",
    "\n",
    "        y_hats = []\n",
    "        for i in range(n_batches):\n",
    "            # Create and normalize windows [Ws, L+H, C]\n",
    "            w_idxs = np.arange(i*windows_batch_size, \n",
    "                    min((i+1)*windows_batch_size, n_windows))\n",
    "            windows = self._create_windows(batch, step='predict', w_idxs=w_idxs)\n",
    "            windows = self._normalization(windows=windows)\n",
    "\n",
    "            # Parse windows\n",
    "            insample_y, insample_mask, _, _, _, futr_exog, stat_exog = self._parse_windows(batch, windows)\n",
    "            windows_batch = dict(insample_y=insample_y, # [Ws, L]\n",
    "                                insample_mask=insample_mask, # [Ws, L]\n",
    "                                futr_exog=futr_exog, # [Ws, L+H]\n",
    "                                stat_exog=stat_exog,\n",
    "                                temporal_cols=batch['temporal_cols']) \n",
    "            \n",
    "            # Model Predictions\n",
    "            y_hat = self(windows_batch)\n",
    "            # # Inverse normalization and sampling\n",
    "            # if self.loss.is_distribution_output:\n",
    "            #     _, y_loc, y_scale = self._inv_normalization(y_hat=output_batch[0],\n",
    "            #                                     temporal_cols=batch['temporal_cols'])\n",
    "            #     distr_args = self.loss.scale_decouple(output=output_batch, loc=y_loc, scale=y_scale)\n",
    "            #     _, sample_mean, quants = self.loss.sample(distr_args=distr_args)\n",
    "            #     y_hat = torch.concat((sample_mean, quants), axis=2)\n",
    "\n",
    "            #     if self.loss.return_params:\n",
    "            #         distr_args = torch.stack(distr_args, dim=-1)\n",
    "            #         distr_args = torch.reshape(distr_args, (len(windows[\"temporal\"]), self.h, -1))\n",
    "            #         y_hat = torch.concat((y_hat, distr_args), axis=2)\n",
    "            # else:\n",
    "            #     y_hat, _, _ = self._inv_normalization(y_hat=output_batch,\n",
    "            #                                     temporal_cols=batch['temporal_cols'])\n",
    "            y_hats.append(y_hat)\n",
    "        y_hat = torch.cat(y_hats, dim=0)\n",
    "        return y_hat\n",
    "\n",
    "    def train_forward(self, windows_batch):\n",
    "\n",
    "        # Parse windows_batch\n",
    "        encoder_input = windows_batch['insample_y'][:,:, None] # <- [B,T,1]\n",
    "        futr_exog  = windows_batch['futr_exog']\n",
    "        stat_exog  = windows_batch['stat_exog']\n",
    "\n",
    "        #[B, seq_len, X]\n",
    "        batch_size, seq_len = encoder_input.shape[:2]\n",
    "        if self.futr_exog_size > 0:\n",
    "            encoder_input = torch.cat((encoder_input, futr_exog), dim=2)\n",
    "        if self.stat_exog_size > 0:\n",
    "            stat_exog = stat_exog.unsqueeze(1).repeat(1, seq_len, 1) # [B, S] -> [B, seq_len, S]\n",
    "            encoder_input = torch.cat((encoder_input, stat_exog), dim=2)\n",
    "\n",
    "        # RNN forward\n",
    "        hidden_state, _ = self.hist_encoder(encoder_input) # [B, seq_len, rnn_hidden_state]\n",
    "\n",
    "        # Decoder forward\n",
    "        output = self.decoder(hidden_state) # [B, seq_len, output_size]\n",
    "        output = output[:,:-1] # Remove last (shift in DeepAr, last output is outside insample_y)\n",
    "        output = self.loss.domain_map(output)\n",
    "        return output\n",
    "    \n",
    "    def forward(self, windows_batch):\n",
    "\n",
    "        # Parse windows_batch\n",
    "        encoder_input = windows_batch['insample_y'][:,:, None] # <- [B,L,1]\n",
    "        futr_exog  = windows_batch['futr_exog'] # <- [B,L+H, n_f]\n",
    "        stat_exog  = windows_batch['stat_exog']\n",
    "        temporal_cols = windows_batch['temporal_cols']\n",
    "\n",
    "        #[B, seq_len, X]\n",
    "        batch_size, seq_len = encoder_input.shape[:2]\n",
    "        if self.futr_exog_size > 0:\n",
    "            futr_exog_input_window = futr_exog[:,:seq_len,:]\n",
    "            encoder_input = torch.cat((encoder_input, futr_exog_input_window), dim=2)\n",
    "        if self.stat_exog_size > 0:\n",
    "            stat_exog = stat_exog.unsqueeze(1).repeat(1, seq_len, 1) # [B, S] -> [B, seq_len, S]\n",
    "            encoder_input = torch.cat((encoder_input, stat_exog), dim=2)\n",
    "\n",
    "        # Use input_size history to predict first h of the forecasting window\n",
    "        _, h_c_tuple = self.hist_encoder(encoder_input)\n",
    "        h_n = h_c_tuple[0] # [n_layers, B, rnn_hidden_state]\n",
    "        c_n = h_c_tuple[1] # [n_layers, B, rnn_hidden_state]\n",
    "\n",
    "        # Vectorizes trayectory samples in batch dimension [1]\n",
    "        h_n = torch.repeat_interleave(h_n, self.trayectory_samples, 1) # [n_layers, B*n_samples, rnn_hidden_state]\n",
    "        c_n = torch.repeat_interleave(c_n, self.trayectory_samples, 1) # [n_layers, B*n_samples, rnn_hidden_state]\n",
    "        last_layer_h = h_n[-1] # [B, rnn_hidden_state]\n",
    "        print('h_c_tuple', h_n.shape, c_n.shape)\n",
    "        print('last_h', last_layer_h.shape)\n",
    "\n",
    "        # Scales for inverse normalization\n",
    "        y_scale = self.scaler.x_scale[:,0,temporal_cols.get_indexer(['y'])].squeeze(-1)\n",
    "        y_loc = self.scaler.x_shift[:,0,temporal_cols.get_indexer(['y'])].squeeze(-1)\n",
    "        y_scale = torch.repeat_interleave(y_scale, self.trayectory_samples, 0)\n",
    "        y_loc = torch.repeat_interleave(y_loc, self.trayectory_samples, 0)\n",
    "        print('y_scale', y_scale.shape)\n",
    "        print('y_loc', y_loc.shape)\n",
    "        print('y_scale', y_scale)\n",
    "        print('y_loc', y_loc)\n",
    "        \n",
    "        # Recursive strategy prediction\n",
    "        samples = torch.zeros(batch_size, self.h, self.trayectory_samples)\n",
    "        for tau in range(self.h):\n",
    "            # Decoder forward\n",
    "            output = self.decoder(last_layer_h) \n",
    "            output = self.loss.domain_map(output)\n",
    "            print('output[0]', output[0].shape)\n",
    "            print('output[1]', output[1].shape)\n",
    "\n",
    "            # Inverse normalization\n",
    "            distr_args = self.loss.scale_decouple(output=output, loc=y_loc, scale=y_scale)\n",
    "            # Add horizon (1) dimension\n",
    "            distr_args = list(distr_args)\n",
    "            for i in range(len(distr_args)):\n",
    "                distr_args[i] = distr_args[i].unsqueeze(-1)\n",
    "            distr_args = tuple(distr_args)\n",
    "            print('distr_args[0].shape', distr_args[0].shape)\n",
    "            print('distr_args[1].shape', distr_args[1].shape)\n",
    "            # Assuming normal for now\n",
    "            samples_tau, _, _ = self.loss.sample(distr_args=distr_args, num_samples=1)\n",
    "            samples[:,tau,:] = samples_tau.reshape(batch_size, self.trayectory_samples)\n",
    "            print('samples_tau.flatten()', samples_tau.flatten())\n",
    "            print(samples.mean(dim=-1))\n",
    "            assert 1<0, 'STOP'\n",
    "            #y_hat = torch.concat((sample_mean, quants), axis=2)\n",
    "\n",
    "            # if self.loss.return_params:\n",
    "            #     distr_args = torch.stack(distr_args, dim=-1)\n",
    "            #     distr_args = torch.reshape(distr_args, (len(windows[\"temporal\"]), self.h, -1))\n",
    "            #     y_hat = torch.concat((y_hat, distr_args), axis=2)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/models/deepar.py#L60){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DeepAR\n",
       "\n",
       ">      DeepAR (h, input_size:int=-1, lstm_n_layers:int=2,\n",
       ">              lstm_hidden_size:int=128, lstm_dropout:float=0.1,\n",
       ">              decoder_hidden_layers:int=0, decoder_hidden_size:int=0,\n",
       ">              trayectory_samples:int=100, futr_exog_list=None,\n",
       ">              stat_exog_list=None, exclude_insample_y=False, loss=MAE(),\n",
       ">              valid_loss=None, max_steps:int=1000, learning_rate:float=0.001,\n",
       ">              num_lr_decays:int=3, early_stop_patience_steps:int=-1,\n",
       ">              val_check_steps:int=100, batch_size:int=32,\n",
       ">              valid_batch_size:Optional[int]=None, windows_batch_size:int=1024,\n",
       ">              inference_windows_batch_size:int=-1, step_size:int=1,\n",
       ">              scaler_type:str='identity', random_seed:int=1,\n",
       ">              num_workers_loader=0, drop_last_loader=False, **trainer_kwargs)\n",
       "\n",
       "DeepAR\n",
       "\n",
       "DeepAR is a method for producing probabilistic forecasts. It uses two recurrent neural networks (RNN) \n",
       "to encode temporal and static variables and generate forecast distribution parameters.\n",
       "\n",
       "**Parameters:**<br>\n",
       "\n",
       "**References**<br>\n",
       "- [David Salinas, Valentin Flunkert, Jan Gasthaus, Tim Januschowski (2020). \"DeepAR: Probabilistic forecasting with autoregressive recurrent networks\". International Journal of Forecasting.](https://www.sciencedirect.com/science/article/pii/S0169207019301888)<br>\n",
       "- [Alexander Alexandrov et. al (2020). \"GluonTS: Probabilistic and Neural Time Series Modeling in Python\". Journal of Machine Learning Research.](https://www.jmlr.org/papers/v21/19-820.html)<br>"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/models/deepar.py#L60){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DeepAR\n",
       "\n",
       ">      DeepAR (h, input_size:int=-1, lstm_n_layers:int=2,\n",
       ">              lstm_hidden_size:int=128, lstm_dropout:float=0.1,\n",
       ">              decoder_hidden_layers:int=0, decoder_hidden_size:int=0,\n",
       ">              trayectory_samples:int=100, futr_exog_list=None,\n",
       ">              stat_exog_list=None, exclude_insample_y=False, loss=MAE(),\n",
       ">              valid_loss=None, max_steps:int=1000, learning_rate:float=0.001,\n",
       ">              num_lr_decays:int=3, early_stop_patience_steps:int=-1,\n",
       ">              val_check_steps:int=100, batch_size:int=32,\n",
       ">              valid_batch_size:Optional[int]=None, windows_batch_size:int=1024,\n",
       ">              inference_windows_batch_size:int=-1, step_size:int=1,\n",
       ">              scaler_type:str='identity', random_seed:int=1,\n",
       ">              num_workers_loader=0, drop_last_loader=False, **trainer_kwargs)\n",
       "\n",
       "DeepAR\n",
       "\n",
       "DeepAR is a method for producing probabilistic forecasts. It uses two recurrent neural networks (RNN) \n",
       "to encode temporal and static variables and generate forecast distribution parameters.\n",
       "\n",
       "**Parameters:**<br>\n",
       "\n",
       "**References**<br>\n",
       "- [David Salinas, Valentin Flunkert, Jan Gasthaus, Tim Januschowski (2020). \"DeepAR: Probabilistic forecasting with autoregressive recurrent networks\". International Journal of Forecasting.](https://www.sciencedirect.com/science/article/pii/S0169207019301888)<br>\n",
       "- [Alexander Alexandrov et. al (2020). \"GluonTS: Probabilistic and Neural Time Series Modeling in Python\". Journal of Machine Learning Research.](https://www.jmlr.org/papers/v21/19-820.html)<br>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DeepAR, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### DeepAR.fit\n",
       "\n",
       ">      DeepAR.fit (dataset, val_size=0, test_size=0, random_seed=None)\n",
       "\n",
       "Fit.\n",
       "\n",
       "The `fit` method, optimizes the neural network's weights using the\n",
       "initialization parameters (`learning_rate`, `windows_batch_size`, ...)\n",
       "and the `loss` function as defined during the initialization.\n",
       "Within `fit` we use a PyTorch Lightning `Trainer` that\n",
       "inherits the initialization's `self.trainer_kwargs`, to customize\n",
       "its inputs, see [PL's trainer arguments](https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.trainer.trainer.Trainer.html?highlight=trainer).\n",
       "\n",
       "The method is designed to be compatible with SKLearn-like classes\n",
       "and in particular to be compatible with the StatsForecast library.\n",
       "\n",
       "By default the `model` is not saving training checkpoints to protect\n",
       "disk memory, to get them change `enable_checkpointing=True` in `__init__`.\n",
       "\n",
       "**Parameters:**<br>\n",
       "`dataset`: NeuralForecast's `TimeSeriesDataset`, see [documentation](https://nixtla.github.io/neuralforecast/tsdataset.html).<br>\n",
       "`val_size`: int, validation size for temporal cross-validation.<br>\n",
       "`random_seed`: int=None, random_seed for pytorch initializer and numpy generators, overwrites model.__init__'s.<br>\n",
       "`test_size`: int, test size for temporal cross-validation.<br>"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### DeepAR.fit\n",
       "\n",
       ">      DeepAR.fit (dataset, val_size=0, test_size=0, random_seed=None)\n",
       "\n",
       "Fit.\n",
       "\n",
       "The `fit` method, optimizes the neural network's weights using the\n",
       "initialization parameters (`learning_rate`, `windows_batch_size`, ...)\n",
       "and the `loss` function as defined during the initialization.\n",
       "Within `fit` we use a PyTorch Lightning `Trainer` that\n",
       "inherits the initialization's `self.trainer_kwargs`, to customize\n",
       "its inputs, see [PL's trainer arguments](https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.trainer.trainer.Trainer.html?highlight=trainer).\n",
       "\n",
       "The method is designed to be compatible with SKLearn-like classes\n",
       "and in particular to be compatible with the StatsForecast library.\n",
       "\n",
       "By default the `model` is not saving training checkpoints to protect\n",
       "disk memory, to get them change `enable_checkpointing=True` in `__init__`.\n",
       "\n",
       "**Parameters:**<br>\n",
       "`dataset`: NeuralForecast's `TimeSeriesDataset`, see [documentation](https://nixtla.github.io/neuralforecast/tsdataset.html).<br>\n",
       "`val_size`: int, validation size for temporal cross-validation.<br>\n",
       "`random_seed`: int=None, random_seed for pytorch initializer and numpy generators, overwrites model.__init__'s.<br>\n",
       "`test_size`: int, test size for temporal cross-validation.<br>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DeepAR.fit, name='DeepAR.fit', title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### DeepAR.predict\n",
       "\n",
       ">      DeepAR.predict (dataset, test_size=None, step_size=1, random_seed=None,\n",
       ">                      **data_module_kwargs)\n",
       "\n",
       "Predict.\n",
       "\n",
       "Neural network prediction with PL's `Trainer` execution of `predict_step`.\n",
       "\n",
       "**Parameters:**<br>\n",
       "`dataset`: NeuralForecast's `TimeSeriesDataset`, see [documentation](https://nixtla.github.io/neuralforecast/tsdataset.html).<br>\n",
       "`test_size`: int=None, test size for temporal cross-validation.<br>\n",
       "`step_size`: int=1, Step size between each window.<br>\n",
       "`random_seed`: int=None, random_seed for pytorch initializer and numpy generators, overwrites model.__init__'s.<br>\n",
       "`**data_module_kwargs`: PL's TimeSeriesDataModule args, see [documentation](https://pytorch-lightning.readthedocs.io/en/1.6.1/extensions/datamodules.html#using-a-datamodule)."
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### DeepAR.predict\n",
       "\n",
       ">      DeepAR.predict (dataset, test_size=None, step_size=1, random_seed=None,\n",
       ">                      **data_module_kwargs)\n",
       "\n",
       "Predict.\n",
       "\n",
       "Neural network prediction with PL's `Trainer` execution of `predict_step`.\n",
       "\n",
       "**Parameters:**<br>\n",
       "`dataset`: NeuralForecast's `TimeSeriesDataset`, see [documentation](https://nixtla.github.io/neuralforecast/tsdataset.html).<br>\n",
       "`test_size`: int=None, test size for temporal cross-validation.<br>\n",
       "`step_size`: int=1, Step size between each window.<br>\n",
       "`random_seed`: int=None, random_seed for pytorch initializer and numpy generators, overwrites model.__init__'s.<br>\n",
       "`**data_module_kwargs`: PL's TimeSeriesDataModule args, see [documentation](https://pytorch-lightning.readthedocs.io/en/1.6.1/extensions/datamodules.html#using-a-datamodule)."
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DeepAR.predict, name='DeepAR.predict', title_level=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralforecast import NeuralForecast\n",
    "from neuralforecast.losses.pytorch import MQLoss, DistributionLoss, GMM, PMM\n",
    "from neuralforecast.tsdataset import TimeSeriesDataset\n",
    "from neuralforecast.utils import AirPassengers, AirPassengersPanel, AirPassengersStatic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 1/1 [00:00<00:00,  1.67it/s, v_num=65, train_loss_step=5.160, train_loss_epoch=5.160]\n",
      "Predicting DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]h_c_tuple torch.Size([3, 200, 128]) torch.Size([3, 200, 128])\n",
      "last_h torch.Size([200, 128])\n",
      "y_scale torch.Size([200])\n",
      "y_loc torch.Size([200])\n",
      "y_scale tensor([68.1163, 68.1163, 68.1163, 68.1163, 68.1163, 68.1163, 68.1163, 68.1163,\n",
      "        68.1163, 68.1163, 68.1163, 68.1163, 68.1163, 68.1163, 68.1163, 68.1163,\n",
      "        68.1163, 68.1163, 68.1163, 68.1163, 68.1163, 68.1163, 68.1163, 68.1163,\n",
      "        68.1163, 68.1163, 68.1163, 68.1163, 68.1163, 68.1163, 68.1163, 68.1163,\n",
      "        68.1163, 68.1163, 68.1163, 68.1163, 68.1163, 68.1163, 68.1163, 68.1163,\n",
      "        68.1163, 68.1163, 68.1163, 68.1163, 68.1163, 68.1163, 68.1163, 68.1163,\n",
      "        68.1163, 68.1163, 68.1163, 68.1163, 68.1163, 68.1163, 68.1163, 68.1163,\n",
      "        68.1163, 68.1163, 68.1163, 68.1163, 68.1163, 68.1163, 68.1163, 68.1163,\n",
      "        68.1163, 68.1163, 68.1163, 68.1163, 68.1163, 68.1163, 68.1163, 68.1163,\n",
      "        68.1163, 68.1163, 68.1163, 68.1163, 68.1163, 68.1163, 68.1163, 68.1163,\n",
      "        68.1163, 68.1163, 68.1163, 68.1163, 68.1163, 68.1163, 68.1163, 68.1163,\n",
      "        68.1163, 68.1163, 68.1163, 68.1163, 68.1163, 68.1163, 68.1163, 68.1163,\n",
      "        68.1163, 68.1163, 68.1163, 68.1163, 68.1163, 68.1163, 68.1163, 68.1163,\n",
      "        68.1163, 68.1163, 68.1163, 68.1163, 68.1163, 68.1163, 68.1163, 68.1163,\n",
      "        68.1163, 68.1163, 68.1163, 68.1163, 68.1163, 68.1163, 68.1163, 68.1163,\n",
      "        68.1163, 68.1163, 68.1163, 68.1163, 68.1163, 68.1163, 68.1163, 68.1163,\n",
      "        68.1163, 68.1163, 68.1163, 68.1163, 68.1163, 68.1163, 68.1163, 68.1163,\n",
      "        68.1163, 68.1163, 68.1163, 68.1163, 68.1163, 68.1163, 68.1163, 68.1163,\n",
      "        68.1163, 68.1163, 68.1163, 68.1163, 68.1163, 68.1163, 68.1163, 68.1163,\n",
      "        68.1163, 68.1163, 68.1163, 68.1163, 68.1163, 68.1163, 68.1163, 68.1163,\n",
      "        68.1163, 68.1163, 68.1163, 68.1163, 68.1163, 68.1163, 68.1163, 68.1163,\n",
      "        68.1163, 68.1163, 68.1163, 68.1163, 68.1163, 68.1163, 68.1163, 68.1163,\n",
      "        68.1163, 68.1163, 68.1163, 68.1163, 68.1163, 68.1163, 68.1163, 68.1163,\n",
      "        68.1163, 68.1163, 68.1163, 68.1163, 68.1163, 68.1163, 68.1163, 68.1163,\n",
      "        68.1163, 68.1163, 68.1163, 68.1163, 68.1163, 68.1163, 68.1163, 68.1163])\n",
      "y_loc tensor([376.5000, 376.5000, 376.5000, 376.5000, 376.5000, 376.5000, 376.5000,\n",
      "        376.5000, 376.5000, 376.5000, 376.5000, 376.5000, 376.5000, 376.5000,\n",
      "        376.5000, 376.5000, 376.5000, 376.5000, 376.5000, 376.5000, 376.5000,\n",
      "        376.5000, 376.5000, 376.5000, 376.5000, 376.5000, 376.5000, 376.5000,\n",
      "        376.5000, 376.5000, 376.5000, 376.5000, 376.5000, 376.5000, 376.5000,\n",
      "        376.5000, 376.5000, 376.5000, 376.5000, 376.5000, 376.5000, 376.5000,\n",
      "        376.5000, 376.5000, 376.5000, 376.5000, 376.5000, 376.5000, 376.5000,\n",
      "        376.5000, 376.5000, 376.5000, 376.5000, 376.5000, 376.5000, 376.5000,\n",
      "        376.5000, 376.5000, 376.5000, 376.5000, 376.5000, 376.5000, 376.5000,\n",
      "        376.5000, 376.5000, 376.5000, 376.5000, 376.5000, 376.5000, 376.5000,\n",
      "        376.5000, 376.5000, 376.5000, 376.5000, 376.5000, 376.5000, 376.5000,\n",
      "        376.5000, 376.5000, 376.5000, 376.5000, 376.5000, 376.5000, 376.5000,\n",
      "        376.5000, 376.5000, 376.5000, 376.5000, 376.5000, 376.5000, 376.5000,\n",
      "        376.5000, 376.5000, 376.5000, 376.5000, 376.5000, 376.5000, 376.5000,\n",
      "        376.5000, 376.5000, 676.5000, 676.5000, 676.5000, 676.5000, 676.5000,\n",
      "        676.5000, 676.5000, 676.5000, 676.5000, 676.5000, 676.5000, 676.5000,\n",
      "        676.5000, 676.5000, 676.5000, 676.5000, 676.5000, 676.5000, 676.5000,\n",
      "        676.5000, 676.5000, 676.5000, 676.5000, 676.5000, 676.5000, 676.5000,\n",
      "        676.5000, 676.5000, 676.5000, 676.5000, 676.5000, 676.5000, 676.5000,\n",
      "        676.5000, 676.5000, 676.5000, 676.5000, 676.5000, 676.5000, 676.5000,\n",
      "        676.5000, 676.5000, 676.5000, 676.5000, 676.5000, 676.5000, 676.5000,\n",
      "        676.5000, 676.5000, 676.5000, 676.5000, 676.5000, 676.5000, 676.5000,\n",
      "        676.5000, 676.5000, 676.5000, 676.5000, 676.5000, 676.5000, 676.5000,\n",
      "        676.5000, 676.5000, 676.5000, 676.5000, 676.5000, 676.5000, 676.5000,\n",
      "        676.5000, 676.5000, 676.5000, 676.5000, 676.5000, 676.5000, 676.5000,\n",
      "        676.5000, 676.5000, 676.5000, 676.5000, 676.5000, 676.5000, 676.5000,\n",
      "        676.5000, 676.5000, 676.5000, 676.5000, 676.5000, 676.5000, 676.5000,\n",
      "        676.5000, 676.5000, 676.5000, 676.5000, 676.5000, 676.5000, 676.5000,\n",
      "        676.5000, 676.5000, 676.5000, 676.5000])\n",
      "output[0] torch.Size([200])\n",
      "output[1] torch.Size([200])\n",
      "distr_args[0].shape torch.Size([200, 1])\n",
      "distr_args[1].shape torch.Size([200, 1])\n",
      "samples_tau.flatten() tensor([438.8217, 407.6120, 384.8904, 346.3045, 373.4700, 379.0668, 525.7023,\n",
      "        403.1629, 431.3352, 548.4055, 458.9688, 473.6355, 409.9547, 445.7903,\n",
      "        411.2722, 382.9585, 498.0416, 459.9485, 404.3181, 494.4619, 471.8884,\n",
      "        516.7733, 553.3298, 439.1164, 448.7129, 407.0874, 396.0311, 436.8931,\n",
      "        396.0084, 465.5480, 461.8448, 550.1952, 457.1681, 447.7654, 454.3416,\n",
      "        415.7979, 329.5633, 397.3509, 569.0735, 439.6572, 589.4852, 381.3608,\n",
      "        541.5191, 406.3454, 592.5427, 462.0431, 460.0925, 517.5781, 491.7207,\n",
      "        472.0337, 516.4016, 568.5333, 458.1568, 412.8233, 334.3934, 364.7460,\n",
      "        539.3393, 391.6461, 240.7198, 399.1089, 513.4715, 463.2429, 451.7007,\n",
      "        405.3343, 432.0504, 444.5005, 547.5663, 445.5974, 505.2776, 498.8751,\n",
      "        356.0355, 372.6006, 416.6966, 410.6229, 469.2173, 384.7368, 526.2887,\n",
      "        497.3690, 427.7635, 487.8835, 357.0166, 414.2569, 455.4867, 435.1536,\n",
      "        414.0202, 354.5526, 424.0201, 426.5535, 380.6863, 416.0923, 462.8138,\n",
      "        567.0197, 388.2672, 387.9359, 532.6500, 470.7622, 529.8909, 395.0638,\n",
      "        388.0907, 513.1412, 784.3840, 758.5970, 588.7897, 798.9613, 618.2093,\n",
      "        708.8329, 656.3560, 677.4403, 740.5674, 767.7404, 681.3011, 693.2868,\n",
      "        759.2990, 765.4017, 779.5010, 720.3797, 756.3626, 823.4211, 827.9190,\n",
      "        716.4528, 625.6495, 856.7233, 752.9708, 767.5357, 707.3260, 729.1948,\n",
      "        949.8278, 722.1606, 728.2180, 639.0141, 785.5642, 697.5612, 683.3288,\n",
      "        681.5073, 733.9960, 733.5668, 887.0906, 788.0138, 851.5098, 637.0310,\n",
      "        653.7530, 802.9746, 713.2153, 725.5127, 775.6656, 720.5384, 780.4600,\n",
      "        652.1283, 793.0630, 721.8731, 738.9349, 763.8889, 853.5092, 860.1711,\n",
      "        703.5530, 903.1035, 793.8611, 681.0790, 734.4236, 712.4871, 708.5874,\n",
      "        686.9866, 800.2394, 807.1146, 762.0607, 698.2538, 778.7314, 821.5651,\n",
      "        722.0749, 688.0966, 746.4269, 741.9231, 722.9519, 741.8781, 793.2463,\n",
      "        685.4444, 687.8011, 755.3989, 736.7380, 724.8411, 743.1105, 594.3708,\n",
      "        761.5590, 764.3746, 851.2214, 795.1295, 809.2993, 806.9089, 802.1150,\n",
      "        810.7231, 714.9124, 829.5863, 780.8643, 691.3533, 720.1096, 710.2933,\n",
      "        687.4934, 823.2791, 820.7531, 714.4646])\n",
      "tensor([[446.4316,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "        [747.0747,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000]])\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "STOP",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[85], line 34\u001b[0m\n\u001b[1;32m     16\u001b[0m nf \u001b[39m=\u001b[39m NeuralForecast(\n\u001b[1;32m     17\u001b[0m     models\u001b[39m=\u001b[39m[DeepAR(h\u001b[39m=\u001b[39m\u001b[39m12\u001b[39m,\n\u001b[1;32m     18\u001b[0m                    input_size\u001b[39m=\u001b[39m\u001b[39m48\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m     freq\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mM\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     32\u001b[0m )\n\u001b[1;32m     33\u001b[0m nf\u001b[39m.\u001b[39mfit(df\u001b[39m=\u001b[39mY_train_df, static_df\u001b[39m=\u001b[39mAirPassengersStatic, val_size\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m---> 34\u001b[0m Y_hat_df \u001b[39m=\u001b[39m nf\u001b[39m.\u001b[39;49mpredict(futr_df\u001b[39m=\u001b[39;49mY_test_df)\n\u001b[1;32m     36\u001b[0m \u001b[39m# # Plot quantile predictions\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[39m# Y_hat_df = Y_hat_df.reset_index(drop=False).drop(columns=['unique_id','ds'])\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[39m# plot_df = pd.concat([Y_test_df, Y_hat_df], axis=1)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39m# plt.grid()\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[39m# plt.plot()\u001b[39;00m\n",
      "File \u001b[0;32m~/Nixtla/neuralforecast/neuralforecast/core.py:345\u001b[0m, in \u001b[0;36mNeuralForecast.predict\u001b[0;34m(self, df, static_df, futr_df, sort_df, verbose, **data_kwargs)\u001b[0m\n\u001b[1;32m    343\u001b[0m old_test_size \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mget_test_size()\n\u001b[1;32m    344\u001b[0m model\u001b[39m.\u001b[39mset_test_size(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mh)  \u001b[39m# To predict h steps ahead\u001b[39;00m\n\u001b[0;32m--> 345\u001b[0m model_fcsts \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict(dataset\u001b[39m=\u001b[39;49mdataset, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mdata_kwargs)\n\u001b[1;32m    346\u001b[0m \u001b[39m# Append predictions in memory placeholder\u001b[39;00m\n\u001b[1;32m    347\u001b[0m output_length \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(model\u001b[39m.\u001b[39mloss\u001b[39m.\u001b[39moutput_names)\n",
      "File \u001b[0;32m~/Nixtla/neuralforecast/neuralforecast/common/_base_windows.py:735\u001b[0m, in \u001b[0;36mBaseWindows.predict\u001b[0;34m(self, dataset, test_size, step_size, random_seed, **data_module_kwargs)\u001b[0m\n\u001b[1;32m    732\u001b[0m     pred_trainer_kwargs[\u001b[39m\"\u001b[39m\u001b[39mdevices\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m [\u001b[39m0\u001b[39m]\n\u001b[1;32m    734\u001b[0m trainer \u001b[39m=\u001b[39m pl\u001b[39m.\u001b[39mTrainer(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpred_trainer_kwargs)\n\u001b[0;32m--> 735\u001b[0m fcsts \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39;49mpredict(\u001b[39mself\u001b[39;49m, datamodule\u001b[39m=\u001b[39;49mdatamodule)\n\u001b[1;32m    736\u001b[0m fcsts \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mvstack(fcsts)\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mflatten()\n\u001b[1;32m    737\u001b[0m fcsts \u001b[39m=\u001b[39m fcsts\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss\u001b[39m.\u001b[39moutput_names))\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/neuralforecast/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:805\u001b[0m, in \u001b[0;36mTrainer.predict\u001b[0;34m(self, model, dataloaders, datamodule, return_predictions, ckpt_path)\u001b[0m\n\u001b[1;32m    803\u001b[0m     model \u001b[39m=\u001b[39m _maybe_unwrap_optimized(model)\n\u001b[1;32m    804\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m_lightning_module \u001b[39m=\u001b[39m model\n\u001b[0;32m--> 805\u001b[0m \u001b[39mreturn\u001b[39;00m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    806\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_predict_impl, model, dataloaders, datamodule, return_predictions, ckpt_path\n\u001b[1;32m    807\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/neuralforecast/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39mtrainer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     43\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 44\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     46\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/neuralforecast/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:847\u001b[0m, in \u001b[0;36mTrainer._predict_impl\u001b[0;34m(self, model, dataloaders, datamodule, return_predictions, ckpt_path)\u001b[0m\n\u001b[1;32m    842\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_connector\u001b[39m.\u001b[39mattach_data(model, predict_dataloaders\u001b[39m=\u001b[39mdataloaders, datamodule\u001b[39m=\u001b[39mdatamodule)\n\u001b[1;32m    844\u001b[0m ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    845\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn, ckpt_path, model_provided\u001b[39m=\u001b[39mmodel_provided, model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    846\u001b[0m )\n\u001b[0;32m--> 847\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49mckpt_path)\n\u001b[1;32m    849\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m    850\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredicting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/neuralforecast/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:935\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_signal_connector\u001b[39m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    932\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    933\u001b[0m \u001b[39m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    934\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 935\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[1;32m    937\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    938\u001b[0m \u001b[39m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    939\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    940\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/neuralforecast/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:973\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    971\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_evaluation_loop\u001b[39m.\u001b[39mrun()\n\u001b[1;32m    972\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredicting:\n\u001b[0;32m--> 973\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict_loop\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m    974\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining:\n\u001b[1;32m    975\u001b[0m     \u001b[39mwith\u001b[39;00m isolate_rng():\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/neuralforecast/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:177\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    175\u001b[0m     context_manager \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mno_grad\n\u001b[1;32m    176\u001b[0m \u001b[39mwith\u001b[39;00m context_manager():\n\u001b[0;32m--> 177\u001b[0m     \u001b[39mreturn\u001b[39;00m loop_run(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/neuralforecast/lib/python3.10/site-packages/pytorch_lightning/loops/prediction_loop.py:112\u001b[0m, in \u001b[0;36m_PredictionLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m     batch, batch_idx, dataloader_idx \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(data_fetcher)\n\u001b[1;32m    111\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mis_last_batch \u001b[39m=\u001b[39m data_fetcher\u001b[39m.\u001b[39mdone\n\u001b[0;32m--> 112\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_predict_step(batch, batch_idx, dataloader_idx)\n\u001b[1;32m    113\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m    114\u001b[0m     \u001b[39m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n\u001b[1;32m    115\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/neuralforecast/lib/python3.10/site-packages/pytorch_lightning/loops/prediction_loop.py:228\u001b[0m, in \u001b[0;36m_PredictionLoop._predict_step\u001b[0;34m(self, batch, batch_idx, dataloader_idx)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_started()\n\u001b[1;32m    227\u001b[0m \u001b[39m# configure step_kwargs\u001b[39;00m\n\u001b[0;32m--> 228\u001b[0m predictions \u001b[39m=\u001b[39m call\u001b[39m.\u001b[39;49m_call_strategy_hook(trainer, \u001b[39m\"\u001b[39;49m\u001b[39mpredict_step\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m*\u001b[39;49mstep_kwargs\u001b[39m.\u001b[39;49mvalues())\n\u001b[1;32m    230\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_processed()\n\u001b[1;32m    232\u001b[0m \u001b[39mif\u001b[39;00m predictions \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/neuralforecast/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:288\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[39mwith\u001b[39;00m trainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[Strategy]\u001b[39m\u001b[39m{\u001b[39;00mtrainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 288\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    290\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    291\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/neuralforecast/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py:396\u001b[0m, in \u001b[0;36mStrategy.predict_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprecision_plugin\u001b[39m.\u001b[39mpredict_step_context():\n\u001b[1;32m    395\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, PredictStep)\n\u001b[0;32m--> 396\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mpredict_step(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "Cell \u001b[0;32mIn[80], line 179\u001b[0m, in \u001b[0;36mDeepAR.predict_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m    172\u001b[0m windows_batch \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(insample_y\u001b[39m=\u001b[39minsample_y, \u001b[39m# [Ws, L]\u001b[39;00m\n\u001b[1;32m    173\u001b[0m                     insample_mask\u001b[39m=\u001b[39minsample_mask, \u001b[39m# [Ws, L]\u001b[39;00m\n\u001b[1;32m    174\u001b[0m                     futr_exog\u001b[39m=\u001b[39mfutr_exog, \u001b[39m# [Ws, L+H]\u001b[39;00m\n\u001b[1;32m    175\u001b[0m                     stat_exog\u001b[39m=\u001b[39mstat_exog,\n\u001b[1;32m    176\u001b[0m                     temporal_cols\u001b[39m=\u001b[39mbatch[\u001b[39m'\u001b[39m\u001b[39mtemporal_cols\u001b[39m\u001b[39m'\u001b[39m]) \n\u001b[1;32m    178\u001b[0m \u001b[39m# Model Predictions\u001b[39;00m\n\u001b[0;32m--> 179\u001b[0m y_hat \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(windows_batch)\n\u001b[1;32m    180\u001b[0m \u001b[39m# # Inverse normalization and sampling\u001b[39;00m\n\u001b[1;32m    181\u001b[0m \u001b[39m# if self.loss.is_distribution_output:\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \u001b[39m#     _, y_loc, y_scale = self._inv_normalization(y_hat=output_batch[0],\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[39m#     y_hat, _, _ = self._inv_normalization(y_hat=output_batch,\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \u001b[39m#                                     temporal_cols=batch['temporal_cols'])\u001b[39;00m\n\u001b[1;32m    195\u001b[0m y_hats\u001b[39m.\u001b[39mappend(y_hat)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/neuralforecast/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[80], line 285\u001b[0m, in \u001b[0;36mDeepAR.forward\u001b[0;34m(self, windows_batch)\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39msamples_tau.flatten()\u001b[39m\u001b[39m'\u001b[39m, samples_tau\u001b[39m.\u001b[39mflatten())\n\u001b[1;32m    284\u001b[0m     \u001b[39mprint\u001b[39m(samples\u001b[39m.\u001b[39mmean(dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[0;32m--> 285\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39m1\u001b[39m\u001b[39m<\u001b[39m\u001b[39m0\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mSTOP\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    286\u001b[0m     \u001b[39m#y_hat = torch.concat((sample_mean, quants), axis=2)\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \n\u001b[1;32m    288\u001b[0m     \u001b[39m# if self.loss.return_params:\u001b[39;00m\n\u001b[1;32m    289\u001b[0m     \u001b[39m#     distr_args = torch.stack(distr_args, dim=-1)\u001b[39;00m\n\u001b[1;32m    290\u001b[0m     \u001b[39m#     distr_args = torch.reshape(distr_args, (len(windows[\"temporal\"]), self.h, -1))\u001b[39;00m\n\u001b[1;32m    291\u001b[0m     \u001b[39m#     y_hat = torch.concat((y_hat, distr_args), axis=2)\u001b[39;00m\n\u001b[1;32m    293\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "\u001b[0;31mAssertionError\u001b[0m: STOP"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from neuralforecast import NeuralForecast\n",
    "#from neuralforecast.models import DeepAR\n",
    "from neuralforecast.losses.pytorch import DistributionLoss, HuberMQLoss\n",
    "from neuralforecast.tsdataset import TimeSeriesDataset\n",
    "from neuralforecast.utils import AirPassengers, AirPassengersPanel, AirPassengersStatic\n",
    "\n",
    "#AirPassengersPanel['y'] = AirPassengersPanel['y'] + 10\n",
    "Y_train_df = AirPassengersPanel[AirPassengersPanel.ds<AirPassengersPanel['ds'].values[-12]] # 132 train\n",
    "Y_test_df = AirPassengersPanel[AirPassengersPanel.ds>=AirPassengersPanel['ds'].values[-12]].reset_index(drop=True) # 12 test\n",
    "\n",
    "nf = NeuralForecast(\n",
    "    models=[DeepAR(h=12,\n",
    "                   input_size=48,\n",
    "                   lstm_n_layers=3,\n",
    "                   trayectory_samples=100,\n",
    "                   loss=DistributionLoss(distribution='Normal', level=[80, 90]),\n",
    "                   learning_rate=0.005,\n",
    "                   stat_exog_list=['airline1'],\n",
    "                   futr_exog_list=['y_[lag12]'],\n",
    "                   max_steps=10,\n",
    "                   val_check_steps=5,\n",
    "                   early_stop_patience_steps=-1,\n",
    "                   scaler_type='standard',\n",
    "                   enable_progress_bar=True),\n",
    "    ],\n",
    "    freq='M'\n",
    ")\n",
    "nf.fit(df=Y_train_df, static_df=AirPassengersStatic, val_size=0)\n",
    "Y_hat_df = nf.predict(futr_df=Y_test_df)\n",
    "\n",
    "# # Plot quantile predictions\n",
    "# Y_hat_df = Y_hat_df.reset_index(drop=False).drop(columns=['unique_id','ds'])\n",
    "# plot_df = pd.concat([Y_test_df, Y_hat_df], axis=1)\n",
    "# plot_df = pd.concat([Y_train_df, plot_df])\n",
    "\n",
    "# plot_df = plot_df[plot_df.unique_id=='Airline1'].drop('unique_id', axis=1)\n",
    "# plt.plot(plot_df['ds'], plot_df['y'], c='black', label='True')\n",
    "# #plt.plot(plot_df['ds'], plot_df['DeepAR'], c='purple', label='mean')\n",
    "# plt.plot(plot_df['ds'], plot_df['DeepAR-median'], c='blue', label='median')\n",
    "# plt.fill_between(x=plot_df['ds'][-12:], \n",
    "#                  y1=plot_df['DeepAR-lo-90'][-12:].values, \n",
    "#                  y2=plot_df['DeepAR-hi-90'][-12:].values,\n",
    "#                  alpha=0.4, label='level 90')\n",
    "# plt.legend()\n",
    "# plt.grid()\n",
    "# plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
