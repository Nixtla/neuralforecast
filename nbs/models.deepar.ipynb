{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp models.deepar"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepAR"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DeepAR methodology produces conditional probabilistic forecasts based on an Seq2Seq autoregressive neural network optimized on panel data using cross-learning.\n",
    "$$\\mathbb{P}(\\mathbf{y}_{[t+1:t+H]}|\\;\\mathbf{y}_{[:t]},\\; \\mathbf{x}^{(h)}_{[:t]},\\; \\mathbf{x}^{(f)}_{[:t+H]},\\; \\mathbf{x}^{(s)})$$\n",
    "\n",
    "where $\\mathbf{x}^{(s)}$ are static exogenous inputs, $\\mathbf{x}^{(h)}_{t}$ historic exogenous, $\\mathbf{x}^{(f)}_{[:t+H]}$ are future exogenous available at the time of the prediction.\n",
    "\n",
    "Our version of DeepAR first encodes the temporal variables with a RNN and then obtains its forecast distribution parameters using a recurrent decoder.\n",
    "The predictions are obtained by transforming the hidden states $\\mathbf{h}_{t}$ into recurrent predictive distribution parameters $\\theta_{\\tau}$.\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{h}_{t} &= \\textrm{EncoderRNN}([\\mathbf{y}_{t},\\mathbf{x}^{(h)}_{t},\\mathbf{x}^{(s)}], \\mathbf{h}_{t-1})\\\\\n",
    "\\mathbf{\\theta}_{\\tau+1}&=\\textrm{DecoderRNN}([\\mathbf{h}_{t}, \\mathbf{x}^{(f)}_{\\tau}], \\mathbf{\\theta}_{\\tau}) \\qquad \\tau \\in [t+1:t+h]\\\\\n",
    "\\hat{y}_{\\tau} &\\sim \\;\\mathrm{P}(y_{\\tau}\\;|\\;\\mathbf{\\theta}_{\\tau})\\;\n",
    "\\end{align}\n",
    "\n",
    "We deviate minimally from the original DeepAR implementation that in contrast uses a Markov Chain Monte Carlo sampler that is fed into the model inputs\n",
    "recursively.\n",
    "\n",
    "**References**<br>\n",
    "- [David Salinas, Valentin Flunkert, Jan Gasthaus, Tim Januschowski (2020). \"DeepAR: Probabilistic forecasting with autoregressive recurrent networks\". International Journal of Forecasting.](https://www.sciencedirect.com/science/article/pii/S0169207019301888)<br>\n",
    "- [Alexander Alexandrov et. al (2020). \"GluonTS: Probabilistic and Neural Time Series Modeling in Python\". Journal of Machine Learning Research.](https://www.jmlr.org/papers/v21/19-820.html)<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Figure 1. DeepAR model, during training the optimization signal comes from likelihood of observations, during inference a recurrent multi-step strategy is used to generate predictive distributions.](imgs_models/deepar.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#TODO: Rethink the usage of Variable Selection Networks\n",
    "#TODO: Rethink the usage of TFT embeddings.\n",
    "#TODO: Add GRN, VSN into common to share across DeepAR/TFT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import os\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch.nn import LayerNorm\n",
    "\n",
    "import logging\n",
    "import warnings\n",
    "logging.getLogger(\"pytorch_lightning\").setLevel(logging.ERROR)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "from neuralforecast.losses.pytorch import MAE\n",
    "from neuralforecast.common._base_windows import BaseWindows\n",
    "from neuralforecast.common._modules import MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from fastcore.test import test_eq\n",
    "from nbdev.showdoc import show_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import logging\n",
    "import warnings\n",
    "logging.getLogger(\"pytorch_lightning\").setLevel(logging.ERROR)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Auxiliary Functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Gating Mechanisms\n",
    "\n",
    "The Gated Residual Network (GRN) provides adaptive depth and network complexity capable of accommodating different size datasets. As residual connections allow for the network to skip the non-linear transformation of input $\\mathbf{a}$ and context $\\mathbf{c}$.\n",
    "\n",
    "\\begin{align}\n",
    "\\eta_{1} &= \\mathrm{ELU}(\\mathbf{W}_{1}\\mathbf{a}+\\mathbf{W}_{2}\\mathbf{c}+\\mathbf{b}_{1}) \\\\\n",
    "\\eta_{2} &= \\mathbf{W}_{2}\\eta_{1}+b_{2} \\\\\n",
    "\\mathrm{GRN}(\\mathbf{a}, \\mathbf{c}) &= \\mathrm{LayerNorm}(a + \\textrm{GLU}(\\eta_{2}))\n",
    "\\end{align}\n",
    "\n",
    "The Gated Linear Unit (GLU) provides the flexibility of supressing unnecesary parts of the GRN. Consider GRN's output $\\gamma$ then GLU transformation is defined by:\n",
    "\n",
    "$$\\mathrm{GLU}(\\gamma) = \\sigma(\\mathbf{W}_{4}\\gamma +b_{4}) \\odot (\\mathbf{W}_{5}\\gamma +b_{5})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "class MaybeLayerNorm(nn.Module):\n",
    "    def __init__(self, output_size, hidden_size, eps):\n",
    "        super().__init__()\n",
    "        if output_size and output_size == 1:\n",
    "            self.ln = nn.Identity()\n",
    "        else:\n",
    "            self.ln = LayerNorm(output_size if output_size else hidden_size,\n",
    "                                eps=eps)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.ln(x)\n",
    "\n",
    "class GLU(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(hidden_size, output_size * 2)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = self.lin(x)\n",
    "        x = F.glu(x)\n",
    "        return x\n",
    "\n",
    "class GRN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_size,\n",
    "                 hidden_size, \n",
    "                 output_size=None,\n",
    "                 context_hidden_size=None,\n",
    "                 dropout=0):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layer_norm = MaybeLayerNorm(output_size, hidden_size, eps=1e-3)\n",
    "        self.lin_a = nn.Linear(input_size, hidden_size)\n",
    "        if context_hidden_size is not None:\n",
    "            self.lin_c = nn.Linear(context_hidden_size, hidden_size, bias=False)\n",
    "        self.lin_i = nn.Linear(hidden_size, hidden_size)\n",
    "        self.glu = GLU(hidden_size, output_size if output_size else hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out_proj = nn.Linear(input_size, output_size) if output_size else None\n",
    "\n",
    "    def forward(self, a: Tensor, c: Optional[Tensor] = None):\n",
    "        x = self.lin_a(a)\n",
    "        if c is not None:\n",
    "            x = x + self.lin_c(c).unsqueeze(1)\n",
    "        x = F.elu(x)\n",
    "        x = self.lin_i(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.glu(x)\n",
    "        y = a if not self.out_proj else self.out_proj(a)\n",
    "        x = x + y\n",
    "        x = self.layer_norm(x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Variable Selection Networks\n",
    "\n",
    "DeepAR and TFT include automated variable selection capabilities, through its variable selection network (VSN) components. The VSN takes the original input $\\{\\mathbf{x}^{(s)}, \\mathbf{x}^{(h)}_{[:t]}, \\mathbf{x}^{(f)}_{[:t]}\\}$ and transforms it through embeddings or linear transformations into a high dimensional space\n",
    "$\\{\\mathbf{E}^{(s)}, \\mathbf{E}^{(h)}_{[:t]}, \\mathbf{E}^{(f)}_{[:t+H]}\\}$. \n",
    "\n",
    "For the observed historic data, the embedding matrix $\\mathbf{E}^{(h)}_{t}$ at time $t$ is a concatenation of $j$ variable $e^{(h)}_{t,j}$ embeddings:\n",
    "\\begin{align}\n",
    "\\mathbf{E}^{(h)}_{t} &= [e^{(h)}_{t,1},\\dots,e^{(h)}_{t,j},\\dots,e^{(h)}_{t,n_{h}}] \\\\\n",
    "\\mathbf{\\tilde{e}}^{(h)}_{t,j} &= \\mathrm{GRN}(e^{(h)}_{t,j})\n",
    "\\end{align}\n",
    "\n",
    "The variable selection weights are given by:\n",
    "$$s^{(h)}_{t}=\\mathrm{SoftMax}(\\mathrm{GRN}(\\mathbf{E}^{(h)}_{t},\\mathbf{E}^{(s)}))$$\n",
    "\n",
    "The VSN processed features are then:\n",
    "$$\\tilde{\\mathbf{E}}^{(h)}_{t}= \\sum_{j} s^{(h)}_{j} \\tilde{e}^{(h)}_{t,j}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "class TFTEmbedding(nn.Module):\n",
    "    def __init__(self, hidden_size, stat_input_size, futr_input_size, hist_input_size, tgt_size):\n",
    "        super().__init__()\n",
    "        # There are 4 types of input:\n",
    "        # 1. Static continuous\n",
    "        # 2. Temporal known a priori continuous\n",
    "        # 3. Temporal observed continuous\n",
    "        # 4. Temporal observed targets (time series obseved so far)\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.stat_input_size = stat_input_size\n",
    "        self.futr_input_size = futr_input_size\n",
    "        self.hist_input_size = hist_input_size\n",
    "        self.tgt_size        = tgt_size\n",
    "\n",
    "        # Instantiate Continuous Embeddings if size is not None\n",
    "        for attr, size in [('stat_exog_embedding', stat_input_size), \n",
    "                           ('futr_exog_embedding', futr_input_size),\n",
    "                           ('hist_exog_embedding', hist_input_size),\n",
    "                           ('tgt_embedding', tgt_size)]:\n",
    "            if size:\n",
    "                vectors = nn.Parameter(torch.Tensor(size, hidden_size))\n",
    "                bias = nn.Parameter(torch.zeros(size, hidden_size))\n",
    "                torch.nn.init.xavier_normal_(vectors)\n",
    "                setattr(self, attr+'_vectors', vectors)\n",
    "                setattr(self, attr+'_bias', bias)\n",
    "            else:\n",
    "                setattr(self, attr+'_vectors', None)\n",
    "                setattr(self, attr+'_bias', None)\n",
    "\n",
    "    def _apply_embedding(self,\n",
    "                         cont: Optional[Tensor],\n",
    "                         cont_emb: Tensor,\n",
    "                         cont_bias: Tensor,\n",
    "                         ):\n",
    "\n",
    "        if (cont is not None):\n",
    "            #the line below is equivalent to following einsums\n",
    "            #e_cont = torch.einsum('btf,fh->bthf', cont, cont_emb)\n",
    "            #e_cont = torch.einsum('bf,fh->bhf', cont, cont_emb)          \n",
    "            e_cont = torch.mul(cont.unsqueeze(-1), cont_emb)\n",
    "            e_cont = e_cont + cont_bias\n",
    "            return e_cont\n",
    "        \n",
    "        return None\n",
    "\n",
    "    def forward(self, target_inp, \n",
    "                stat_exog=None, futr_exog=None, hist_exog=None):\n",
    "        # temporal/static categorical/continuous known/observed input \n",
    "        # tries to get input, if fails returns None\n",
    "\n",
    "        # Static inputs are expected to be equal for all timesteps\n",
    "        # For memory efficiency there is no assert statement\n",
    "        stat_exog = stat_exog[:,:] if stat_exog is not None else None\n",
    "\n",
    "        s_inp = self._apply_embedding(cont=stat_exog,\n",
    "                                      cont_emb=self.stat_exog_embedding_vectors,\n",
    "                                      cont_bias=self.stat_exog_embedding_bias)\n",
    "        k_inp = self._apply_embedding(cont=futr_exog,\n",
    "                                      cont_emb=self.futr_exog_embedding_vectors,\n",
    "                                      cont_bias=self.futr_exog_embedding_bias)\n",
    "        o_inp = self._apply_embedding(cont=hist_exog,\n",
    "                                      cont_emb=self.hist_exog_embedding_vectors,\n",
    "                                      cont_bias=self.hist_exog_embedding_bias)\n",
    "\n",
    "        # Temporal observed targets\n",
    "        # t_observed_tgt = torch.einsum('btf,fh->btfh', \n",
    "        #                               target_inp, self.tgt_embedding_vectors)        \n",
    "        target_inp = torch.matmul(target_inp.unsqueeze(3).unsqueeze(4),\n",
    "                          self.tgt_embedding_vectors.unsqueeze(1)).squeeze(3)\n",
    "        target_inp = target_inp + self.tgt_embedding_bias\n",
    "\n",
    "        return s_inp, k_inp, o_inp, target_inp\n",
    "\n",
    "class VariableSelectionNetwork(nn.Module):\n",
    "    def __init__(self, hidden_size, num_inputs, dropout):\n",
    "        super().__init__()\n",
    "        self.joint_grn = GRN(input_size=hidden_size*num_inputs, \n",
    "                             hidden_size=hidden_size, \n",
    "                             output_size=num_inputs, \n",
    "                             context_hidden_size=hidden_size)\n",
    "        self.var_grns = nn.ModuleList(\n",
    "                        [GRN(input_size=hidden_size, \n",
    "                             hidden_size=hidden_size, dropout=dropout)\n",
    "                         for _ in range(num_inputs)])\n",
    "\n",
    "    def forward(self, x: Tensor, context: Optional[Tensor] = None):\n",
    "        Xi = x.reshape(*x.shape[:-2], -1)\n",
    "        grn_outputs = self.joint_grn(Xi, c=context)\n",
    "        sparse_weights = F.softmax(grn_outputs, dim=-1)\n",
    "        transformed_embed_list = [m(x[...,i,:])\n",
    "                                     for i, m in enumerate(self.var_grns)]\n",
    "        transformed_embed = torch.stack(transformed_embed_list, dim=-1)\n",
    "        #the line below performs batched matrix vector multiplication\n",
    "        #for temporal features it's bthf,btf->bth\n",
    "        #for static features it's bhf,bf->bh\n",
    "        variable_ctx = torch.matmul(transformed_embed, \n",
    "                                    sparse_weights.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "        return variable_ctx, sparse_weights"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Static Covariate Encoder\n",
    "\n",
    "The static embedding $\\mathbf{E}^{(s)}$ is transformed by the StaticCovariateEncoder into contexts $c_{s}, c_{e}, c_{h}, c_{c}$. Where $c_{s}$ are temporal variable selection contexts, $c_{e}$ are TemporalFusionDecoder enriching contexts, and $c_{h}, c_{c}$ are LSTM's hidden/contexts for the TemporalCovariateEncoder.\n",
    "\n",
    "\\begin{align}\n",
    "c_{s}, c_{e}, (c_{h}, c_{c}) & = \\textrm{GRN}(\\textrm{VSN}(\\mathbf{E}^{(s)}))\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "class StaticCovariateEncoder(nn.Module):\n",
    "    def __init__(self, hidden_size, num_static_vars, dropout):\n",
    "        super().__init__()\n",
    "        self.vsn = VariableSelectionNetwork(hidden_size=hidden_size,\n",
    "                                            num_inputs=num_static_vars,\n",
    "                                            dropout=dropout)\n",
    "        self.context_grns = nn.ModuleList(\n",
    "                              [GRN(input_size=hidden_size,\n",
    "                                   hidden_size=hidden_size,\n",
    "                                   dropout=dropout) for _ in range(4)])\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tuple[Tensor, Tensor, Tensor, Tensor]:\n",
    "        variable_ctx, sparse_weights = self.vsn(x)\n",
    "\n",
    "        # Context vectors:\n",
    "        # variable selection context\n",
    "        # enrichment context\n",
    "        # state_c context\n",
    "        # state_h context\n",
    "        cs, ce, ch, cc = tuple(m(variable_ctx) for m in self.context_grns)\n",
    "\n",
    "        return cs, ce, ch, cc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Temporal Covariate Encoder\n",
    "\n",
    "TemporalCovariateEncoder encodes the embeddings $\\mathbf{E}^{(h)}, \\mathbf{E}^{(f)}$ and contexts  $(c_{h}, c_{c})$ with an LSTM.\n",
    "\n",
    "\\begin{align}\n",
    "\\tilde{\\mathbf{E}}^{(h)}_{[:t]} & = \\textrm{VSN}(\\mathbf{E}^{(h)}_{[:t]}, c_{s}) \\\\\n",
    "\\tilde{\\mathbf{E}}^{(h)}_{[:t]} &= \\mathrm{LSTM}(\\tilde{\\mathbf{E}}^{(h)}_{[:t]}, (c_{h}, c_{c})) \\\\\n",
    "h_{[:t]} &= \\mathrm{Gate}(\\mathrm{LayerNorm}(\\tilde{\\mathbf{E}}^{(h)}_{[:t]}))\n",
    "\\end{align}\n",
    "\n",
    "An analogous process is repeated for the future data, with the main difference that $\\mathbf{E}^{(f)}$ contains the future available information.\n",
    "\n",
    "\\begin{align}\n",
    "\\tilde{\\mathbf{E}}^{(f)}_{[t+1:t+h]} & = \\textrm{VSN}(\\mathbf{E}^{(h)}_{t+1:t+H}, \\mathbf{E}^{(f)}_{t+1:t+H}, c_{s}) \\\\\n",
    "\\tilde{\\mathbf{E}}^{(f)}_{[t+1:t+h]} &= \\mathrm{LSTM}(\\tilde{\\mathbf{E}}^{(h)}_{[t+1:t+h]}, (c_{h}, c_{c})) \\\\\n",
    "h_{[t+1:t+H]} &= \\mathrm{Gate}(\\mathrm{LayerNorm}(\\tilde{\\mathbf{E}}^{(f)}_{[t+1:t+h]}))\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "class TemporalCovariateEncoder(nn.Module):\n",
    "    def __init__(self, hidden_size, \n",
    "                 num_historic_vars, num_future_vars, dropout):\n",
    "        super(TemporalCovariateEncoder, self).__init__()\n",
    "\n",
    "        self.history_vsn = VariableSelectionNetwork(\n",
    "                                       hidden_size=hidden_size,\n",
    "                                       num_inputs=num_historic_vars,\n",
    "                                       dropout=dropout)\n",
    "        self.history_encoder = nn.LSTM(input_size=hidden_size,\n",
    "                                       hidden_size=hidden_size,\n",
    "                                       batch_first=True)\n",
    "        \n",
    "        self.future_vsn = VariableSelectionNetwork(hidden_size=hidden_size,\n",
    "                                                   num_inputs=num_future_vars,\n",
    "                                                   dropout=dropout)\n",
    "        self.future_encoder = nn.LSTM(input_size=hidden_size,\n",
    "                                      hidden_size=hidden_size,\n",
    "                                      batch_first=True)\n",
    "        \n",
    "        # Shared Gated-Skip Connection\n",
    "        self.input_gate = GLU(hidden_size, hidden_size)\n",
    "        self.input_gate_ln = LayerNorm(hidden_size, eps=1e-3)\n",
    "    \n",
    "    def forward(self, historical_inputs, future_inputs, cs, ch, cc):\n",
    "        # [N,X_in,L] -> [N,hidden_size,L]\n",
    "        historical_features, _ = self.history_vsn(historical_inputs, cs)\n",
    "        history, state = self.history_encoder(historical_features, (ch, cc))\n",
    "\n",
    "        future_features, _ = self.future_vsn(future_inputs, cs)\n",
    "        future, _ = self.future_encoder(future_features, state)\n",
    "        #torch.cuda.synchronize() # this call gives prf boost for unknown reasons\n",
    "\n",
    "        input_embedding = torch.cat([historical_features, future_features], dim=1)\n",
    "        temporal_features = torch.cat([history, future], dim=1)\n",
    "        temporal_features = self.input_gate(temporal_features)\n",
    "        temporal_features = temporal_features + input_embedding\n",
    "        temporal_features = self.input_gate_ln(temporal_features)      \n",
    "        return temporal_features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. DeepAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class DeepAR(BaseWindows):\n",
    "    \"\"\" DeepAR\n",
    "\n",
    "    DeepAR is a method for producing probabilistic forecasts. It uses two recurrent neural networks (RNN) \n",
    "    to encode temporal and static variables and generate forecast distribution parameters.\n",
    "    \n",
    "    Our version of DeepAR deviates slightly from the original GluonTS approach, which incorporates a \n",
    "    Markov Chain sampler in the optimization, while we employ a full Seq2Seq approach.\n",
    "\n",
    "    **Parameters:**<br>\n",
    "    `h`: int, Forecast horizon. <br>\n",
    "    `input_size`: int, autorregresive inputs size, y=[1,2,3,4] input_size=2 -> y_[t-2:t]=[1,2].<br>\n",
    "    `decoder_layers`: int=2, number of layers for the MLP decoder.<br>    \n",
    "    `stat_exog_list`: str list, static continuous columns.<br>\n",
    "    `hist_exog_list`: str list, historic continuous columns.<br>\n",
    "    `futr_exog_list`: str list, future continuous columns.<br>\n",
    "    `hidden_size`: int, units of embeddings and encoders.<br>\n",
    "    `dropout`: float (0, 1), dropout of inputs VSNs.<br>\n",
    "    `shared_weights`: bool, If True, all blocks within each stack will share parameters. <br>\n",
    "    `activation`: str, activation from ['ReLU', 'Softplus', 'Tanh', 'SELU', 'LeakyReLU', 'PReLU', 'Sigmoid'].<br>\n",
    "    `loss`: PyTorch module, instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
    "    `valid_loss`: PyTorch module=`loss`, instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
    "    `max_steps`: int=1000, maximum number of training steps.<br>\n",
    "    `learning_rate`: float=1e-3, Learning rate between (0, 1).<br>\n",
    "    `num_lr_decays`: int=-1, Number of learning rate decays, evenly distributed across max_steps.<br>\n",
    "    `early_stop_patience_steps`: int=-1, Number of validation iterations before early stopping.<br>\n",
    "    `val_check_steps`: int=100, Number of training steps between every validation loss check.<br>\n",
    "    `batch_size`: int, number of different series in each batch.<br>\n",
    "    `windows_batch_size`: int=None, windows sampled from rolled data, default uses all.<br>\n",
    "    `valid_batch_size`: int=None, number of different series in each validation and test batch.<br>\n",
    "    `step_size`: int=1, step size between each window of temporal data.<br>\n",
    "    `scaler_type`: str='robust', type of scaler for temporal inputs normalization see [temporal scalers](https://nixtla.github.io/neuralforecast/common.scalers.html).<br>\n",
    "    `random_seed`: int, random seed initialization for replicability.<br>\n",
    "    `num_workers_loader`: int=os.cpu_count(), workers to be used by `TimeSeriesDataLoader`.<br>\n",
    "    `drop_last_loader`: bool=False, if True `TimeSeriesDataLoader` drops last non-full batch.<br>\n",
    "    `alias`: str, optional,  Custom name of the model.<br>\n",
    "    `**trainer_kwargs`: int,  keyword trainer arguments inherited from [PyTorch Lighning's trainer](https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.trainer.trainer.Trainer.html?highlight=trainer).<br>    \n",
    "\n",
    "    **References**<br>\n",
    "    - [David Salinas, Valentin Flunkert, Jan Gasthaus, Tim Januschowski (2020). \"DeepAR: Probabilistic forecasting with autoregressive recurrent networks\". International Journal of Forecasting.](https://www.sciencedirect.com/science/article/pii/S0169207019301888)<br>\n",
    "    - [Alexander Alexandrov et. al (2020). \"GluonTS: Probabilistic and Neural Time Series Modeling in Python\". Journal of Machine Learning Research.](https://www.jmlr.org/papers/v21/19-820.html)<br>\n",
    "    \"\"\"\n",
    "    # Class attributes\n",
    "    SAMPLING_TYPE = 'windows'\n",
    "    \n",
    "    def __init__(self,\n",
    "                 h,\n",
    "                 input_size,\n",
    "                 tgt_size: int = 1,\n",
    "                 decoder_layers: int=2,\n",
    "                 stat_exog_list = None,\n",
    "                 hist_exog_list = None,\n",
    "                 futr_exog_list = None,\n",
    "                 hidden_size: int = 128,\n",
    "                 dropout: float = 0.1,\n",
    "                 loss = MAE(),\n",
    "                 valid_loss = None,\n",
    "                 max_steps: int = 1000,\n",
    "                 learning_rate: float = 1e-3,\n",
    "                 num_lr_decays: int = -1,\n",
    "                 early_stop_patience_steps: int =-1,\n",
    "                 val_check_steps: int = 100,\n",
    "                 batch_size: int = 32,\n",
    "                 valid_batch_size: Optional[int] = None,\n",
    "                 windows_batch_size: int = 1024,\n",
    "                 inference_windows_batch_size: int = 1024,\n",
    "                 step_size: int = 1,\n",
    "                 scaler_type: str = 'robust',\n",
    "                 num_workers_loader = 0,\n",
    "                 drop_last_loader = False,\n",
    "                 random_seed: int = 1,\n",
    "                 **trainer_kwargs\n",
    "                 ):\n",
    "\n",
    "        # Inherit BaseWindows class\n",
    "        super(DeepAR, self).__init__(h=h,\n",
    "                                  input_size=input_size,\n",
    "                                  loss=loss,\n",
    "                                  valid_loss=valid_loss,\n",
    "                                  max_steps=max_steps,\n",
    "                                  learning_rate=learning_rate,\n",
    "                                  num_lr_decays=num_lr_decays,\n",
    "                                  early_stop_patience_steps=early_stop_patience_steps,\n",
    "                                  val_check_steps=val_check_steps,\n",
    "                                  batch_size=batch_size,\n",
    "                                  valid_batch_size=valid_batch_size,\n",
    "                                  windows_batch_size=windows_batch_size,\n",
    "                                  inference_windows_batch_size=inference_windows_batch_size,\n",
    "                                  step_size=step_size,\n",
    "                                  scaler_type=scaler_type,\n",
    "                                  num_workers_loader=num_workers_loader,\n",
    "                                  drop_last_loader=drop_last_loader,\n",
    "                                  random_seed=random_seed,\n",
    "                                  **trainer_kwargs)\n",
    "        self.example_length = input_size + h\n",
    "        self.decoder_layers = decoder_layers\n",
    "\n",
    "        # Parse lists hyperparameters\n",
    "        self.stat_exog_list = [] if stat_exog_list is None else stat_exog_list\n",
    "        self.hist_exog_list = [] if hist_exog_list is None else hist_exog_list\n",
    "        self.futr_exog_list = [] if futr_exog_list is None else futr_exog_list\n",
    "\n",
    "        stat_input_size = len(self.stat_exog_list)\n",
    "        futr_input_size = max(len(self.futr_exog_list), 1)\n",
    "        hist_input_size = len(self.hist_exog_list)\n",
    "        num_historic_vars = futr_input_size + hist_input_size + tgt_size\n",
    "\n",
    "        #------------------------------- Encoders -----------------------------#\n",
    "        self.embedding = TFTEmbedding(hidden_size=hidden_size,\n",
    "                                      stat_input_size=stat_input_size,\n",
    "                                      futr_input_size=futr_input_size,\n",
    "                                      hist_input_size=hist_input_size,\n",
    "                                      tgt_size=tgt_size)\n",
    "        \n",
    "        self.static_encoder = StaticCovariateEncoder(\n",
    "                                      hidden_size=hidden_size,\n",
    "                                      num_static_vars=stat_input_size,\n",
    "                                      dropout=dropout)\n",
    "\n",
    "        self.temporal_encoder = TemporalCovariateEncoder(\n",
    "                                      hidden_size=hidden_size,\n",
    "                                      num_historic_vars=num_historic_vars,\n",
    "                                      num_future_vars=futr_input_size,\n",
    "                                      dropout=dropout)\n",
    "\n",
    "        #------------------------------ Decoders -----------------------------#\n",
    "        # Decoder MLP\n",
    "        #self.enrichment_grn = GRN(input_size=hidden_size,\n",
    "        #                          hidden_size=hidden_size,\n",
    "        #                          context_hidden_size=hidden_size, \n",
    "        #                          dropout=dropout)\n",
    "        self.mlp_decoder = MLP(in_features=hidden_size,\n",
    "                               out_features=self.loss.outputsize_multiplier,\n",
    "                               hidden_size=hidden_size,\n",
    "                               num_layers=self.decoder_layers,\n",
    "                               activation='ReLU',\n",
    "                               dropout=dropout)\n",
    "\n",
    "    def forward(self, windows_batch):\n",
    "\n",
    "        # Parsiw windows_batch\n",
    "        y_insample = windows_batch['insample_y'][:,:, None] # <- [B,T,1]\n",
    "        futr_exog  = windows_batch['futr_exog']\n",
    "        hist_exog  = windows_batch['hist_exog']\n",
    "        stat_exog  = windows_batch['stat_exog']\n",
    "\n",
    "        if futr_exog is None:\n",
    "            futr_exog = y_insample[:, [-1]]\n",
    "            futr_exog = futr_exog.repeat(1, self.example_length, 1)\n",
    "\n",
    "        s_inp, k_inp, o_inp, t_observed_tgt = self.embedding(target_inp=y_insample, \n",
    "                                                             hist_exog=hist_exog,\n",
    "                                                             futr_exog=futr_exog,\n",
    "                                                             stat_exog=stat_exog)\n",
    "\n",
    "        #-------------------------------- Inputs ------------------------------#\n",
    "        # Static context\n",
    "        if s_inp is not None:\n",
    "            #cs, ce, ch, cc = self.static_encoder(s_inp)\n",
    "            cs, _, ch, cc = self.static_encoder(s_inp)\n",
    "            ch, cc = ch.unsqueeze(0), cc.unsqueeze(0) # LSTM initial states\n",
    "        else:\n",
    "            # If None add zeros\n",
    "            batch_size, example_length, target_size, hidden_size = t_observed_tgt.shape\n",
    "            cs = torch.zeros(size=(batch_size, hidden_size)).to(y_insample.device)\n",
    "            #ce = torch.zeros(size=(batch_size, hidden_size)).to(y_insample.device)\n",
    "            ch = torch.zeros(size=(1, batch_size, hidden_size)).to(y_insample.device)\n",
    "            cc = torch.zeros(size=(1, batch_size, hidden_size)).to(y_insample.device)\n",
    "\n",
    "        # Historical inputs\n",
    "        _historical_inputs = [k_inp[:,:self.input_size,:],\n",
    "                              t_observed_tgt[:,:self.input_size,:]]\n",
    "        if o_inp is not None:\n",
    "            _historical_inputs.insert(0,o_inp[:,:self.input_size,:])\n",
    "        historical_inputs = torch.cat(_historical_inputs, dim=-2)\n",
    "\n",
    "        # Future inputs\n",
    "        future_inputs = k_inp[:, self.input_size:]\n",
    "\n",
    "        #---------------------------- Encode/Decode ---------------------------#\n",
    "        # Embeddings + VSN + LSTM encoders\n",
    "        temporal_features = self.temporal_encoder(historical_inputs=historical_inputs,\n",
    "                                                  future_inputs=future_inputs,\n",
    "                                                  cs=cs, ch=ch, cc=cc)\n",
    "\n",
    "        # Rethink fusion with static variables, to enrich temporal features (see TFT)\n",
    "        # In the abscence of temporal_fusion_decoder residual connections still needed\n",
    "        temporal_features = temporal_features[:, self.input_size:, :]\n",
    "        #temporal_features = self.enrichment_grn(temporal_features, c=ce)\n",
    "\n",
    "        # Adapt output to loss\n",
    "        output = self.mlp_decoder(temporal_features)\n",
    "        output = self.loss.domain_map(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### DeepAR\n",
       "\n",
       ">      DeepAR (h, input_size, tgt_size:int=1, decoder_layers:int=2,\n",
       ">              stat_exog_list=None, hist_exog_list=None, futr_exog_list=None,\n",
       ">              hidden_size:int=128, dropout:float=0.1, loss=MAE(),\n",
       ">              valid_loss=None, max_steps:int=1000, learning_rate:float=0.001,\n",
       ">              num_lr_decays:int=-1, early_stop_patience_steps:int=-1,\n",
       ">              val_check_steps:int=100, batch_size:int=32,\n",
       ">              valid_batch_size:Optional[int]=None, windows_batch_size:int=1024,\n",
       ">              inference_windows_batch_size:int=1024, step_size:int=1,\n",
       ">              scaler_type:str='robust', num_workers_loader=0,\n",
       ">              drop_last_loader=False, random_seed:int=1, **trainer_kwargs)\n",
       "\n",
       "DeepAR\n",
       "\n",
       "DeepAR is a method for producing probabilistic forecasts. It uses two recurrent neural networks (RNN) \n",
       "to encode temporal and static variables and generate forecast distribution parameters.\n",
       "\n",
       "Our version of DeepAR deviates slightly from the original GluonTS approach, which incorporates a \n",
       "Markov Chain sampler in the optimization, while we employ a full Seq2Seq approach.\n",
       "\n",
       "**Parameters:**<br>\n",
       "`h`: int, Forecast horizon. <br>\n",
       "`input_size`: int, autorregresive inputs size, y=[1,2,3,4] input_size=2 -> y_[t-2:t]=[1,2].<br>\n",
       "`decoder_layers`: int=2, number of layers for the MLP decoder.<br>    \n",
       "`stat_exog_list`: str list, static continuous columns.<br>\n",
       "`hist_exog_list`: str list, historic continuous columns.<br>\n",
       "`futr_exog_list`: str list, future continuous columns.<br>\n",
       "`hidden_size`: int, units of embeddings and encoders.<br>\n",
       "`dropout`: float (0, 1), dropout of inputs VSNs.<br>\n",
       "`shared_weights`: bool, If True, all blocks within each stack will share parameters. <br>\n",
       "`activation`: str, activation from ['ReLU', 'Softplus', 'Tanh', 'SELU', 'LeakyReLU', 'PReLU', 'Sigmoid'].<br>\n",
       "`loss`: PyTorch module, instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
       "`valid_loss`: PyTorch module=`loss`, instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
       "`max_steps`: int=1000, maximum number of training steps.<br>\n",
       "`learning_rate`: float=1e-3, Learning rate between (0, 1).<br>\n",
       "`num_lr_decays`: int=-1, Number of learning rate decays, evenly distributed across max_steps.<br>\n",
       "`early_stop_patience_steps`: int=-1, Number of validation iterations before early stopping.<br>\n",
       "`val_check_steps`: int=100, Number of training steps between every validation loss check.<br>\n",
       "`batch_size`: int, number of different series in each batch.<br>\n",
       "`windows_batch_size`: int=None, windows sampled from rolled data, default uses all.<br>\n",
       "`valid_batch_size`: int=None, number of different series in each validation and test batch.<br>\n",
       "`step_size`: int=1, step size between each window of temporal data.<br>\n",
       "`scaler_type`: str='robust', type of scaler for temporal inputs normalization see [temporal scalers](https://nixtla.github.io/neuralforecast/common.scalers.html).<br>\n",
       "`random_seed`: int, random seed initialization for replicability.<br>\n",
       "`num_workers_loader`: int=os.cpu_count(), workers to be used by `TimeSeriesDataLoader`.<br>\n",
       "`drop_last_loader`: bool=False, if True `TimeSeriesDataLoader` drops last non-full batch.<br>\n",
       "`alias`: str, optional,  Custom name of the model.<br>\n",
       "`**trainer_kwargs`: int,  keyword trainer arguments inherited from [PyTorch Lighning's trainer](https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.trainer.trainer.Trainer.html?highlight=trainer).<br>    \n",
       "\n",
       "**References**<br>\n",
       "- [David Salinas, Valentin Flunkert, Jan Gasthaus, Tim Januschowski (2020). \"DeepAR: Probabilistic forecasting with autoregressive recurrent networks\". International Journal of Forecasting.](https://www.sciencedirect.com/science/article/pii/S0169207019301888)<br>\n",
       "- [Alexander Alexandrov et. al (2020). \"GluonTS: Probabilistic and Neural Time Series Modeling in Python\". Journal of Machine Learning Research.](https://www.jmlr.org/papers/v21/19-820.html)<br>"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### DeepAR\n",
       "\n",
       ">      DeepAR (h, input_size, tgt_size:int=1, decoder_layers:int=2,\n",
       ">              stat_exog_list=None, hist_exog_list=None, futr_exog_list=None,\n",
       ">              hidden_size:int=128, dropout:float=0.1, loss=MAE(),\n",
       ">              valid_loss=None, max_steps:int=1000, learning_rate:float=0.001,\n",
       ">              num_lr_decays:int=-1, early_stop_patience_steps:int=-1,\n",
       ">              val_check_steps:int=100, batch_size:int=32,\n",
       ">              valid_batch_size:Optional[int]=None, windows_batch_size:int=1024,\n",
       ">              inference_windows_batch_size:int=1024, step_size:int=1,\n",
       ">              scaler_type:str='robust', num_workers_loader=0,\n",
       ">              drop_last_loader=False, random_seed:int=1, **trainer_kwargs)\n",
       "\n",
       "DeepAR\n",
       "\n",
       "DeepAR is a method for producing probabilistic forecasts. It uses two recurrent neural networks (RNN) \n",
       "to encode temporal and static variables and generate forecast distribution parameters.\n",
       "\n",
       "Our version of DeepAR deviates slightly from the original GluonTS approach, which incorporates a \n",
       "Markov Chain sampler in the optimization, while we employ a full Seq2Seq approach.\n",
       "\n",
       "**Parameters:**<br>\n",
       "`h`: int, Forecast horizon. <br>\n",
       "`input_size`: int, autorregresive inputs size, y=[1,2,3,4] input_size=2 -> y_[t-2:t]=[1,2].<br>\n",
       "`decoder_layers`: int=2, number of layers for the MLP decoder.<br>    \n",
       "`stat_exog_list`: str list, static continuous columns.<br>\n",
       "`hist_exog_list`: str list, historic continuous columns.<br>\n",
       "`futr_exog_list`: str list, future continuous columns.<br>\n",
       "`hidden_size`: int, units of embeddings and encoders.<br>\n",
       "`dropout`: float (0, 1), dropout of inputs VSNs.<br>\n",
       "`shared_weights`: bool, If True, all blocks within each stack will share parameters. <br>\n",
       "`activation`: str, activation from ['ReLU', 'Softplus', 'Tanh', 'SELU', 'LeakyReLU', 'PReLU', 'Sigmoid'].<br>\n",
       "`loss`: PyTorch module, instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
       "`valid_loss`: PyTorch module=`loss`, instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
       "`max_steps`: int=1000, maximum number of training steps.<br>\n",
       "`learning_rate`: float=1e-3, Learning rate between (0, 1).<br>\n",
       "`num_lr_decays`: int=-1, Number of learning rate decays, evenly distributed across max_steps.<br>\n",
       "`early_stop_patience_steps`: int=-1, Number of validation iterations before early stopping.<br>\n",
       "`val_check_steps`: int=100, Number of training steps between every validation loss check.<br>\n",
       "`batch_size`: int, number of different series in each batch.<br>\n",
       "`windows_batch_size`: int=None, windows sampled from rolled data, default uses all.<br>\n",
       "`valid_batch_size`: int=None, number of different series in each validation and test batch.<br>\n",
       "`step_size`: int=1, step size between each window of temporal data.<br>\n",
       "`scaler_type`: str='robust', type of scaler for temporal inputs normalization see [temporal scalers](https://nixtla.github.io/neuralforecast/common.scalers.html).<br>\n",
       "`random_seed`: int, random seed initialization for replicability.<br>\n",
       "`num_workers_loader`: int=os.cpu_count(), workers to be used by `TimeSeriesDataLoader`.<br>\n",
       "`drop_last_loader`: bool=False, if True `TimeSeriesDataLoader` drops last non-full batch.<br>\n",
       "`alias`: str, optional,  Custom name of the model.<br>\n",
       "`**trainer_kwargs`: int,  keyword trainer arguments inherited from [PyTorch Lighning's trainer](https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.trainer.trainer.Trainer.html?highlight=trainer).<br>    \n",
       "\n",
       "**References**<br>\n",
       "- [David Salinas, Valentin Flunkert, Jan Gasthaus, Tim Januschowski (2020). \"DeepAR: Probabilistic forecasting with autoregressive recurrent networks\". International Journal of Forecasting.](https://www.sciencedirect.com/science/article/pii/S0169207019301888)<br>\n",
       "- [Alexander Alexandrov et. al (2020). \"GluonTS: Probabilistic and Neural Time Series Modeling in Python\". Journal of Machine Learning Research.](https://www.jmlr.org/papers/v21/19-820.html)<br>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DeepAR, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### DeepAR.fit\n",
       "\n",
       ">      DeepAR.fit (dataset, val_size=0, test_size=0, random_seed=None)\n",
       "\n",
       "Fit.\n",
       "\n",
       "The `fit` method, optimizes the neural network's weights using the\n",
       "initialization parameters (`learning_rate`, `windows_batch_size`, ...)\n",
       "and the `loss` function as defined during the initialization.\n",
       "Within `fit` we use a PyTorch Lightning `Trainer` that\n",
       "inherits the initialization's `self.trainer_kwargs`, to customize\n",
       "its inputs, see [PL's trainer arguments](https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.trainer.trainer.Trainer.html?highlight=trainer).\n",
       "\n",
       "The method is designed to be compatible with SKLearn-like classes\n",
       "and in particular to be compatible with the StatsForecast library.\n",
       "\n",
       "By default the `model` is not saving training checkpoints to protect\n",
       "disk memory, to get them change `enable_checkpointing=True` in `__init__`.\n",
       "\n",
       "**Parameters:**<br>\n",
       "`dataset`: NeuralForecast's `TimeSeriesDataset`, see [documentation](https://nixtla.github.io/neuralforecast/tsdataset.html).<br>\n",
       "`val_size`: int, validation size for temporal cross-validation.<br>\n",
       "`random_seed`: int=None, random_seed for pytorch initializer and numpy generators, overwrites model.__init__'s.<br>\n",
       "`test_size`: int, test size for temporal cross-validation.<br>"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### DeepAR.fit\n",
       "\n",
       ">      DeepAR.fit (dataset, val_size=0, test_size=0, random_seed=None)\n",
       "\n",
       "Fit.\n",
       "\n",
       "The `fit` method, optimizes the neural network's weights using the\n",
       "initialization parameters (`learning_rate`, `windows_batch_size`, ...)\n",
       "and the `loss` function as defined during the initialization.\n",
       "Within `fit` we use a PyTorch Lightning `Trainer` that\n",
       "inherits the initialization's `self.trainer_kwargs`, to customize\n",
       "its inputs, see [PL's trainer arguments](https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.trainer.trainer.Trainer.html?highlight=trainer).\n",
       "\n",
       "The method is designed to be compatible with SKLearn-like classes\n",
       "and in particular to be compatible with the StatsForecast library.\n",
       "\n",
       "By default the `model` is not saving training checkpoints to protect\n",
       "disk memory, to get them change `enable_checkpointing=True` in `__init__`.\n",
       "\n",
       "**Parameters:**<br>\n",
       "`dataset`: NeuralForecast's `TimeSeriesDataset`, see [documentation](https://nixtla.github.io/neuralforecast/tsdataset.html).<br>\n",
       "`val_size`: int, validation size for temporal cross-validation.<br>\n",
       "`random_seed`: int=None, random_seed for pytorch initializer and numpy generators, overwrites model.__init__'s.<br>\n",
       "`test_size`: int, test size for temporal cross-validation.<br>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DeepAR.fit, name='DeepAR.fit', title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### DeepAR.predict\n",
       "\n",
       ">      DeepAR.predict (dataset, test_size=None, step_size=1, random_seed=None,\n",
       ">                      **data_module_kwargs)\n",
       "\n",
       "Predict.\n",
       "\n",
       "Neural network prediction with PL's `Trainer` execution of `predict_step`.\n",
       "\n",
       "**Parameters:**<br>\n",
       "`dataset`: NeuralForecast's `TimeSeriesDataset`, see [documentation](https://nixtla.github.io/neuralforecast/tsdataset.html).<br>\n",
       "`test_size`: int=None, test size for temporal cross-validation.<br>\n",
       "`step_size`: int=1, Step size between each window.<br>\n",
       "`random_seed`: int=None, random_seed for pytorch initializer and numpy generators, overwrites model.__init__'s.<br>\n",
       "`**data_module_kwargs`: PL's TimeSeriesDataModule args, see [documentation](https://pytorch-lightning.readthedocs.io/en/1.6.1/extensions/datamodules.html#using-a-datamodule)."
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### DeepAR.predict\n",
       "\n",
       ">      DeepAR.predict (dataset, test_size=None, step_size=1, random_seed=None,\n",
       ">                      **data_module_kwargs)\n",
       "\n",
       "Predict.\n",
       "\n",
       "Neural network prediction with PL's `Trainer` execution of `predict_step`.\n",
       "\n",
       "**Parameters:**<br>\n",
       "`dataset`: NeuralForecast's `TimeSeriesDataset`, see [documentation](https://nixtla.github.io/neuralforecast/tsdataset.html).<br>\n",
       "`test_size`: int=None, test size for temporal cross-validation.<br>\n",
       "`step_size`: int=1, Step size between each window.<br>\n",
       "`random_seed`: int=None, random_seed for pytorch initializer and numpy generators, overwrites model.__init__'s.<br>\n",
       "`**data_module_kwargs`: PL's TimeSeriesDataModule args, see [documentation](https://pytorch-lightning.readthedocs.io/en/1.6.1/extensions/datamodules.html#using-a-datamodule)."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DeepAR.predict, name='DeepAR.predict', title_level=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralforecast import NeuralForecast\n",
    "from neuralforecast.losses.pytorch import MQLoss, DistributionLoss, GMM, PMM\n",
    "from neuralforecast.tsdataset import TimeSeriesDataset\n",
    "from neuralforecast.utils import AirPassengers, AirPassengersPanel, AirPassengersStatic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 309: 100%|| 1/1 [00:00<00:00,  7.17it/s, v_num=32, train_loss_step=4.510, train_loss_epoch=4.510, valid_loss=5.360]\n",
      "Predicting DataLoader 0: 100%|| 1/1 [00:00<00:00, 26.91it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| eval: false\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from neuralforecast import NeuralForecast\n",
    "#from neuralforecast.models import DeepAR\n",
    "from neuralforecast.losses.pytorch import DistributionLoss, HuberMQLoss\n",
    "from neuralforecast.tsdataset import TimeSeriesDataset\n",
    "from neuralforecast.utils import AirPassengers, AirPassengersPanel, AirPassengersStatic\n",
    "\n",
    "#AirPassengersPanel['y'] = AirPassengersPanel['y'] + 10\n",
    "Y_train_df = AirPassengersPanel[AirPassengersPanel.ds<AirPassengersPanel['ds'].values[-12]] # 132 train\n",
    "Y_test_df = AirPassengersPanel[AirPassengersPanel.ds>=AirPassengersPanel['ds'].values[-12]].reset_index(drop=True) # 12 test\n",
    "\n",
    "nf = NeuralForecast(\n",
    "    models=[DeepAR(h=12, input_size=48,\n",
    "                   hidden_size=10,\n",
    "                   #loss=DistributionLoss(distribution='Poisson', level=[80, 90]),\n",
    "                   #loss=DistributionLoss(distribution='Normal', level=[80, 90]),\n",
    "                   loss=DistributionLoss(distribution='StudentT', level=[80, 90]),\n",
    "                   #loss=HuberMQLoss(level=[80, 90]),\n",
    "                   learning_rate=0.005,\n",
    "                   stat_exog_list=['airline1'],\n",
    "                   #futr_exog_list=['y_[lag12]'],\n",
    "                   hist_exog_list=['trend'],\n",
    "                   max_steps=500,\n",
    "                   val_check_steps=10,\n",
    "                   early_stop_patience_steps=10,\n",
    "                   scaler_type='robust',\n",
    "                   windows_batch_size=None,\n",
    "                   enable_progress_bar=True),\n",
    "    ],\n",
    "    freq='M'\n",
    ")\n",
    "nf.fit(df=Y_train_df, static_df=AirPassengersStatic, val_size=12)\n",
    "Y_hat_df = nf.predict(futr_df=Y_test_df)\n",
    "\n",
    "# Plot quantile predictions\n",
    "Y_hat_df = Y_hat_df.reset_index(drop=False).drop(columns=['unique_id','ds'])\n",
    "plot_df = pd.concat([Y_test_df, Y_hat_df], axis=1)\n",
    "plot_df = pd.concat([Y_train_df, plot_df])\n",
    "\n",
    "plot_df = plot_df[plot_df.unique_id=='Airline1'].drop('unique_id', axis=1)\n",
    "plt.plot(plot_df['ds'], plot_df['y'], c='black', label='True')\n",
    "#plt.plot(plot_df['ds'], plot_df['DeepAR'], c='purple', label='mean')\n",
    "plt.plot(plot_df['ds'], plot_df['DeepAR-median'], c='blue', label='median')\n",
    "plt.fill_between(x=plot_df['ds'][-12:], \n",
    "                 y1=plot_df['DeepAR-lo-90'][-12:].values, \n",
    "                 y2=plot_df['DeepAR-hi-90'][-12:].values,\n",
    "                 alpha=0.4, label='level 90')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
