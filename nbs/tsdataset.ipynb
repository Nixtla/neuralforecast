{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524620c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp tsdataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15392f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fa25a4",
   "metadata": {},
   "source": [
    "# PyTorch Dataset/Loader\n",
    "> Torch Dataset for Time Series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2508f7a9-1433-4ad8-8f2f-0078c6ed6c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from fastcore.test import test_eq\n",
    "from nbdev.showdoc import show_doc\n",
    "from neuralforecast.utils import generate_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44065066-e72a-431f-938f-1528adef9fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import warnings\n",
    "from collections.abc import Mapping\n",
    "from typing import Dict, Optional, TYPE_CHECKING\n",
    "if TYPE_CHECKING:\n",
    "    from utilsforecast.target_transforms import BaseTargetTransform\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from utilsforecast.grouped_array import GroupedArray\n",
    "from utilsforecast.processing import DataFrameProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323a7a6e-38c3-496d-8f1e-cad05f643d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TimeSeriesLoader(DataLoader):\n",
    "    \"\"\"TimeSeriesLoader DataLoader.\n",
    "    [Source code](https://github.com/Nixtla/neuralforecast1/blob/main/neuralforecast/tsdataset.py).\n",
    "\n",
    "    Small change to PyTorch's Data loader. \n",
    "    Combines a dataset and a sampler, and provides an iterable over the given dataset.\n",
    "\n",
    "    The class `~torch.utils.data.DataLoader` supports both map-style and\n",
    "    iterable-style datasets with single- or multi-process loading, customizing\n",
    "    loading order and optional automatic batching (collation) and memory pinning.    \n",
    "    \n",
    "    **Parameters:**<br>\n",
    "    `batch_size`: (int, optional): how many samples per batch to load (default: 1).<br>\n",
    "    `shuffle`: (bool, optional): set to `True` to have the data reshuffled at every epoch (default: `False`).<br>\n",
    "    `sampler`: (Sampler or Iterable, optional): defines the strategy to draw samples from the dataset.<br>\n",
    "                Can be any `Iterable` with `__len__` implemented. If specified, `shuffle` must not be specified.<br>\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, **kwargs):\n",
    "        if 'collate_fn' in kwargs:\n",
    "            kwargs.pop('collate_fn')\n",
    "        kwargs_ = {**kwargs, **dict(collate_fn=self._collate_fn)}\n",
    "        DataLoader.__init__(self, dataset=dataset, **kwargs_)\n",
    "    \n",
    "    def _collate_fn(self, batch):\n",
    "        elem = batch[0]\n",
    "        elem_type = type(elem)\n",
    "\n",
    "        if isinstance(elem, torch.Tensor):\n",
    "            out = None\n",
    "            if torch.utils.data.get_worker_info() is not None:\n",
    "                # If we're in a background process, concatenate directly into a\n",
    "                # shared memory tensor to avoid an extra copy\n",
    "                numel = sum(x.numel() for x in batch)\n",
    "                storage = elem.storage()._new_shared(numel, device=elem.device)\n",
    "                out = elem.new(storage).resize_(len(batch), *list(elem.size()))\n",
    "            return torch.stack(batch, 0, out=out)\n",
    "\n",
    "        elif isinstance(elem, Mapping):\n",
    "            if elem['static'] is None:\n",
    "                return dict(temporal=self.collate_fn([d['temporal'] for d in batch]),\n",
    "                            temporal_cols = elem['temporal_cols'])\n",
    "            \n",
    "            return dict(static=self.collate_fn([d['static'] for d in batch]),\n",
    "                        static_cols = elem['static_cols'],\n",
    "                        temporal=self.collate_fn([d['temporal'] for d in batch]),\n",
    "                        temporal_cols = elem['temporal_cols'])\n",
    "\n",
    "        raise TypeError(f'Unknown {elem_type}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e94050-0290-43ad-9a73-c4626bba9541",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(TimeSeriesLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05687429-c139-44c0-adb9-097c616908cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TimeSeriesDataset(Dataset):\n",
    "\n",
    "    def __init__(self,\n",
    "                 temporal,\n",
    "                 temporal_cols,\n",
    "                 indptr,\n",
    "                 max_size: int,\n",
    "                 min_size: int,\n",
    "                 static=None,\n",
    "                 static_cols=None,\n",
    "                 sorted=False,\n",
    "                 scaler_type=None):\n",
    "        super().__init__()\n",
    "\n",
    "        if scaler_type is None:\n",
    "            self.scalers_: Optional[Dict[str, 'BaseTargetTransform']] = None\n",
    "        else:\n",
    "            # delay the import because these require numba, which isn't a requirement\n",
    "            from utilsforecast.target_transforms import (\n",
    "                LocalBoxCox,\n",
    "                LocalMinMaxScaler,            \n",
    "                LocalRobustScaler,            \n",
    "                LocalStandardScaler,\n",
    "            )\n",
    "\n",
    "            type2scaler = {\n",
    "                'standard': LocalStandardScaler,\n",
    "                'robust': lambda: LocalRobustScaler(scale='mad'),\n",
    "                'robust-iqr': lambda: LocalRobustScaler(scale='iqr'),\n",
    "                'minmax': LocalMinMaxScaler,\n",
    "                'boxcox': LocalBoxCox,\n",
    "            }\n",
    "            if scaler_type not in type2scaler:\n",
    "                raise ValueError(f'scaler_type must be one of {type2scaler.keys()}')\n",
    "            self.scalers_ = {}\n",
    "            for i, col in enumerate(temporal_cols):\n",
    "                if col == 'available_mask':\n",
    "                    continue\n",
    "                ga = GroupedArray(temporal[:, i], indptr)\n",
    "                self.scalers_[col] = type2scaler[scaler_type]()                \n",
    "                temporal[:, i] = self.scalers_[col].fit_transform(ga)\n",
    "\n",
    "        self.temporal = torch.tensor(temporal, dtype=torch.float)\n",
    "        self.temporal_cols = pd.Index(list(temporal_cols))\n",
    "\n",
    "        if static is not None:\n",
    "            self.static = torch.tensor(static, dtype=torch.float)\n",
    "            self.static_cols = static_cols\n",
    "        else:\n",
    "            self.static = static\n",
    "            self.static_cols = static_cols\n",
    "\n",
    "        self.indptr = indptr\n",
    "        self.n_groups = self.indptr.size - 1\n",
    "        self.max_size = max_size\n",
    "        self.min_size = min_size\n",
    "\n",
    "        # Upadated flag. To protect consistency, dataset can only be updated once\n",
    "        self.updated = False\n",
    "        self.sorted = sorted\n",
    "            \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if isinstance(idx, int):\n",
    "            # Parse temporal data and pad its left\n",
    "            temporal = torch.zeros(size=(len(self.temporal_cols), self.max_size),\n",
    "                                   dtype=torch.float32)\n",
    "            ts = self.temporal[self.indptr[idx] : self.indptr[idx + 1], :]\n",
    "            temporal[:len(self.temporal_cols), -len(ts):] = ts.permute(1, 0)\n",
    "\n",
    "            # Add static data if available\n",
    "            static = None if self.static is None else self.static[idx,:]\n",
    "\n",
    "            item = dict(temporal=temporal, temporal_cols=self.temporal_cols,\n",
    "                        static=static, static_cols=self.static_cols)\n",
    "\n",
    "            return item\n",
    "        raise ValueError(f'idx must be int, got {type(idx)}')\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_groups\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'TimeSeriesDataset(n_data={self.temporal.shape[0]:,}, n_groups={self.n_groups:,})'\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        if not hasattr(other, 'data') or not hasattr(other, 'indptr'):\n",
    "            return False\n",
    "        return np.allclose(self.data, other.data) and np.array_equal(self.indptr, other.indptr)\n",
    "\n",
    "    def _invert_target_transform(self, data: np.ndarray, indptr: np.ndarray) -> np.ndarray:\n",
    "        if self.scalers_ is None:\n",
    "            return data\n",
    "        for i in range(data.shape[1]):\n",
    "            ga = GroupedArray(data[:, i], indptr)\n",
    "            data[:, i] = self.scalers_['y'].inverse_transform(ga)\n",
    "        return data\n",
    "\n",
    "    def _transform_temporal(self) -> None:\n",
    "        if self.scalers_ is None:\n",
    "            return\n",
    "        for i, col in enumerate(self.temporal_cols):\n",
    "            if col == 'available_mask':\n",
    "                continue\n",
    "            scaler = self.scalers_.get(col, None)\n",
    "            if scaler is None:\n",
    "                continue\n",
    "            ga = GroupedArray(self.temporal[:, i].numpy(), self.indptr)\n",
    "            self.temporal[:, i] = torch.from_numpy(scaler.transform(ga))\n",
    "\n",
    "    @staticmethod\n",
    "    def update_dataset(dataset, future_df):\n",
    "        \"\"\"Add future observations to the dataset.\n",
    "        \"\"\"        \n",
    "        \n",
    "        # Protect consistency\n",
    "        future_df = future_df.copy()\n",
    "\n",
    "        # Add Nones to missing columns (without available_mask)\n",
    "        temporal_cols = dataset.temporal_cols.copy()\n",
    "        for col in temporal_cols:\n",
    "            if col not in future_df.columns:\n",
    "                future_df[col] = np.nan\n",
    "            if col == 'available_mask':\n",
    "                future_df[col] = 1\n",
    "        \n",
    "        # Sort columns to match self.temporal_cols (without available_mask)\n",
    "        future_df = future_df[ ['unique_id','ds'] + temporal_cols.tolist() ]\n",
    "\n",
    "        # Process future_df\n",
    "        futr_dataset, *_ = dataset.from_df(df=future_df, sort_df=dataset.sorted)\n",
    "        futr_dataset.scalers_ = dataset.scalers_\n",
    "        futr_dataset._transform_temporal()\n",
    "\n",
    "        # Define and fill new temporal with updated information\n",
    "        len_temporal, col_temporal = dataset.temporal.shape\n",
    "        new_temporal = torch.zeros(size=(len_temporal+len(future_df), col_temporal))\n",
    "        new_indptr = [0]\n",
    "        new_max_size = 0\n",
    "\n",
    "        acum = 0\n",
    "        for i in range(dataset.n_groups):\n",
    "            series_length = dataset.indptr[i + 1] - dataset.indptr[i]\n",
    "            new_length = series_length + futr_dataset.indptr[i + 1] - futr_dataset.indptr[i]\n",
    "            new_temporal[acum:(acum+series_length), :] = dataset.temporal[dataset.indptr[i] : dataset.indptr[i + 1], :]\n",
    "            new_temporal[(acum+series_length):(acum+new_length), :] = \\\n",
    "                                 futr_dataset.temporal[futr_dataset.indptr[i] : futr_dataset.indptr[i + 1], :]\n",
    "            \n",
    "            acum += new_length\n",
    "            new_indptr.append(acum)\n",
    "            if new_length > new_max_size:\n",
    "                new_max_size = new_length\n",
    "        \n",
    "        # Define new dataset\n",
    "        updated_dataset = TimeSeriesDataset(temporal=new_temporal,\n",
    "                                            temporal_cols=dataset.temporal_cols.copy(),\n",
    "                                            indptr=np.array(new_indptr).astype(np.int32),\n",
    "                                            max_size=new_max_size,\n",
    "                                            min_size=dataset.min_size,\n",
    "                                            static=dataset.static,\n",
    "                                            static_cols=dataset.static_cols,\n",
    "                                            sorted=dataset.sorted)\n",
    "\n",
    "        return updated_dataset\n",
    "    \n",
    "    @staticmethod\n",
    "    def trim_dataset(dataset, left_trim: int = 0, right_trim: int = 0):\n",
    "        \"\"\"\n",
    "        Trim temporal information from a dataset.\n",
    "        Returns temporal indexes [t+left:t-right] for all series.\n",
    "        \"\"\"\n",
    "        if dataset.min_size <= left_trim + right_trim:\n",
    "            raise Exception(f'left_trim + right_trim ({left_trim} + {right_trim}) \\\n",
    "                                must be lower than the shorter time series ({dataset.min_size})')\n",
    "\n",
    "        # Define and fill new temporal with trimmed information        \n",
    "        len_temporal, col_temporal = dataset.temporal.shape\n",
    "        total_trim = (left_trim + right_trim) * dataset.n_groups\n",
    "        new_temporal = torch.zeros(size=(len_temporal-total_trim, col_temporal))\n",
    "        new_indptr = [0]\n",
    "\n",
    "        acum = 0\n",
    "        for i in range(dataset.n_groups):\n",
    "            series_length = dataset.indptr[i + 1] - dataset.indptr[i]\n",
    "            new_length = series_length - left_trim - right_trim\n",
    "            new_temporal[acum:(acum+new_length), :] = dataset.temporal[dataset.indptr[i]+left_trim : \\\n",
    "                                                                       dataset.indptr[i + 1]-right_trim, :]\n",
    "            acum += new_length\n",
    "            new_indptr.append(acum)\n",
    "\n",
    "        new_max_size = dataset.max_size-left_trim-right_trim\n",
    "        new_min_size = dataset.min_size-left_trim-right_trim\n",
    "        \n",
    "        # Define new dataset\n",
    "        updated_dataset = TimeSeriesDataset(temporal=new_temporal,\n",
    "                                            temporal_cols= dataset.temporal_cols.copy(),\n",
    "                                            indptr=np.array(new_indptr).astype(np.int32),\n",
    "                                            max_size=new_max_size,\n",
    "                                            min_size=new_min_size,\n",
    "                                            static=dataset.static,\n",
    "                                            static_cols=dataset.static_cols,\n",
    "                                            sorted=dataset.sorted)\n",
    "\n",
    "        return updated_dataset\n",
    "\n",
    "    @staticmethod\n",
    "    def from_df(df, static_df=None, sort_df=False, scaler_type=None):\n",
    "        # TODO: protect on equality of static_df + df indexes\n",
    "        if df.index.name == 'unique_id':\n",
    "            warnings.warn(\n",
    "                \"Passing the id as index is deprecated, please provide it as a column instead.\",\n",
    "                DeprecationWarning,\n",
    "            )\n",
    "            df = df.reset_index('unique_id')\n",
    "        # Define indexes if not given\n",
    "        if static_df is not None:\n",
    "            if static_df.index.name == 'unique_id':\n",
    "                warnings.warn(\n",
    "                    \"Passing the id as index is deprecated, please provide it as a column instead.\",\n",
    "                    DeprecationWarning,\n",
    "                )\n",
    "            else:\n",
    "                static_df = static_df.set_index('unique_id')\n",
    "            if sort_df:\n",
    "                static_df = static_df.sort_index()\n",
    "\n",
    "        proc = DataFrameProcessor('unique_id', 'ds', 'y')\n",
    "        ids, times, data, indptr, sort_idxs = proc.process(df)\n",
    "        # processor sets y as the first column\n",
    "        temporal_cols = pd.Index(['y'] + df.columns.drop(['unique_id', 'ds', 'y']).tolist())\n",
    "        temporal = data.astype(np.float32, copy=False)\n",
    "        indices = pd.Index(ids)\n",
    "        dates = pd.Index(times, name='ds')\n",
    "        sizes = np.diff(indptr)\n",
    "        max_size = max(sizes)\n",
    "        min_size = min(sizes)\n",
    "\n",
    "        # Add Available mask efficiently (without adding column to df)\n",
    "        if 'available_mask' not in df.columns:\n",
    "            available_mask = np.ones((len(temporal),1), dtype=np.float32)\n",
    "            temporal = np.append(temporal, available_mask, axis=1)\n",
    "            temporal_cols = temporal_cols.append(pd.Index(['available_mask']))\n",
    "\n",
    "        # Static features\n",
    "        if static_df is not None:\n",
    "            static = static_df.values\n",
    "            static_cols = static_df.columns\n",
    "        else:\n",
    "            static = None\n",
    "            static_cols = None\n",
    "\n",
    "        dataset = TimeSeriesDataset(\n",
    "            temporal=temporal,\n",
    "            temporal_cols=temporal_cols,\n",
    "            static=static,\n",
    "            static_cols=static_cols,\n",
    "            indptr=indptr,\n",
    "            max_size=max_size,\n",
    "            min_size=min_size,\n",
    "            sorted=sort_df,\n",
    "            scaler_type=scaler_type,\n",
    "        )\n",
    "        ds = pd.MultiIndex.from_frame(df[['unique_id', 'ds']])\n",
    "        if sort_idxs is not None:\n",
    "            ds = ds[sort_idxs]\n",
    "        return dataset, indices, dates, ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a818bf-28d2-4561-8036-475f6fe78d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(TimeSeriesDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c07552-b6fa-4d10-8792-71743dcdfd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "# Testing sort_df=True functionality\n",
    "temporal_df = generate_series(n_series=1000, \n",
    "                         n_temporal_features=0, equal_ends=False)\n",
    "sorted_temporal_df = temporal_df.sort_values(['unique_id', 'ds'])\n",
    "unsorted_temporal_df = sorted_temporal_df.sample(frac=1.0)\n",
    "dataset, indices, dates, ds = TimeSeriesDataset.from_df(df=unsorted_temporal_df,\n",
    "                                                        sort_df=True)\n",
    "\n",
    "np.testing.assert_allclose(dataset.temporal[:,:-1], \n",
    "                           sorted_temporal_df.drop(columns='ds').values)\n",
    "test_eq(indices, sorted_temporal_df.index.unique(level='unique_id'))\n",
    "test_eq(dates, temporal_df.groupby('unique_id')['ds'].max().values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca22c35-2806-4a84-b6f1-34104a63acee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# scaler_type\n",
    "temporal_df_with_features = generate_series(n_series=10, n_temporal_features=2, equal_ends=False)\n",
    "dataset, *_ = TimeSeriesDataset.from_df(df=temporal_df_with_features, scaler_type='standard')\n",
    "grouped_y = temporal_df_with_features.groupby('unique_id')['y']\n",
    "\n",
    "# y was scaled\n",
    "np.testing.assert_allclose(\n",
    "    temporal_df_with_features['y'].sub(grouped_y.transform('mean')).div(grouped_y.transform('std', ddof=0)).values,\n",
    "    dataset.temporal[:, 0].numpy(),\n",
    "    atol=1e-5,\n",
    ")\n",
    "\n",
    "# mask is all 1s\n",
    "assert dataset.temporal[:, -1].eq(1).all().item()\n",
    "\n",
    "# inverse tfm\n",
    "np.testing.assert_allclose(\n",
    "    dataset.scalers_['y'].inverse_transform(GroupedArray(dataset.temporal[:, 0].numpy(), dataset.indptr)),\n",
    "    temporal_df_with_features['y'].values.astype('float32'),\n",
    "    atol=1e-5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4dae43c-4d11-4bbc-a431-ac33b004859a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TimeSeriesDataModule(pl.LightningDataModule):\n",
    "    \n",
    "    def __init__(\n",
    "            self, \n",
    "            dataset: TimeSeriesDataset,\n",
    "            batch_size=32, \n",
    "            valid_batch_size=1024,\n",
    "            num_workers=0,\n",
    "            drop_last=False\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.valid_batch_size = valid_batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.drop_last = drop_last\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        loader = TimeSeriesLoader(\n",
    "            self.dataset, \n",
    "            batch_size=self.batch_size, \n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=True,\n",
    "            drop_last=self.drop_last\n",
    "        )\n",
    "        return loader\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        loader = TimeSeriesLoader(\n",
    "            self.dataset, \n",
    "            batch_size=self.valid_batch_size, \n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False,\n",
    "            drop_last=self.drop_last\n",
    "        )\n",
    "        return loader\n",
    "    \n",
    "    def predict_dataloader(self):\n",
    "        loader = TimeSeriesLoader(\n",
    "            self.dataset,\n",
    "            batch_size=self.valid_batch_size, \n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False\n",
    "        )\n",
    "        return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8535a15f-b5cf-4ca1-bfa2-e53a9e8c3bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(TimeSeriesDataModule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b534d29d-eecc-43ba-8468-c23305fa24a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "batch_size = 128\n",
    "data = TimeSeriesDataModule(dataset=dataset, \n",
    "                            batch_size=batch_size, drop_last=True)\n",
    "for batch in data.train_dataloader():\n",
    "    test_eq(batch['temporal'].shape, (batch_size, 2, 500))\n",
    "    test_eq(batch['temporal_cols'], ['y', 'available_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4481272a-ea3a-4b63-8f14-9445d8f41338",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "batch_size = 128\n",
    "n_static_features = 2\n",
    "n_temporal_features = 4\n",
    "temporal_df, static_df = generate_series(n_series=1000,\n",
    "                                         n_static_features=n_static_features,\n",
    "                                         n_temporal_features=n_temporal_features, \n",
    "                                         equal_ends=False)\n",
    "\n",
    "dataset, indices, dates, ds = TimeSeriesDataset.from_df(df=temporal_df,\n",
    "                                                        static_df=static_df,\n",
    "                                                        sort_df=True)\n",
    "data = TimeSeriesDataModule(dataset=dataset,\n",
    "                            batch_size=batch_size, drop_last=True)\n",
    "\n",
    "for batch in data.train_dataloader():\n",
    "    test_eq(batch['temporal'].shape, (batch_size, n_temporal_features + 2, 500))\n",
    "    test_eq(batch['temporal_cols'],\n",
    "            ['y'] + [f'temporal_{i}' for i in range(n_temporal_features)] + ['available_mask'])\n",
    "    \n",
    "    test_eq(batch['static'].shape, (batch_size, n_static_features))\n",
    "    test_eq(batch['static_cols'], [f'static_{i}' for i in range(n_static_features)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252b59f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "# Testing sort_df=True functionality\n",
    "temporal_df = generate_series(n_series=2,\n",
    "                              n_temporal_features=2, equal_ends=True)\n",
    "temporal_df = temporal_df.groupby('unique_id').tail(10)\n",
    "temporal_df = temporal_df.reset_index()\n",
    "temporal_full_df = temporal_df.sort_values(['unique_id', 'ds']).reset_index(drop=True)\n",
    "temporal_full_df.loc[temporal_full_df.ds > '2001-05-11', ['y', 'temporal_0']] = None\n",
    "\n",
    "split1_df = temporal_full_df.loc[temporal_full_df.ds <= '2001-05-11']\n",
    "split2_df = temporal_full_df.loc[temporal_full_df.ds > '2001-05-11']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eab7367",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "# Testing available mask\n",
    "temporal_df_w_mask = temporal_df.copy()\n",
    "temporal_df_w_mask['available_mask'] = 1\n",
    "\n",
    "# Mask with all 1's\n",
    "dataset, indices, dates, ds = TimeSeriesDataset.from_df(df=temporal_df_w_mask,\n",
    "                                                        sort_df=True)\n",
    "mask_average = dataset.temporal[:, -1].mean()\n",
    "np.testing.assert_almost_equal(mask_average, 1.0000)\n",
    "\n",
    "# Add 0's to available mask\n",
    "temporal_df_w_mask.loc[temporal_df_w_mask.ds > '2001-05-11', 'available_mask'] = 0\n",
    "dataset, indices, dates, ds = TimeSeriesDataset.from_df(df=temporal_df_w_mask,\n",
    "                                                        sort_df=True)\n",
    "mask_average = dataset.temporal[:, -1].mean()\n",
    "np.testing.assert_almost_equal(mask_average, 0.7000)\n",
    "\n",
    "# Available mask not in last column\n",
    "temporal_df_w_mask = temporal_df_w_mask[['unique_id','ds','y','available_mask', 'temporal_0','temporal_1']]\n",
    "dataset, indices, dates, ds = TimeSeriesDataset.from_df(df=temporal_df_w_mask,\n",
    "                                                        sort_df=True)\n",
    "mask_average = dataset.temporal[:, 1].mean()\n",
    "np.testing.assert_almost_equal(mask_average, 0.7000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d23f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To test correct future_df wrangling of the `update_df` method\n",
    "# We are checking that we are able to recover the AirPassengers dataset\n",
    "# using the dataframe or splitting it into parts and initializing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f999c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "# FULL DATASET\n",
    "dataset_full, indices_full, dates_full, ds_full = TimeSeriesDataset.from_df(df=temporal_full_df,\n",
    "                                                                            sort_df=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f927e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "# SPLIT_1 DATASET\n",
    "dataset_1, indices_1, dates_1, ds_1 = TimeSeriesDataset.from_df(df=split1_df,\n",
    "                                                                sort_df=False)\n",
    "dataset_1 = dataset_1.update_dataset(dataset_1, split2_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468a6879",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "np.testing.assert_almost_equal(dataset_full.temporal.numpy(), dataset_1.temporal.numpy())\n",
    "test_eq(dataset_full.max_size, dataset_1.max_size)\n",
    "test_eq(dataset_full.indptr, dataset_1.indptr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556f852c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "# Testing trim_dataset functionality\n",
    "n_static_features = 0\n",
    "n_temporal_features = 2\n",
    "temporal_df = generate_series(n_series=100,\n",
    "                              min_length=50,\n",
    "                              max_length=100,\n",
    "                              n_static_features=n_static_features,\n",
    "                              n_temporal_features=n_temporal_features, \n",
    "                              equal_ends=False)\n",
    "dataset, indices, dates, ds = TimeSeriesDataset.from_df(df=temporal_df,\n",
    "                                                        static_df=static_df,\n",
    "                                                        sort_df=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7b1a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "left_trim = 10\n",
    "right_trim = 20\n",
    "dataset_trimmed = dataset.trim_dataset(dataset, left_trim=left_trim, right_trim=right_trim)\n",
    "\n",
    "np.testing.assert_almost_equal(dataset.temporal[dataset.indptr[50]+left_trim:dataset.indptr[51]-right_trim].numpy(),\n",
    "                               dataset_trimmed.temporal[dataset_trimmed.indptr[50]:dataset_trimmed.indptr[51]].numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
