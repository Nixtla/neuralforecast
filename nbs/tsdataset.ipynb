{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "524620c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp tsdataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15392f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fa25a4",
   "metadata": {},
   "source": [
    "# PyTorch Dataset/Loader\n",
    "> Torch Dataset for Time Series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2508f7a9-1433-4ad8-8f2f-0078c6ed6c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cchallu/opt/anaconda3/envs/neuralforecast/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "from fastcore.test import test_eq\n",
    "from nbdev.showdoc import show_doc\n",
    "from neuralforecast.utils import generate_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44065066-e72a-431f-938f-1528adef9fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from collections.abc import Mapping\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "323a7a6e-38c3-496d-8f1e-cad05f643d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TimeSeriesLoader(DataLoader):\n",
    "    \"\"\"TimeSeriesLoader DataLoader.\n",
    "    [Source code](https://github.com/Nixtla/neuralforecast1/blob/main/neuralforecast/tsdataset.py).\n",
    "\n",
    "    Small change to PyTorch's Data loader. \n",
    "    Combines a dataset and a sampler, and provides an iterable over the given dataset.\n",
    "\n",
    "    The class `~torch.utils.data.DataLoader` supports both map-style and\n",
    "    iterable-style datasets with single- or multi-process loading, customizing\n",
    "    loading order and optional automatic batching (collation) and memory pinning.    \n",
    "    \n",
    "    **Parameters:**<br>\n",
    "    `batch_size`: (int, optional): how many samples per batch to load (default: 1).<br>\n",
    "    `shuffle`: (bool, optional): set to `True` to have the data reshuffled at every epoch (default: `False`).<br>\n",
    "    `sampler`: (Sampler or Iterable, optional): defines the strategy to draw samples from the dataset.<br>\n",
    "                Can be any `Iterable` with `__len__` implemented. If specified, `shuffle` must not be specified.<br>\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, **kwargs):\n",
    "        if 'collate_fn' in kwargs:\n",
    "            kwargs.pop('collate_fn')\n",
    "        kwargs_ = {**kwargs, **dict(collate_fn=self._collate_fn)}\n",
    "        DataLoader.__init__(self, dataset=dataset, **kwargs_)\n",
    "    \n",
    "    def _collate_fn(self, batch):\n",
    "        elem = batch[0]\n",
    "        elem_type = type(elem)\n",
    "\n",
    "        if isinstance(elem, torch.Tensor):\n",
    "            out = None\n",
    "            if torch.utils.data.get_worker_info() is not None:\n",
    "                # If we're in a background process, concatenate directly into a\n",
    "                # shared memory tensor to avoid an extra copy\n",
    "                numel = sum(x.numel() for x in batch)\n",
    "                storage = elem.storage()._new_shared(numel, device=elem.device)\n",
    "                out = elem.new(storage).resize_(len(batch), *list(elem.size()))\n",
    "            return torch.stack(batch, 0, out=out)\n",
    "\n",
    "        elif isinstance(elem, Mapping):\n",
    "            if elem['static'] is None:\n",
    "                return dict(temporal=self.collate_fn([d['temporal'] for d in batch]),\n",
    "                            temporal_cols = elem['temporal_cols'])\n",
    "            \n",
    "            return dict(static=self.collate_fn([d['static'] for d in batch]),\n",
    "                        static_cols = elem['static_cols'],\n",
    "                        temporal=self.collate_fn([d['temporal'] for d in batch]),\n",
    "                        temporal_cols = elem['temporal_cols'])\n",
    "\n",
    "        raise TypeError(f'Unknown {elem_type}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93e94050-0290-43ad-9a73-c4626bba9541",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/tsdataset.py#L16){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TimeSeriesLoader\n",
       "\n",
       ">      TimeSeriesLoader (dataset, **kwargs)\n",
       "\n",
       "TimeSeriesLoader DataLoader.\n",
       "[Source code](https://github.com/Nixtla/neuralforecast1/blob/main/neuralforecast/tsdataset.py).\n",
       "\n",
       "Small change to PyTorch's Data loader. \n",
       "Combines a dataset and a sampler, and provides an iterable over the given dataset.\n",
       "\n",
       "The class `~torch.utils.data.DataLoader` supports both map-style and\n",
       "iterable-style datasets with single- or multi-process loading, customizing\n",
       "loading order and optional automatic batching (collation) and memory pinning.    \n",
       "\n",
       "**Parameters:**<br>\n",
       "`batch_size`: (int, optional): how many samples per batch to load (default: 1).<br>\n",
       "`shuffle`: (bool, optional): set to `True` to have the data reshuffled at every epoch (default: `False`).<br>\n",
       "`sampler`: (Sampler or Iterable, optional): defines the strategy to draw samples from the dataset.<br>\n",
       "            Can be any `Iterable` with `__len__` implemented. If specified, `shuffle` must not be specified.<br>"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/tsdataset.py#L16){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TimeSeriesLoader\n",
       "\n",
       ">      TimeSeriesLoader (dataset, **kwargs)\n",
       "\n",
       "TimeSeriesLoader DataLoader.\n",
       "[Source code](https://github.com/Nixtla/neuralforecast1/blob/main/neuralforecast/tsdataset.py).\n",
       "\n",
       "Small change to PyTorch's Data loader. \n",
       "Combines a dataset and a sampler, and provides an iterable over the given dataset.\n",
       "\n",
       "The class `~torch.utils.data.DataLoader` supports both map-style and\n",
       "iterable-style datasets with single- or multi-process loading, customizing\n",
       "loading order and optional automatic batching (collation) and memory pinning.    \n",
       "\n",
       "**Parameters:**<br>\n",
       "`batch_size`: (int, optional): how many samples per batch to load (default: 1).<br>\n",
       "`shuffle`: (bool, optional): set to `True` to have the data reshuffled at every epoch (default: `False`).<br>\n",
       "`sampler`: (Sampler or Iterable, optional): defines the strategy to draw samples from the dataset.<br>\n",
       "            Can be any `Iterable` with `__len__` implemented. If specified, `shuffle` must not be specified.<br>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(TimeSeriesLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05687429-c139-44c0-adb9-097c616908cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TimeSeriesDataset(Dataset):\n",
    "\n",
    "    def __init__(self,\n",
    "                 temporal,\n",
    "                 temporal_cols,\n",
    "                 indptr,\n",
    "                 max_size: int,\n",
    "                 min_size: int,\n",
    "                 static=None,\n",
    "                 static_cols=None,\n",
    "                 sorted=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.temporal = torch.tensor(temporal, dtype=torch.float)\n",
    "        self.temporal_cols = pd.Index(list(temporal_cols)+\\\n",
    "                                      ['available_mask'])\n",
    "        if static is not None:\n",
    "            self.static = torch.tensor(static, dtype=torch.float)\n",
    "            self.static_cols = static_cols\n",
    "        else:\n",
    "            self.static = static\n",
    "            self.static_cols = static_cols\n",
    "\n",
    "        self.indptr = indptr\n",
    "        self.n_groups = self.indptr.size - 1\n",
    "        self.max_size = max_size\n",
    "        self.min_size = min_size\n",
    "\n",
    "        # Upadated flag. To protect consistency, dataset can only be updated once\n",
    "        self.updated = False\n",
    "        self.sorted = sorted\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if isinstance(idx, int):\n",
    "            # Parse temporal data and pad its left\n",
    "            temporal = torch.zeros(size=(len(self.temporal_cols), self.max_size),\n",
    "                                   dtype=torch.float32)\n",
    "            ts = self.temporal[self.indptr[idx] : self.indptr[idx + 1], :]\n",
    "            temporal[:len(self.temporal_cols)-1, -len(ts):] = ts.permute(1, 0)\n",
    "\n",
    "            # Add available_mask\n",
    "            temporal[len(self.temporal_cols)-1, -len(ts):] = 1\n",
    "\n",
    "            # Add static data if available\n",
    "            static = None if self.static is None else self.static[idx,:]\n",
    "\n",
    "            item = dict(temporal=temporal, temporal_cols=self.temporal_cols,\n",
    "                        static=static, static_cols=self.static_cols)\n",
    "\n",
    "            return item\n",
    "        raise ValueError(f'idx must be int, got {type(idx)}')\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_groups\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'TimeSeriesDataset(n_data={self.data.size:,}, n_groups={self.n_groups:,})'\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        if not hasattr(other, 'data') or not hasattr(other, 'indptr'):\n",
    "            return False\n",
    "        return np.allclose(self.data, other.data) and np.array_equal(self.indptr, other.indptr)\n",
    "\n",
    "    @staticmethod\n",
    "    def update_dataset(dataset, future_df):\n",
    "        \"\"\"Add future observations to the dataset.\n",
    "        \"\"\"        \n",
    "\n",
    "        # Add Nones to missing columns (without available_mask)\n",
    "        temporal_cols = dataset.temporal_cols.copy()\n",
    "        temporal_cols = temporal_cols.delete(len(temporal_cols)-1)\n",
    "        for col in temporal_cols:\n",
    "            if col not in future_df.columns:\n",
    "                future_df[col] = None\n",
    "        \n",
    "        # Sort columns to match self.temporal_cols\n",
    "        future_df = future_df[ ['unique_id','ds'] + temporal_cols.tolist() ]\n",
    "\n",
    "        # Process future_df\n",
    "        futr_dataset, indices, futr_dates, futr_index = dataset.from_df(df=future_df, sort_df=dataset.sorted)\n",
    "\n",
    "        # Define and fill new temporal with updated information\n",
    "        len_temporal, col_temporal = dataset.temporal.shape\n",
    "        new_temporal = torch.zeros(size=(len_temporal+len(future_df), col_temporal))\n",
    "        new_indptr = [0]\n",
    "        new_max_size = 0\n",
    "\n",
    "        acum = 0\n",
    "        for i in range(dataset.n_groups):\n",
    "            series_length = dataset.indptr[i + 1] - dataset.indptr[i]\n",
    "            new_length = series_length + futr_dataset.indptr[i + 1] - futr_dataset.indptr[i]\n",
    "            new_temporal[acum:(acum+series_length), :] = dataset.temporal[dataset.indptr[i] : dataset.indptr[i + 1], :]\n",
    "            new_temporal[(acum+series_length):(acum+new_length), :] = \\\n",
    "                                 futr_dataset.temporal[futr_dataset.indptr[i] : futr_dataset.indptr[i + 1], :]\n",
    "            \n",
    "            acum += new_length\n",
    "            new_indptr.append(acum)\n",
    "            if new_length > new_max_size:\n",
    "                new_max_size = new_length\n",
    "        \n",
    "        # Define new dataset\n",
    "        updated_dataset = TimeSeriesDataset(temporal=new_temporal,\n",
    "                                            temporal_cols=temporal_cols,\n",
    "                                            indptr=np.array(new_indptr).astype(np.int32),\n",
    "                                            max_size=new_max_size,\n",
    "                                            min_size=dataset.min_size,\n",
    "                                            static=dataset.static,\n",
    "                                            static_cols=dataset.static_cols,\n",
    "                                            sorted=dataset.sorted)\n",
    "\n",
    "        return updated_dataset\n",
    "    \n",
    "    @staticmethod\n",
    "    def trim_dataset(dataset, left_trim: int = 0, right_trim: int = 0):\n",
    "        \"\"\"Trim temporal information from a dataset.\n",
    "        \"\"\"\n",
    "\n",
    "        if dataset.min_size <= left_trim + right_trim:\n",
    "            raise Exception(f'left_trim + right_trim ({left_trim} + {right_trim}) \\\n",
    "                                must be lower than the shorter time series ({dataset.min_size})')\n",
    "\n",
    "\n",
    "        # Remove available mask from temporal_cols\n",
    "        temporal_cols = dataset.temporal_cols.copy()\n",
    "        temporal_cols = temporal_cols.delete(len(temporal_cols)-1)\n",
    "\n",
    "        # Define and fill new temporal with trimmed information        \n",
    "        len_temporal, col_temporal = dataset.temporal.shape\n",
    "        total_trim = (left_trim + right_trim) * dataset.n_groups\n",
    "        new_temporal = torch.zeros(size=(len_temporal-total_trim, col_temporal))\n",
    "        new_indptr = [0]\n",
    "\n",
    "        acum = 0\n",
    "        for i in range(dataset.n_groups):\n",
    "            series_length = dataset.indptr[i + 1] - dataset.indptr[i]\n",
    "            new_length = series_length - left_trim - right_trim\n",
    "            new_temporal[acum:(acum+new_length), :] = dataset.temporal[dataset.indptr[i]+left_trim : \\\n",
    "                                                                       dataset.indptr[i + 1]-right_trim, :]\n",
    "            acum += new_length\n",
    "            new_indptr.append(acum)\n",
    "\n",
    "        new_max_size = dataset.max_size-left_trim-right_trim\n",
    "        new_min_size = dataset.min_size-left_trim-right_trim\n",
    "        \n",
    "        # Define new dataset\n",
    "        updated_dataset = TimeSeriesDataset(temporal=new_temporal,\n",
    "                                            temporal_cols=temporal_cols,\n",
    "                                            indptr=np.array(new_indptr).astype(np.int32),\n",
    "                                            max_size=new_max_size,\n",
    "                                            min_size=new_min_size,\n",
    "                                            static=dataset.static,\n",
    "                                            static_cols=dataset.static_cols,\n",
    "                                            sorted=dataset.sorted)\n",
    "\n",
    "        return updated_dataset\n",
    "\n",
    "    @staticmethod\n",
    "    def from_df(df, static_df=None, sort_df=False):\n",
    "        # TODO: protect on equality of static_df + df indexes\n",
    "        # Define indexes if not given\n",
    "        if df.index.name != 'unique_id':\n",
    "            df = df.set_index('unique_id')\n",
    "            if static_df is not None:\n",
    "                static_df = static_df.set_index('unique_id')\n",
    "\n",
    "        df = df.set_index('ds', append=True)\n",
    "        \n",
    "        # Sort data by index\n",
    "        if not df.index.is_monotonic_increasing and sort_df:\n",
    "            df = df.sort_index()\n",
    "\n",
    "            if static_df is not None:\n",
    "                static_df = static_df.sort_index()\n",
    "\n",
    "        # Create auxiliary temporal indices 'indptr'\n",
    "        temporal = df.values.astype(np.float32)\n",
    "        temporal_cols = df.columns\n",
    "        indices_sizes = df.index.get_level_values('unique_id').value_counts(sort=False)\n",
    "        indices = indices_sizes.index\n",
    "        sizes = indices_sizes.values\n",
    "        max_size = max(sizes)\n",
    "        min_size = min(sizes)\n",
    "        cum_sizes = sizes.cumsum()\n",
    "        dates = df.index.get_level_values('ds')[cum_sizes - 1]\n",
    "        indptr = np.append(0, cum_sizes).astype(np.int32)\n",
    "\n",
    "        # Static features\n",
    "        if static_df is not None:\n",
    "            static = static_df.values\n",
    "            static_cols = static_df.columns\n",
    "        else:\n",
    "            static = None\n",
    "            static_cols = None\n",
    "\n",
    "        dataset = TimeSeriesDataset(\n",
    "                    temporal=temporal, temporal_cols=temporal_cols,\n",
    "                    static=static, static_cols=static_cols,\n",
    "                    indptr=indptr, max_size=max_size, min_size=min_size, sorted=sort_df)\n",
    "        return dataset, indices, dates, df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61a818bf-28d2-4561-8036-475f6fe78d0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/tsdataset.py#L71){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TimeSeriesDataset\n",
       "\n",
       ">      TimeSeriesDataset (temporal, temporal_cols, indptr, max_size:int,\n",
       ">                         min_size:int, static=None, static_cols=None,\n",
       ">                         sorted=False)\n",
       "\n",
       "An abstract class representing a :class:`Dataset`.\n",
       "\n",
       "All datasets that represent a map from keys to data samples should subclass\n",
       "it. All subclasses should overwrite :meth:`__getitem__`, supporting fetching a\n",
       "data sample for a given key. Subclasses could also optionally overwrite\n",
       ":meth:`__len__`, which is expected to return the size of the dataset by many\n",
       ":class:`~torch.utils.data.Sampler` implementations and the default options\n",
       "of :class:`~torch.utils.data.DataLoader`.\n",
       "\n",
       ".. note::\n",
       "  :class:`~torch.utils.data.DataLoader` by default constructs a index\n",
       "  sampler that yields integral indices.  To make it work with a map-style\n",
       "  dataset with non-integral indices/keys, a custom sampler must be provided."
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/tsdataset.py#L71){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TimeSeriesDataset\n",
       "\n",
       ">      TimeSeriesDataset (temporal, temporal_cols, indptr, max_size:int,\n",
       ">                         min_size:int, static=None, static_cols=None,\n",
       ">                         sorted=False)\n",
       "\n",
       "An abstract class representing a :class:`Dataset`.\n",
       "\n",
       "All datasets that represent a map from keys to data samples should subclass\n",
       "it. All subclasses should overwrite :meth:`__getitem__`, supporting fetching a\n",
       "data sample for a given key. Subclasses could also optionally overwrite\n",
       ":meth:`__len__`, which is expected to return the size of the dataset by many\n",
       ":class:`~torch.utils.data.Sampler` implementations and the default options\n",
       "of :class:`~torch.utils.data.DataLoader`.\n",
       "\n",
       ".. note::\n",
       "  :class:`~torch.utils.data.DataLoader` by default constructs a index\n",
       "  sampler that yields integral indices.  To make it work with a map-style\n",
       "  dataset with non-integral indices/keys, a custom sampler must be provided."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(TimeSeriesDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52c07552-b6fa-4d10-8792-71743dcdfd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "# Testing sort_df=True functionality\n",
    "temporal_df = generate_series(n_series=1000, \n",
    "                         n_temporal_features=0, equal_ends=False)\n",
    "sorted_temporal_df = temporal_df.sort_values(['unique_id', 'ds'])\n",
    "unsorted_temporal_df = sorted_temporal_df.sample(frac=1.0)\n",
    "dataset, indices, dates, ds = TimeSeriesDataset.from_df(df=unsorted_temporal_df,\n",
    "                                                        sort_df=True)\n",
    "\n",
    "np.testing.assert_allclose(dataset.temporal, \n",
    "                           sorted_temporal_df.drop(columns='ds').values)\n",
    "test_eq(indices, sorted_temporal_df.index.unique(level='unique_id'))\n",
    "test_eq(dates, temporal_df.groupby('unique_id')['ds'].max().values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4dae43c-4d11-4bbc-a431-ac33b004859a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TimeSeriesDataModule(pl.LightningDataModule):\n",
    "    \n",
    "    def __init__(\n",
    "            self, \n",
    "            dataset: TimeSeriesDataset,\n",
    "            batch_size=32, \n",
    "            valid_batch_size=1024,\n",
    "            num_workers=0,\n",
    "            drop_last=False\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.valid_batch_size = valid_batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.drop_last = drop_last\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        loader = TimeSeriesLoader(\n",
    "            self.dataset, \n",
    "            batch_size=self.batch_size, \n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=True,\n",
    "            drop_last=self.drop_last\n",
    "        )\n",
    "        return loader\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        loader = TimeSeriesLoader(\n",
    "            self.dataset, \n",
    "            batch_size=self.valid_batch_size, \n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False,\n",
    "            drop_last=self.drop_last\n",
    "        )\n",
    "        return loader\n",
    "    \n",
    "    def predict_dataloader(self):\n",
    "        loader = TimeSeriesLoader(\n",
    "            self.dataset,\n",
    "            batch_size=self.valid_batch_size, \n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False\n",
    "        )\n",
    "        return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8535a15f-b5cf-4ca1-bfa2-e53a9e8c3bc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/tsdataset.py#L253){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TimeSeriesDataModule\n",
       "\n",
       ">      TimeSeriesDataModule (dataset:__main__.TimeSeriesDataset, batch_size=32,\n",
       ">                            valid_batch_size=1024, num_workers=0,\n",
       ">                            drop_last=False)\n",
       "\n",
       "A DataModule standardizes the training, val, test splits, data preparation and transforms. The main\n",
       "advantage is consistent data splits, data preparation and transforms across models.\n",
       "\n",
       "Example::\n",
       "\n",
       "    class MyDataModule(LightningDataModule):\n",
       "        def __init__(self):\n",
       "            super().__init__()\n",
       "        def prepare_data(self):\n",
       "            # download, split, etc...\n",
       "            # only called on 1 GPU/TPU in distributed\n",
       "        def setup(self, stage):\n",
       "            # make assignments here (val/train/test split)\n",
       "            # called on every process in DDP\n",
       "        def train_dataloader(self):\n",
       "            train_split = Dataset(...)\n",
       "            return DataLoader(train_split)\n",
       "        def val_dataloader(self):\n",
       "            val_split = Dataset(...)\n",
       "            return DataLoader(val_split)\n",
       "        def test_dataloader(self):\n",
       "            test_split = Dataset(...)\n",
       "            return DataLoader(test_split)\n",
       "        def teardown(self):\n",
       "            # clean up after fit or test\n",
       "            # called on every process in DDP"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/tsdataset.py#L253){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TimeSeriesDataModule\n",
       "\n",
       ">      TimeSeriesDataModule (dataset:__main__.TimeSeriesDataset, batch_size=32,\n",
       ">                            valid_batch_size=1024, num_workers=0,\n",
       ">                            drop_last=False)\n",
       "\n",
       "A DataModule standardizes the training, val, test splits, data preparation and transforms. The main\n",
       "advantage is consistent data splits, data preparation and transforms across models.\n",
       "\n",
       "Example::\n",
       "\n",
       "    class MyDataModule(LightningDataModule):\n",
       "        def __init__(self):\n",
       "            super().__init__()\n",
       "        def prepare_data(self):\n",
       "            # download, split, etc...\n",
       "            # only called on 1 GPU/TPU in distributed\n",
       "        def setup(self, stage):\n",
       "            # make assignments here (val/train/test split)\n",
       "            # called on every process in DDP\n",
       "        def train_dataloader(self):\n",
       "            train_split = Dataset(...)\n",
       "            return DataLoader(train_split)\n",
       "        def val_dataloader(self):\n",
       "            val_split = Dataset(...)\n",
       "            return DataLoader(val_split)\n",
       "        def test_dataloader(self):\n",
       "            test_split = Dataset(...)\n",
       "            return DataLoader(test_split)\n",
       "        def teardown(self):\n",
       "            # clean up after fit or test\n",
       "            # called on every process in DDP"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(TimeSeriesDataModule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b534d29d-eecc-43ba-8468-c23305fa24a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "batch_size = 128\n",
    "data = TimeSeriesDataModule(dataset=dataset, \n",
    "                            batch_size=batch_size, drop_last=True)\n",
    "for batch in data.train_dataloader():\n",
    "    test_eq(batch['temporal'].shape, (batch_size, 2, 500))\n",
    "    test_eq(batch['temporal_cols'], ['y', 'available_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4481272a-ea3a-4b63-8f14-9445d8f41338",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "batch_size = 128\n",
    "n_static_features = 2\n",
    "n_temporal_features = 4\n",
    "temporal_df, static_df = generate_series(n_series=1000,\n",
    "                                         n_static_features=n_static_features,\n",
    "                                         n_temporal_features=n_temporal_features, \n",
    "                                         equal_ends=False)\n",
    "\n",
    "dataset, indices, dates, ds = TimeSeriesDataset.from_df(df=temporal_df,\n",
    "                                                        static_df=static_df,\n",
    "                                                        sort_df=True)\n",
    "data = TimeSeriesDataModule(dataset=dataset,\n",
    "                            batch_size=batch_size, drop_last=True)\n",
    "\n",
    "for batch in data.train_dataloader():\n",
    "    test_eq(batch['temporal'].shape, (batch_size, n_temporal_features + 2, 500))\n",
    "    test_eq(batch['temporal_cols'],\n",
    "            ['y'] + [f'temporal_{i}' for i in range(n_temporal_features)] + ['available_mask'])\n",
    "    \n",
    "    test_eq(batch['static'].shape, (batch_size, n_static_features))\n",
    "    test_eq(batch['static_cols'], [f'static_{i}' for i in range(n_static_features)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "252b59f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "# Testing sort_df=True functionality\n",
    "temporal_df = generate_series(n_series=2,\n",
    "                              n_temporal_features=2, equal_ends=True)\n",
    "temporal_df = temporal_df.groupby('unique_id').tail(10)\n",
    "temporal_df = temporal_df.reset_index()\n",
    "temporal_full_df = temporal_df.sort_values(['unique_id', 'ds']).reset_index(drop=True)\n",
    "temporal_full_df.loc[temporal_full_df.ds > '2001-05-11', ['y', 'temporal_0']] = None\n",
    "\n",
    "split1_df = temporal_full_df.loc[temporal_full_df.ds <= '2001-05-11']\n",
    "split2_df = temporal_full_df.loc[temporal_full_df.ds > '2001-05-11']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a0d23f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To test correct future_df wrangling of the `update_df` method\n",
    "# We are checking that we are able to recover the AirPassengers dataset\n",
    "# using the dataframe or splitting it into parts and initializing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "39f999c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "# FULL DATASET\n",
    "dataset_full, indices_full, dates_full, ds_full = TimeSeriesDataset.from_df(df=temporal_full_df,\n",
    "                                                                            sort_df=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0b1b3b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "# SPLIT_1 DATASET\n",
    "dataset_1, indices_1, dates_1, ds_1 = TimeSeriesDataset.from_df(df=split1_df,\n",
    "                                                                sort_df=False)\n",
    "dataset_1 = dataset_1.update_dataset(dataset_1, split2_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "75d12062",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "np.testing.assert_almost_equal(dataset_full.temporal.numpy(), dataset_1.temporal.numpy())\n",
    "test_eq(dataset_full.max_size, dataset_1.max_size)\n",
    "test_eq(dataset_full.indptr, dataset_1.indptr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c72f3b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "# Testing trim_dataset functionality\n",
    "n_static_features = 0\n",
    "n_temporal_features = 2\n",
    "temporal_df = generate_series(n_series=100,\n",
    "                              min_length=50,\n",
    "                              max_length=100,\n",
    "                              n_static_features=n_static_features,\n",
    "                              n_temporal_features=n_temporal_features, \n",
    "                              equal_ends=False)\n",
    "dataset, indices, dates, ds = TimeSeriesDataset.from_df(df=temporal_df,\n",
    "                                                        static_df=static_df,\n",
    "                                                        sort_df=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3d9be28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "left_trim = 10\n",
    "right_trim = 20\n",
    "dataset_trimmed = dataset.trim_dataset(dataset, left_trim=left_trim, right_trim=right_trim)\n",
    "\n",
    "np.testing.assert_almost_equal(dataset.temporal[dataset.indptr[50]+left_trim:dataset.indptr[51]-right_trim].numpy(),\n",
    "                               dataset_trimmed.temporal[dataset_trimmed.indptr[50]:dataset_trimmed.indptr[51]].numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
