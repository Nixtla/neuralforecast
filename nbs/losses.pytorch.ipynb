{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524620c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp losses.pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15392f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fa25a4",
   "metadata": {},
   "source": [
    "# PyTorch Losses\n",
    "\n",
    "> NeuralForecast contains a collection PyTorch Loss classes aimed to be used during the models' optimization. The most important train signal is the forecast error, which is the difference between the observed value $y_{\\tau}$ and the prediction $\\hat{y}_{\\tau}$, at time $y_{\\tau}$:$$e_{\\tau} = y_{\\tau}-\\hat{y}_{\\tau} \\qquad \\qquad \\tau \\in \\{t+1,\\dots,t+H \\}$$ The train loss summarizes the forecast errors in different train optimization objectives.<br><br>All the losses are `torch.nn.modules` which helps to automatically moved them across CPU/GPU/TPU devices with Pytorch Lightning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfa68dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from typing import Optional, Union, Tuple\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal, StudentT, Poisson\n",
    "from torch.distributions import (\n",
    "    AffineTransform,\n",
    "    Distribution,\n",
    "    TransformedDistribution,\n",
    ")\n",
    "# from torch.distributions import (\n",
    "#     Beta,\n",
    "#     Distribution,\n",
    "#     Gamma,\n",
    "#     NegativeBinomial,\n",
    "#     Normal,\n",
    "#     Poisson,\n",
    "#     StudentT,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2508f7a9-1433-4ad8-8f2f-0078c6ed6c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import matplotlib.pyplot as plt\n",
    "from fastcore.test import test_eq\n",
    "from nbdev.showdoc import show_doc\n",
    "from neuralforecast.utils import generate_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e07e98-b4c8-4ade-b3b6-1d27f367aa0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def _divide_no_nan(a: torch.Tensor, b: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Auxiliary funtion to handle divide by 0\n",
    "    \"\"\"\n",
    "    div = a / b\n",
    "    div[div != div] = 0.0\n",
    "    div[div == float('inf')] = 0.0\n",
    "    return div"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a94d7d",
   "metadata": {},
   "source": [
    "# <span style=\"color:DarkBlue\">1. Scale-dependent Errors </span>\n",
    "\n",
    "These metrics are on the same scale as the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fc4679",
   "metadata": {},
   "source": [
    "## Mean Absolute Error (MAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e413fae-c590-4713-aab9-37c61ed37dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class MAE(torch.nn.Module):\n",
    "    \"\"\"Mean Absolute Error\n",
    "\n",
    "    Calculates Mean Absolute Error between\n",
    "    `y` and `y_hat`. MAE measures the relative prediction\n",
    "    accuracy of a forecasting method by calculating the\n",
    "    deviation of the prediction and the true\n",
    "    value at a given time and averages these devations\n",
    "    over the length of the series.\n",
    "\n",
    "    $$ \\mathrm{MAE}(\\\\mathbf{y}_{\\\\tau}, \\\\mathbf{\\hat{y}}_{\\\\tau}) = \\\\frac{1}{H} \\\\sum^{t+H}_{\\\\tau=t+1} |y_{\\\\tau} - \\hat{y}_{\\\\tau}| $$\n",
    "    \"\"\"    \n",
    "    def __init__(self):\n",
    "        super(MAE, self).__init__()\n",
    "        self.outputsize_multiplier = 1\n",
    "        self.output_names = ['']\n",
    "        self.is_distribution_output = False\n",
    "\n",
    "    def domain_map(self, y_hat: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Univariate loss operates in dimension [B,T,H]/[B,H]\n",
    "        This changes the network's output from [B,H,1]->[B,H]\n",
    "        \"\"\"\n",
    "        return y_hat.squeeze(-1)\n",
    "\n",
    "    def __call__(self, y: torch.Tensor, y_hat: torch.Tensor,\n",
    "                 mask: Union[torch.Tensor, None] = None):\n",
    "        \"\"\"\n",
    "        **Parameters:**<br>\n",
    "        `y`: tensor, Actual values.<br>\n",
    "        `y_hat`: tensor, Predicted values.<br>\n",
    "        `mask`: tensor, Specifies date stamps per serie to consider in loss.<br>\n",
    "\n",
    "        **Returns:**<br>\n",
    "        `mae`: tensor (single value).\n",
    "        \"\"\"\n",
    "        if mask is None:\n",
    "            mask = torch.ones_like(y)\n",
    "\n",
    "        mae = torch.abs(y - y_hat) * mask\n",
    "        mae = torch.mean(mae)\n",
    "        return mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d004cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(MAE, name='MAE.__init__', title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a20a273",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(MAE.__call__, name='MAE.__call__', title_level=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0292c74d",
   "metadata": {},
   "source": [
    "![](imgs_losses/mae_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f31cc3d",
   "metadata": {},
   "source": [
    "## Mean Squared Error (MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472b63b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class MSE(torch.nn.Module):\n",
    "    \"\"\"  Mean Squared Error\n",
    "\n",
    "    Calculates Mean Squared Error between\n",
    "    `y` and `y_hat`. MSE measures the relative prediction\n",
    "    accuracy of a forecasting method by calculating the \n",
    "    squared deviation of the prediction and the true\n",
    "    value at a given time, and averages these devations\n",
    "    over the length of the series.\n",
    "    \n",
    "    $$ \\mathrm{MSE}(\\\\mathbf{y}_{\\\\tau}, \\\\mathbf{\\hat{y}}_{\\\\tau}) = \\\\frac{1}{H} \\\\sum^{t+H}_{\\\\tau=t+1} (y_{\\\\tau} - \\hat{y}_{\\\\tau})^{2} $$\n",
    "    \"\"\"    \n",
    "    def __init__(self):\n",
    "        super(MSE, self).__init__()\n",
    "        self.outputsize_multiplier = 1\n",
    "        self.output_names = ['']\n",
    "        self.is_distribution_output = False\n",
    "\n",
    "    def domain_map(self, y_hat: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Univariate loss operates in dimension [B,T,H]/[B,H]\n",
    "        This changes the network's output from [B,H,1]->[B,H]\n",
    "        \"\"\"\n",
    "        return y_hat.squeeze(-1)\n",
    "\n",
    "    def __call__(self, y: torch.Tensor, y_hat: torch.Tensor,\n",
    "                 mask: Union[torch.Tensor, None] = None):\n",
    "        \"\"\"\n",
    "        **Parameters:**<br>\n",
    "        `y`: tensor, Actual values.<br>\n",
    "        `y_hat`: tensor, Predicted values.<br>\n",
    "        `mask`: tensor, Specifies date stamps per serie to consider in loss.<br>\n",
    "\n",
    "        **Returns:**<br>\n",
    "        `mse`: tensor (single value).\n",
    "        \"\"\"\n",
    "        if mask is None:\n",
    "            mask = torch.ones_like(y_hat)\n",
    "\n",
    "        mse = (y - y_hat)**2\n",
    "        mse = mask * mse\n",
    "        mse = torch.mean(mse)\n",
    "        return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c65b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(MSE, name='MSE.__init__', title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0126a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(MSE.__call__, name='MSE.__call__', title_level=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b23f9c1",
   "metadata": {},
   "source": [
    "![](imgs_losses/mse_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b160140b",
   "metadata": {},
   "source": [
    "## Root Mean Squared Error (RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab87149d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class RMSE(torch.nn.Module):\n",
    "    \"\"\" Root Mean Squared Error\n",
    "\n",
    "    Calculates Root Mean Squared Error between\n",
    "    `y` and `y_hat`. RMSE measures the relative prediction\n",
    "    accuracy of a forecasting method by calculating the squared deviation\n",
    "    of the prediction and the observed value at a given time and\n",
    "    averages these devations over the length of the series.\n",
    "    Finally the RMSE will be in the same scale\n",
    "    as the original time series so its comparison with other\n",
    "    series is possible only if they share a common scale. \n",
    "    RMSE has a direct connection to the L2 norm.\n",
    "    \n",
    "    $$ \\mathrm{RMSE}(\\\\mathbf{y}_{\\\\tau}, \\\\mathbf{\\hat{y}}_{\\\\tau}) = \\\\sqrt{\\\\frac{1}{H} \\\\sum^{t+H}_{\\\\tau=t+1} (y_{\\\\tau} - \\hat{y}_{\\\\tau})^{2}} $$\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(RMSE, self).__init__()\n",
    "        self.outputsize_multiplier = 1\n",
    "        self.output_names = ['']\n",
    "        self.is_distribution_output = False\n",
    "\n",
    "    def domain_map(self, y_hat: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Univariate loss operates in dimension [B,T,H]/[B,H]\n",
    "        This changes the network's output from [B,H,1]->[B,H]\n",
    "        \"\"\"\n",
    "        return y_hat.squeeze(-1)\n",
    "\n",
    "    def __call__(self, y: torch.Tensor, y_hat: torch.Tensor, \n",
    "                 mask: Union[torch.Tensor, None] = None):\n",
    "        \"\"\"\n",
    "        **Parameters:**<br>\n",
    "        `y`: tensor, Actual values.<br>\n",
    "        `y_hat`: tensor, Predicted values.<br>\n",
    "        `mask`: tensor, Specifies date stamps per serie to consider in loss.<br>\n",
    "\n",
    "        **Returns:**<br>\n",
    "        `rmse`: tensor (single value).\n",
    "        \"\"\"\n",
    "        if mask is None: \n",
    "            mask = torch.ones_like(y_hat)\n",
    "\n",
    "        mse = (y - y_hat)**2\n",
    "        mse = mask * mse\n",
    "        mse = torch.mean(mse)\n",
    "        mse = torch.sqrt(mse)\n",
    "        return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d961d383",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(RMSE, name='RMSE.__init__', title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d398d3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(RMSE.__call__, name='RMSE.__call__', title_level=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4539e38",
   "metadata": {},
   "source": [
    "![](imgs_losses/rmse_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcf5488",
   "metadata": {},
   "source": [
    "# <span style=\"color:DarkBlue\"> 2. Percentage errors </span>\n",
    "\n",
    "These metrics are unit-free, suitable for comparisons across series."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eab97ec",
   "metadata": {},
   "source": [
    "## Mean Absolute Percentage Error (MAPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbddc1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class MAPE(torch.nn.Module):\n",
    "    \"\"\" Mean Absolute Percentage Error\n",
    "\n",
    "    Calculates Mean Absolute Percentage Error  between\n",
    "    `y` and `y_hat`. MAPE measures the relative prediction\n",
    "    accuracy of a forecasting method by calculating the percentual deviation\n",
    "    of the prediction and the observed value at a given time and\n",
    "    averages these devations over the length of the series.\n",
    "    The closer to zero an observed value is, the higher penalty MAPE loss\n",
    "    assigns to the corresponding error.\n",
    "    \n",
    "    $$ \\mathrm{MAPE}(\\\\mathbf{y}_{\\\\tau}, \\\\mathbf{\\hat{y}}_{\\\\tau}) = \\\\frac{1}{H} \\\\sum^{t+H}_{\\\\tau=t+1} \\\\frac{|y_{\\\\tau}-\\hat{y}_{\\\\tau}|}{|y_{\\\\tau}|} $$\n",
    "    \"\"\"    \n",
    "    def __init__(self):\n",
    "        super(MAPE, self).__init__()\n",
    "        self.outputsize_multiplier = 1\n",
    "        self.output_names = ['']\n",
    "        self.is_distribution_output = False\n",
    "\n",
    "    def domain_map(self, y_hat: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Univariate loss operates in dimension [B,T,H]/[B,H]\n",
    "        This changes the network's output from [B,H,1]->[B,H]\n",
    "        \"\"\"\n",
    "        return y_hat.squeeze(-1)\n",
    "\n",
    "    def __call__(self, y: torch.Tensor, y_hat: torch.Tensor, \n",
    "                 mask: Union[torch.Tensor, None] = None):\n",
    "        \"\"\"\n",
    "        **Parameters:**<br>\n",
    "        `y`: tensor, Actual values.<br>\n",
    "        `y_hat`: tensor, Predicted values.<br>\n",
    "        `mask`: tensor, Specifies date stamps per serie to consider in loss.<br>\n",
    "\n",
    "        **Returns:**<br>\n",
    "        `mape`: tensor (single value).\n",
    "        \"\"\"\n",
    "        if mask is None: \n",
    "            mask = torch.ones_like(y_hat)\n",
    "\n",
    "        mask = _divide_no_nan(mask, torch.abs(y))\n",
    "        mape = torch.abs(y - y_hat) * mask\n",
    "        mape = torch.mean(mape)\n",
    "        return mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174e8042",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(MAPE, name='MAPE.__init__', title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da63f136",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(MAPE.__call__, name='MAPE.__call__', title_level=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ccdc69",
   "metadata": {},
   "source": [
    "![](imgs_losses/mape_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb245891",
   "metadata": {},
   "source": [
    "## Symmetric MAPE (sMAPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71497ee-4485-4a17-97d9-81e324fade3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class SMAPE(torch.nn.Module):\n",
    "    \"\"\" Symmetric Mean Absolute Percentage Error\n",
    "\n",
    "    Calculates Symmetric Mean Absolute Percentage Error between\n",
    "    `y` and `y_hat`. SMAPE measures the relative prediction\n",
    "    accuracy of a forecasting method by calculating the relative deviation\n",
    "    of the prediction and the observed value scaled by the sum of the\n",
    "    absolute values for the prediction and observed value at a\n",
    "    given time, then averages these devations over the length\n",
    "    of the series. This allows the SMAPE to have bounds between\n",
    "    0% and 200% which is desireble compared to normal MAPE that\n",
    "    may be undetermined when the target is zero.\n",
    "\n",
    "    $$ \\mathrm{sMAPE}_{2}(\\\\mathbf{y}_{\\\\tau}, \\\\mathbf{\\hat{y}}_{\\\\tau}) = \\\\frac{1}{H} \\\\sum^{t+H}_{\\\\tau=t+1} \\\\frac{|y_{\\\\tau}-\\hat{y}_{\\\\tau}|}{|y_{\\\\tau}|+|\\hat{y}_{\\\\tau}|} $$\n",
    "\n",
    "    **References:**<br>\n",
    "    [Makridakis S., \"Accuracy measures: theoretical and practical concerns\".](https://www.sciencedirect.com/science/article/pii/0169207093900793)\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(SMAPE, self).__init__()\n",
    "        self.outputsize_multiplier = 1\n",
    "        self.output_names = ['']\n",
    "        self.is_distribution_output = False\n",
    "\n",
    "    def domain_map(self, y_hat: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Univariate loss operates in dimension [B,T,H]/[B,H]\n",
    "        This changes the network's output from [B,H,1]->[B,H]\n",
    "        \"\"\"\n",
    "        return y_hat.squeeze(-1)\n",
    "\n",
    "    def __call__(self, y: torch.Tensor, y_hat: torch.Tensor, \n",
    "                 mask: Union[torch.Tensor, None] = None):\n",
    "        \"\"\"\n",
    "        **Parameters:**<br>\n",
    "        `y`: tensor, Actual values.<br>\n",
    "        `y_hat`: tensor, Predicted values.<br>\n",
    "        `mask`: tensor, Specifies date stamps per serie to consider in loss.<br>\n",
    "\n",
    "        **Returns:**<br>\n",
    "        `smape`: tensor (single value).\n",
    "        \"\"\"\n",
    "        if mask is None: \n",
    "            mask = torch.ones_like(y_hat)\n",
    "\n",
    "        delta_y = torch.abs((y - y_hat))\n",
    "        scale = torch.abs(y) + torch.abs(y_hat)\n",
    "        smape = _divide_no_nan(delta_y, scale)\n",
    "        smape = smape * mask\n",
    "        smape = 2 * torch.mean(smape)\n",
    "        return smape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee99fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(SMAPE, name='SMAPE.__init__', title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db62a845",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(SMAPE.__call__, name='SMAPE.__call__', title_level=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3f2d6f",
   "metadata": {},
   "source": [
    "# <span style=\"color:DarkBlue\"> 3. Scale-independent Errors </span>\n",
    "\n",
    "These metrics measure the relative improvements versus baselines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2dee1f",
   "metadata": {},
   "source": [
    "## Mean Absolute Scaled Error (MASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7618c9-648a-41d5-9d17-dd7027a452ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class MASE(torch.nn.Module):\n",
    "    \"\"\" Mean Absolute Scaled Error \n",
    "    Calculates the Mean Absolute Scaled Error between\n",
    "    `y` and `y_hat`. MASE measures the relative prediction\n",
    "    accuracy of a forecasting method by comparinng the mean absolute errors\n",
    "    of the prediction and the observed value against the mean\n",
    "    absolute errors of the seasonal naive model.\n",
    "    The MASE partially composed the Overall Weighted Average (OWA), \n",
    "    used in the M4 Competition.\n",
    "    \n",
    "    $$ \\mathrm{MASE}(\\\\mathbf{y}_{\\\\tau}, \\\\mathbf{\\hat{y}}_{\\\\tau}, \\\\mathbf{\\hat{y}}^{season}_{\\\\tau}) = \\\\frac{1}{H} \\sum^{t+H}_{\\\\tau=t+1} \\\\frac{|y_{\\\\tau}-\\hat{y}_{\\\\tau}|}{\\mathrm{MAE}(\\\\mathbf{y}_{\\\\tau}, \\\\mathbf{\\hat{y}}^{season}_{\\\\tau})} $$\n",
    "\n",
    "    **Parameters:**<br>\n",
    "    `seasonality`: int. Main frequency of the time series; Hourly 24,  Daily 7, Weekly 52, Monthly 12, Quarterly 4, Yearly 1.\n",
    "    \n",
    "    **References:**<br>\n",
    "    [Rob J. Hyndman, & Koehler, A. B. \"Another look at measures of forecast accuracy\".](https://www.sciencedirect.com/science/article/pii/S0169207006000239)<br>\n",
    "    [Spyros Makridakis, Evangelos Spiliotis, Vassilios Assimakopoulos, \"The M4 Competition: 100,000 time series and 61 forecasting methods\".](https://www.sciencedirect.com/science/article/pii/S0169207019301128)\n",
    "    \"\"\"\n",
    "    def __init__(self, seasonality: int):\n",
    "        super(MASE, self).__init__()\n",
    "        self.outputsize_multiplier = 1\n",
    "        self.seasonality = seasonality\n",
    "        self.output_names = ['']\n",
    "        self.is_distribution_output = False\n",
    "\n",
    "    def domain_map(self, y_hat: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Univariate loss operates in dimension [B,T,H]/[B,H]\n",
    "        This changes the network's output from [B,H,1]->[B,H]\n",
    "        \"\"\"\n",
    "        return y_hat.squeeze(-1)\n",
    "\n",
    "    def __call__(self, y: torch.Tensor, y_hat: torch.Tensor,  y_insample: torch.Tensor, \n",
    "                 mask: Union[torch.Tensor, None] = None):\n",
    "        \"\"\"\n",
    "        **Parameters:**<br>\n",
    "        `y`: tensor (batch_size, output_size), Actual values.<br>\n",
    "        `y_hat`: tensor (batch_size, output_size)), Predicted values.<br>\n",
    "        `y_insample`: tensor (batch_size, input_size), Actual insample Seasonal Naive predictions.<br>\n",
    "        `mask`: tensor, Specifies date stamps per serie to consider in loss.<br>\n",
    "\n",
    "        **Returns:**<br>\n",
    "        `mase`: tensor (single value).\n",
    "        \"\"\"\n",
    "        if mask is None: \n",
    "            mask = torch.ones_like(y_hat)\n",
    "\n",
    "        delta_y = torch.abs(y - y_hat)\n",
    "        scale = torch.mean(torch.abs(y_insample[:, self.seasonality:] - \\\n",
    "                                     y_insample[:, :-self.seasonality]), axis=1)\n",
    "        mase = _divide_no_nan(delta_y, scale[:, None])\n",
    "        mase = mase * mask\n",
    "        mase = torch.mean(mase)\n",
    "        return mase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a4cf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(MASE, name='MASE.__init__', title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a2c11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(MASE.__call__, name='MASE.__call__', title_level=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0c8fe5",
   "metadata": {},
   "source": [
    "![](imgs_losses/mase_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c828438e",
   "metadata": {},
   "source": [
    "# <span style=\"color:DarkBlue\"> 4. Probabilistic Errors </span>\n",
    "\n",
    "These measure absolute deviation non-symmetrically, that produce under/over estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999d8cb2",
   "metadata": {},
   "source": [
    "## Quantile Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcf7e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class QuantileLoss(torch.nn.Module):\n",
    "    \"\"\" Quantile Loss\n",
    "\n",
    "    Computes the quantile loss between `y` and `y_hat`.\n",
    "    QL measures the deviation of a quantile forecast.\n",
    "    By weighting the absolute deviation in a non symmetric way, the\n",
    "    loss pays more attention to under or over estimation.\n",
    "    A common value for q is 0.5 for the deviation from the median (Pinball loss).\n",
    "\n",
    "    $$ \\mathrm{QL}(\\\\mathbf{y}_{\\\\tau}, \\\\mathbf{\\hat{y}}^{(q)}_{\\\\tau}) = \\\\frac{1}{H} \\\\sum^{t+H}_{\\\\tau=t+1} \\Big( (1-q)\\,( \\hat{y}^{(q)}_{\\\\tau} - y_{\\\\tau} )_{+} + q\\,( y_{\\\\tau} - \\hat{y}^{(q)}_{\\\\tau} )_{+} \\Big) $$\n",
    "\n",
    "    **Parameters:**<br>\n",
    "    `q`: float, between 0 and 1. The slope of the quantile loss, in the context of quantile regression, the q determines the conditional quantile level.<br>\n",
    "\n",
    "    **References:**<br>\n",
    "    [Roger Koenker and Gilbert Bassett, Jr., \"Regression Quantiles\".](https://www.jstor.org/stable/1913643)\n",
    "    \"\"\"\n",
    "    def __init__(self, q):\n",
    "        super(QuantileLoss, self).__init__()\n",
    "        self.outputsize_multiplier = 1\n",
    "        self.q = q\n",
    "        self.output_names = [f'_ql{q}']\n",
    "        self.is_distribution_output = False\n",
    "\n",
    "    def domain_map(self, y_hat: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Univariate loss operates in dimension [B,T,H]/[B,H]\n",
    "        This changes the network's output from [B,H,1]->[B,H]\n",
    "        \"\"\"\n",
    "        return y_hat.squeeze(-1)\n",
    "\n",
    "    def __call__(self, y: torch.Tensor, y_hat: torch.Tensor, \n",
    "                 mask: Union[torch.Tensor, None] = None):\n",
    "        \"\"\"\n",
    "        **Parameters:**<br>\n",
    "        `y`: tensor, Actual values.<br>\n",
    "        `y_hat`: tensor, Predicted values.<br>\n",
    "        `mask`: tensor, Specifies date stamps per serie to consider in loss.<br>\n",
    "\n",
    "        **Returns:**<br>\n",
    "        `quantile_loss`: tensor (single value).\n",
    "        \"\"\"\n",
    "        if mask is None: \n",
    "            mask = torch.ones_like(y_hat)\n",
    "\n",
    "        delta_y = y - y_hat\n",
    "        loss = torch.max(torch.mul(self.q, delta_y), torch.mul((self.q - 1), delta_y))\n",
    "        loss = loss * mask\n",
    "        quantile_loss = torch.mean(loss)\n",
    "        return quantile_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bd46d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(QuantileLoss, name='QuantileLoss.__init__', title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1588e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(QuantileLoss.__call__, name='QuantileLoss.__call__', title_level=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ac874f",
   "metadata": {},
   "source": [
    "![](imgs_losses/q_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92dbb002",
   "metadata": {},
   "source": [
    "## Multi Quantile Loss (MQLoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291a0530",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def level_to_outputs(level):\n",
    "    qs = sum([[50-l/2, 50+l/2] for l in level], [])\n",
    "    output_names = sum([[f'-lo-{l}', f'-hi-{l}'] for l in level], [])\n",
    "\n",
    "    sort_idx = np.argsort(qs)\n",
    "    quantiles = np.array(qs)[sort_idx]\n",
    "\n",
    "    # Add default median\n",
    "    quantiles = np.concatenate([np.array([50]), quantiles])\n",
    "    quantiles = torch.Tensor(quantiles) / 100\n",
    "    output_names = list(np.array(output_names)[sort_idx])\n",
    "    output_names.insert(0, '-median')\n",
    "    \n",
    "    return quantiles, output_names\n",
    "\n",
    "def quantiles_to_outputs(quantiles):\n",
    "    output_names = []\n",
    "    for q in quantiles:\n",
    "        if q<.50:\n",
    "            output_names.append(f'-lo-{np.round(100-200*q,2)}')\n",
    "        elif q>.50:\n",
    "            output_names.append(f'-hi-{np.round(100-200*(1-q),2)}')\n",
    "        else:\n",
    "            output_names.append('-median')\n",
    "    return quantiles, output_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3a5b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class MQLoss(torch.nn.Module):\n",
    "    \"\"\"  Multi-Quantile loss\n",
    "\n",
    "    Calculates the Multi-Quantile loss (MQL) between `y` and `y_hat`.\n",
    "    MQL calculates the average multi-quantile Loss for\n",
    "    a given set of quantiles, based on the absolute \n",
    "    difference between predicted quantiles and observed values.\n",
    "    \n",
    "    $$ \\mathrm{MQL}(\\\\mathbf{y}_{\\\\tau},[\\\\mathbf{\\hat{y}}^{(q_{1})}_{\\\\tau}, ... ,\\hat{y}^{(q_{n})}_{\\\\tau}]) = \\\\frac{1}{n} \\\\sum_{q_{i}} \\mathrm{QL}(\\\\mathbf{y}_{\\\\tau}, \\\\mathbf{\\hat{y}}^{(q_{i})}_{\\\\tau}) $$\n",
    "    \n",
    "    The limit behavior of MQL allows to measure the accuracy \n",
    "    of a full predictive distribution $\\mathbf{\\hat{F}}_{\\\\tau}$ with \n",
    "    the continuous ranked probability score (CRPS). This can be achieved \n",
    "    through a numerical integration technique, that discretizes the quantiles \n",
    "    and treats the CRPS integral with a left Riemann approximation, averaging over \n",
    "    uniformly distanced quantiles.    \n",
    "    \n",
    "    $$ \\mathrm{CRPS}(y_{\\\\tau}, \\mathbf{\\hat{F}}_{\\\\tau}) = \\int^{1}_{0} \\mathrm{QL}(y_{\\\\tau}, \\hat{y}^{(q)}_{\\\\tau}) dq $$\n",
    "\n",
    "    **Parameters:**<br>\n",
    "    `level`: int list [0,100]. Probability levels for prediction intervals (Defaults median).\n",
    "    `quantiles`: float list [0., 1.]. Alternative to level, quantiles to estimate from y distribution.\n",
    "\n",
    "    **References:**<br>\n",
    "    [Roger Koenker and Gilbert Bassett, Jr., \"Regression Quantiles\".](https://www.jstor.org/stable/1913643)<br>\n",
    "    [James E. Matheson and Robert L. Winkler, \"Scoring Rules for Continuous Probability Distributions\".](https://www.jstor.org/stable/2629907)\n",
    "    \"\"\"\n",
    "    def __init__(self, level=[80, 90], quantiles=None):\n",
    "        super(MQLoss, self).__init__()\n",
    "        # Transform level to MQLoss parameters\n",
    "        if level:\n",
    "            qs, self.output_names = level_to_outputs(level)\n",
    "            quantiles = torch.Tensor(qs)\n",
    "\n",
    "        # Transform quantiles to homogeneus output names\n",
    "        if quantiles is not None:\n",
    "            _, self.output_names = quantiles_to_outputs(quantiles)\n",
    "            quantiles = torch.Tensor(quantiles)\n",
    "\n",
    "        self.quantiles = torch.nn.Parameter(quantiles, requires_grad=False)\n",
    "        self.outputsize_multiplier = len(self.quantiles)\n",
    "        self.is_distribution_output = False\n",
    "\n",
    "    def domain_map(self, y_hat: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Identity domain map [B,T,H,Q]/[B,H,Q]\n",
    "        \"\"\"\n",
    "        return y_hat\n",
    "\n",
    "    def __call__(self, y: torch.Tensor, y_hat: torch.Tensor, \n",
    "                 mask: Union[torch.Tensor, None] = None):\n",
    "        \"\"\"\n",
    "        **Parameters:**<br>\n",
    "        `y`: tensor, Actual values.<br>\n",
    "        `y_hat`: tensor, Predicted values.<br>\n",
    "        `mask`: tensor, Specifies date stamps per serie to consider in loss.<br>\n",
    "\n",
    "        **Returns:**<br>\n",
    "        `mqloss`: tensor (single value).\n",
    "        \"\"\"\n",
    "        if mask is None: \n",
    "            mask = torch.ones_like(y_hat)\n",
    "\n",
    "        n_q = len(self.quantiles)\n",
    "        \n",
    "        error  = y_hat - y.unsqueeze(-1)\n",
    "        sq     = torch.maximum(-error, torch.zeros_like(error))\n",
    "        s1_q   = torch.maximum(error, torch.zeros_like(error))\n",
    "        mqloss = (self.quantiles * sq + (1 - self.quantiles) * s1_q)\n",
    "            \n",
    "        # Match y/weights dimensions and compute weighted average\n",
    "        mask = mask / torch.sum(mask)\n",
    "        mask = mask.unsqueeze(-1)\n",
    "        mqloss = (1/n_q) * mqloss * mask\n",
    "        return torch.sum(mqloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f42ec82",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(MQLoss, name='MQLoss.__init__', title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac2237a",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(MQLoss.__call__, name='MQLoss.__call__', title_level=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b66b0e",
   "metadata": {},
   "source": [
    "![](imgs_losses/mq_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df65d575",
   "metadata": {},
   "source": [
    "## Weighted MQLoss (wMQLoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d34c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class wMQLoss(torch.nn.Module):\n",
    "    \"\"\" Weighted Multi-Quantile loss\n",
    "    \n",
    "    Calculates the Weighted Multi-Quantile loss (WMQL) between `y` and `y_hat`.\n",
    "    WMQL calculates the weighted average multi-quantile Loss for\n",
    "    a given set of quantiles, based on the absolute \n",
    "    difference between predicted quantiles and observed values.  \n",
    "        \n",
    "    $$ \\mathrm{wMQL}(\\\\mathbf{y}_{\\\\tau},[\\\\mathbf{\\hat{y}}^{(q_{1})}_{\\\\tau}, ... ,\\hat{y}^{(q_{n})}_{\\\\tau}]) = \\\\frac{1}{n} \\\\sum_{q_{i}} \\\\frac{\\mathrm{QL}(\\\\mathbf{y}_{\\\\tau}, \\\\mathbf{\\hat{y}}^{(q_{i})}_{\\\\tau})}{\\\\sum^{t+H}_{\\\\tau=t+1} |y_{\\\\tau}|} $$\n",
    "    \n",
    "    **Parameters:**<br>\n",
    "    `level`: int list [0,100]. Probability levels for prediction intervals (Defaults median).\n",
    "    `quantiles`: float list [0., 1.]. Alternative to level, quantiles to estimate from y distribution.\n",
    "\n",
    "    **References:**<br>\n",
    "    [Roger Koenker and Gilbert Bassett, Jr., \"Regression Quantiles\".](https://www.jstor.org/stable/1913643)<br>\n",
    "    [James E. Matheson and Robert L. Winkler, \"Scoring Rules for Continuous Probability Distributions\".](https://www.jstor.org/stable/2629907)\n",
    "    \"\"\"\n",
    "    def __init__(self, level=[80, 90], quantiles=None):\n",
    "        super(wMQLoss, self).__init__()\n",
    "        # Transform level to MQLoss parameters\n",
    "        if level:\n",
    "            qs, self.output_names = level_to_outputs(level)\n",
    "            quantiles = torch.Tensor(qs)\n",
    "\n",
    "        # Transform quantiles to homogeneus output names\n",
    "        if quantiles is not None:\n",
    "            _, self.output_names = quantiles_to_outputs(quantiles)\n",
    "            quantiles = torch.Tensor(quantiles)\n",
    "\n",
    "        self.quantiles = torch.nn.Parameter(quantiles, requires_grad=False)\n",
    "        self.outputsize_multiplier = len(self.quantiles)\n",
    "        self.is_distribution_output = False\n",
    "\n",
    "    def domain_map(self, y_hat: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Identity domain map [B,T,H,Q]/[B,H,Q]\n",
    "        \"\"\"\n",
    "        return y_hat\n",
    "\n",
    "    def __call__(self, y: torch.Tensor, y_hat: torch.Tensor, \n",
    "                 mask: Union[torch.Tensor, None] = None):\n",
    "        \"\"\"\n",
    "        **Parameters:**<br>\n",
    "        `y`: tensor, Actual values.<br>\n",
    "        `y_hat`: tensor, Predicted values.<br>\n",
    "        `mask`: tensor, Specifies date stamps per serie to consider in loss.<br>\n",
    "\n",
    "        **Returns:**<br>\n",
    "        `mqloss`: tensor (single value).\n",
    "        \"\"\"\n",
    "        if mask is None: \n",
    "            mask = torch.ones_like(y_hat)\n",
    "\n",
    "        error = y_hat - y.unsqueeze(-1)\n",
    "        \n",
    "        sq = torch.maximum(-error, torch.zeros_like(error))\n",
    "        s1_q = torch.maximum(error, torch.zeros_like(error))\n",
    "        loss = (self.quantiles * sq + (1 - self.quantiles) * s1_q)\n",
    "        \n",
    "        mask = mask.unsqueeze(-1)\n",
    "        wmqloss = _divide_no_nan(torch.sum(loss * mask, axis=-2), \n",
    "                                 torch.sum(torch.abs(y.unsqueeze(-1)) * mask, axis=-2))\n",
    "        return torch.mean(wmqloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d916af",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(wMQLoss, name='wMQLoss.__init__', title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bdcbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(wMQLoss.__call__, name='wMQLoss.__call__', title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da37f2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "# check = MQLoss(level=[80, 90])\n",
    "check = MQLoss(quantiles=[0.0500, 0.1000, 0.50, 0.9000, 0.9500])\n",
    "print(check.output_names)\n",
    "print(check.quantiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895ec0c0",
   "metadata": {},
   "source": [
    "## DistributionLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801785b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def weighted_average(x: torch.Tensor, \n",
    "                     weights: Optional[torch.Tensor]=None, dim=None) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Computes the weighted average of a given tensor across a given dim, masking\n",
    "    values associated with weight zero,\n",
    "    meaning instead of `nan * 0 = nan` you will get `0 * 0 = 0`.\n",
    "\n",
    "    **Parameters:**<br>\n",
    "    `x`: Input tensor, of which the average must be computed.<br>\n",
    "    `weights`: Weights tensor, of the same shape as `x`.<br>\n",
    "    `dim`: The dim along which to average `x`.<br>\n",
    "\n",
    "    **Returns:**<br>\n",
    "    `Tensor`: The tensor with values averaged along the specified `dim`.<br>\n",
    "    \"\"\"\n",
    "    if weights is not None:\n",
    "        weighted_tensor = torch.where(\n",
    "            weights != 0, x * weights, torch.zeros_like(x)\n",
    "        )\n",
    "        sum_weights = torch.clamp(\n",
    "            weights.sum(dim=dim) if dim else weights.sum(), min=1.0\n",
    "        )\n",
    "        return (\n",
    "            weighted_tensor.sum(dim=dim) if dim else weighted_tensor.sum()\n",
    "        ) / sum_weights\n",
    "    else:\n",
    "        return x.mean(dim=dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b90c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def student_domain_map(input: torch.Tensor, eps: float=0.1):\n",
    "    \"\"\"\n",
    "    Maps input into distribution constraints, by construction input's \n",
    "    last dimension is of matching `distr_args` length.\n",
    "\n",
    "    **Parameters:**<br>\n",
    "    `input`: tensor, of dimensions [B,T,H,theta] or [B,H,theta].<br>\n",
    "    `eps`: float, helps the initialization of scale for easier optimization.<br>\n",
    "\n",
    "    **Returns:**<br>\n",
    "    `(df, loc, scale)`: tuple with tensors of StudentT distribution arguments.<br>\n",
    "    \"\"\"\n",
    "    df, loc, scale = torch.tensor_split(input, 3, dim=-1)\n",
    "    scale = F.softplus(scale) + eps\n",
    "    df = 2.0 + F.softplus(df)\n",
    "    return df.squeeze(-1), loc.squeeze(-1), scale.squeeze(-1)\n",
    "\n",
    "def normal_domain_map(input: torch.Tensor, eps: float=0.1):\n",
    "    \"\"\"\n",
    "    Maps input into distribution constraints, by construction input's \n",
    "    last dimension is of matching `distr_args` length.\n",
    "\n",
    "    **Parameters:**<br>\n",
    "    `input`: tensor, of dimensions [B,T,H,theta] or [B,H,theta].<br>\n",
    "    `eps`: float, helps the initialization of scale for easier optimization.<br>\n",
    "\n",
    "    **Returns:**<br>\n",
    "    `(loc, scale)`: tuple with tensors of Normal distribution arguments.<br>\n",
    "    \"\"\"\n",
    "    loc, scale = torch.tensor_split(input, 2, dim=-1)\n",
    "    scale = F.softplus(scale) + eps\n",
    "    return loc.squeeze(-1), scale.squeeze(-1)\n",
    "\n",
    "def poisson_domain_map(input: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Maps input into distribution constraints, by construction input's \n",
    "    last dimension is of matching `distr_args` length.\n",
    "\n",
    "    **Parameters:**<br>\n",
    "    `input`: tensor, of dimensions [B,T,H,theta] or [B,H,theta].<br>\n",
    "\n",
    "    **Returns:**<br>\n",
    "    `(loc,)`: tuple with tensors of Poisson distribution arguments.<br>\n",
    "    \"\"\"\n",
    "    rate_pos = F.softplus(input).clone()\n",
    "    return (rate_pos.squeeze(-1),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5931f6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class DistributionLoss(torch.nn.Module):\n",
    "    \"\"\" DistributionLoss\n",
    "\n",
    "    This PyTorch module wraps the `torch.distribution` classes allowing it to \n",
    "    interact with NeuralForecast models modularly. It shares the negative \n",
    "    log-likelihood as the optimization objective and a sample method to \n",
    "    generate empirically the quantiles defined by the `level` list.\n",
    "\n",
    "    Additionally, it implements a distribution transformation that factorizes the\n",
    "    scale-dependent likelihood parameters into a base scale and a multiplier \n",
    "    efficiently learnable within the network's non-linearities operating ranges.\n",
    "\n",
    "    Available distributions:\n",
    "    - Poisson\n",
    "    - Normal\n",
    "    - StudentT\n",
    "\n",
    "    **Parameters:**<br>\n",
    "    `distribution`: str, identifier of a torch.distributions.Distribution class.<br>\n",
    "    `level`: float list [0,100], confidence levels for prediction intervals.<br>\n",
    "    `quantiles`: float list [0,1], alternative to level list, target quantiles.<br>\n",
    "    `num_samples`: int=500, number of samples for the empirical quantiles.<br>\n",
    "    `return_params`: bool=False, wether or not return the Distribution parameters.<br><br>\n",
    "\n",
    "    **References:**<br>\n",
    "    - [PyTorch Probability Distributions Package: StudentT.](https://pytorch.org/docs/stable/distributions.html#studentt)<br>\n",
    "    - [David Salinas, Valentin Flunkert, Jan Gasthaus, Tim Januschowski (2020).\n",
    "       \"DeepAR: Probabilistic forecasting with autoregressive recurrent networks\". International Journal of Forecasting.](https://www.sciencedirect.com/science/article/pii/S0169207019301888)<br>\n",
    "    \"\"\"\n",
    "    def __init__(self, distribution, level=[80, 90], quantiles=None,\n",
    "                 num_samples=500, return_params=False):\n",
    "        super(DistributionLoss, self).__init__()\n",
    "\n",
    "        available_distributions = dict(Normal=Normal,\n",
    "                                       Poisson=Poisson,\n",
    "                                       StudentT=StudentT,)\n",
    "        domain_maps = dict(Normal=normal_domain_map,\n",
    "                           Poisson=poisson_domain_map,\n",
    "                           StudentT=student_domain_map,)\n",
    "        param_names = dict(Normal=[\"-loc\", \"-scale\"],\n",
    "                           Poisson=[\"-loc\"],\n",
    "                           StudentT=[\"-df\", \"-loc\", \"-scale\"],)\n",
    "        assert (distribution in available_distributions.keys()), f'{distribution} not available'\n",
    "\n",
    "        self._base_distribution = available_distributions[distribution]\n",
    "        self.domain_map = domain_maps[distribution]\n",
    "        self.param_names = param_names[distribution]\n",
    "\n",
    "        if level:\n",
    "            qs, self.output_names = level_to_outputs(level)\n",
    "            quantiles = torch.Tensor(qs)\n",
    "\n",
    "        # Transform quantiles to homogeneus output names\n",
    "        if quantiles is not None:\n",
    "            _, self.output_names = quantiles_to_outputs(quantiles)\n",
    "            quantiles = torch.Tensor(quantiles)\n",
    "        self.quantiles = torch.nn.Parameter(quantiles, requires_grad=False)\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "        # If True, predict_step will return Distribution's parameters\n",
    "        self.return_params = return_params\n",
    "        if self.return_params:\n",
    "            self.output_names = self.output_names + self.param_names\n",
    "\n",
    "        self.outputsize_multiplier = len(self._base_distribution.arg_constraints.keys())\n",
    "        self.is_distribution_output = True\n",
    "\n",
    "    def get_distribution(self,\n",
    "                         distr_args,\n",
    "                         loc: Optional[torch.Tensor] = None,\n",
    "                         scale: Optional[torch.Tensor] = None,) -> Distribution:\n",
    "        \"\"\"\n",
    "        Construct the associated Pytorch Distribution, given the collection of\n",
    "        constructor arguments and, optionally, location and scale tensors.\n",
    "\n",
    "        **Parameters**<br>\n",
    "        `distr_args`: Constructor arguments for the underlying Distribution type.<br>\n",
    "        `loc`: Optional tensor, of the same shape as the batch_shape + event_shape\n",
    "               of the resulting distribution.<br>\n",
    "        `scale`: Optional tensor, of the same shape as the batch_shape+event_shape \n",
    "               of the resulting distribution.<br>\n",
    "\n",
    "        **Returns**<br>\n",
    "        `Distribution`: AffineTransformed distribution.<br>\n",
    "        \"\"\"\n",
    "        # TODO: domain_map output dictionary rather instead of ordered tuple\n",
    "        # to improve instantiation of the base distributions.\n",
    "        distr = self._base_distribution(*distr_args)\n",
    "        if loc is None and scale is None:\n",
    "            return distr\n",
    "        else:\n",
    "            return TransformedDistribution(distr, [AffineTransform(loc=loc, scale=scale)])\n",
    "\n",
    "    def sample(self,\n",
    "               distr_args: torch.Tensor,\n",
    "               loc: torch.Tensor,\n",
    "               scale: torch.Tensor,\n",
    "               num_samples: Optional[int] = None):\n",
    "        \"\"\"\n",
    "        Construct the empirical quantiles from the estimated Distribution,\n",
    "        sampling from it `num_samples` independently.\n",
    "\n",
    "        **Parameters**<br>\n",
    "        `distr_args`: Constructor arguments for the underlying Distribution type.<br>\n",
    "        `loc`: Optional tensor, of the same shape as the batch_shape + event_shape\n",
    "               of the resulting distribution.<br>\n",
    "        `scale`: Optional tensor, of the same shape as the batch_shape+event_shape \n",
    "               of the resulting distribution.<br>\n",
    "        `num_samples`: int=500, overwrite number of samples for the empirical quantiles.<br>\n",
    "\n",
    "        **Returns**<br>\n",
    "        `samples`: tensor, shape [B,H,`num_samples`].<br>\n",
    "        `quantiles`: tensor, empirical quantiles defined by `levels`.<br>\n",
    "        \"\"\"\n",
    "        if num_samples is None:\n",
    "            num_samples = self.num_samples\n",
    "\n",
    "        B, H = distr_args[0].size()\n",
    "        Q = len(self.quantiles)\n",
    "\n",
    "        # Construct AffineTransformed Distribution to\n",
    "        # sample y ~ loc + Distribution(distr_args) * scale independently\n",
    "        distr = self.get_distribution(distr_args=distr_args, loc=loc, scale=scale)\n",
    "        samples = distr.sample(sample_shape=(num_samples,))\n",
    "        samples = samples.permute(1,2,0) # [samples,B,H] -> [B,H,samples]\n",
    "        samples = samples.to(distr_args[0].device)\n",
    "        samples = samples.view(B*H, num_samples)\n",
    "\n",
    "        # Compute quantiles\n",
    "        quantiles_device = self.quantiles.to(distr_args[0].device)\n",
    "        quants = torch.quantile(input=samples, \n",
    "                                q=quantiles_device, dim=1)\n",
    "        quants = quants.permute((1,0)) # [Q, B*H] -> [B*H, Q]\n",
    "\n",
    "        # Final reshapes\n",
    "        samples = samples.view(B, H, num_samples)\n",
    "        quants  = quants.view(B, H, Q)\n",
    "        return samples, quants\n",
    "\n",
    "    def __call__(self,\n",
    "                 y: torch.Tensor,\n",
    "                 distr_args: torch.Tensor,\n",
    "                 loc: torch.Tensor,\n",
    "                 scale: torch.Tensor,\n",
    "                 mask: Union[torch.Tensor, None] = None):\n",
    "        \"\"\"\n",
    "        Computes the negative log-likelihood objective function. \n",
    "        To estimate the following predictive distribution:\n",
    "\n",
    "        $$\\mathrm{P}(\\mathbf{y}_{\\\\tau}\\,|\\,\\\\theta) \\\\quad \\mathrm{and} \\\\quad -\\log(\\mathrm{P}(\\mathbf{y}_{\\\\tau}\\,|\\,\\\\theta))$$\n",
    "\n",
    "        where $\\\\theta$ represents the distributions parameters. It aditionally \n",
    "        summarizes the objective signal using a weighted average using the `mask` tensor. \n",
    "\n",
    "        **Parameters**<br>\n",
    "        `y`: tensor, Actual values.<br>\n",
    "        `distr_args`: Constructor arguments for the underlying Distribution type.<br>\n",
    "        `loc`: Optional tensor, of the same shape as the batch_shape + event_shape\n",
    "               of the resulting distribution.<br>\n",
    "        `scale`: Optional tensor, of the same shape as the batch_shape+event_shape \n",
    "               of the resulting distribution.<br>\n",
    "        `mask`: tensor, Specifies date stamps per serie to consider in loss.<br>\n",
    "\n",
    "        **Returns**<br>\n",
    "        `loss`: scalar, weighted loss function against which backpropagation will be performed.<br>\n",
    "        \"\"\"\n",
    "        # Construct AffineTransformed Distribution\n",
    "        distr = self.get_distribution(distr_args=distr_args, loc=loc, scale=scale)\n",
    "        loss_values = -distr.log_prob(y)\n",
    "        loss_weights = mask\n",
    "        return weighted_average(loss_values, weights=loss_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a462101b",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(DistributionLoss, name='DistributionLoss.__init__', title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c367f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(DistributionLoss.sample, name='DistributionLoss.sample', title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e32679",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(DistributionLoss.__call__, name='DistributionLoss.__call__', title_level=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f459b8",
   "metadata": {},
   "source": [
    "## Poisson Mixture Mesh (PMM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ec688f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class PMM(torch.nn.Module):\n",
    "    \"\"\" Poisson Mixture Mesh\n",
    "\n",
    "    This Poisson Mixture statistical model assumes independence across groups of \n",
    "    data $\\mathcal{G}=\\{[g_{i}]\\}$, and estimates relationships within the group.\n",
    "\n",
    "    $$ \\mathrm{P}\\\\left(\\mathbf{y}_{[b][t+1:t+H]}\\\\right) = \n",
    "    \\prod_{ [g_{i}] \\in \\mathcal{G}} \\mathrm{P} \\\\left(\\mathbf{y}_{[g_{i}][\\\\tau]} \\\\right) =\n",
    "    \\prod_{\\\\beta\\in[g_{i}]} \n",
    "    \\\\left(\\sum_{k=1}^{K} w_k \\prod_{(\\\\beta,\\\\tau) \\in [g_i][t+1:t+H]} \\mathrm{Poisson}(y_{\\\\beta,\\\\tau}, \\hat{\\\\lambda}_{\\\\beta,\\\\tau,k}) \\\\right)$$\n",
    "\n",
    "    **Parameters:**<br>\n",
    "    `n_components`: int=10, the number of mixture components.<br>\n",
    "    `level`: float list [0,100], confidence levels for prediction intervals.<br>\n",
    "    `quantiles`: float list [0,1], alternative to level list, target quantiles.<br>\n",
    "    `return_params`: bool=False, wether or not return the Distribution parameters.<br><br>\n",
    "\n",
    "    **References:**<br>\n",
    "    [Kin G. Olivares, O. Nganba Meetei, Ruijun Ma, Rohan Reddy, Mengfei Cao, Lee Dicker. \n",
    "    Probabilistic Hierarchical Forecasting with Deep Poisson Mixtures. Submitted to the International \n",
    "    Journal Forecasting, Working paper available at arxiv.](https://arxiv.org/pdf/2110.13179.pdf)\n",
    "    \"\"\"\n",
    "    def __init__(self, n_components=10, level=[80, 90], quantiles=None, \n",
    "                 num_samples=500, return_params=False):\n",
    "        super(PMM, self).__init__()\n",
    "        # Transform level to MQLoss parameters\n",
    "        if level:\n",
    "            qs, self.output_names = level_to_outputs(level)\n",
    "            quantiles = torch.Tensor(qs)\n",
    "\n",
    "        # Transform quantiles to homogeneus output names\n",
    "        if quantiles is not None:\n",
    "            _, self.output_names = quantiles_to_outputs(quantiles)\n",
    "            quantiles = torch.Tensor(quantiles)\n",
    "        self.quantiles = torch.nn.Parameter(quantiles, requires_grad=False)\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "        # If True, predict_step will return Distribution's parameters\n",
    "        self.return_params = return_params\n",
    "        if self.return_params:\n",
    "            self.param_names = [f\"-lambda-{i}\" for i in range(1, n_components + 1)]\n",
    "            self.output_names = self.output_names + self.param_names\n",
    "\n",
    "        self.outputsize_multiplier = n_components\n",
    "        self.is_distribution_output = True\n",
    "\n",
    "    def domain_map(self, lambdas_hat: torch.Tensor):\n",
    "        lambdas_hat = F.softplus(lambdas_hat)\n",
    "        return (lambdas_hat,)#, weights\n",
    "\n",
    "    def sample(self, distr_args, loc=None, scale=None, num_samples=None):\n",
    "        \"\"\"\n",
    "        Construct the empirical quantiles from the estimated Distribution,\n",
    "        sampling from it `num_samples` independently.\n",
    "\n",
    "        **Parameters**<br>\n",
    "        `distr_args`: Constructor arguments for the underlying Distribution type.<br>\n",
    "        `loc`: Optional tensor, of the same shape as the batch_shape + event_shape\n",
    "               of the resulting distribution.<br>\n",
    "        `scale`: Optional tensor, of the same shape as the batch_shape+event_shape \n",
    "               of the resulting distribution.<br>\n",
    "        `num_samples`: int=500, overwrites number of samples for the empirical quantiles.<br>\n",
    "\n",
    "        **Returns**<br>\n",
    "        `samples`: tensor, shape [B,H,`num_samples`].<br>\n",
    "        `quantiles`: tensor, empirical quantiles defined by `levels`.<br>\n",
    "        \"\"\"\n",
    "        if num_samples is None:\n",
    "            num_samples = self.num_samples\n",
    "\n",
    "        lambdas = distr_args[0]\n",
    "        B, H, K = lambdas.size()\n",
    "        Q = len(self.quantiles)\n",
    "\n",
    "        # Sample K ~ Mult(weights)\n",
    "        # shared across B, H\n",
    "        # weights = torch.repeat_interleave(input=weights, repeats=H, dim=2)\n",
    "        weights = (1/K) * torch.ones_like(lambdas).to(lambdas.device)\n",
    "\n",
    "        # Avoid loop, vectorize\n",
    "        weights = weights.reshape(-1, K)\n",
    "        lambdas = lambdas.flatten()        \n",
    "\n",
    "        # Vectorization trick to recover row_idx\n",
    "        sample_idxs = torch.multinomial(input=weights, \n",
    "                                        num_samples=num_samples,\n",
    "                                        replacement=True)\n",
    "        aux_col_idx = torch.unsqueeze(torch.arange(B*H),-1) * K\n",
    "\n",
    "        # To device\n",
    "        sample_idxs = sample_idxs.to(lambdas.device)\n",
    "        aux_col_idx = aux_col_idx.to(lambdas.device)\n",
    "\n",
    "        sample_idxs = sample_idxs + aux_col_idx\n",
    "        sample_idxs = sample_idxs.flatten()\n",
    "\n",
    "        sample_lambdas = lambdas[sample_idxs]\n",
    "\n",
    "        # Sample y ~ Poisson(lambda) independently\n",
    "        samples = torch.poisson(sample_lambdas).to(lambdas.device)\n",
    "        samples = samples.view(B*H, num_samples)\n",
    "\n",
    "        # Compute quantiles\n",
    "        quantiles_device = self.quantiles.to(lambdas.device)\n",
    "        quants = torch.quantile(input=samples, q=quantiles_device, dim=1)\n",
    "        quants = quants.permute((1,0)) # Q, B*H\n",
    "\n",
    "        # Final reshapes\n",
    "        samples = samples.view(B, H, num_samples)\n",
    "        quants  = quants.view(B, H, Q)\n",
    "\n",
    "        return samples, quants        \n",
    "    \n",
    "    def neglog_likelihood(self,\n",
    "                          y: torch.Tensor,\n",
    "                          distr_args: Tuple[torch.Tensor],\n",
    "                          mask: Union[torch.Tensor, None] = None,\n",
    "                          loc: Union[torch.Tensor, None] = None,\n",
    "                          scale: Union[torch.Tensor, None] = None):\n",
    "        if mask is None: \n",
    "            mask = torch.ones_like(y)\n",
    "\n",
    "        eps  = 1e-10\n",
    "        lambdas = distr_args[0]\n",
    "        B, H, K = lambdas.size()\n",
    "\n",
    "        lambdas = distr_args[0]\n",
    "        weights = (1/K) * torch.ones_like(lambdas).to(lambdas.device)\n",
    "\n",
    "        y = y[:,:,None]\n",
    "        mask = mask[:,:,None]\n",
    "        \n",
    "        log = y * torch.log(lambdas + eps) - lambdas\\\n",
    "              - ( (y) * torch.log(y + eps) - y )   # Stirling's Factorial\n",
    "\n",
    "        #log  = torch.sum(log, dim=0, keepdim=True) # Joint within batch/group\n",
    "        #log  = torch.sum(log, dim=1, keepdim=True) # Joint within horizon\n",
    "        \n",
    "        # Numerical stability mixture and loglik\n",
    "        log_max = torch.amax(log, dim=2, keepdim=True) # [1,1,K] (collapsed joints)\n",
    "        lik     = weights * torch.exp(log-log_max)     # Take max\n",
    "        loglik  = torch.log(torch.sum(lik, dim=2, keepdim=True)) + log_max # Return max\n",
    "        loglik  = loglik * mask #replace with mask\n",
    "        \n",
    "        loss = -torch.mean(loglik)\n",
    "        return loss\n",
    "    \n",
    "    def __call__(self, y: torch.Tensor,\n",
    "                 distr_args: Tuple[torch.Tensor],\n",
    "                 mask: Union[torch.Tensor, None] = None,\n",
    "                 loc: Union[torch.Tensor, None] = None,\n",
    "                 scale: Union[torch.Tensor, None] = None):\n",
    "\n",
    "        return self.neglog_likelihood(y=y, distr_args=distr_args, mask=mask,\n",
    "                                      loc=loc, scale=scale)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d7daba",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(PMM, name='PMM.__init__', title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8da65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(PMM.sample, name='PMM.sample', title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba75717c",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(PMM.__call__, name='PMM.__call__', title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba6a127",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# Create single mixture and broadcast to N,H,K\n",
    "weights = torch.ones((1,3))[None, :, :]\n",
    "lambdas = torch.Tensor([[5,10,15], [10,20,30]])[None, :, :]\n",
    "\n",
    "# Create repetitions for the batch dimension N.\n",
    "N=2\n",
    "weights = torch.repeat_interleave(input=weights, repeats=N, dim=0)\n",
    "lambdas = torch.repeat_interleave(input=lambdas, repeats=N, dim=0)\n",
    "\n",
    "print('weights.shape (N,H,K) \\t', weights.shape)\n",
    "print('lambdas.shape (N,H,K) \\t', lambdas.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56a2fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "model = PMM(quantiles=[0.1, 0.40, 0.5, 0.60, 0.9])\n",
    "distr_args = (lambdas,)\n",
    "samples, quants = model.sample(distr_args)\n",
    "\n",
    "print('samples.shape (N,H,num_samples) ', samples.shape)\n",
    "print('quants.shape  (N,H,Q) \\t\\t', quants.shape)\n",
    "\n",
    "# Plot synthethic data\n",
    "x_plot = range(quants.shape[1]) # H length\n",
    "y_plot_hat = quants[0,:,:]  # Filter N,G,T -> H,Q\n",
    "samples_hat = samples[0,:,:]  # Filter N,G,T -> H,num_samples\n",
    "\n",
    "# Kernel density plot for single forecast horizon \\tau = t+1\n",
    "fig, ax = plt.subplots(figsize=(3.7, 2.9))\n",
    "\n",
    "ax.hist(samples_hat[0,:], alpha=0.5, label=r'Horizon $\\tau+1$')\n",
    "ax.hist(samples_hat[1,:], alpha=0.5, label=r'Horizon $\\tau+2$')\n",
    "ax.set(xlabel='Y values', ylabel='Probability')\n",
    "plt.title('Single horizon Distributions')\n",
    "plt.legend(bbox_to_anchor=(1, 1), loc='upper left', ncol=1)\n",
    "plt.grid()\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Plot simulated trajectory\n",
    "fig, ax = plt.subplots(figsize=(3.7, 2.9))\n",
    "plt.plot(x_plot, y_plot_hat[:,2], color='black', label='median [q50]')\n",
    "plt.fill_between(x_plot,\n",
    "                 y1=y_plot_hat[:,1], y2=y_plot_hat[:,3],\n",
    "                 facecolor='blue', alpha=0.4, label='[p25-p75]')\n",
    "plt.fill_between(x_plot,\n",
    "                 y1=y_plot_hat[:,0], y2=y_plot_hat[:,4],\n",
    "                 facecolor='blue', alpha=0.2, label='[p1-p99]')\n",
    "ax.set(xlabel='Horizon', ylabel='Y values')\n",
    "plt.title('PMM Probabilistic Predictions')\n",
    "plt.legend(bbox_to_anchor=(1, 1), loc='upper left', ncol=1)\n",
    "plt.grid()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84e0dd4",
   "metadata": {},
   "source": [
    "## Gaussian Mixture Mesh (GMM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6928b0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class GMM(torch.nn.Module):\n",
    "    \"\"\" Gaussian Mixture Mesh\n",
    "\n",
    "    This Gaussian Mixture statistical model assumes independence across groups of \n",
    "    data $\\mathcal{G}=\\{[g_{i}]\\}$, and estimates relationships within the group.\n",
    "\n",
    "    $$ \\mathrm{P}\\\\left(\\mathbf{y}_{[b][t+1:t+H]}\\\\right) = \n",
    "    \\prod_{ [g_{i}] \\in \\mathcal{G}} \\mathrm{P}\\left(\\mathbf{y}_{[g_{i}][\\\\tau]}\\\\right)=\n",
    "    \\prod_{\\\\beta\\in[g_{i}]}\n",
    "    \\\\left(\\sum_{k=1}^{K} w_k \\prod_{(\\\\beta,\\\\tau) \\in [g_i][t+1:t+H]} \n",
    "    \\mathrm{Gaussian}(y_{\\\\beta,\\\\tau}, \\hat{\\mu}_{\\\\beta,\\\\tau,k}, \\sigma_{\\\\beta,\\\\tau,k})\\\\right)$$\n",
    "\n",
    "    **Parameters:**<br>\n",
    "    `n_components`: int=10, the number of mixture components.<br>\n",
    "    `level`: float list [0,100], confidence levels for prediction intervals.<br>\n",
    "    `quantiles`: float list [0,1], alternative to level list, target quantiles.<br>\n",
    "    `return_params`: bool=False, wether or not return the Distribution parameters.<br><br>\n",
    "\n",
    "    **References:**<br>\n",
    "    [Kin G. Olivares, O. Nganba Meetei, Ruijun Ma, Rohan Reddy, Mengfei Cao, Lee Dicker. \n",
    "    Probabilistic Hierarchical Forecasting with Deep Poisson Mixtures. Submitted to the International \n",
    "    Journal Forecasting, Working paper available at arxiv.](https://arxiv.org/pdf/2110.13179.pdf)\n",
    "    \"\"\"\n",
    "    def __init__(self, n_components=1, level=[80, 90], quantiles=None, \n",
    "                 num_samples=500, return_params=False):\n",
    "        super(GMM, self).__init__()\n",
    "        # Transform level to MQLoss parameters\n",
    "        if level:\n",
    "            qs, self.output_names = level_to_outputs(level)\n",
    "            quantiles = torch.Tensor(qs)\n",
    "\n",
    "        # Transform quantiles to homogeneus output names\n",
    "        if quantiles is not None:\n",
    "            _, self.output_names = quantiles_to_outputs(quantiles)\n",
    "            quantiles = torch.Tensor(quantiles)\n",
    "        self.quantiles = torch.nn.Parameter(quantiles, requires_grad=False)\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "        # If True, predict_step will return Distribution's parameters\n",
    "        self.return_params = return_params\n",
    "        if self.return_params:\n",
    "            self.param_names = [f\"-lambda-{i}\" for i in range(1, n_components + 1)]\n",
    "            self.output_names = self.output_names + self.param_names        \n",
    "\n",
    "        self.outputsize_multiplier = n_components\n",
    "        self.is_distribution_output = True\n",
    "\n",
    "    def sample(self, weights, means, stds, num_samples=None):\n",
    "        \"\"\"\n",
    "        Construct the empirical quantiles from the estimated Distribution,\n",
    "        sampling from it `num_samples` independently.\n",
    "\n",
    "        **Parameters**<br>\n",
    "        `distr_args`: Constructor arguments for the underlying Distribution type.<br>\n",
    "        `loc`: Optional tensor, of the same shape as the batch_shape + event_shape\n",
    "               of the resulting distribution.<br>\n",
    "        `scale`: Optional tensor, of the same shape as the batch_shape+event_shape \n",
    "               of the resulting distribution.<br>\n",
    "        `num_samples`: int=500, number of samples for the empirical quantiles.<br>\n",
    "\n",
    "        **Returns**<br>\n",
    "        `samples`: tensor, shape [B,H,`num_samples`].<br>\n",
    "        `quantiles`: tensor, empirical quantiles defined by `levels`.<br>\n",
    "        \"\"\"\n",
    "        if num_samples is None:\n",
    "            num_samples = self.num_samples\n",
    "        \n",
    "        B, H, K = means.size()\n",
    "        Q = len(self.quantiles)\n",
    "        assert means.shape == stds.shape\n",
    "\n",
    "        # Sample K ~ Mult(weights)\n",
    "        # shared across B, H\n",
    "        # weights = torch.repeat_interleave(input=weights, repeats=H, dim=2)\n",
    "\n",
    "        # Avoid loop, vectorize\n",
    "        weights = weights.reshape(-1, K)\n",
    "        means = means.flatten()\n",
    "        stds = stds.flatten()\n",
    "\n",
    "        # Vectorization trick to recover row_idx\n",
    "        sample_idxs = torch.multinomial(input=weights, \n",
    "                                        num_samples=num_samples,\n",
    "                                        replacement=True)\n",
    "        aux_col_idx = torch.unsqueeze(torch.arange(B*H),-1) * K\n",
    "\n",
    "        # To device\n",
    "        sample_idxs = sample_idxs.to(means.device)\n",
    "        aux_col_idx = aux_col_idx.to(means.device)\n",
    "\n",
    "        sample_idxs = sample_idxs + aux_col_idx\n",
    "        sample_idxs = sample_idxs.flatten()\n",
    "\n",
    "        sample_means = means[sample_idxs]\n",
    "        sample_stds  = stds[sample_idxs]\n",
    "\n",
    "        # Sample y ~ Normal(mu, std) independently\n",
    "        samples = torch.normal(sample_means, sample_stds).to(means.device)\n",
    "        samples = samples.view(B*H, num_samples)\n",
    "\n",
    "        # Compute quantiles\n",
    "        quantiles_device = self.quantiles.to(means.device)\n",
    "        quants = torch.quantile(input=samples, q=quantiles_device, dim=1)\n",
    "        quants = quants.permute((1,0)) # Q, B*H\n",
    "\n",
    "        # Final reshapes\n",
    "        samples = samples.view(B, H, num_samples)\n",
    "        quants  = quants.view(B, H, Q)\n",
    "\n",
    "        return samples, quants        \n",
    "    \n",
    "    def neglog_likelihood(self,\n",
    "                          y: torch.Tensor,\n",
    "                          weights: torch.Tensor,\n",
    "                          means: torch.Tensor,\n",
    "                          stds: torch.Tensor,\n",
    "                          mask: Union[torch.Tensor, None] = None):\n",
    "\n",
    "        if mask is None: \n",
    "            mask = torch.ones_like(means)\n",
    "\n",
    "        B, H, K = means.size()\n",
    "        # eps  = 1e-10\n",
    "        \n",
    "        log  = -0.5 * ((1/stds)*(y - means))**2\\\n",
    "                - torch.log(((2*math.pi)**(0.5)) * stds)\n",
    "\n",
    "        #log  = torch.sum(log, dim=0, keepdim=True) # Joint within batch/group\n",
    "        #log  = torch.sum(log, dim=1, keepdim=True) # Joint within horizon\n",
    "\n",
    "        # Numerical stability mixture and loglik\n",
    "        log_max = torch.amax(log, dim=2, keepdim=True) # [1,1,K] (collapsed joints)\n",
    "        lik     = weights * torch.exp(log-log_max)     # Take max\n",
    "        loglik  = torch.log(torch.sum(lik, dim=2, keepdim=True)) + log_max # Return max\n",
    "        \n",
    "        loglik  = loglik * mask #replace with mask\n",
    "\n",
    "        loss = -torch.mean(loglik)\n",
    "        return loss\n",
    "    \n",
    "    def __call__(self, y: torch.Tensor,\n",
    "                 weights: torch.Tensor,\n",
    "                 means: torch.Tensor,\n",
    "                 stds: torch.Tensor,\n",
    "                 mask: Union[torch.Tensor, None] = None):\n",
    "\n",
    "        return self.neglog_likelihood(y=y, weights=weights,\n",
    "                                 means=means, stds=stds, mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4ebf3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(GMM, name='GMM.__init__', title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea56d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(GMM.sample, name='GMM.sample', title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f16e4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(GMM.__call__, name='GMM.__call__', title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5493e651",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# Create single mixture and broadcast to N,H,K\n",
    "means   = torch.Tensor([[5,10,15], [10,20,30]])[None, :, :]\n",
    "\n",
    "# # Create repetitions for the batch dimension N.\n",
    "N=2\n",
    "means = torch.repeat_interleave(input=means, repeats=N, dim=0)\n",
    "weights = torch.ones_like(means)\n",
    "stds  = torch.ones_like(means)\n",
    "\n",
    "print('weights.shape (N,H,K) \\t', weights.shape)\n",
    "print('means.shape (N,H,K) \\t', means.shape)\n",
    "print('stds.shape (N,H,K) \\t', stds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684d2382",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "model = GMM(quantiles=[0.1, 0.40, 0.5, 0.60, 0.9])\n",
    "distr_args = (lambdas, weights, stds)\n",
    "samples, quants = model.sample(weights=weights,\n",
    "                               means=means, stds=stds)\n",
    "\n",
    "print('samples.shape (N,H,num_samples) ', samples.shape)\n",
    "print('quants.shape  (N,H,Q) \\t\\t', quants.shape)\n",
    "\n",
    "# Plot synthethic data\n",
    "x_plot = range(quants.shape[1]) # H length\n",
    "y_plot_hat = quants[0,:,:]  # Filter N,G,T -> H,Q\n",
    "samples_hat = samples[0,:,:]  # Filter N,G,T -> H,num_samples\n",
    "\n",
    "# Kernel density plot for single forecast horizon \\tau = t+1\n",
    "fig, ax = plt.subplots(figsize=(3.7, 2.9))\n",
    "\n",
    "ax.hist(samples_hat[0,:], alpha=0.5, bins=50,\n",
    "        label=r'Horizon $\\tau+1$')\n",
    "ax.hist(samples_hat[1,:], alpha=0.5, bins=50,\n",
    "        label=r'Horizon $\\tau+2$')\n",
    "ax.set(xlabel='Y values', ylabel='Probability')\n",
    "plt.title('Single horizon Distributions')\n",
    "plt.legend(bbox_to_anchor=(1, 1), loc='upper left', ncol=1)\n",
    "plt.grid()\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Plot simulated trajectory\n",
    "fig, ax = plt.subplots(figsize=(3.7, 2.9))\n",
    "plt.plot(x_plot, y_plot_hat[:,2], color='black', label='median [q50]')\n",
    "plt.fill_between(x_plot,\n",
    "                 y1=y_plot_hat[:,1], y2=y_plot_hat[:,3],\n",
    "                 facecolor='blue', alpha=0.4, label='[p25-p75]')\n",
    "plt.fill_between(x_plot,\n",
    "                 y1=y_plot_hat[:,0], y2=y_plot_hat[:,4],\n",
    "                 facecolor='blue', alpha=0.2, label='[p1-p99]')\n",
    "ax.set(xlabel='Horizon', ylabel='Y values')\n",
    "plt.title('PMM Probabilistic Predictions')\n",
    "plt.legend(bbox_to_anchor=(1, 1), loc='upper left', ncol=1)\n",
    "plt.grid()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdbe5c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('neuralforecast')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
