{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524620c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp losses.pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15392f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd532cb1-d11d-468e-a0e5-eb1101ba6662",
   "metadata": {},
   "source": [
    "# PyTorch Losses\n",
    "\n",
    "> NeuralForecast contains a collection PyTorch Loss classes aimed to be used during the models' optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096cfbec-1d59-454a-b572-5890103b2f1f",
   "metadata": {},
   "source": [
    "The most important train signal is the forecast error, which is the difference between the observed value $y_{\\tau}$ and the prediction $\\hat{y}_{\\tau}$, at time $y_{\\tau}$:\n",
    "\n",
    "$$e_{\\tau} = y_{\\tau}-\\hat{y}_{\\tau} \\qquad \\qquad \\tau \\in \\{t+1,\\dots,t+H \\}$$\n",
    "\n",
    "The train loss summarizes the forecast errors in different train optimization objectives.\n",
    "\n",
    "All the losses are `torch.nn.modules` which helps to automatically moved them across CPU/GPU/TPU devices with Pytorch Lightning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfa68dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from typing import Optional, Union, Tuple, List\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Distribution\n",
    "from torch.distributions import (\n",
    "    Bernoulli,\n",
    "    Normal, \n",
    "    StudentT, \n",
    "    Poisson,\n",
    "    NegativeBinomial,\n",
    "    Beta,\n",
    "    Gamma,\n",
    "    MixtureSameFamily,\n",
    "    Categorical,\n",
    "    AffineTransform, \n",
    "    TransformedDistribution,\n",
    ")\n",
    "\n",
    "from torch.distributions import constraints\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2508f7a9-1433-4ad8-8f2f-0078c6ed6c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ospra\\miniconda3\\envs\\neuralforecast-backup\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-04-01 17:20:21,171\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2025-04-01 17:20:21,397\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "import matplotlib.pyplot as plt\n",
    "from fastcore.test import test_eq\n",
    "from nbdev.showdoc import show_doc\n",
    "from neuralforecast.utils import generate_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e07e98-b4c8-4ade-b3b6-1d27f367aa0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def _divide_no_nan(a: torch.Tensor, b: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Auxiliary funtion to handle divide by 0\n",
    "    \"\"\"\n",
    "    div = a / b\n",
    "    return torch.nan_to_num(div, nan=0.0, posinf=0.0, neginf=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132db0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def _weighted_mean(losses, weights):\n",
    "    \"\"\"\n",
    "    Compute weighted mean of losses per datapoint.\n",
    "    \"\"\"\n",
    "    return _divide_no_nan(torch.sum(losses * weights), torch.sum(weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41562a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BasePointLoss(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Base class for point loss functions.\n",
    "\n",
    "    **Parameters:**<br>\n",
    "    `horizon_weight`: Tensor of size h, weight for each timestamp of the forecasting window. <br>\n",
    "    `outputsize_multiplier`: Multiplier for the output size. <br>\n",
    "    `output_names`: Names of the outputs. <br>\n",
    "    \"\"\"\n",
    "    def __init__(self, horizon_weight=None, outputsize_multiplier=None, output_names=None):\n",
    "        super(BasePointLoss, self).__init__()\n",
    "        if horizon_weight is not None:\n",
    "            horizon_weight = torch.Tensor(horizon_weight.flatten())\n",
    "        self.horizon_weight = horizon_weight\n",
    "        self.outputsize_multiplier = outputsize_multiplier\n",
    "        self.output_names = output_names\n",
    "        self.is_distribution_output = False\n",
    "\n",
    "    def domain_map(self, y_hat: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "        Univariate: [B, H, 1]\n",
    "        Multivariate: [B, H, N]\n",
    "\n",
    "        Output: [B, H, N]\n",
    "        \"\"\"\n",
    "        return y_hat\n",
    "\n",
    "    def _compute_weights(self, y, mask):\n",
    "        \"\"\"\n",
    "        Compute final weights for each datapoint (based on all weights and all masks)\n",
    "        Set horizon_weight to a ones[H] tensor if not set.\n",
    "        If set, check that it has the same length as the horizon in x.\n",
    "        \"\"\"\n",
    "        if mask is None:\n",
    "            mask = torch.ones_like(y)\n",
    "\n",
    "        if self.horizon_weight is None:\n",
    "            weights = torch.ones_like(mask)\n",
    "        else:\n",
    "            assert mask.shape[1] == len(self.horizon_weight), \\\n",
    "                'horizon_weight must have same length as Y'\n",
    "            weights = self.horizon_weight.clone()\n",
    "            weights = weights[None, :, None].to(mask.device)\n",
    "            weights = torch.ones_like(mask, device=mask.device) * weights\n",
    "        \n",
    "        return weights * mask"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b8a94d7d",
   "metadata": {},
   "source": [
    "# 1. Scale-dependent Errors\n",
    "\n",
    "These metrics are on the same scale as the data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "82fc4679",
   "metadata": {},
   "source": [
    "## Mean Absolute Error (MAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e413fae-c590-4713-aab9-37c61ed37dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class MAE(BasePointLoss):\n",
    "    \"\"\"Mean Absolute Error\n",
    "\n",
    "    Calculates Mean Absolute Error between\n",
    "    `y` and `y_hat`. MAE measures the relative prediction\n",
    "    accuracy of a forecasting method by calculating the\n",
    "    deviation of the prediction and the true\n",
    "    value at a given time and averages these devations\n",
    "    over the length of the series.\n",
    "\n",
    "    $$ \\mathrm{MAE}(\\\\mathbf{y}_{\\\\tau}, \\\\mathbf{\\hat{y}}_{\\\\tau}) = \\\\frac{1}{H} \\\\sum^{t+H}_{\\\\tau=t+1} |y_{\\\\tau} - \\hat{y}_{\\\\tau}| $$\n",
    "\n",
    "    **Parameters:**<br>\n",
    "    `horizon_weight`: Tensor of size h, weight for each timestamp of the forecasting window. <br>\n",
    "    \"\"\"    \n",
    "    def __init__(self, horizon_weight=None):\n",
    "        super(MAE, self).__init__(horizon_weight=horizon_weight,\n",
    "                                  outputsize_multiplier=1,\n",
    "                                  output_names=[''])\n",
    "\n",
    "    def __call__(self,\n",
    "                 y: torch.Tensor,\n",
    "                 y_hat: torch.Tensor,\n",
    "                 mask: Union[torch.Tensor, None] = None,\n",
    "                 y_insample: Union[torch.Tensor, None] = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        **Parameters:**<br>\n",
    "        `y`: tensor, Actual values.<br>\n",
    "        `y_hat`: tensor, Predicted values.<br>\n",
    "        `mask`: tensor, Specifies datapoints to consider in loss.<br>\n",
    "\n",
    "        **Returns:**<br>\n",
    "        `mae`: tensor (single value).\n",
    "        \"\"\"\n",
    "        losses = torch.abs(y - y_hat)\n",
    "        weights = self._compute_weights(y=y, mask=mask)\n",
    "        return _weighted_mean(losses=losses, weights=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d004cd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/losses/pytorch.py#L95){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### MAE.__init__\n",
       "\n",
       ">      MAE.__init__ (horizon_weight=None)\n",
       "\n",
       "*Mean Absolute Error\n",
       "\n",
       "Calculates Mean Absolute Error between\n",
       "`y` and `y_hat`. MAE measures the relative prediction\n",
       "accuracy of a forecasting method by calculating the\n",
       "deviation of the prediction and the true\n",
       "value at a given time and averages these devations\n",
       "over the length of the series.\n",
       "\n",
       "$$ \\mathrm{MAE}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}_{\\tau}) = \\frac{1}{H} \\sum^{t+H}_{\\tau=t+1} |y_{\\tau} - \\hat{y}_{\\tau}| $$\n",
       "\n",
       "**Parameters:**<br>\n",
       "`horizon_weight`: Tensor of size h, weight for each timestamp of the forecasting window. <br>*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/losses/pytorch.py#L95){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### MAE.__init__\n",
       "\n",
       ">      MAE.__init__ (horizon_weight=None)\n",
       "\n",
       "*Mean Absolute Error\n",
       "\n",
       "Calculates Mean Absolute Error between\n",
       "`y` and `y_hat`. MAE measures the relative prediction\n",
       "accuracy of a forecasting method by calculating the\n",
       "deviation of the prediction and the true\n",
       "value at a given time and averages these devations\n",
       "over the length of the series.\n",
       "\n",
       "$$ \\mathrm{MAE}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}_{\\tau}) = \\frac{1}{H} \\sum^{t+H}_{\\tau=t+1} |y_{\\tau} - \\hat{y}_{\\tau}| $$\n",
       "\n",
       "**Parameters:**<br>\n",
       "`horizon_weight`: Tensor of size h, weight for each timestamp of the forecasting window. <br>*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(MAE, name='MAE.__init__', title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a20a273",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/losses/pytorch.py#L116){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### MAE.__call__\n",
       "\n",
       ">      MAE.__call__ (y:torch.Tensor, y_hat:torch.Tensor,\n",
       ">                    mask:Optional[torch.Tensor]=None,\n",
       ">                    y_insample:Optional[torch.Tensor]=None)\n",
       "\n",
       "***Parameters:**<br>\n",
       "`y`: tensor, Actual values.<br>\n",
       "`y_hat`: tensor, Predicted values.<br>\n",
       "`mask`: tensor, Specifies datapoints to consider in loss.<br>\n",
       "\n",
       "**Returns:**<br>\n",
       "`mae`: tensor (single value).*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/losses/pytorch.py#L116){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### MAE.__call__\n",
       "\n",
       ">      MAE.__call__ (y:torch.Tensor, y_hat:torch.Tensor,\n",
       ">                    mask:Optional[torch.Tensor]=None,\n",
       ">                    y_insample:Optional[torch.Tensor]=None)\n",
       "\n",
       "***Parameters:**<br>\n",
       "`y`: tensor, Actual values.<br>\n",
       "`y_hat`: tensor, Predicted values.<br>\n",
       "`mask`: tensor, Specifies datapoints to consider in loss.<br>\n",
       "\n",
       "**Returns:**<br>\n",
       "`mae`: tensor (single value).*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(MAE.__call__, name='MAE.__call__', title_level=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0292c74d",
   "metadata": {},
   "source": [
    "![](imgs_losses/mae_loss.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4f31cc3d",
   "metadata": {},
   "source": [
    "## Mean Squared Error (MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cfe937",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class MSE(BasePointLoss):\n",
    "    \"\"\"  Mean Squared Error\n",
    "\n",
    "    Calculates Mean Squared Error between\n",
    "    `y` and `y_hat`. MSE measures the relative prediction\n",
    "    accuracy of a forecasting method by calculating the \n",
    "    squared deviation of the prediction and the true\n",
    "    value at a given time, and averages these devations\n",
    "    over the length of the series.\n",
    "    \n",
    "    $$ \\mathrm{MSE}(\\\\mathbf{y}_{\\\\tau}, \\\\mathbf{\\hat{y}}_{\\\\tau}) = \\\\frac{1}{H} \\\\sum^{t+H}_{\\\\tau=t+1} (y_{\\\\tau} - \\hat{y}_{\\\\tau})^{2} $$\n",
    "\n",
    "    **Parameters:**<br>\n",
    "    `horizon_weight`: Tensor of size h, weight for each timestamp of the forecasting window. <br>\n",
    "    \"\"\"\n",
    "    def __init__(self, horizon_weight=None):\n",
    "        super(MSE, self).__init__(horizon_weight=horizon_weight,\n",
    "                                  outputsize_multiplier=1,\n",
    "                                  output_names=[''])\n",
    "\n",
    "    def __call__(self,\n",
    "                 y: torch.Tensor,\n",
    "                 y_hat: torch.Tensor,\n",
    "                 y_insample: torch.Tensor,\n",
    "                 mask: Union[torch.Tensor, None] = None,\n",
    "                 ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        **Parameters:**<br>\n",
    "        `y`: tensor, Actual values.<br>\n",
    "        `y_hat`: tensor, Predicted values.<br>\n",
    "        `mask`: tensor, Specifies datapoints to consider in loss.<br>\n",
    "\n",
    "        **Returns:**<br>\n",
    "        `mse`: tensor (single value).\n",
    "        \"\"\"\n",
    "        losses = (y - y_hat)**2\n",
    "        weights = self._compute_weights(y=y, mask=mask)\n",
    "        return _weighted_mean(losses=losses, weights=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c65b82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/losses/pytorch.py#L136){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### MSE.__init__\n",
       "\n",
       ">      MSE.__init__ (horizon_weight=None)\n",
       "\n",
       "*Mean Squared Error\n",
       "\n",
       "Calculates Mean Squared Error between\n",
       "`y` and `y_hat`. MSE measures the relative prediction\n",
       "accuracy of a forecasting method by calculating the \n",
       "squared deviation of the prediction and the true\n",
       "value at a given time, and averages these devations\n",
       "over the length of the series.\n",
       "\n",
       "$$ \\mathrm{MSE}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}_{\\tau}) = \\frac{1}{H} \\sum^{t+H}_{\\tau=t+1} (y_{\\tau} - \\hat{y}_{\\tau})^{2} $$\n",
       "\n",
       "**Parameters:**<br>\n",
       "`horizon_weight`: Tensor of size h, weight for each timestamp of the forecasting window. <br>*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/losses/pytorch.py#L136){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### MSE.__init__\n",
       "\n",
       ">      MSE.__init__ (horizon_weight=None)\n",
       "\n",
       "*Mean Squared Error\n",
       "\n",
       "Calculates Mean Squared Error between\n",
       "`y` and `y_hat`. MSE measures the relative prediction\n",
       "accuracy of a forecasting method by calculating the \n",
       "squared deviation of the prediction and the true\n",
       "value at a given time, and averages these devations\n",
       "over the length of the series.\n",
       "\n",
       "$$ \\mathrm{MSE}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}_{\\tau}) = \\frac{1}{H} \\sum^{t+H}_{\\tau=t+1} (y_{\\tau} - \\hat{y}_{\\tau})^{2} $$\n",
       "\n",
       "**Parameters:**<br>\n",
       "`horizon_weight`: Tensor of size h, weight for each timestamp of the forecasting window. <br>*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(MSE, name='MSE.__init__', title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0126a7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/losses/pytorch.py#L157){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### MSE.__call__\n",
       "\n",
       ">      MSE.__call__ (y:torch.Tensor, y_hat:torch.Tensor,\n",
       ">                    y_insample:torch.Tensor, mask:Optional[torch.Tensor]=None)\n",
       "\n",
       "***Parameters:**<br>\n",
       "`y`: tensor, Actual values.<br>\n",
       "`y_hat`: tensor, Predicted values.<br>\n",
       "`mask`: tensor, Specifies datapoints to consider in loss.<br>\n",
       "\n",
       "**Returns:**<br>\n",
       "`mse`: tensor (single value).*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/losses/pytorch.py#L157){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### MSE.__call__\n",
       "\n",
       ">      MSE.__call__ (y:torch.Tensor, y_hat:torch.Tensor,\n",
       ">                    y_insample:torch.Tensor, mask:Optional[torch.Tensor]=None)\n",
       "\n",
       "***Parameters:**<br>\n",
       "`y`: tensor, Actual values.<br>\n",
       "`y_hat`: tensor, Predicted values.<br>\n",
       "`mask`: tensor, Specifies datapoints to consider in loss.<br>\n",
       "\n",
       "**Returns:**<br>\n",
       "`mse`: tensor (single value).*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(MSE.__call__, name='MSE.__call__', title_level=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7b23f9c1",
   "metadata": {},
   "source": [
    "![](imgs_losses/mse_loss.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b160140b",
   "metadata": {},
   "source": [
    "## Root Mean Squared Error (RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545ebfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class RMSE(BasePointLoss):\n",
    "    \"\"\" Root Mean Squared Error\n",
    "\n",
    "    Calculates Root Mean Squared Error between\n",
    "    `y` and `y_hat`. RMSE measures the relative prediction\n",
    "    accuracy of a forecasting method by calculating the squared deviation\n",
    "    of the prediction and the observed value at a given time and\n",
    "    averages these devations over the length of the series.\n",
    "    Finally the RMSE will be in the same scale\n",
    "    as the original time series so its comparison with other\n",
    "    series is possible only if they share a common scale. \n",
    "    RMSE has a direct connection to the L2 norm.\n",
    "    \n",
    "    $$ \\mathrm{RMSE}(\\\\mathbf{y}_{\\\\tau}, \\\\mathbf{\\hat{y}}_{\\\\tau}) = \\\\sqrt{\\\\frac{1}{H} \\\\sum^{t+H}_{\\\\tau=t+1} (y_{\\\\tau} - \\hat{y}_{\\\\tau})^{2}} $$\n",
    "\n",
    "    **Parameters:**<br>\n",
    "    `horizon_weight`: Tensor of size h, weight for each timestamp of the forecasting window. <br>\n",
    "    \"\"\"\n",
    "    def __init__(self, horizon_weight=None):\n",
    "        super(RMSE, self).__init__(horizon_weight=horizon_weight,\n",
    "                                  outputsize_multiplier=1,\n",
    "                                  output_names=[''])\n",
    "\n",
    "    def __call__(self,\n",
    "                 y: torch.Tensor,\n",
    "                 y_hat: torch.Tensor,\n",
    "                 mask: Union[torch.Tensor, None] = None,\n",
    "                 y_insample: Union[torch.Tensor, None] = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        **Parameters:**<br>\n",
    "        `y`: tensor, Actual values.<br>\n",
    "        `y_hat`: tensor, Predicted values.<br>\n",
    "        `mask`: tensor, Specifies datapoints to consider in loss.<br>\n",
    "\n",
    "        **Returns:**<br>\n",
    "        `rmse`: tensor (single value).\n",
    "        \"\"\"\n",
    "        losses = (y - y_hat)**2\n",
    "        weights = self._compute_weights(y=y, mask=mask)\n",
    "        losses = _weighted_mean(losses=losses, weights=weights)\n",
    "        return torch.sqrt(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d961d383",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/losses/pytorch.py#L177){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### RMSE.__init__\n",
       "\n",
       ">      RMSE.__init__ (horizon_weight=None)\n",
       "\n",
       "*Root Mean Squared Error\n",
       "\n",
       "Calculates Root Mean Squared Error between\n",
       "`y` and `y_hat`. RMSE measures the relative prediction\n",
       "accuracy of a forecasting method by calculating the squared deviation\n",
       "of the prediction and the observed value at a given time and\n",
       "averages these devations over the length of the series.\n",
       "Finally the RMSE will be in the same scale\n",
       "as the original time series so its comparison with other\n",
       "series is possible only if they share a common scale. \n",
       "RMSE has a direct connection to the L2 norm.\n",
       "\n",
       "$$ \\mathrm{RMSE}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}_{\\tau}) = \\sqrt{\\frac{1}{H} \\sum^{t+H}_{\\tau=t+1} (y_{\\tau} - \\hat{y}_{\\tau})^{2}} $$\n",
       "\n",
       "**Parameters:**<br>\n",
       "`horizon_weight`: Tensor of size h, weight for each timestamp of the forecasting window. <br>*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/losses/pytorch.py#L177){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### RMSE.__init__\n",
       "\n",
       ">      RMSE.__init__ (horizon_weight=None)\n",
       "\n",
       "*Root Mean Squared Error\n",
       "\n",
       "Calculates Root Mean Squared Error between\n",
       "`y` and `y_hat`. RMSE measures the relative prediction\n",
       "accuracy of a forecasting method by calculating the squared deviation\n",
       "of the prediction and the observed value at a given time and\n",
       "averages these devations over the length of the series.\n",
       "Finally the RMSE will be in the same scale\n",
       "as the original time series so its comparison with other\n",
       "series is possible only if they share a common scale. \n",
       "RMSE has a direct connection to the L2 norm.\n",
       "\n",
       "$$ \\mathrm{RMSE}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}_{\\tau}) = \\sqrt{\\frac{1}{H} \\sum^{t+H}_{\\tau=t+1} (y_{\\tau} - \\hat{y}_{\\tau})^{2}} $$\n",
       "\n",
       "**Parameters:**<br>\n",
       "`horizon_weight`: Tensor of size h, weight for each timestamp of the forecasting window. <br>*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(RMSE, name='RMSE.__init__', title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d398d3e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/losses/pytorch.py#L201){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### RMSE.__call__\n",
       "\n",
       ">      RMSE.__call__ (y:torch.Tensor, y_hat:torch.Tensor,\n",
       ">                     mask:Optional[torch.Tensor]=None,\n",
       ">                     y_insample:Optional[torch.Tensor]=None)\n",
       "\n",
       "***Parameters:**<br>\n",
       "`y`: tensor, Actual values.<br>\n",
       "`y_hat`: tensor, Predicted values.<br>\n",
       "`mask`: tensor, Specifies datapoints to consider in loss.<br>\n",
       "\n",
       "**Returns:**<br>\n",
       "`rmse`: tensor (single value).*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/losses/pytorch.py#L201){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### RMSE.__call__\n",
       "\n",
       ">      RMSE.__call__ (y:torch.Tensor, y_hat:torch.Tensor,\n",
       ">                     mask:Optional[torch.Tensor]=None,\n",
       ">                     y_insample:Optional[torch.Tensor]=None)\n",
       "\n",
       "***Parameters:**<br>\n",
       "`y`: tensor, Actual values.<br>\n",
       "`y_hat`: tensor, Predicted values.<br>\n",
       "`mask`: tensor, Specifies datapoints to consider in loss.<br>\n",
       "\n",
       "**Returns:**<br>\n",
       "`rmse`: tensor (single value).*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(RMSE.__call__, name='RMSE.__call__', title_level=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d4539e38",
   "metadata": {},
   "source": [
    "![](imgs_losses/rmse_loss.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8bcf5488",
   "metadata": {},
   "source": [
    "# 2. Percentage errors\n",
    "\n",
    "These metrics are unit-free, suitable for comparisons across series."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8eab97ec",
   "metadata": {},
   "source": [
    "## Mean Absolute Percentage Error (MAPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adecb6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class MAPE(BasePointLoss):\n",
    "    \"\"\" Mean Absolute Percentage Error\n",
    "\n",
    "    Calculates Mean Absolute Percentage Error  between\n",
    "    `y` and `y_hat`. MAPE measures the relative prediction\n",
    "    accuracy of a forecasting method by calculating the percentual deviation\n",
    "    of the prediction and the observed value at a given time and\n",
    "    averages these devations over the length of the series.\n",
    "    The closer to zero an observed value is, the higher penalty MAPE loss\n",
    "    assigns to the corresponding error.\n",
    "\n",
    "    $$ \\mathrm{MAPE}(\\\\mathbf{y}_{\\\\tau}, \\\\mathbf{\\hat{y}}_{\\\\tau}) = \\\\frac{1}{H} \\\\sum^{t+H}_{\\\\tau=t+1} \\\\frac{|y_{\\\\tau}-\\hat{y}_{\\\\tau}|}{|y_{\\\\tau}|} $$\n",
    "\n",
    "    **Parameters:**<br>\n",
    "    `horizon_weight`: Tensor of size h, weight for each timestamp of the forecasting window. <br>\n",
    "\n",
    "    **References:**<br>\n",
    "    [Makridakis S., \"Accuracy measures: theoretical and practical concerns\".](https://www.sciencedirect.com/science/article/pii/0169207093900793)    \n",
    "    \"\"\"\n",
    "    def __init__(self, horizon_weight=None):\n",
    "        super(MAPE, self).__init__(horizon_weight=horizon_weight,\n",
    "                                  outputsize_multiplier=1,\n",
    "                                  output_names=[''])\n",
    "\n",
    "    def __call__(self,\n",
    "                 y: torch.Tensor,\n",
    "                 y_hat: torch.Tensor,\n",
    "                 y_insample: torch.Tensor,\n",
    "                 mask: Union[torch.Tensor, None] = None,\n",
    "                 ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        **Parameters:**<br>\n",
    "        `y`: tensor, Actual values.<br>\n",
    "        `y_hat`: tensor, Predicted values.<br>\n",
    "        `mask`: tensor, Specifies date stamps per serie to consider in loss.<br>\n",
    "\n",
    "        **Returns:**<br>\n",
    "        `mape`: tensor (single value).\n",
    "        \"\"\"\n",
    "        scale = _divide_no_nan(torch.ones_like(y, device=y.device), torch.abs(y))\n",
    "        losses = torch.abs(y - y_hat) * scale\n",
    "        weights = self._compute_weights(y=y, mask=mask)\n",
    "        mape = _weighted_mean(losses=losses, weights=weights)\n",
    "        return mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174e8042",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/losses/pytorch.py#L222){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### MAPE.__init__\n",
       "\n",
       ">      MAPE.__init__ (horizon_weight=None)\n",
       "\n",
       "*Mean Absolute Percentage Error\n",
       "\n",
       "Calculates Mean Absolute Percentage Error  between\n",
       "`y` and `y_hat`. MAPE measures the relative prediction\n",
       "accuracy of a forecasting method by calculating the percentual deviation\n",
       "of the prediction and the observed value at a given time and\n",
       "averages these devations over the length of the series.\n",
       "The closer to zero an observed value is, the higher penalty MAPE loss\n",
       "assigns to the corresponding error.\n",
       "\n",
       "$$ \\mathrm{MAPE}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}_{\\tau}) = \\frac{1}{H} \\sum^{t+H}_{\\tau=t+1} \\frac{|y_{\\tau}-\\hat{y}_{\\tau}|}{|y_{\\tau}|} $$\n",
       "\n",
       "**Parameters:**<br>\n",
       "`horizon_weight`: Tensor of size h, weight for each timestamp of the forecasting window. <br>\n",
       "\n",
       "**References:**<br>\n",
       "[Makridakis S., \"Accuracy measures: theoretical and practical concerns\".](https://www.sciencedirect.com/science/article/pii/0169207093900793)*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/losses/pytorch.py#L222){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### MAPE.__init__\n",
       "\n",
       ">      MAPE.__init__ (horizon_weight=None)\n",
       "\n",
       "*Mean Absolute Percentage Error\n",
       "\n",
       "Calculates Mean Absolute Percentage Error  between\n",
       "`y` and `y_hat`. MAPE measures the relative prediction\n",
       "accuracy of a forecasting method by calculating the percentual deviation\n",
       "of the prediction and the observed value at a given time and\n",
       "averages these devations over the length of the series.\n",
       "The closer to zero an observed value is, the higher penalty MAPE loss\n",
       "assigns to the corresponding error.\n",
       "\n",
       "$$ \\mathrm{MAPE}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}_{\\tau}) = \\frac{1}{H} \\sum^{t+H}_{\\tau=t+1} \\frac{|y_{\\tau}-\\hat{y}_{\\tau}|}{|y_{\\tau}|} $$\n",
       "\n",
       "**Parameters:**<br>\n",
       "`horizon_weight`: Tensor of size h, weight for each timestamp of the forecasting window. <br>\n",
       "\n",
       "**References:**<br>\n",
       "[Makridakis S., \"Accuracy measures: theoretical and practical concerns\".](https://www.sciencedirect.com/science/article/pii/0169207093900793)*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(MAPE, name='MAPE.__init__', title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da63f136",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/losses/pytorch.py#L247){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### MAPE.__call__\n",
       "\n",
       ">      MAPE.__call__ (y:torch.Tensor, y_hat:torch.Tensor,\n",
       ">                     y_insample:torch.Tensor, mask:Optional[torch.Tensor]=None)\n",
       "\n",
       "***Parameters:**<br>\n",
       "`y`: tensor, Actual values.<br>\n",
       "`y_hat`: tensor, Predicted values.<br>\n",
       "`mask`: tensor, Specifies date stamps per serie to consider in loss.<br>\n",
       "\n",
       "**Returns:**<br>\n",
       "`mape`: tensor (single value).*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/losses/pytorch.py#L247){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### MAPE.__call__\n",
       "\n",
       ">      MAPE.__call__ (y:torch.Tensor, y_hat:torch.Tensor,\n",
       ">                     y_insample:torch.Tensor, mask:Optional[torch.Tensor]=None)\n",
       "\n",
       "***Parameters:**<br>\n",
       "`y`: tensor, Actual values.<br>\n",
       "`y_hat`: tensor, Predicted values.<br>\n",
       "`mask`: tensor, Specifies date stamps per serie to consider in loss.<br>\n",
       "\n",
       "**Returns:**<br>\n",
       "`mape`: tensor (single value).*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(MAPE.__call__, name='MAPE.__call__', title_level=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c8ccdc69",
   "metadata": {},
   "source": [
    "![](imgs_losses/mape_loss.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cb245891",
   "metadata": {},
   "source": [
    "## Symmetric MAPE (sMAPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7566e649",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class SMAPE(BasePointLoss):\n",
    "    \"\"\" Symmetric Mean Absolute Percentage Error\n",
    "\n",
    "    Calculates Symmetric Mean Absolute Percentage Error between\n",
    "    `y` and `y_hat`. SMAPE measures the relative prediction\n",
    "    accuracy of a forecasting method by calculating the relative deviation\n",
    "    of the prediction and the observed value scaled by the sum of the\n",
    "    absolute values for the prediction and observed value at a\n",
    "    given time, then averages these devations over the length\n",
    "    of the series. This allows the SMAPE to have bounds between\n",
    "    0% and 200% which is desireble compared to normal MAPE that\n",
    "    may be undetermined when the target is zero.\n",
    "\n",
    "    $$ \\mathrm{sMAPE}_{2}(\\\\mathbf{y}_{\\\\tau}, \\\\mathbf{\\hat{y}}_{\\\\tau}) = \\\\frac{1}{H} \\\\sum^{t+H}_{\\\\tau=t+1} \\\\frac{|y_{\\\\tau}-\\hat{y}_{\\\\tau}|}{|y_{\\\\tau}|+|\\hat{y}_{\\\\tau}|} $$\n",
    "\n",
    "    **Parameters:**<br>\n",
    "    `horizon_weight`: Tensor of size h, weight for each timestamp of the forecasting window. <br>\n",
    "\n",
    "    **References:**<br>\n",
    "    [Makridakis S., \"Accuracy measures: theoretical and practical concerns\".](https://www.sciencedirect.com/science/article/pii/0169207093900793)\n",
    "    \"\"\"\n",
    "    def __init__(self, horizon_weight=None):\n",
    "        super(SMAPE, self).__init__(horizon_weight=horizon_weight,\n",
    "                                  outputsize_multiplier=1,\n",
    "                                  output_names=[''])\n",
    "\n",
    "    def __call__(self,\n",
    "                 y: torch.Tensor,\n",
    "                 y_hat: torch.Tensor,\n",
    "                 mask: Union[torch.Tensor, None] = None,\n",
    "                 y_insample: Union[torch.Tensor, None] = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        **Parameters:**<br>\n",
    "        `y`: tensor, Actual values.<br>\n",
    "        `y_hat`: tensor, Predicted values.<br>\n",
    "        `mask`: tensor, Specifies date stamps per serie to consider in loss.<br>\n",
    "\n",
    "        **Returns:**<br>\n",
    "        `smape`: tensor (single value).\n",
    "        \"\"\"\n",
    "        delta_y = torch.abs((y - y_hat))\n",
    "        scale = torch.abs(y) + torch.abs(y_hat)\n",
    "        losses = _divide_no_nan(delta_y, scale)\n",
    "        weights = self._compute_weights(y=y, mask=mask)\n",
    "        return 2*_weighted_mean(losses=losses, weights=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee99fb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/losses/pytorch.py#L269){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### SMAPE.__init__\n",
       "\n",
       ">      SMAPE.__init__ (horizon_weight=None)\n",
       "\n",
       "*Symmetric Mean Absolute Percentage Error\n",
       "\n",
       "Calculates Symmetric Mean Absolute Percentage Error between\n",
       "`y` and `y_hat`. SMAPE measures the relative prediction\n",
       "accuracy of a forecasting method by calculating the relative deviation\n",
       "of the prediction and the observed value scaled by the sum of the\n",
       "absolute values for the prediction and observed value at a\n",
       "given time, then averages these devations over the length\n",
       "of the series. This allows the SMAPE to have bounds between\n",
       "0% and 200% which is desireble compared to normal MAPE that\n",
       "may be undetermined when the target is zero.\n",
       "\n",
       "$$ \\mathrm{sMAPE}_{2}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}_{\\tau}) = \\frac{1}{H} \\sum^{t+H}_{\\tau=t+1} \\frac{|y_{\\tau}-\\hat{y}_{\\tau}|}{|y_{\\tau}|+|\\hat{y}_{\\tau}|} $$\n",
       "\n",
       "**Parameters:**<br>\n",
       "`horizon_weight`: Tensor of size h, weight for each timestamp of the forecasting window. <br>\n",
       "\n",
       "**References:**<br>\n",
       "[Makridakis S., \"Accuracy measures: theoretical and practical concerns\".](https://www.sciencedirect.com/science/article/pii/0169207093900793)*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/losses/pytorch.py#L269){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### SMAPE.__init__\n",
       "\n",
       ">      SMAPE.__init__ (horizon_weight=None)\n",
       "\n",
       "*Symmetric Mean Absolute Percentage Error\n",
       "\n",
       "Calculates Symmetric Mean Absolute Percentage Error between\n",
       "`y` and `y_hat`. SMAPE measures the relative prediction\n",
       "accuracy of a forecasting method by calculating the relative deviation\n",
       "of the prediction and the observed value scaled by the sum of the\n",
       "absolute values for the prediction and observed value at a\n",
       "given time, then averages these devations over the length\n",
       "of the series. This allows the SMAPE to have bounds between\n",
       "0% and 200% which is desireble compared to normal MAPE that\n",
       "may be undetermined when the target is zero.\n",
       "\n",
       "$$ \\mathrm{sMAPE}_{2}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}_{\\tau}) = \\frac{1}{H} \\sum^{t+H}_{\\tau=t+1} \\frac{|y_{\\tau}-\\hat{y}_{\\tau}|}{|y_{\\tau}|+|\\hat{y}_{\\tau}|} $$\n",
       "\n",
       "**Parameters:**<br>\n",
       "`horizon_weight`: Tensor of size h, weight for each timestamp of the forecasting window. <br>\n",
       "\n",
       "**References:**<br>\n",
       "[Makridakis S., \"Accuracy measures: theoretical and practical concerns\".](https://www.sciencedirect.com/science/article/pii/0169207093900793)*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(SMAPE, name='SMAPE.__init__', title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db62a845",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/losses/pytorch.py#L296){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### SMAPE.__call__\n",
       "\n",
       ">      SMAPE.__call__ (y:torch.Tensor, y_hat:torch.Tensor,\n",
       ">                      mask:Optional[torch.Tensor]=None,\n",
       ">                      y_insample:Optional[torch.Tensor]=None)\n",
       "\n",
       "***Parameters:**<br>\n",
       "`y`: tensor, Actual values.<br>\n",
       "`y_hat`: tensor, Predicted values.<br>\n",
       "`mask`: tensor, Specifies date stamps per serie to consider in loss.<br>\n",
       "\n",
       "**Returns:**<br>\n",
       "`smape`: tensor (single value).*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/losses/pytorch.py#L296){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### SMAPE.__call__\n",
       "\n",
       ">      SMAPE.__call__ (y:torch.Tensor, y_hat:torch.Tensor,\n",
       ">                      mask:Optional[torch.Tensor]=None,\n",
       ">                      y_insample:Optional[torch.Tensor]=None)\n",
       "\n",
       "***Parameters:**<br>\n",
       "`y`: tensor, Actual values.<br>\n",
       "`y_hat`: tensor, Predicted values.<br>\n",
       "`mask`: tensor, Specifies date stamps per serie to consider in loss.<br>\n",
       "\n",
       "**Returns:**<br>\n",
       "`smape`: tensor (single value).*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(SMAPE.__call__, name='SMAPE.__call__', title_level=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bc3f2d6f",
   "metadata": {},
   "source": [
    "# 3. Scale-independent Errors\n",
    "\n",
    "These metrics measure the relative improvements versus baselines."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5b2dee1f",
   "metadata": {},
   "source": [
    "## Mean Absolute Scaled Error (MASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc34fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class MASE(BasePointLoss):\n",
    "    \"\"\" Mean Absolute Scaled Error \n",
    "    Calculates the Mean Absolute Scaled Error between\n",
    "    `y` and `y_hat`. MASE measures the relative prediction\n",
    "    accuracy of a forecasting method by comparinng the mean absolute errors\n",
    "    of the prediction and the observed value against the mean\n",
    "    absolute errors of the seasonal naive model.\n",
    "    The MASE partially composed the Overall Weighted Average (OWA), \n",
    "    used in the M4 Competition.\n",
    "    \n",
    "    $$ \\mathrm{MASE}(\\\\mathbf{y}_{\\\\tau}, \\\\mathbf{\\hat{y}}_{\\\\tau}, \\\\mathbf{\\hat{y}}^{season}_{\\\\tau}) = \\\\frac{1}{H} \\sum^{t+H}_{\\\\tau=t+1} \\\\frac{|y_{\\\\tau}-\\hat{y}_{\\\\tau}|}{\\mathrm{MAE}(\\\\mathbf{y}_{\\\\tau}, \\\\mathbf{\\hat{y}}^{season}_{\\\\tau})} $$\n",
    "\n",
    "    **Parameters:**<br>\n",
    "    `seasonality`: int. Main frequency of the time series; Hourly 24,  Daily 7, Weekly 52, Monthly 12, Quarterly 4, Yearly 1.\n",
    "    `horizon_weight`: Tensor of size h, weight for each timestamp of the forecasting window. <br>\n",
    "    \n",
    "    **References:**<br>\n",
    "    [Rob J. Hyndman, & Koehler, A. B. \"Another look at measures of forecast accuracy\".](https://www.sciencedirect.com/science/article/pii/S0169207006000239)<br>\n",
    "    [Spyros Makridakis, Evangelos Spiliotis, Vassilios Assimakopoulos, \"The M4 Competition: 100,000 time series and 61 forecasting methods\".](https://www.sciencedirect.com/science/article/pii/S0169207019301128)\n",
    "    \"\"\"\n",
    "    def __init__(self, seasonality: int, horizon_weight=None):\n",
    "        super(MASE, self).__init__(horizon_weight=horizon_weight,\n",
    "                                   outputsize_multiplier=1,\n",
    "                                   output_names=[''])\n",
    "        self.seasonality = seasonality\n",
    "\n",
    "    def __call__(self,\n",
    "                 y: torch.Tensor,\n",
    "                 y_hat: torch.Tensor,\n",
    "                 y_insample: torch.Tensor,\n",
    "                 mask: Union[torch.Tensor, None] = None,\n",
    "                 ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        **Parameters:**<br>\n",
    "        `y`: tensor (batch_size, output_size), Actual values.<br>\n",
    "        `y_hat`: tensor (batch_size, output_size)), Predicted values.<br>\n",
    "        `y_insample`: tensor (batch_size, input_size), Actual insample values.<br>\n",
    "        `mask`: tensor, Specifies date stamps per serie to consider in loss.<br>\n",
    "\n",
    "        **Returns:**<br>\n",
    "        `mase`: tensor (single value).\n",
    "        \"\"\"\n",
    "        delta_y = torch.abs(y - y_hat)\n",
    "        scale = torch.mean(torch.abs(y_insample[:, self.seasonality:] - \\\n",
    "                                     y_insample[:, :-self.seasonality]), axis=1)\n",
    "        losses = _divide_no_nan(delta_y, scale[:, None, None])\n",
    "        weights = self._compute_weights(y=y, mask=mask)\n",
    "        return _weighted_mean(losses=losses, weights=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a4cf21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/losses/pytorch.py#L318){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### MASE.__init__\n",
       "\n",
       ">      MASE.__init__ (seasonality:int, horizon_weight=None)\n",
       "\n",
       "*Mean Absolute Scaled Error \n",
       "Calculates the Mean Absolute Scaled Error between\n",
       "`y` and `y_hat`. MASE measures the relative prediction\n",
       "accuracy of a forecasting method by comparinng the mean absolute errors\n",
       "of the prediction and the observed value against the mean\n",
       "absolute errors of the seasonal naive model.\n",
       "The MASE partially composed the Overall Weighted Average (OWA), \n",
       "used in the M4 Competition.\n",
       "\n",
       "$$ \\mathrm{MASE}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}_{\\tau}, \\mathbf{\\hat{y}}^{season}_{\\tau}) = \\frac{1}{H} \\sum^{t+H}_{\\tau=t+1} \\frac{|y_{\\tau}-\\hat{y}_{\\tau}|}{\\mathrm{MAE}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}^{season}_{\\tau})} $$\n",
       "\n",
       "**Parameters:**<br>\n",
       "`seasonality`: int. Main frequency of the time series; Hourly 24,  Daily 7, Weekly 52, Monthly 12, Quarterly 4, Yearly 1.\n",
       "`horizon_weight`: Tensor of size h, weight for each timestamp of the forecasting window. <br>\n",
       "\n",
       "**References:**<br>\n",
       "[Rob J. Hyndman, & Koehler, A. B. \"Another look at measures of forecast accuracy\".](https://www.sciencedirect.com/science/article/pii/S0169207006000239)<br>\n",
       "[Spyros Makridakis, Evangelos Spiliotis, Vassilios Assimakopoulos, \"The M4 Competition: 100,000 time series and 61 forecasting methods\".](https://www.sciencedirect.com/science/article/pii/S0169207019301128)*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/losses/pytorch.py#L318){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### MASE.__init__\n",
       "\n",
       ">      MASE.__init__ (seasonality:int, horizon_weight=None)\n",
       "\n",
       "*Mean Absolute Scaled Error \n",
       "Calculates the Mean Absolute Scaled Error between\n",
       "`y` and `y_hat`. MASE measures the relative prediction\n",
       "accuracy of a forecasting method by comparinng the mean absolute errors\n",
       "of the prediction and the observed value against the mean\n",
       "absolute errors of the seasonal naive model.\n",
       "The MASE partially composed the Overall Weighted Average (OWA), \n",
       "used in the M4 Competition.\n",
       "\n",
       "$$ \\mathrm{MASE}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}_{\\tau}, \\mathbf{\\hat{y}}^{season}_{\\tau}) = \\frac{1}{H} \\sum^{t+H}_{\\tau=t+1} \\frac{|y_{\\tau}-\\hat{y}_{\\tau}|}{\\mathrm{MAE}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}^{season}_{\\tau})} $$\n",
       "\n",
       "**Parameters:**<br>\n",
       "`seasonality`: int. Main frequency of the time series; Hourly 24,  Daily 7, Weekly 52, Monthly 12, Quarterly 4, Yearly 1.\n",
       "`horizon_weight`: Tensor of size h, weight for each timestamp of the forecasting window. <br>\n",
       "\n",
       "**References:**<br>\n",
       "[Rob J. Hyndman, & Koehler, A. B. \"Another look at measures of forecast accuracy\".](https://www.sciencedirect.com/science/article/pii/S0169207006000239)<br>\n",
       "[Spyros Makridakis, Evangelos Spiliotis, Vassilios Assimakopoulos, \"The M4 Competition: 100,000 time series and 61 forecasting methods\".](https://www.sciencedirect.com/science/article/pii/S0169207019301128)*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(MASE, name='MASE.__init__', title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a2c11b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/losses/pytorch.py#L345){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### MASE.__call__\n",
       "\n",
       ">      MASE.__call__ (y:torch.Tensor, y_hat:torch.Tensor,\n",
       ">                     y_insample:torch.Tensor, mask:Optional[torch.Tensor]=None)\n",
       "\n",
       "***Parameters:**<br>\n",
       "`y`: tensor (batch_size, output_size), Actual values.<br>\n",
       "`y_hat`: tensor (batch_size, output_size)), Predicted values.<br>\n",
       "`y_insample`: tensor (batch_size, input_size), Actual insample values.<br>\n",
       "`mask`: tensor, Specifies date stamps per serie to consider in loss.<br>\n",
       "\n",
       "**Returns:**<br>\n",
       "`mase`: tensor (single value).*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/losses/pytorch.py#L345){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### MASE.__call__\n",
       "\n",
       ">      MASE.__call__ (y:torch.Tensor, y_hat:torch.Tensor,\n",
       ">                     y_insample:torch.Tensor, mask:Optional[torch.Tensor]=None)\n",
       "\n",
       "***Parameters:**<br>\n",
       "`y`: tensor (batch_size, output_size), Actual values.<br>\n",
       "`y_hat`: tensor (batch_size, output_size)), Predicted values.<br>\n",
       "`y_insample`: tensor (batch_size, input_size), Actual insample values.<br>\n",
       "`mask`: tensor, Specifies date stamps per serie to consider in loss.<br>\n",
       "\n",
       "**Returns:**<br>\n",
       "`mase`: tensor (single value).*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(MASE.__call__, name='MASE.__call__', title_level=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6e0c8fe5",
   "metadata": {},
   "source": [
    "![](imgs_losses/mase_loss.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "73bbdc4e",
   "metadata": {},
   "source": [
    "## Relative Mean Squared Error (relMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954911d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class relMSE(BasePointLoss):\n",
    "    \"\"\"Relative Mean Squared Error\n",
    "    Computes Relative Mean Squared Error (relMSE), as proposed by Hyndman & Koehler (2006)\n",
    "    as an alternative to percentage errors, to avoid measure unstability.\n",
    "    $$ \\mathrm{relMSE}(\\\\mathbf{y}, \\\\mathbf{\\hat{y}}, \\\\mathbf{\\hat{y}}^{benchmark}) =\n",
    "    \\\\frac{\\mathrm{MSE}(\\\\mathbf{y}, \\\\mathbf{\\hat{y}})}{\\mathrm{MSE}(\\\\mathbf{y}, \\\\mathbf{\\hat{y}}^{benchmark})} $$\n",
    "\n",
    "    **Parameters:**<br>\n",
    "    `y_train`: numpy array, deprecated.<br>\n",
    "    `horizon_weight`: Tensor of size h, weight for each timestamp of the forecasting window. <br>\n",
    "\n",
    "    **References:**<br>\n",
    "    - [Hyndman, R. J and Koehler, A. B. (2006).\n",
    "       \"Another look at measures of forecast accuracy\",\n",
    "       International Journal of Forecasting, Volume 22, Issue 4.](https://www.sciencedirect.com/science/article/pii/S0169207006000239)<br>\n",
    "    - [Kin G. Olivares, O. Nganba Meetei, Ruijun Ma, Rohan Reddy, Mengfei Cao, Lee Dicker. \n",
    "       \"Probabilistic Hierarchical Forecasting with Deep Poisson Mixtures. \n",
    "       Submitted to the International Journal Forecasting, Working paper available at arxiv.](https://arxiv.org/pdf/2110.13179.pdf)\n",
    "    \"\"\"\n",
    "    def __init__(self, y_train=None, horizon_weight=None):\n",
    "        super(relMSE, self).__init__(horizon_weight=horizon_weight,\n",
    "                                     outputsize_multiplier=1,\n",
    "                                     output_names=[''])\n",
    "        if y_train is not None:\n",
    "            raise DeprecationWarning(\"y_train will be deprecated in a future release.\")\n",
    "        self.mse = MSE(horizon_weight=horizon_weight)\n",
    "\n",
    "    def __call__(self,\n",
    "                 y: torch.Tensor,\n",
    "                 y_hat: torch.Tensor,\n",
    "                 y_benchmark: torch.Tensor,\n",
    "                 mask: Union[torch.Tensor, None] = None\n",
    "                 ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        **Parameters:**<br>\n",
    "        `y`: tensor (batch_size, output_size), Actual values.<br>\n",
    "        `y_hat`: tensor (batch_size, output_size)), Predicted values.<br>\n",
    "        `y_benchmark`: tensor (batch_size, output_size), Benchmark predicted values.<br>\n",
    "        `mask`: tensor, Specifies date stamps per serie to consider in loss.<br>\n",
    "\n",
    "        **Returns:**<br>\n",
    "        `relMSE`: tensor (single value).\n",
    "        \"\"\"\n",
    "        norm = self.mse(y=y, y_hat=y_benchmark, mask=mask) # Already weighted\n",
    "        norm = norm + 1e-5 # Numerical stability\n",
    "        loss = self.mse(y=y, y_hat=y_hat, mask=mask) # Already weighted\n",
    "        loss = _divide_no_nan(loss, norm)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edeb6f9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/losses/pytorch.py#L374){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### relMSE.__init__\n",
       "\n",
       ">      relMSE.__init__ (y_train=None, horizon_weight=None)\n",
       "\n",
       "*Relative Mean Squared Error\n",
       "Computes Relative Mean Squared Error (relMSE), as proposed by Hyndman & Koehler (2006)\n",
       "as an alternative to percentage errors, to avoid measure unstability.\n",
       "$$ \\mathrm{relMSE}(\\mathbf{y}, \\mathbf{\\hat{y}}, \\mathbf{\\hat{y}}^{benchmark}) =\n",
       "\\frac{\\mathrm{MSE}(\\mathbf{y}, \\mathbf{\\hat{y}})}{\\mathrm{MSE}(\\mathbf{y}, \\mathbf{\\hat{y}}^{benchmark})} $$\n",
       "\n",
       "**Parameters:**<br>\n",
       "`y_train`: numpy array, deprecated.<br>\n",
       "`horizon_weight`: Tensor of size h, weight for each timestamp of the forecasting window. <br>\n",
       "\n",
       "**References:**<br>\n",
       "- [Hyndman, R. J and Koehler, A. B. (2006).\n",
       "   \"Another look at measures of forecast accuracy\",\n",
       "   International Journal of Forecasting, Volume 22, Issue 4.](https://www.sciencedirect.com/science/article/pii/S0169207006000239)<br>\n",
       "- [Kin G. Olivares, O. Nganba Meetei, Ruijun Ma, Rohan Reddy, Mengfei Cao, Lee Dicker. \n",
       "   \"Probabilistic Hierarchical Forecasting with Deep Poisson Mixtures. \n",
       "   Submitted to the International Journal Forecasting, Working paper available at arxiv.](https://arxiv.org/pdf/2110.13179.pdf)*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/losses/pytorch.py#L374){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### relMSE.__init__\n",
       "\n",
       ">      relMSE.__init__ (y_train=None, horizon_weight=None)\n",
       "\n",
       "*Relative Mean Squared Error\n",
       "Computes Relative Mean Squared Error (relMSE), as proposed by Hyndman & Koehler (2006)\n",
       "as an alternative to percentage errors, to avoid measure unstability.\n",
       "$$ \\mathrm{relMSE}(\\mathbf{y}, \\mathbf{\\hat{y}}, \\mathbf{\\hat{y}}^{benchmark}) =\n",
       "\\frac{\\mathrm{MSE}(\\mathbf{y}, \\mathbf{\\hat{y}})}{\\mathrm{MSE}(\\mathbf{y}, \\mathbf{\\hat{y}}^{benchmark})} $$\n",
       "\n",
       "**Parameters:**<br>\n",
       "`y_train`: numpy array, deprecated.<br>\n",
       "`horizon_weight`: Tensor of size h, weight for each timestamp of the forecasting window. <br>\n",
       "\n",
       "**References:**<br>\n",
       "- [Hyndman, R. J and Koehler, A. B. (2006).\n",
       "   \"Another look at measures of forecast accuracy\",\n",
       "   International Journal of Forecasting, Volume 22, Issue 4.](https://www.sciencedirect.com/science/article/pii/S0169207006000239)<br>\n",
       "- [Kin G. Olivares, O. Nganba Meetei, Ruijun Ma, Rohan Reddy, Mengfei Cao, Lee Dicker. \n",
       "   \"Probabilistic Hierarchical Forecasting with Deep Poisson Mixtures. \n",
       "   Submitted to the International Journal Forecasting, Working paper available at arxiv.](https://arxiv.org/pdf/2110.13179.pdf)*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(relMSE, name='relMSE.__init__', title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a317b5c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/losses/pytorch.py#L401){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### relMSE.__call__\n",
       "\n",
       ">      relMSE.__call__ (y:torch.Tensor, y_hat:torch.Tensor,\n",
       ">                       y_benchmark:torch.Tensor,\n",
       ">                       mask:Optional[torch.Tensor]=None)\n",
       "\n",
       "***Parameters:**<br>\n",
       "`y`: tensor (batch_size, output_size), Actual values.<br>\n",
       "`y_hat`: tensor (batch_size, output_size)), Predicted values.<br>\n",
       "`y_benchmark`: tensor (batch_size, output_size), Benchmark predicted values.<br>\n",
       "`mask`: tensor, Specifies date stamps per serie to consider in loss.<br>\n",
       "\n",
       "**Returns:**<br>\n",
       "`relMSE`: tensor (single value).*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/losses/pytorch.py#L401){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### relMSE.__call__\n",
       "\n",
       ">      relMSE.__call__ (y:torch.Tensor, y_hat:torch.Tensor,\n",
       ">                       y_benchmark:torch.Tensor,\n",
       ">                       mask:Optional[torch.Tensor]=None)\n",
       "\n",
       "***Parameters:**<br>\n",
       "`y`: tensor (batch_size, output_size), Actual values.<br>\n",
       "`y_hat`: tensor (batch_size, output_size)), Predicted values.<br>\n",
       "`y_benchmark`: tensor (batch_size, output_size), Benchmark predicted values.<br>\n",
       "`mask`: tensor, Specifies date stamps per serie to consider in loss.<br>\n",
       "\n",
       "**Returns:**<br>\n",
       "`relMSE`: tensor (single value).*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(relMSE.__call__, name='relMSE.__call__', title_level=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c828438e",
   "metadata": {},
   "source": [
    "# 4. Probabilistic Errors\n",
    "\n",
    "These methods use statistical approaches for estimating unknown probability distributions using observed data. \n",
    "\n",
    "Maximum likelihood estimation involves finding the parameter values that maximize the likelihood function, which measures the probability of obtaining the observed data given the parameter values. MLE has good theoretical properties and efficiency under certain satisfied assumptions.\n",
    "\n",
    "On the non-parametric approach, quantile regression measures non-symmetrically deviation, producing under/over estimation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "999d8cb2",
   "metadata": {},
   "source": [
    "## Quantile Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd296fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class QuantileLoss(BasePointLoss):\n",
    "    \"\"\" Quantile Loss\n",
    "\n",
    "    Computes the quantile loss between `y` and `y_hat`.\n",
    "    QL measures the deviation of a quantile forecast.\n",
    "    By weighting the absolute deviation in a non symmetric way, the\n",
    "    loss pays more attention to under or over estimation.\n",
    "    A common value for q is 0.5 for the deviation from the median (Pinball loss).\n",
    "\n",
    "    $$ \\mathrm{QL}(\\\\mathbf{y}_{\\\\tau}, \\\\mathbf{\\hat{y}}^{(q)}_{\\\\tau}) = \\\\frac{1}{H} \\\\sum^{t+H}_{\\\\tau=t+1} \\Big( (1-q)\\,( \\hat{y}^{(q)}_{\\\\tau} - y_{\\\\tau} )_{+} + q\\,( y_{\\\\tau} - \\hat{y}^{(q)}_{\\\\tau} )_{+} \\Big) $$\n",
    "\n",
    "    **Parameters:**<br>\n",
    "    `q`: float, between 0 and 1. The slope of the quantile loss, in the context of quantile regression, the q determines the conditional quantile level.<br>\n",
    "    `horizon_weight`: Tensor of size h, weight for each timestamp of the forecasting window. <br>\n",
    "\n",
    "    **References:**<br>\n",
    "    [Roger Koenker and Gilbert Bassett, Jr., \"Regression Quantiles\".](https://www.jstor.org/stable/1913643)\n",
    "    \"\"\"\n",
    "    def __init__(self, q, horizon_weight=None):\n",
    "        super(QuantileLoss, self).__init__(horizon_weight=horizon_weight,\n",
    "                                           outputsize_multiplier=1,\n",
    "                                           output_names=[f'_ql{q}'])\n",
    "        self.q = q\n",
    "\n",
    "    def __call__(self,\n",
    "                 y: torch.Tensor,\n",
    "                 y_hat: torch.Tensor,\n",
    "                 y_insample: torch.Tensor,\n",
    "                 mask: Union[torch.Tensor, None] = None,\n",
    "                 ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        **Parameters:**<br>\n",
    "        `y`: tensor, Actual values.<br>\n",
    "        `y_hat`: tensor, Predicted values.<br>\n",
    "        `mask`: tensor, Specifies datapoints to consider in loss.<br>\n",
    "\n",
    "        **Returns:**<br>\n",
    "        `quantile_loss`: tensor (single value).\n",
    "        \"\"\"\n",
    "        delta_y = y - y_hat\n",
    "        losses = torch.max(torch.mul(self.q, delta_y), torch.mul((self.q - 1), delta_y))\n",
    "        weights = self._compute_weights(y=y, mask=mask)\n",
    "        return _weighted_mean(losses=losses, weights=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bd46d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/losses/pytorch.py#L428){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### QuantileLoss.__init__\n",
       "\n",
       ">      QuantileLoss.__init__ (q, horizon_weight=None)\n",
       "\n",
       "*Quantile Loss\n",
       "\n",
       "Computes the quantile loss between `y` and `y_hat`.\n",
       "QL measures the deviation of a quantile forecast.\n",
       "By weighting the absolute deviation in a non symmetric way, the\n",
       "loss pays more attention to under or over estimation.\n",
       "A common value for q is 0.5 for the deviation from the median (Pinball loss).\n",
       "\n",
       "$$ \\mathrm{QL}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}^{(q)}_{\\tau}) = \\frac{1}{H} \\sum^{t+H}_{\\tau=t+1} \\Big( (1-q)\\,( \\hat{y}^{(q)}_{\\tau} - y_{\\tau} )_{+} + q\\,( y_{\\tau} - \\hat{y}^{(q)}_{\\tau} )_{+} \\Big) $$\n",
       "\n",
       "**Parameters:**<br>\n",
       "`q`: float, between 0 and 1. The slope of the quantile loss, in the context of quantile regression, the q determines the conditional quantile level.<br>\n",
       "`horizon_weight`: Tensor of size h, weight for each timestamp of the forecasting window. <br>\n",
       "\n",
       "**References:**<br>\n",
       "[Roger Koenker and Gilbert Bassett, Jr., \"Regression Quantiles\".](https://www.jstor.org/stable/1913643)*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/losses/pytorch.py#L428){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### QuantileLoss.__init__\n",
       "\n",
       ">      QuantileLoss.__init__ (q, horizon_weight=None)\n",
       "\n",
       "*Quantile Loss\n",
       "\n",
       "Computes the quantile loss between `y` and `y_hat`.\n",
       "QL measures the deviation of a quantile forecast.\n",
       "By weighting the absolute deviation in a non symmetric way, the\n",
       "loss pays more attention to under or over estimation.\n",
       "A common value for q is 0.5 for the deviation from the median (Pinball loss).\n",
       "\n",
       "$$ \\mathrm{QL}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}^{(q)}_{\\tau}) = \\frac{1}{H} \\sum^{t+H}_{\\tau=t+1} \\Big( (1-q)\\,( \\hat{y}^{(q)}_{\\tau} - y_{\\tau} )_{+} + q\\,( y_{\\tau} - \\hat{y}^{(q)}_{\\tau} )_{+} \\Big) $$\n",
       "\n",
       "**Parameters:**<br>\n",
       "`q`: float, between 0 and 1. The slope of the quantile loss, in the context of quantile regression, the q determines the conditional quantile level.<br>\n",
       "`horizon_weight`: Tensor of size h, weight for each timestamp of the forecasting window. <br>\n",
       "\n",
       "**References:**<br>\n",
       "[Roger Koenker and Gilbert Bassett, Jr., \"Regression Quantiles\".](https://www.jstor.org/stable/1913643)*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(QuantileLoss, name='QuantileLoss.__init__', title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1588e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/losses/pytorch.py#L455){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### QuantileLoss.__call__\n",
       "\n",
       ">      QuantileLoss.__call__ (y:torch.Tensor, y_hat:torch.Tensor,\n",
       ">                             y_insample:torch.Tensor,\n",
       ">                             mask:Optional[torch.Tensor]=None)\n",
       "\n",
       "***Parameters:**<br>\n",
       "`y`: tensor, Actual values.<br>\n",
       "`y_hat`: tensor, Predicted values.<br>\n",
       "`mask`: tensor, Specifies datapoints to consider in loss.<br>\n",
       "\n",
       "**Returns:**<br>\n",
       "`quantile_loss`: tensor (single value).*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/losses/pytorch.py#L455){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### QuantileLoss.__call__\n",
       "\n",
       ">      QuantileLoss.__call__ (y:torch.Tensor, y_hat:torch.Tensor,\n",
       ">                             y_insample:torch.Tensor,\n",
       ">                             mask:Optional[torch.Tensor]=None)\n",
       "\n",
       "***Parameters:**<br>\n",
       "`y`: tensor, Actual values.<br>\n",
       "`y_hat`: tensor, Predicted values.<br>\n",
       "`mask`: tensor, Specifies datapoints to consider in loss.<br>\n",
       "\n",
       "**Returns:**<br>\n",
       "`quantile_loss`: tensor (single value).*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(QuantileLoss.__call__, name='QuantileLoss.__call__', title_level=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "51ac874f",
   "metadata": {},
   "source": [
    "![](imgs_losses/q_loss.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "92dbb002",
   "metadata": {},
   "source": [
    "## Multi Quantile Loss (MQLoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291a0530",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def level_to_outputs(level):\n",
    "    qs = sum([[50-l/2, 50+l/2] for l in level], [])\n",
    "    output_names = sum([[f'-lo-{l}', f'-hi-{l}'] for l in level], [])\n",
    "\n",
    "    sort_idx = np.argsort(qs)\n",
    "    quantiles = np.array(qs)[sort_idx]\n",
    "\n",
    "    # Add default median\n",
    "    quantiles = np.concatenate([np.array([50]), quantiles])\n",
    "    quantiles = torch.Tensor(quantiles) / 100\n",
    "    output_names = list(np.array(output_names)[sort_idx])\n",
    "    output_names.insert(0, '-median')\n",
    "    \n",
    "    return quantiles, output_names\n",
    "\n",
    "def quantiles_to_outputs(quantiles):\n",
    "    output_names = []\n",
    "    for q in quantiles:\n",
    "        if q<.50:\n",
    "            output_names.append(f'-lo-{np.round(100-200*q,2)}')\n",
    "        elif q>.50:\n",
    "            output_names.append(f'-hi-{np.round(100-200*(1-q),2)}')\n",
    "        else:\n",
    "            output_names.append('-median')\n",
    "    return quantiles, output_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21dc7968",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class MQLoss(BasePointLoss):\n",
    "    \"\"\"  Multi-Quantile loss\n",
    "\n",
    "    Calculates the Multi-Quantile loss (MQL) between `y` and `y_hat`.\n",
    "    MQL calculates the average multi-quantile Loss for\n",
    "    a given set of quantiles, based on the absolute \n",
    "    difference between predicted quantiles and observed values.\n",
    "    \n",
    "    $$ \\mathrm{MQL}(\\\\mathbf{y}_{\\\\tau},[\\\\mathbf{\\hat{y}}^{(q_{1})}_{\\\\tau}, ... ,\\hat{y}^{(q_{n})}_{\\\\tau}]) = \\\\frac{1}{n} \\\\sum_{q_{i}} \\mathrm{QL}(\\\\mathbf{y}_{\\\\tau}, \\\\mathbf{\\hat{y}}^{(q_{i})}_{\\\\tau}) $$\n",
    "    \n",
    "    The limit behavior of MQL allows to measure the accuracy \n",
    "    of a full predictive distribution $\\mathbf{\\hat{F}}_{\\\\tau}$ with \n",
    "    the continuous ranked probability score (CRPS). This can be achieved \n",
    "    through a numerical integration technique, that discretizes the quantiles \n",
    "    and treats the CRPS integral with a left Riemann approximation, averaging over \n",
    "    uniformly distanced quantiles.    \n",
    "    \n",
    "    $$ \\mathrm{CRPS}(y_{\\\\tau}, \\mathbf{\\hat{F}}_{\\\\tau}) = \\int^{1}_{0} \\mathrm{QL}(y_{\\\\tau}, \\hat{y}^{(q)}_{\\\\tau}) dq $$\n",
    "\n",
    "    **Parameters:**<br>\n",
    "    `level`: int list [0,100]. Probability levels for prediction intervals (Defaults median).\n",
    "    `quantiles`: float list [0., 1.]. Alternative to level, quantiles to estimate from y distribution.\n",
    "    `horizon_weight`: Tensor of size h, weight for each timestamp of the forecasting window. <br>\n",
    "\n",
    "    **References:**<br>\n",
    "    [Roger Koenker and Gilbert Bassett, Jr., \"Regression Quantiles\".](https://www.jstor.org/stable/1913643)<br>\n",
    "    [James E. Matheson and Robert L. Winkler, \"Scoring Rules for Continuous Probability Distributions\".](https://www.jstor.org/stable/2629907)\n",
    "    \"\"\"\n",
    "    def __init__(self, level=[80, 90], quantiles=None, horizon_weight=None):\n",
    "\n",
    "        qs, output_names = level_to_outputs(level)\n",
    "        qs = torch.Tensor(qs)\n",
    "        # Transform quantiles to homogeneus output names\n",
    "        if quantiles is not None:\n",
    "            _, output_names = quantiles_to_outputs(quantiles)\n",
    "            qs = torch.Tensor(quantiles)\n",
    "\n",
    "        super(MQLoss, self).__init__(horizon_weight=horizon_weight,\n",
    "                                     outputsize_multiplier=len(qs),\n",
    "                                     output_names=output_names)\n",
    "        \n",
    "        self.quantiles = torch.nn.Parameter(qs, requires_grad=False)\n",
    "\n",
    "    def domain_map(self, y_hat: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "        Univariate: [B, H, 1 * Q]\n",
    "        Multivariate: [B, H, N * Q]\n",
    "\n",
    "        Output: [B, H, N, Q]\n",
    "        \"\"\"\n",
    "        output = y_hat.reshape(y_hat.shape[0],\n",
    "                               y_hat.shape[1],\n",
    "                               -1,\n",
    "                               self.outputsize_multiplier)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def _compute_weights(self, y, mask):\n",
    "        \"\"\"\n",
    "        Compute final weights for each datapoint (based on all weights and all masks)\n",
    "        Set horizon_weight to a ones[H] tensor if not set.\n",
    "        If set, check that it has the same length as the horizon in x.\n",
    "\n",
    "        y: [B, h, N, 1]\n",
    "        mask: [B, h, N, 1]\n",
    "        \"\"\"\n",
    "\n",
    "        if self.horizon_weight is None:\n",
    "            weights = torch.ones_like(mask)\n",
    "        else:\n",
    "            assert mask.shape[1] == len(self.horizon_weight), \\\n",
    "                'horizon_weight must have same length as Y'    \n",
    "            weights = self.horizon_weight.clone()\n",
    "            weights = weights[None, :, None, None]\n",
    "            weights = weights.to(mask.device)\n",
    "            weights = torch.ones_like(mask, device=mask.device) * weights\n",
    "        \n",
    "        return weights * mask\n",
    "\n",
    "    def __call__(self,\n",
    "                 y: torch.Tensor,\n",
    "                 y_hat: torch.Tensor,\n",
    "                 y_insample: torch.Tensor,\n",
    "                 mask: Union[torch.Tensor, None] = None,\n",
    "                 ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        **Parameters:**<br>\n",
    "        `y`: tensor, Actual values.<br>\n",
    "        `y_hat`: tensor, Predicted values.<br>\n",
    "        `mask`: tensor, Specifies date stamps per serie to consider in loss.<br>\n",
    "\n",
    "        **Returns:**<br>\n",
    "        `mqloss`: tensor (single value).\n",
    "        \"\"\"\n",
    "        # [B, h, N] -> [B, h, N, 1]\n",
    "        if y_hat.ndim == 3:\n",
    "            y_hat = y_hat.unsqueeze(-1)\n",
    "\n",
    "        y = y.unsqueeze(-1)\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(-1)\n",
    "        else:\n",
    "            mask = torch.ones_like(y, device=y.device)\n",
    "\n",
    "        error  = y_hat - y\n",
    "\n",
    "        sq     = torch.maximum(-error, torch.zeros_like(error))\n",
    "        s1_q   = torch.maximum(error, torch.zeros_like(error))\n",
    "        \n",
    "        quantiles = self.quantiles[None, None, None, :]\n",
    "        losses = (1 / len(quantiles)) * (quantiles * sq + (1 - quantiles) * s1_q)\n",
    "        weights = self._compute_weights(y=losses, mask=mask) # Use losses for extra dim\n",
    "\n",
    "        return _weighted_mean(losses=losses, weights=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f42ec82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/losses/pytorch.py#L504){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### MQLoss.__init__\n",
       "\n",
       ">      MQLoss.__init__ (level=[80, 90], quantiles=None, horizon_weight=None)\n",
       "\n",
       "*Multi-Quantile loss\n",
       "\n",
       "Calculates the Multi-Quantile loss (MQL) between `y` and `y_hat`.\n",
       "MQL calculates the average multi-quantile Loss for\n",
       "a given set of quantiles, based on the absolute \n",
       "difference between predicted quantiles and observed values.\n",
       "\n",
       "$$ \\mathrm{MQL}(\\mathbf{y}_{\\tau},[\\mathbf{\\hat{y}}^{(q_{1})}_{\\tau}, ... ,\\hat{y}^{(q_{n})}_{\\tau}]) = \\frac{1}{n} \\sum_{q_{i}} \\mathrm{QL}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}^{(q_{i})}_{\\tau}) $$\n",
       "\n",
       "The limit behavior of MQL allows to measure the accuracy \n",
       "of a full predictive distribution $\\mathbf{\\hat{F}}_{\\tau}$ with \n",
       "the continuous ranked probability score (CRPS). This can be achieved \n",
       "through a numerical integration technique, that discretizes the quantiles \n",
       "and treats the CRPS integral with a left Riemann approximation, averaging over \n",
       "uniformly distanced quantiles.    \n",
       "\n",
       "$$ \\mathrm{CRPS}(y_{\\tau}, \\mathbf{\\hat{F}}_{\\tau}) = \\int^{1}_{0} \\mathrm{QL}(y_{\\tau}, \\hat{y}^{(q)}_{\\tau}) dq $$\n",
       "\n",
       "**Parameters:**<br>\n",
       "`level`: int list [0,100]. Probability levels for prediction intervals (Defaults median).\n",
       "`quantiles`: float list [0., 1.]. Alternative to level, quantiles to estimate from y distribution.\n",
       "`horizon_weight`: Tensor of size h, weight for each timestamp of the forecasting window. <br>\n",
       "\n",
       "**References:**<br>\n",
       "[Roger Koenker and Gilbert Bassett, Jr., \"Regression Quantiles\".](https://www.jstor.org/stable/1913643)<br>\n",
       "[James E. Matheson and Robert L. Winkler, \"Scoring Rules for Continuous Probability Distributions\".](https://www.jstor.org/stable/2629907)*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/losses/pytorch.py#L504){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### MQLoss.__init__\n",
       "\n",
       ">      MQLoss.__init__ (level=[80, 90], quantiles=None, horizon_weight=None)\n",
       "\n",
       "*Multi-Quantile loss\n",
       "\n",
       "Calculates the Multi-Quantile loss (MQL) between `y` and `y_hat`.\n",
       "MQL calculates the average multi-quantile Loss for\n",
       "a given set of quantiles, based on the absolute \n",
       "difference between predicted quantiles and observed values.\n",
       "\n",
       "$$ \\mathrm{MQL}(\\mathbf{y}_{\\tau},[\\mathbf{\\hat{y}}^{(q_{1})}_{\\tau}, ... ,\\hat{y}^{(q_{n})}_{\\tau}]) = \\frac{1}{n} \\sum_{q_{i}} \\mathrm{QL}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}^{(q_{i})}_{\\tau}) $$\n",
       "\n",
       "The limit behavior of MQL allows to measure the accuracy \n",
       "of a full predictive distribution $\\mathbf{\\hat{F}}_{\\tau}$ with \n",
       "the continuous ranked probability score (CRPS). This can be achieved \n",
       "through a numerical integration technique, that discretizes the quantiles \n",
       "and treats the CRPS integral with a left Riemann approximation, averaging over \n",
       "uniformly distanced quantiles.    \n",
       "\n",
       "$$ \\mathrm{CRPS}(y_{\\tau}, \\mathbf{\\hat{F}}_{\\tau}) = \\int^{1}_{0} \\mathrm{QL}(y_{\\tau}, \\hat{y}^{(q)}_{\\tau}) dq $$\n",
       "\n",
       "**Parameters:**<br>\n",
       "`level`: int list [0,100]. Probability levels for prediction intervals (Defaults median).\n",
       "`quantiles`: float list [0., 1.]. Alternative to level, quantiles to estimate from y distribution.\n",
       "`horizon_weight`: Tensor of size h, weight for each timestamp of the forecasting window. <br>\n",
       "\n",
       "**References:**<br>\n",
       "[Roger Koenker and Gilbert Bassett, Jr., \"Regression Quantiles\".](https://www.jstor.org/stable/1913643)<br>\n",
       "[James E. Matheson and Robert L. Winkler, \"Scoring Rules for Continuous Probability Distributions\".](https://www.jstor.org/stable/2629907)*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(MQLoss, name='MQLoss.__init__', title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac2237a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/losses/pytorch.py#L578){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### MQLoss.__call__\n",
       "\n",
       ">      MQLoss.__call__ (y:torch.Tensor, y_hat:torch.Tensor,\n",
       ">                       y_insample:torch.Tensor,\n",
       ">                       mask:Optional[torch.Tensor]=None)\n",
       "\n",
       "***Parameters:**<br>\n",
       "`y`: tensor, Actual values.<br>\n",
       "`y_hat`: tensor, Predicted values.<br>\n",
       "`mask`: tensor, Specifies date stamps per serie to consider in loss.<br>\n",
       "\n",
       "**Returns:**<br>\n",
       "`mqloss`: tensor (single value).*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/losses/pytorch.py#L578){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### MQLoss.__call__\n",
       "\n",
       ">      MQLoss.__call__ (y:torch.Tensor, y_hat:torch.Tensor,\n",
       ">                       y_insample:torch.Tensor,\n",
       ">                       mask:Optional[torch.Tensor]=None)\n",
       "\n",
       "***Parameters:**<br>\n",
       "`y`: tensor, Actual values.<br>\n",
       "`y_hat`: tensor, Predicted values.<br>\n",
       "`mask`: tensor, Specifies date stamps per serie to consider in loss.<br>\n",
       "\n",
       "**Returns:**<br>\n",
       "`mqloss`: tensor (single value).*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(MQLoss.__call__, name='MQLoss.__call__', title_level=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "33b66b0e",
   "metadata": {},
   "source": [
    "![](imgs_losses/mq_loss.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da37f2ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-lo-98.0', '-lo-80.0', '-median', '-hi-80.0', '-hi-98.0']\n",
      "Parameter containing:\n",
      "tensor([0.0100, 0.1000, 0.5000, 0.9000, 0.9900])\n"
     ]
    }
   ],
   "source": [
    "# | hide\n",
    "# Unit tests to check MQLoss' stored quantiles\n",
    "# attribute is correctly instantiated\n",
    "check = MQLoss(level=[80, 90])\n",
    "test_eq(len(check.quantiles), 5)\n",
    "\n",
    "check = MQLoss(quantiles=[0.0100, 0.1000, 0.5, 0.9000, 0.9900])\n",
    "print(check.output_names)\n",
    "print(check.quantiles)\n",
    "test_eq(len(check.quantiles), 5)\n",
    "\n",
    "check = MQLoss(quantiles=[0.0100, 0.1000, 0.9000, 0.9900])\n",
    "test_eq(len(check.quantiles), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ff6f46",
   "metadata": {},
   "source": [
    "## Implicit Quantile Loss (IQLoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653f081f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class QuantileLayer(nn.Module):\n",
    "    r\"\"\"\n",
    "    Implicit Quantile Layer from the paper ``IQN for Distributional\n",
    "    Reinforcement Learning`` (https://arxiv.org/abs/1806.06923) by\n",
    "    Dabney et al. 2018.\n",
    "\n",
    "    Code from GluonTS: https://github.com/awslabs/gluonts/blob/61133ef6e2d88177b32ace4afc6843ab9a7bc8cd/src/gluonts/torch/distributions/implicit_quantile_network.py\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_output: int, cos_embedding_dim: int = 128):\n",
    "        super().__init__()\n",
    "\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Linear(cos_embedding_dim, cos_embedding_dim),\n",
    "            nn.PReLU(),\n",
    "            nn.Linear(cos_embedding_dim, num_output),\n",
    "        )\n",
    "\n",
    "        self.register_buffer(\"integers\", torch.arange(0, cos_embedding_dim))\n",
    "\n",
    "    def forward(self, tau: torch.Tensor) -> torch.Tensor: \n",
    "        cos_emb_tau = torch.cos(tau * self.integers * torch.pi)\n",
    "        return self.output_layer(cos_emb_tau)\n",
    "\n",
    "\n",
    "class IQLoss(QuantileLoss):\n",
    "    \"\"\"Implicit Quantile Loss\n",
    "\n",
    "    Computes the quantile loss between `y` and `y_hat`, with the quantile `q` provided as an input to the network. \n",
    "    IQL measures the deviation of a quantile forecast.\n",
    "    By weighting the absolute deviation in a non symmetric way, the\n",
    "    loss pays more attention to under or over estimation.\n",
    "\n",
    "    $$ \\mathrm{QL}(\\\\mathbf{y}_{\\\\tau}, \\\\mathbf{\\hat{y}}^{(q)}_{\\\\tau}) = \\\\frac{1}{H} \\\\sum^{t+H}_{\\\\tau=t+1} \\Big( (1-q)\\,( \\hat{y}^{(q)}_{\\\\tau} - y_{\\\\tau} )_{+} + q\\,( y_{\\\\tau} - \\hat{y}^{(q)}_{\\\\tau} )_{+} \\Big) $$\n",
    "\n",
    "    **Parameters:**<br>\n",
    "    `quantile_sampling`: str, default='uniform', sampling distribution used to sample the quantiles during training. Choose from ['uniform', 'beta']. <br>\n",
    "    `horizon_weight`: Tensor of size h, weight for each timestamp of the forecasting window. <br>\n",
    "\n",
    "    **References:**<br>\n",
    "    [Gouttes, Adèle, Kashif Rasul, Mateusz Koren, Johannes Stephan, and Tofigh Naghibi, \"Probabilistic Time Series Forecasting with Implicit Quantile Networks\".](http://arxiv.org/abs/2107.03743)\n",
    "    \"\"\"\n",
    "    def __init__(self, cos_embedding_dim = 64, concentration0 = 1.0, concentration1 = 1.0, horizon_weight=None):\n",
    "        self.update_quantile()\n",
    "        super(IQLoss, self).__init__(\n",
    "            q = self.q,\n",
    "            horizon_weight=horizon_weight\n",
    "        )\n",
    "\n",
    "        self.cos_embedding_dim = cos_embedding_dim\n",
    "        self.concentration0 = concentration0\n",
    "        self.concentration1 = concentration1\n",
    "        self.has_sampled = False\n",
    "        self.has_predicted = False\n",
    "\n",
    "        self.quantile_layer = QuantileLayer(\n",
    "            num_output=1, cos_embedding_dim=self.cos_embedding_dim\n",
    "        )\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Linear(1, 1), nn.PReLU()\n",
    "        )\n",
    "        \n",
    "    def _sample_quantiles(self, sample_size, device):\n",
    "        if not self.has_sampled:\n",
    "            self._init_sampling_distribution(device)\n",
    "\n",
    "        quantiles = self.sampling_distr.sample(sample_size)\n",
    "        self.has_sampled = True        \n",
    "        self.has_predicted = False\n",
    "\n",
    "        return quantiles\n",
    "    \n",
    "    def _init_sampling_distribution(self, device):\n",
    "        concentration0 = torch.tensor([self.concentration0],\n",
    "                                      device=device,\n",
    "                                      dtype=torch.float32)\n",
    "        concentration1 = torch.tensor([self.concentration1],\n",
    "                                      device=device,\n",
    "                                      dtype=torch.float32)        \n",
    "        self.sampling_distr = Beta(concentration0 = concentration0,\n",
    "                                   concentration1 = concentration1)\n",
    "\n",
    "    def update_quantile(self, q: List[float] = [0.5]):\n",
    "        self.q = q[0]\n",
    "        self.output_names = [f\"_ql{q[0]}\"]\n",
    "        self.has_predicted = True\n",
    "\n",
    "    def domain_map(self, y_hat):\n",
    "        \"\"\"\n",
    "        Adds IQN network to output of network\n",
    "\n",
    "        Input shapes to this function:\n",
    "         \n",
    "        Univariate: y_hat = [B, h, 1] \n",
    "        Multivariate: y_hat = [B, h, N]\n",
    "        \"\"\"\n",
    "        if self.eval() and self.has_predicted:\n",
    "            quantiles = torch.full(size=y_hat.shape, \n",
    "                                    fill_value=self.q,\n",
    "                                    device=y_hat.device,\n",
    "                                    dtype=y_hat.dtype) \n",
    "            quantiles = quantiles.unsqueeze(-1)             \n",
    "        else:\n",
    "            quantiles = self._sample_quantiles(sample_size=y_hat.shape,\n",
    "                                        device=y_hat.device)\n",
    "\n",
    "        # Embed the quantiles and add to y_hat\n",
    "        emb_taus = self.quantile_layer(quantiles)\n",
    "        emb_inputs = y_hat.unsqueeze(-1) * (1.0 + emb_taus)\n",
    "        emb_outputs = self.output_layer(emb_inputs)\n",
    "        \n",
    "        # Domain map\n",
    "        y_hat = emb_outputs.squeeze(-1)\n",
    "\n",
    "        return y_hat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fc84ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/losses/pytorch.py#L643){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### IQLoss.__init__\n",
       "\n",
       ">      IQLoss.__init__ (cos_embedding_dim=64, concentration0=1.0,\n",
       ">                       concentration1=1.0, horizon_weight=None)\n",
       "\n",
       "*Implicit Quantile Loss\n",
       "\n",
       "Computes the quantile loss between `y` and `y_hat`, with the quantile `q` provided as an input to the network. \n",
       "IQL measures the deviation of a quantile forecast.\n",
       "By weighting the absolute deviation in a non symmetric way, the\n",
       "loss pays more attention to under or over estimation.\n",
       "\n",
       "$$ \\mathrm{QL}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}^{(q)}_{\\tau}) = \\frac{1}{H} \\sum^{t+H}_{\\tau=t+1} \\Big( (1-q)\\,( \\hat{y}^{(q)}_{\\tau} - y_{\\tau} )_{+} + q\\,( y_{\\tau} - \\hat{y}^{(q)}_{\\tau} )_{+} \\Big) $$\n",
       "\n",
       "**Parameters:**<br>\n",
       "`quantile_sampling`: str, default='uniform', sampling distribution used to sample the quantiles during training. Choose from ['uniform', 'beta']. <br>\n",
       "`horizon_weight`: Tensor of size h, weight for each timestamp of the forecasting window. <br>\n",
       "\n",
       "**References:**<br>\n",
       "[Gouttes, Adèle, Kashif Rasul, Mateusz Koren, Johannes Stephan, and Tofigh Naghibi, \"Probabilistic Time Series Forecasting with Implicit Quantile Networks\".](http://arxiv.org/abs/2107.03743)*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/losses/pytorch.py#L643){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### IQLoss.__init__\n",
       "\n",
       ">      IQLoss.__init__ (cos_embedding_dim=64, concentration0=1.0,\n",
       ">                       concentration1=1.0, horizon_weight=None)\n",
       "\n",
       "*Implicit Quantile Loss\n",
       "\n",
       "Computes the quantile loss between `y` and `y_hat`, with the quantile `q` provided as an input to the network. \n",
       "IQL measures the deviation of a quantile forecast.\n",
       "By weighting the absolute deviation in a non symmetric way, the\n",
       "loss pays more attention to under or over estimation.\n",
       "\n",
       "$$ \\mathrm{QL}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}^{(q)}_{\\tau}) = \\frac{1}{H} \\sum^{t+H}_{\\tau=t+1} \\Big( (1-q)\\,( \\hat{y}^{(q)}_{\\tau} - y_{\\tau} )_{+} + q\\,( y_{\\tau} - \\hat{y}^{(q)}_{\\tau} )_{+} \\Big) $$\n",
       "\n",
       "**Parameters:**<br>\n",
       "`quantile_sampling`: str, default='uniform', sampling distribution used to sample the quantiles during training. Choose from ['uniform', 'beta']. <br>\n",
       "`horizon_weight`: Tensor of size h, weight for each timestamp of the forecasting window. <br>\n",
       "\n",
       "**References:**<br>\n",
       "[Gouttes, Adèle, Kashif Rasul, Mateusz Koren, Johannes Stephan, and Tofigh Naghibi, \"Probabilistic Time Series Forecasting with Implicit Quantile Networks\".](http://arxiv.org/abs/2107.03743)*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(IQLoss, name='IQLoss.__init__', title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de58e80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/losses/pytorch.py#L455){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### IQLoss.__call__\n",
       "\n",
       ">      IQLoss.__call__ (y:torch.Tensor, y_hat:torch.Tensor,\n",
       ">                       y_insample:torch.Tensor,\n",
       ">                       mask:Optional[torch.Tensor]=None)\n",
       "\n",
       "***Parameters:**<br>\n",
       "`y`: tensor, Actual values.<br>\n",
       "`y_hat`: tensor, Predicted values.<br>\n",
       "`mask`: tensor, Specifies datapoints to consider in loss.<br>\n",
       "\n",
       "**Returns:**<br>\n",
       "`quantile_loss`: tensor (single value).*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/losses/pytorch.py#L455){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### IQLoss.__call__\n",
       "\n",
       ">      IQLoss.__call__ (y:torch.Tensor, y_hat:torch.Tensor,\n",
       ">                       y_insample:torch.Tensor,\n",
       ">                       mask:Optional[torch.Tensor]=None)\n",
       "\n",
       "***Parameters:**<br>\n",
       "`y`: tensor, Actual values.<br>\n",
       "`y_hat`: tensor, Predicted values.<br>\n",
       "`mask`: tensor, Specifies datapoints to consider in loss.<br>\n",
       "\n",
       "**Returns:**<br>\n",
       "`quantile_loss`: tensor (single value).*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(IQLoss.__call__, name='IQLoss.__call__', title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee2992b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "# Unit tests\n",
    "# Check that default quantile is set to 0.5 at initialization\n",
    "check = IQLoss()\n",
    "test_eq(check.q, 0.5)\n",
    "\n",
    "# Check that quantiles are correctly updated - prediction\n",
    "check = IQLoss()\n",
    "check.update_quantile([0.7])\n",
    "test_eq(check.q, 0.7)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "895ec0c0",
   "metadata": {},
   "source": [
    "## DistributionLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801785b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def weighted_average(x: torch.Tensor, \n",
    "                     weights: Optional[torch.Tensor]=None, dim=None) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Computes the weighted average of a given tensor across a given dim, masking\n",
    "    values associated with weight zero,\n",
    "    meaning instead of `nan * 0 = nan` you will get `0 * 0 = 0`.\n",
    "\n",
    "    **Parameters:**<br>\n",
    "    `x`: Input tensor, of which the average must be computed.<br>\n",
    "    `weights`: Weights tensor, of the same shape as `x`.<br>\n",
    "    `dim`: The dim along which to average `x`.<br>\n",
    "\n",
    "    **Returns:**<br>\n",
    "    `Tensor`: The tensor with values averaged along the specified `dim`.<br>\n",
    "    \"\"\"\n",
    "    if weights is not None:\n",
    "        weighted_tensor = torch.where(\n",
    "            weights != 0, x * weights, torch.zeros_like(x)\n",
    "        )\n",
    "        sum_weights = torch.clamp(\n",
    "            weights.sum(dim=dim) if dim else weights.sum(), min=1.0\n",
    "        )\n",
    "        return (\n",
    "            weighted_tensor.sum(dim=dim) if dim else weighted_tensor.sum()\n",
    "        ) / sum_weights\n",
    "    else:\n",
    "        return x.mean(dim=dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b90c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def bernoulli_scale_decouple(output, loc=None, scale=None):\n",
    "    \"\"\" Bernoulli Scale Decouple\n",
    "\n",
    "    Stabilizes model's output optimization, by learning residual\n",
    "    variance and residual location based on anchoring `loc`, `scale`.\n",
    "    Also adds Bernoulli domain protection to the distribution parameters.\n",
    "    \"\"\"\n",
    "    probs = output[0]\n",
    "    #if (loc is not None) and (scale is not None):\n",
    "    #    rate = (rate * scale) + loc\n",
    "    probs = F.sigmoid(probs)#.clone()\n",
    "    return (probs,)\n",
    "\n",
    "def student_scale_decouple(output, loc=None, scale=None, eps: float=0.1):\n",
    "    \"\"\" Normal Scale Decouple\n",
    "\n",
    "    Stabilizes model's output optimization, by learning residual\n",
    "    variance and residual location based on anchoring `loc`, `scale`.\n",
    "    Also adds StudentT domain protection to the distribution parameters.\n",
    "    \"\"\"\n",
    "    df, mean, tscale = output\n",
    "    tscale = F.softplus(tscale)\n",
    "    if (loc is not None) and (scale is not None):\n",
    "        mean = (mean * scale) + loc\n",
    "        tscale = (tscale + eps) * scale\n",
    "    df = 3.0 + F.softplus(df)\n",
    "    return (df, mean, tscale)\n",
    "\n",
    "def normal_scale_decouple(output, loc=None, scale=None, eps: float=0.2):\n",
    "    \"\"\" Normal Scale Decouple\n",
    "\n",
    "    Stabilizes model's output optimization, by learning residual\n",
    "    variance and residual location based on anchoring `loc`, `scale`.\n",
    "    Also adds Normal domain protection to the distribution parameters.\n",
    "    \"\"\"\n",
    "    mean, std = output\n",
    "    std = F.softplus(std)\n",
    "    if (loc is not None) and (scale is not None):\n",
    "        mean = (mean * scale) + loc\n",
    "        std = (std + eps) * scale\n",
    "    return (mean, std)\n",
    "\n",
    "def poisson_scale_decouple(output, loc=None, scale=None):\n",
    "    \"\"\" Poisson Scale Decouple\n",
    "\n",
    "    Stabilizes model's output optimization, by learning residual\n",
    "    variance and residual location based on anchoring `loc`, `scale`.\n",
    "    Also adds Poisson domain protection to the distribution parameters.\n",
    "    \"\"\"\n",
    "    eps  = 1e-10\n",
    "    rate = output[0]\n",
    "    if (loc is not None) and (scale is not None):\n",
    "        rate = (rate * scale) + loc\n",
    "    rate = F.softplus(rate) + eps\n",
    "    return (rate, )\n",
    "\n",
    "def nbinomial_scale_decouple(output, loc=None, scale=None):\n",
    "    \"\"\" Negative Binomial Scale Decouple\n",
    "\n",
    "    Stabilizes model's output optimization, by learning total\n",
    "    count and logits based on anchoring `loc`, `scale`.\n",
    "    Also adds Negative Binomial domain protection to the distribution parameters.\n",
    "    \"\"\"\n",
    "    mu, alpha = output\n",
    "    mu = F.softplus(mu) + 1e-8\n",
    "    alpha = F.softplus(alpha) + 1e-8    # alpha = 1/total_counts\n",
    "    if (loc is not None) and (scale is not None):\n",
    "        mu = mu * scale + loc\n",
    "        alpha /= (scale + 1.)\n",
    "\n",
    "    # mu = total_count * (probs/(1-probs))\n",
    "    # => probs = mu / (total_count + mu)\n",
    "    # => probs = mu / [total_count * (1 + mu * (1/total_count))]\n",
    "    total_count = 1.0 / alpha\n",
    "    probs = (mu * alpha / (1.0 + mu * alpha)) + 1e-8\n",
    "    return (total_count, probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03294edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def est_lambda(mu, rho):\n",
    "    return mu ** (2 - rho) / (2 - rho)\n",
    "\n",
    "def est_alpha(rho):\n",
    "    return (2 - rho) / (rho - 1)\n",
    "\n",
    "def est_beta(mu, rho):\n",
    "    return mu ** (1 - rho) / (rho - 1)\n",
    "\n",
    "\n",
    "class Tweedie(Distribution):\n",
    "    \"\"\" Tweedie Distribution\n",
    "\n",
    "    The Tweedie distribution is a compound probability, special case of exponential\n",
    "    dispersion models EDMs defined by its mean-variance relationship.\n",
    "    The distribution particularly useful to model sparse series as the probability has\n",
    "    possitive mass at zero but otherwise is continuous.\n",
    "\n",
    "    $Y \\sim \\mathrm{ED}(\\\\mu,\\\\sigma^{2}) \\qquad\n",
    "    \\mathbb{P}(y|\\\\mu ,\\\\sigma^{2})=h(\\\\sigma^{2},y) \\\\exp \\\\left({\\\\frac {\\\\theta y-A(\\\\theta )}{\\\\sigma^{2}}}\\\\right)$<br>\n",
    "    \n",
    "    $\\mu =A'(\\\\theta ) \\qquad \\mathrm{Var}(Y) = \\\\sigma^{2} \\\\mu^{\\\\rho}$\n",
    "    \n",
    "    Cases of the variance relationship include Normal (`rho` = 0), Poisson (`rho` = 1),\n",
    "    Gamma (`rho` = 2), inverse Gaussian (`rho` = 3).\n",
    "\n",
    "    **Parameters:**<br>\n",
    "    `log_mu`: tensor, with log of means.<br>\n",
    "    `rho`: float, Tweedie variance power (1,2). Fixed across all observations.<br>\n",
    "    `sigma2`: tensor, Tweedie variance. Currently fixed in 1.<br>\n",
    "\n",
    "    **References:**<br>\n",
    "    - [Tweedie, M. C. K. (1984). An index which distinguishes between some important exponential families. Statistics: Applications and New Directions. \n",
    "    Proceedings of the Indian Statistical Institute Golden Jubilee International Conference (Eds. J. K. Ghosh and J. Roy), pp. 579-604. Calcutta: Indian Statistical Institute.]()<br>\n",
    "    - [Jorgensen, B. (1987). Exponential Dispersion Models. Journal of the Royal Statistical Society. \n",
    "       Series B (Methodological), 49(2), 127–162. http://www.jstor.org/stable/2345415](http://www.jstor.org/stable/2345415)<br>\n",
    "    \"\"\"\n",
    "    arg_constraints = {'log_mu': constraints.real}\n",
    "    support = constraints.nonnegative\n",
    "\n",
    "    def __init__(self, log_mu, rho, validate_args=None):\n",
    "        # TODO: add sigma2 dispersion\n",
    "        # TODO add constraints\n",
    "        # support = constraints.real\n",
    "        self.log_mu = log_mu\n",
    "        self.rho = rho\n",
    "        assert rho>1 and rho<2, f'rho={rho} parameter needs to be between (1,2).'\n",
    "\n",
    "        batch_shape = log_mu.size()\n",
    "        super(Tweedie, self).__init__(batch_shape, validate_args=validate_args)\n",
    "\n",
    "    @property\n",
    "    def mean(self):\n",
    "        return torch.exp(self.log_mu)\n",
    "\n",
    "    @property\n",
    "    def variance(self):\n",
    "        return torch.ones_line(self.log_mu) #TODO need to be assigned\n",
    "\n",
    "    def sample(self, sample_shape=torch.Size()):\n",
    "        shape = self._extended_shape(sample_shape)\n",
    "        with torch.no_grad():\n",
    "            mu   = self.mean\n",
    "            rho  = self.rho * torch.ones_like(mu)\n",
    "            sigma2 = 1 #TODO\n",
    "\n",
    "            rate  = est_lambda(mu, rho) / sigma2  # rate for poisson\n",
    "            alpha = est_alpha(rho)                # alpha for Gamma distribution\n",
    "            beta  = est_beta(mu, rho) / sigma2    # beta for Gamma distribution\n",
    "            \n",
    "            # Expand for sample\n",
    "            rate = rate.expand(shape)\n",
    "            alpha = alpha.expand(shape)\n",
    "            beta = beta.expand(shape)\n",
    "\n",
    "            N = torch.poisson(rate) + 1e-5\n",
    "            gamma = Gamma(N*alpha, beta)\n",
    "            samples = gamma.sample()\n",
    "            samples[N==0] = 0\n",
    "\n",
    "            return samples\n",
    "\n",
    "    def log_prob(self, y_true):\n",
    "        rho = self.rho\n",
    "        y_pred = self.log_mu\n",
    "\n",
    "        a = y_true * torch.exp((1 - rho) * y_pred) / (1 - rho)\n",
    "        b = torch.exp((2 - rho) * y_pred) / (2 - rho)\n",
    "\n",
    "        return a - b\n",
    "\n",
    "def tweedie_domain_map(input: torch.Tensor, rho: float = 1.5):\n",
    "    \"\"\"\n",
    "    Maps output of neural network to domain of distribution loss\n",
    "\n",
    "    \"\"\"\n",
    "    return (input, rho)\n",
    "\n",
    "def tweedie_scale_decouple(output, loc=None, scale=None):\n",
    "    \"\"\"Tweedie Scale Decouple\n",
    "\n",
    "    Stabilizes model's output optimization, by learning total\n",
    "    count and logits based on anchoring `loc`, `scale`.\n",
    "    Also adds Tweedie domain protection to the distribution parameters.\n",
    "    \"\"\"\n",
    "    log_mu, rho = output\n",
    "    log_mu = F.softplus(log_mu)\n",
    "    log_mu = torch.clamp(log_mu, 1e-9, 37)\n",
    "    if (loc is not None) and (scale is not None):\n",
    "        log_mu += torch.log(loc)\n",
    "\n",
    "    log_mu = torch.clamp(log_mu, 1e-9, 37)\n",
    "    return (log_mu, rho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5949c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "# Code adapted from: https://github.com/awslabs/gluonts/blob/61133ef6e2d88177b32ace4afc6843ab9a7bc8cd/src/gluonts/torch/distributions/isqf.py\n",
    "\n",
    "class ISQF(TransformedDistribution):\n",
    "    \"\"\"\n",
    "    Distribution class for the Incremental (Spline) Quantile Function.\n",
    "    \n",
    "    **Parameters:**<br>\n",
    "    `spline_knots`: Tensor parametrizing the x-positions of the spline knots. Shape: (*batch_shape, (num_qk-1), num_pieces)\n",
    "    `spline_heights`: Tensor parametrizing the y-positions of the spline knots. Shape: (*batch_shape, (num_qk-1), num_pieces)\n",
    "    `beta_l`: Tensor containing the non-negative learnable parameter of the left tail. Shape: (*batch_shape,)\n",
    "    `beta_r`: Tensor containing the non-negative learnable parameter of the right tail. Shape: (*batch_shape,)\n",
    "    `qk_y`: Tensor containing the increasing y-positions of the quantile knots. Shape: (*batch_shape, num_qk)\n",
    "    `qk_x`: Tensor containing the increasing x-positions of the quantile knots. Shape: (*batch_shape, num_qk)\n",
    "    `loc`: Tensor containing the location in case of a transformed random variable. Shape: (*batch_shape,)\n",
    "    `scale`: Tensor containing the scale in case of a transformed random variable. Shape: (*batch_shape,)\n",
    "\n",
    "    **References:**<br>\n",
    "    [Park, Youngsuk, Danielle Maddix, François-Xavier Aubet, Kelvin Kan, Jan Gasthaus, and Yuyang Wang (2022). \"Learning Quantile Functions without Quantile Crossing for Distribution-free Time Series Forecasting\".](https://proceedings.mlr.press/v151/park22a.html)\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        spline_knots: torch.Tensor,\n",
    "        spline_heights: torch.Tensor,\n",
    "        beta_l: torch.Tensor,\n",
    "        beta_r: torch.Tensor,\n",
    "        qk_y: torch.Tensor,\n",
    "        qk_x: torch.Tensor,\n",
    "        loc: torch.Tensor,\n",
    "        scale: torch.Tensor,        \n",
    "        validate_args=None,\n",
    "    ) -> None:\n",
    "        base_distribution = BaseISQF(spline_knots=spline_knots,\n",
    "                                 spline_heights=spline_heights,\n",
    "                                 beta_l=beta_l,\n",
    "                                 beta_r=beta_r,\n",
    "                                 qk_y=qk_y,\n",
    "                                 qk_x=qk_x,\n",
    "                                 validate_args=validate_args)\n",
    "        transforms = AffineTransform(loc = loc, scale = scale)\n",
    "        super().__init__(\n",
    "            base_distribution, transforms, validate_args=validate_args\n",
    "        )\n",
    "\n",
    "    def crps(self, y: torch.Tensor) -> torch.Tensor:\n",
    "        z = y\n",
    "        scale = 1.0\n",
    "        t = self.transforms[0]\n",
    "        z = t._inverse(z)\n",
    "        scale *= t.scale\n",
    "        p = self.base_dist.crps(z)\n",
    "        return p * scale\n",
    "    \n",
    "    @property\n",
    "    def mean(self):\n",
    "        \"\"\"\n",
    "        Function used to compute the empirical mean\n",
    "        \"\"\"\n",
    "        samples = self.sample([1000])\n",
    "        return samples.mean(dim=0)\n",
    "        \n",
    "\n",
    "class BaseISQF(Distribution):\n",
    "    \"\"\"\n",
    "    Base distribution class for the Incremental (Spline) Quantile Function.\n",
    "    \n",
    "    **Parameters:**<br>\n",
    "    `spline_knots`: Tensor parametrizing the x-positions of the spline knots. Shape: (*batch_shape, (num_qk-1), num_pieces)\n",
    "    `spline_heights`: Tensor parametrizing the y-positions of the spline knots. Shape: (*batch_shape, (num_qk-1), num_pieces)\n",
    "    `beta_l`: Tensor containing the non-negative learnable parameter of the left tail. (*batch_shape,)\n",
    "    `beta_r`: Tensor containing the non-negative learnable parameter of the right tail. (*batch_shape,)\n",
    "    `qk_y`: Tensor containing the increasing y-positions of the quantile knots. Shape: (*batch_shape, num_qk)\n",
    "    `qk_x`: Tensor containing the increasing x-positions of the quantile knots. Shape: (*batch_shape, num_qk)\n",
    "\n",
    "    **References:**<br>\n",
    "    [Park, Youngsuk, Danielle Maddix, François-Xavier Aubet, Kelvin Kan, Jan Gasthaus, and Yuyang Wang (2022). \"Learning Quantile Functions without Quantile Crossing for Distribution-free Time Series Forecasting\".](https://proceedings.mlr.press/v151/park22a.html)\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        spline_knots: torch.Tensor,\n",
    "        spline_heights: torch.Tensor,\n",
    "        beta_l: torch.Tensor,\n",
    "        beta_r: torch.Tensor,\n",
    "        qk_y: torch.Tensor,\n",
    "        qk_x: torch.Tensor,\n",
    "        tol: float = 1e-4,\n",
    "        validate_args: bool = False,\n",
    "    ) -> None:\n",
    "        self.num_qk, self.num_pieces = qk_y.shape[-1], spline_knots.shape[-1]\n",
    "        self.spline_knots, self.spline_heights = spline_knots, spline_heights\n",
    "        self.beta_l, self.beta_r = beta_l, beta_r\n",
    "        self.qk_y_all = qk_y\n",
    "        self.tol = tol\n",
    "\n",
    "        super().__init__(\n",
    "            batch_shape=self.batch_shape, validate_args=validate_args\n",
    "        )\n",
    "\n",
    "        # Get quantile knots (qk) parameters\n",
    "        (\n",
    "            self.qk_x,\n",
    "            self.qk_x_plus,\n",
    "            self.qk_x_l,\n",
    "            self.qk_x_r,\n",
    "        ) = BaseISQF.parameterize_qk(qk_x)\n",
    "        (\n",
    "            self.qk_y,\n",
    "            self.qk_y_plus,\n",
    "            self.qk_y_l,\n",
    "            self.qk_y_r,\n",
    "        ) = BaseISQF.parameterize_qk(qk_y)\n",
    "\n",
    "        # Get spline knots (sk) parameters\n",
    "        self.sk_y, self.delta_sk_y = BaseISQF.parameterize_spline(\n",
    "            self.spline_heights,\n",
    "            self.qk_y,\n",
    "            self.qk_y_plus,\n",
    "            self.tol,\n",
    "        )\n",
    "        self.sk_x, self.delta_sk_x = BaseISQF.parameterize_spline(\n",
    "            self.spline_knots,\n",
    "            self.qk_x,\n",
    "            self.qk_x_plus,\n",
    "            self.tol,\n",
    "        )\n",
    "\n",
    "        if self.num_pieces > 1:\n",
    "            self.sk_x_plus = torch.cat(\n",
    "                [self.sk_x[..., 1:], self.qk_x_plus.unsqueeze(dim=-1)], dim=-1\n",
    "            )\n",
    "        else:\n",
    "            self.sk_x_plus = self.qk_x_plus.unsqueeze(dim=-1)\n",
    "\n",
    "        # Get tails parameters\n",
    "        self.tail_al, self.tail_bl = BaseISQF.parameterize_tail(\n",
    "            self.beta_l, self.qk_x_l, self.qk_y_l\n",
    "        )\n",
    "        self.tail_ar, self.tail_br = BaseISQF.parameterize_tail(\n",
    "            -self.beta_r, 1 - self.qk_x_r, self.qk_y_r\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def parameterize_qk(\n",
    "        quantile_knots: torch.Tensor,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        r\"\"\"\n",
    "        Function to parameterize the x or y positions\n",
    "        of the num_qk quantile knots\n",
    "        Parameters\n",
    "        ----------\n",
    "        quantile_knots\n",
    "            x or y positions of the quantile knots\n",
    "            shape: (*batch_shape, num_qk)\n",
    "        Returns\n",
    "        -------\n",
    "        qk\n",
    "            x or y positions of the quantile knots (qk),\n",
    "            with index=1, ..., num_qk-1,\n",
    "            shape: (*batch_shape, num_qk-1)\n",
    "        qk_plus\n",
    "            x or y positions of the quantile knots (qk),\n",
    "            with index=2, ..., num_qk,\n",
    "            shape: (*batch_shape, num_qk-1)\n",
    "        qk_l\n",
    "            x or y positions of the left-most quantile knot (qk),\n",
    "            shape: (*batch_shape)\n",
    "        qk_r\n",
    "            x or y positions of the right-most quantile knot (qk),\n",
    "            shape: (*batch_shape)\n",
    "        \"\"\"\n",
    "\n",
    "        qk, qk_plus = quantile_knots[..., :-1], quantile_knots[..., 1:]\n",
    "        qk_l, qk_r = quantile_knots[..., 0], quantile_knots[..., -1]\n",
    "\n",
    "        return qk, qk_plus, qk_l, qk_r\n",
    "\n",
    "    @staticmethod\n",
    "    def parameterize_spline(\n",
    "        spline_knots: torch.Tensor,\n",
    "        qk: torch.Tensor,\n",
    "        qk_plus: torch.Tensor,\n",
    "        tol: float = 1e-4,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        r\"\"\"\n",
    "        Function to parameterize the x or y positions of the spline knots\n",
    "        Parameters\n",
    "        ----------\n",
    "        spline_knots\n",
    "            variable that parameterizes the spline knot positions\n",
    "        qk\n",
    "            x or y positions of the quantile knots (qk),\n",
    "            with index=1, ..., num_qk-1,\n",
    "            shape: (*batch_shape, num_qk-1)\n",
    "        qk_plus\n",
    "            x or y positions of the quantile knots (qk),\n",
    "            with index=2, ..., num_qk,\n",
    "            shape: (*batch_shape, num_qk-1)\n",
    "        num_pieces\n",
    "            number of spline knot pieces\n",
    "        tol\n",
    "            tolerance hyperparameter for numerical stability\n",
    "        Returns\n",
    "        -------\n",
    "        sk\n",
    "            x or y positions of the spline knots (sk),\n",
    "            shape: (*batch_shape, num_qk-1, num_pieces)\n",
    "        delta_sk\n",
    "            difference of x or y positions of the spline knots (sk),\n",
    "            shape: (*batch_shape, num_qk-1, num_pieces)\n",
    "        \"\"\"\n",
    "\n",
    "        # The spacing between spline knots is parameterized\n",
    "        # by softmax function (in [0,1] and sum to 1)\n",
    "        # We add tol to prevent overflow in computing 1/spacing in spline CRPS\n",
    "        # After adding tol, it is normalized by\n",
    "        # (1 + num_pieces * tol) to keep the sum-to-1 property\n",
    "\n",
    "        num_pieces = spline_knots.shape[-1]\n",
    "\n",
    "        delta_x = (F.softmax(spline_knots, dim=-1) + tol) / (\n",
    "            1 + num_pieces * tol\n",
    "        )\n",
    "\n",
    "        zero_tensor = torch.zeros_like(\n",
    "            delta_x[..., 0:1]\n",
    "        )  # 0:1 for keeping dimension\n",
    "        x = torch.cat(\n",
    "            [zero_tensor, torch.cumsum(delta_x, dim=-1)[..., :-1]], dim=-1\n",
    "        )\n",
    "\n",
    "        qk, qk_plus = qk.unsqueeze(dim=-1), qk_plus.unsqueeze(dim=-1)\n",
    "        sk = x * (qk_plus - qk) + qk\n",
    "        delta_sk = delta_x * (qk_plus - qk)\n",
    "\n",
    "        return sk, delta_sk\n",
    "\n",
    "    @staticmethod\n",
    "    def parameterize_tail(\n",
    "        beta: torch.Tensor, qk_x: torch.Tensor, qk_y: torch.Tensor\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        r\"\"\"\n",
    "        Function to parameterize the tail parameters\n",
    "        Note that the exponential tails are given by\n",
    "        q(alpha)\n",
    "        = a_l log(alpha) + b_l if left tail\n",
    "        = a_r log(1-alpha) + b_r if right tail\n",
    "        where\n",
    "        a_l=1/beta_l, b_l=-a_l*log(qk_x_l)+q(qk_x_l)\n",
    "        a_r=1/beta_r, b_r=a_r*log(1-qk_x_r)+q(qk_x_r)\n",
    "        Parameters\n",
    "        ----------\n",
    "        beta\n",
    "            parameterizes the left or right tail, shape: (*batch_shape,)\n",
    "        qk_x\n",
    "            left- or right-most x-positions of the quantile knots,\n",
    "            shape: (*batch_shape,)\n",
    "        qk_y\n",
    "            left- or right-most y-positions of the quantile knots,\n",
    "            shape: (*batch_shape,)\n",
    "        Returns\n",
    "        -------\n",
    "        tail_a\n",
    "            a_l or a_r as described above\n",
    "        tail_b\n",
    "            b_l or b_r as described above\n",
    "        \"\"\"\n",
    "\n",
    "        tail_a = 1 / beta\n",
    "        tail_b = -tail_a * torch.log(qk_x) + qk_y\n",
    "\n",
    "        return tail_a, tail_b\n",
    "\n",
    "    def quantile(self, alpha: torch.Tensor) -> torch.Tensor:\n",
    "        return self.quantile_internal(alpha, dim=0)\n",
    "\n",
    "    def quantile_internal(\n",
    "        self, alpha: torch.Tensor, dim: Optional[int] = None\n",
    "    ) -> torch.Tensor:\n",
    "        r\"\"\"\n",
    "        Evaluates the quantile function at the quantile levels input_alpha\n",
    "        Parameters\n",
    "        ----------\n",
    "        alpha\n",
    "            Tensor of shape = (*batch_shape,) if axis=None, or containing an\n",
    "            additional axis on the specified position, otherwise\n",
    "        dim\n",
    "            Index of the axis containing the different quantile levels which\n",
    "            are to be computed.\n",
    "            Read the description below for detailed information\n",
    "        Returns\n",
    "        -------\n",
    "        Tensor\n",
    "            Quantiles tensor, of the same shape as alpha\n",
    "        \"\"\"\n",
    "\n",
    "        qk_x, qk_x_l, qk_x_plus = self.qk_x, self.qk_x_l, self.qk_x_plus\n",
    "\n",
    "        # The following describes the parameters reshaping in\n",
    "        # quantile_internal, quantile_spline and quantile_tail\n",
    "\n",
    "        # tail parameters: tail_al, tail_ar, tail_bl, tail_br,\n",
    "        # shape = (*batch_shape,)\n",
    "        # spline parameters: sk_x, sk_x_plus, sk_y, sk_y_plus,\n",
    "        # shape = (*batch_shape, num_qk-1, num_pieces)\n",
    "        # quantile knots parameters: qk_x, qk_x_plus, qk_y, qk_y_plus,\n",
    "        # shape = (*batch_shape, num_qk-1)\n",
    "\n",
    "        # dim=None - passed at inference when num_samples is None\n",
    "        # shape of input_alpha = (*batch_shape,), will be expanded to\n",
    "        # (*batch_shape, 1, 1) to perform operation\n",
    "        # The shapes of parameters are as described above,\n",
    "        # no reshaping is needed\n",
    "\n",
    "        # dim=0 - passed at inference when num_samples is not None\n",
    "        # shape of input_alpha = (num_samples, *batch_shape)\n",
    "        # it will be expanded to\n",
    "        # (num_samples, *batch_shape, 1, 1) to perform operation\n",
    "        #\n",
    "        # The shapes of tail parameters\n",
    "        # should be (num_samples, *batch_shape)\n",
    "        #\n",
    "        # The shapes of spline parameters\n",
    "        # should be (num_samples, *batch_shape, num_qk-1, num_pieces)\n",
    "        #\n",
    "        # The shapes of quantile knots parameters\n",
    "        # should be (num_samples, *batch_shape, num_qk-1)\n",
    "        #\n",
    "        # We expand at dim=0 for all of them\n",
    "\n",
    "        # dim=-2 - passed at training when we evaluate quantiles at\n",
    "        # spline knots in order to compute alpha_tilde\n",
    "        #\n",
    "        # This is only for the quantile_spline function\n",
    "        # shape of input_alpha = (*batch_shape, num_qk-1, num_pieces)\n",
    "        # it will be expanded to\n",
    "        # (*batch_shape, num_qk-1, num_pieces, 1) to perform operation\n",
    "        #\n",
    "        # The shapes of spline and quantile knots parameters should be\n",
    "        # (*batch_shape, num_qk-1, 1, num_pieces)\n",
    "        # and (*batch_shape, num_qk-1, 1), respectively\n",
    "        #\n",
    "        # We expand at dim=-2 and dim=-1 for\n",
    "        # spline and quantile knots parameters, respectively\n",
    "\n",
    "        if dim is not None:\n",
    "            qk_x_l = qk_x_l.unsqueeze(dim=dim)\n",
    "            qk_x = qk_x.unsqueeze(dim=dim)\n",
    "            qk_x_plus = qk_x_plus.unsqueeze(dim=dim)\n",
    "\n",
    "        quantile = torch.where(\n",
    "            alpha < qk_x_l,\n",
    "            self.quantile_tail(alpha, dim=dim, left_tail=True),\n",
    "            self.quantile_tail(alpha, dim=dim, left_tail=False),\n",
    "        )\n",
    "\n",
    "        spline_val = self.quantile_spline(alpha, dim=dim)\n",
    "\n",
    "        for spline_idx in range(self.num_qk - 1):\n",
    "            is_in_between = torch.logical_and(\n",
    "                qk_x[..., spline_idx] <= alpha,\n",
    "                alpha < qk_x_plus[..., spline_idx],\n",
    "            )\n",
    "\n",
    "            quantile = torch.where(\n",
    "                is_in_between,\n",
    "                spline_val[..., spline_idx],\n",
    "                quantile,\n",
    "            )\n",
    "\n",
    "        return quantile\n",
    "\n",
    "    def quantile_spline(\n",
    "        self,\n",
    "        alpha: torch.Tensor,\n",
    "        dim: Optional[int] = None,\n",
    "    ) -> torch.Tensor:\n",
    "        # Refer to the description in quantile_internal\n",
    "\n",
    "        qk_y = self.qk_y\n",
    "        sk_x, delta_sk_x, delta_sk_y = (\n",
    "            self.sk_x,\n",
    "            self.delta_sk_x,\n",
    "            self.delta_sk_y,\n",
    "        )\n",
    "\n",
    "        if dim is not None:\n",
    "            qk_y = qk_y.unsqueeze(dim=0 if dim == 0 else -1)\n",
    "            sk_x = sk_x.unsqueeze(dim=dim)\n",
    "            delta_sk_x = delta_sk_x.unsqueeze(dim=dim)\n",
    "            delta_sk_y = delta_sk_y.unsqueeze(dim=dim)\n",
    "\n",
    "        if dim is None or dim == 0:\n",
    "            alpha = alpha.unsqueeze(dim=-1)\n",
    "\n",
    "        alpha = alpha.unsqueeze(dim=-1)\n",
    "\n",
    "        spline_val = (alpha - sk_x) / delta_sk_x\n",
    "        spline_val = torch.maximum(\n",
    "            torch.minimum(spline_val, torch.ones_like(spline_val)),\n",
    "            torch.zeros_like(spline_val),\n",
    "        )\n",
    "\n",
    "        return qk_y + torch.sum(spline_val * delta_sk_y, dim=-1)\n",
    "\n",
    "    def quantile_tail(\n",
    "        self,\n",
    "        alpha: torch.Tensor,\n",
    "        dim: Optional[int] = None,\n",
    "        left_tail: bool = True,\n",
    "    ) -> torch.Tensor:\n",
    "        # Refer to the description in quantile_internal\n",
    "\n",
    "        if left_tail:\n",
    "            tail_a, tail_b = self.tail_al, self.tail_bl\n",
    "        else:\n",
    "            tail_a, tail_b = self.tail_ar, self.tail_br\n",
    "            alpha = 1 - alpha\n",
    "\n",
    "        if dim is not None:\n",
    "            tail_a, tail_b = tail_a.unsqueeze(dim=dim), tail_b.unsqueeze(\n",
    "                dim=dim\n",
    "            )\n",
    "\n",
    "        return tail_a * torch.log(alpha) + tail_b\n",
    "\n",
    "    def cdf_spline(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        r\"\"\"\n",
    "        For observations z and splines defined in [qk_x[k], qk_x[k+1]]\n",
    "        Computes the quantile level alpha_tilde such that\n",
    "        alpha_tilde\n",
    "        = q^{-1}(z) if z is in-between qk_x[k] and qk_x[k+1]\n",
    "        = qk_x[k] if z<qk_x[k]\n",
    "        = qk_x[k+1] if z>qk_x[k+1]\n",
    "        Parameters\n",
    "        ----------\n",
    "        z\n",
    "            Observation, shape = (*batch_shape,)\n",
    "        Returns\n",
    "        -------\n",
    "        alpha_tilde\n",
    "            Corresponding quantile level, shape = (*batch_shape, num_qk-1)\n",
    "        \"\"\"\n",
    "\n",
    "        qk_y, qk_y_plus = self.qk_y, self.qk_y_plus\n",
    "        qk_x, qk_x_plus = self.qk_x, self.qk_x_plus\n",
    "        sk_x, delta_sk_x, delta_sk_y = (\n",
    "            self.sk_x,\n",
    "            self.delta_sk_x,\n",
    "            self.delta_sk_y,\n",
    "        )\n",
    "\n",
    "        z_expand = z.unsqueeze(dim=-1)\n",
    "\n",
    "        if self.num_pieces > 1:\n",
    "            qk_y_expand = qk_y.unsqueeze(dim=-1)\n",
    "            z_expand_twice = z_expand.unsqueeze(dim=-1)\n",
    "\n",
    "            knots_eval = self.quantile_spline(sk_x, dim=-2)\n",
    "\n",
    "            # Compute \\sum_{s=0}^{s_0-1} \\Delta sk_y[s],\n",
    "            # where \\Delta sk_y[s] = (sk_y[s+1]-sk_y[s])\n",
    "            mask_sum_s0 = torch.lt(knots_eval, z_expand_twice)\n",
    "            mask_sum_s0_minus = torch.cat(\n",
    "                [\n",
    "                    mask_sum_s0[..., 1:],\n",
    "                    torch.zeros_like(qk_y_expand, dtype=torch.bool),\n",
    "                ],\n",
    "                dim=-1,\n",
    "            )\n",
    "            sum_delta_sk_y = torch.sum(mask_sum_s0_minus * delta_sk_y, dim=-1)\n",
    "\n",
    "            mask_s0_only = torch.logical_and(\n",
    "                mask_sum_s0, torch.logical_not(mask_sum_s0_minus)\n",
    "            )\n",
    "            # Compute (sk_x[s_0+1]-sk_x[s_0])/(sk_y[s_0+1]-sk_y[s_0])\n",
    "            frac_s0 = torch.sum(\n",
    "                (mask_s0_only * delta_sk_x) / delta_sk_y, dim=-1\n",
    "            )\n",
    "\n",
    "            # Compute sk_x_{s_0}\n",
    "            sk_x_s0 = torch.sum(mask_s0_only * sk_x, dim=-1)\n",
    "\n",
    "            # Compute alpha_tilde\n",
    "            alpha_tilde = (\n",
    "                sk_x_s0 + (z_expand - qk_y - sum_delta_sk_y) * frac_s0\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            # num_pieces=1, ISQF reduces to IQF\n",
    "            alpha_tilde = qk_x + (z_expand - qk_y) / (qk_y_plus - qk_y) * (\n",
    "                qk_x_plus - qk_x\n",
    "            )\n",
    "\n",
    "        alpha_tilde = torch.minimum(\n",
    "            torch.maximum(alpha_tilde, qk_x), qk_x_plus\n",
    "        )\n",
    "\n",
    "        return alpha_tilde\n",
    "\n",
    "    def cdf_tail(\n",
    "        self, z: torch.Tensor, left_tail: bool = True\n",
    "    ) -> torch.Tensor:\n",
    "        r\"\"\"\n",
    "        Computes the quantile level alpha_tilde such that\n",
    "        alpha_tilde\n",
    "        = q^{-1}(z) if z is in the tail region\n",
    "        = qk_x_l or qk_x_r if z is in the non-tail region\n",
    "        Parameters\n",
    "        ----------\n",
    "        z\n",
    "            Observation, shape = (*batch_shape,)\n",
    "        left_tail\n",
    "            If True, compute alpha_tilde for the left tail\n",
    "            Otherwise, compute alpha_tilde for the right tail\n",
    "        Returns\n",
    "        -------\n",
    "        alpha_tilde\n",
    "            Corresponding quantile level, shape = (*batch_shape,)\n",
    "        \"\"\"\n",
    "\n",
    "        if left_tail:\n",
    "            tail_a, tail_b, qk_x = self.tail_al, self.tail_bl, self.qk_x_l\n",
    "        else:\n",
    "            tail_a, tail_b, qk_x = self.tail_ar, self.tail_br, 1 - self.qk_x_r\n",
    "\n",
    "        log_alpha_tilde = torch.minimum((z - tail_b) / tail_a, torch.log(qk_x))\n",
    "        alpha_tilde = torch.exp(log_alpha_tilde)\n",
    "        return alpha_tilde if left_tail else 1 - alpha_tilde\n",
    "\n",
    "    def crps_tail(\n",
    "        self, z: torch.Tensor, left_tail: bool = True\n",
    "    ) -> torch.Tensor:\n",
    "        r\"\"\"\n",
    "        Compute CRPS in analytical form for left/right tails\n",
    "        Parameters\n",
    "        ----------\n",
    "        z\n",
    "            Observation to evaluate. shape = (*batch_shape,)\n",
    "        left_tail\n",
    "            If True, compute CRPS for the left tail\n",
    "            Otherwise, compute CRPS for the right tail\n",
    "        Returns\n",
    "        -------\n",
    "        Tensor\n",
    "            Tensor containing the CRPS, of the same shape as z\n",
    "        \"\"\"\n",
    "\n",
    "        alpha_tilde = self.cdf_tail(z, left_tail=left_tail)\n",
    "\n",
    "        if left_tail:\n",
    "            tail_a, tail_b, qk_x, qk_y = (\n",
    "                self.tail_al,\n",
    "                self.tail_bl,\n",
    "                self.qk_x_l,\n",
    "                self.qk_y_l,\n",
    "            )\n",
    "            term1 = (z - tail_b) * (qk_x**2 - 2 * qk_x + 2 * alpha_tilde)\n",
    "            term2 = qk_x**2 * tail_a * (-torch.log(qk_x) + 0.5)\n",
    "            term2 = term2 + 2 * torch.where(\n",
    "                z < qk_y,\n",
    "                qk_x * tail_a * (torch.log(qk_x) - 1)\n",
    "                + alpha_tilde * (-z + tail_b + tail_a),\n",
    "                torch.zeros_like(qk_x),\n",
    "            )\n",
    "        else:\n",
    "            tail_a, tail_b, qk_x, qk_y = (\n",
    "                self.tail_ar,\n",
    "                self.tail_br,\n",
    "                self.qk_x_r,\n",
    "                self.qk_y_r,\n",
    "            )\n",
    "            term1 = (z - tail_b) * (-1 - qk_x**2 + 2 * alpha_tilde)\n",
    "            term2 = tail_a * (\n",
    "                -0.5 * (qk_x + 1) ** 2\n",
    "                + (qk_x**2 - 1) * torch.log(1 - qk_x)\n",
    "                + 2 * alpha_tilde\n",
    "            )\n",
    "            term2 = term2 + 2 * torch.where(\n",
    "                z > qk_y,\n",
    "                (1 - alpha_tilde) * (z - tail_b),\n",
    "                tail_a * (1 - qk_x) * torch.log(1 - qk_x),\n",
    "            )\n",
    "\n",
    "        return term1 + term2\n",
    "\n",
    "    def crps_spline(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        \n",
    "        Compute CRPS in analytical form for the spline\n",
    "        \n",
    "        **Parameters**<br>\n",
    "        `z`: Observation to evaluate.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        qk_x, qk_x_plus, qk_y = self.qk_x, self.qk_x_plus, self.qk_y\n",
    "        sk_x, sk_x_plus = self.sk_x, self.sk_x_plus\n",
    "        delta_sk_x, delta_sk_y = self.delta_sk_x, self.delta_sk_y\n",
    "\n",
    "        z_expand = z.unsqueeze(dim=-1)\n",
    "        qk_x_plus_expand = qk_x_plus.unsqueeze(dim=-1)\n",
    "\n",
    "        alpha_tilde = self.cdf_spline(z)\n",
    "        alpha_tilde_expand = alpha_tilde.unsqueeze(dim=-1)\n",
    "\n",
    "        r = torch.minimum(torch.maximum(alpha_tilde_expand, sk_x), sk_x_plus)\n",
    "\n",
    "        coeff1 = (\n",
    "            -2 / 3 * sk_x_plus**3\n",
    "            + sk_x * sk_x_plus**2\n",
    "            + sk_x_plus**2\n",
    "            - (1 / 3) * sk_x**3\n",
    "            - 2 * sk_x * sk_x_plus\n",
    "            - r**2\n",
    "            + 2 * sk_x * r\n",
    "        )\n",
    "\n",
    "        coeff2 = (\n",
    "            -2 * torch.maximum(alpha_tilde_expand, sk_x_plus)\n",
    "            + sk_x_plus**2\n",
    "            + 2 * qk_x_plus_expand\n",
    "            - qk_x_plus_expand**2\n",
    "        )\n",
    "\n",
    "        result = (\n",
    "            (qk_x_plus**2 - qk_x**2) * (z_expand - qk_y)\n",
    "            + 2 * (qk_x_plus - alpha_tilde) * (qk_y - z_expand)\n",
    "            + torch.sum((delta_sk_y / delta_sk_x) * coeff1, dim=-1)\n",
    "            + torch.sum(delta_sk_y * coeff2, dim=-1)\n",
    "        )\n",
    "\n",
    "        return torch.sum(result, dim=-1)\n",
    "\n",
    "    def loss(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        return self.crps(z)\n",
    "    \n",
    "    def log_prob(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        return -self.crps(z)\n",
    "\n",
    "    def crps(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute CRPS in analytical form\n",
    "        \n",
    "        **Parameters**\n",
    "\n",
    "        `z`: Observation to evaluate.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        crps_lt = self.crps_tail(z, left_tail=True)\n",
    "        crps_rt = self.crps_tail(z, left_tail=False)\n",
    "\n",
    "        return crps_lt + crps_rt + self.crps_spline(z)\n",
    "\n",
    "    def cdf(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Computes the quantile level alpha_tilde such that\n",
    "        q(alpha_tilde) = z\n",
    "        \n",
    "        **Parameters**\n",
    "\n",
    "        `z`: Tensor of shape = (*batch_shape,)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        qk_y, qk_y_l, qk_y_plus = self.qk_y, self.qk_y_l, self.qk_y_plus\n",
    "\n",
    "        alpha_tilde = torch.where(\n",
    "            z < qk_y_l,\n",
    "            self.cdf_tail(z, left_tail=True),\n",
    "            self.cdf_tail(z, left_tail=False),\n",
    "        )\n",
    "\n",
    "        spline_alpha_tilde = self.cdf_spline(z)\n",
    "\n",
    "        for spline_idx in range(self.num_qk - 1):\n",
    "            is_in_between = torch.logical_and(\n",
    "                qk_y[..., spline_idx] <= z, z < qk_y_plus[..., spline_idx]\n",
    "            )\n",
    "\n",
    "            alpha_tilde = torch.where(\n",
    "                is_in_between, spline_alpha_tilde[..., spline_idx], alpha_tilde\n",
    "            )\n",
    "\n",
    "        return alpha_tilde\n",
    "\n",
    "    def rsample(self, sample_shape: torch.Size = torch.Size()) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Function used to draw random samples\n",
    "        \n",
    "        **Parameters**\n",
    "\n",
    "        `num_samples`: number of samples\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # if sample_shape=()) then input_alpha should have the same shape\n",
    "        # as beta_l, i.e., (*batch_shape,)\n",
    "        # else u should be (*sample_shape, *batch_shape)\n",
    "        target_shape = (\n",
    "            self.beta_l.shape\n",
    "            if sample_shape == torch.Size()\n",
    "            else torch.Size(sample_shape) + self.beta_l.shape\n",
    "        )\n",
    "\n",
    "        alpha = torch.rand(\n",
    "            target_shape,\n",
    "            dtype=self.beta_l.dtype,\n",
    "            device=self.beta_l.device,\n",
    "            layout=self.beta_l.layout,\n",
    "        )\n",
    "\n",
    "        sample = self.quantile(alpha)\n",
    "\n",
    "        if sample_shape == torch.Size():\n",
    "            sample = sample.squeeze(dim=0)\n",
    "\n",
    "        return sample\n",
    "\n",
    "    @property\n",
    "    def batch_shape(self) -> torch.Size:\n",
    "        return self.beta_l.shape\n",
    "\n",
    "def isqf_domain_map(input: torch.Tensor, tol: float=1e-4, quantiles: torch.Tensor = torch.tensor([0.1, 0.5, 0.9], dtype=torch.float32), num_pieces: int = 5):\n",
    "    \"\"\" ISQF Domain Map\n",
    "    Maps input into distribution constraints, by construction input's \n",
    "    last dimension is of matching `distr_args` length.\n",
    "\n",
    "    **Parameters:**<br>\n",
    "    `input`: tensor, of dimensions [B, H, N * n_outputs].<br>\n",
    "    `tol`: float, tolerance.<br>\n",
    "    `quantiles`: tensor, quantiles used for ISQF (i.e. x-positions for the knots). <br>\n",
    "    `num_pieces`: int, num_pieces used for each quantile spline. <br>\n",
    "\n",
    "    **Returns:**<br>\n",
    "    `(spline_knots, spline_heights, beta_l, beta_r, qk_y, qk_x)`: tuple with tensors of ISQF distribution arguments.<br>\n",
    "    \"\"\"\n",
    "\n",
    "    # Add tol to prevent the y-distance of\n",
    "    # two quantile knots from being too small\n",
    "    #\n",
    "    # Because in this case the spline knots could be squeezed together\n",
    "    # and cause overflow in spline CRPS computation\n",
    "    num_qk = len(quantiles)\n",
    "    n_outputs = 2 * (num_qk - 1) * num_pieces + 2 + num_qk\n",
    "    \n",
    "    # Reshape: [B, h, N * n_outputs] -> [B, h, N, n_outputs]\n",
    "    input = input.reshape(input.shape[0],\n",
    "                          input.shape[1],\n",
    "                          -1,\n",
    "                          n_outputs)\n",
    "    start_index = 0\n",
    "    spline_knots = input[..., start_index: start_index + (num_qk - 1) * num_pieces]\n",
    "    start_index += (num_qk - 1) * num_pieces\n",
    "    spline_heights = input[..., start_index: start_index + (num_qk - 1) * num_pieces]\n",
    "    start_index += (num_qk - 1) * num_pieces\n",
    "    beta_l = input[..., start_index: start_index + 1]\n",
    "    start_index += 1\n",
    "    beta_r = input[..., start_index: start_index + 1]\n",
    "    start_index += 1\n",
    "    quantile_knots = F.softplus(input[..., start_index: start_index + num_qk]) + tol\n",
    "\n",
    "    qk_y = torch.cumsum(quantile_knots, dim=-1)\n",
    "\n",
    "    # Prevent overflow when we compute 1/beta\n",
    "    beta_l = F.softplus(beta_l.squeeze(-1)) + tol\n",
    "    beta_r = F.softplus(beta_r.squeeze(-1)) + tol\n",
    "\n",
    "    # Reshape spline arguments\n",
    "    batch_shape = spline_knots.shape[:-1]\n",
    "\n",
    "    # repeat qk_x from (num_qk,) to (*batch_shape, num_qk)\n",
    "    qk_x_repeat = quantiles\\\n",
    "                       .repeat(*batch_shape, 1)\\\n",
    "                       .to(input.device)\n",
    "\n",
    "    # knots and heights have shape (*batch_shape, (num_qk-1)*num_pieces)\n",
    "    # reshape them to (*batch_shape, (num_qk-1), num_pieces)\n",
    "    spline_knots_reshape = spline_knots.reshape(\n",
    "        *batch_shape, (num_qk - 1), num_pieces\n",
    "    )\n",
    "    spline_heights_reshape = spline_heights.reshape(\n",
    "        *batch_shape, (num_qk - 1), num_pieces\n",
    "    )\n",
    "\n",
    "    return spline_knots_reshape, spline_heights_reshape, beta_l, beta_r, qk_y, qk_x_repeat\n",
    "\n",
    "def isqf_scale_decouple(output, loc=None, scale=None):\n",
    "    \"\"\" ISQF Scale Decouple\n",
    "\n",
    "    Stabilizes model's output optimization. We simply pass through\n",
    "    the location and the scale to the (transformed) distribution constructor\n",
    "    \"\"\"\n",
    "    spline_knots, spline_heights, beta_l, beta_r, qk_y, qk_x_repeat = output\n",
    "    if loc is None:\n",
    "        loc = torch.zeros_like(beta_l)\n",
    "    if scale is None:\n",
    "        scale = torch.ones_like(beta_l)\n",
    "\n",
    "    return (spline_knots, spline_heights, beta_l, beta_r, qk_y, qk_x_repeat, loc, scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5931f6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class DistributionLoss(torch.nn.Module):\n",
    "    \"\"\" DistributionLoss\n",
    "\n",
    "    This PyTorch module wraps the `torch.distribution` classes allowing it to \n",
    "    interact with NeuralForecast models modularly. It shares the negative \n",
    "    log-likelihood as the optimization objective and a sample method to \n",
    "    generate empirically the quantiles defined by the `level` list.\n",
    "\n",
    "    Additionally, it implements a distribution transformation that factorizes the\n",
    "    scale-dependent likelihood parameters into a base scale and a multiplier \n",
    "    efficiently learnable within the network's non-linearities operating ranges.\n",
    "\n",
    "    Available distributions:<br>\n",
    "    - Poisson<br>\n",
    "    - Normal<br>\n",
    "    - StudentT<br>\n",
    "    - NegativeBinomial<br>\n",
    "    - Tweedie<br>\n",
    "    - Bernoulli (Temporal Classifiers)<br>\n",
    "    - ISQF (Incremental Spline Quantile Function) \n",
    "\n",
    "    **Parameters:**<br>\n",
    "    `distribution`: str, identifier of a torch.distributions.Distribution class.<br>\n",
    "    `level`: float list [0,100], confidence levels for prediction intervals.<br>\n",
    "    `quantiles`: float list [0,1], alternative to level list, target quantiles.<br>\n",
    "    `num_samples`: int=500, number of samples for the empirical quantiles.<br>\n",
    "    `return_params`: bool=False, wether or not return the Distribution parameters.<br>\n",
    "    `horizon_weight`: Tensor of size h, weight for each timestamp of the forecasting window.<br><br>\n",
    "\n",
    "    **References:**<br>\n",
    "    - [PyTorch Probability Distributions Package: StudentT.](https://pytorch.org/docs/stable/distributions.html#studentt)<br>\n",
    "    - [David Salinas, Valentin Flunkert, Jan Gasthaus, Tim Januschowski (2020).\n",
    "       \"DeepAR: Probabilistic forecasting with autoregressive recurrent networks\". International Journal of Forecasting.](https://www.sciencedirect.com/science/article/pii/S0169207019301888)<br>\n",
    "    - [Park, Youngsuk, Danielle Maddix, François-Xavier Aubet, Kelvin Kan, Jan Gasthaus, and Yuyang Wang (2022). \"Learning Quantile Functions without Quantile Crossing for Distribution-free Time Series Forecasting\".](https://proceedings.mlr.press/v151/park22a.html)       \n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, distribution, level=[80, 90], quantiles=None,\n",
    "                 num_samples=1000, return_params=False, horizon_weight = None, **distribution_kwargs):\n",
    "       super(DistributionLoss, self).__init__()\n",
    "\n",
    "       qs, self.output_names = level_to_outputs(level)\n",
    "       qs = torch.Tensor(qs)\n",
    "\n",
    "        # Transform quantiles to homogeneus output names\n",
    "       if quantiles is not None:\n",
    "              quantiles = sorted(quantiles)\n",
    "              _, self.output_names = quantiles_to_outputs(quantiles)\n",
    "              qs = torch.Tensor(quantiles)\n",
    "       self.quantiles = torch.nn.Parameter(qs, requires_grad=False)\n",
    "       num_qk = len(self.quantiles)\n",
    "\n",
    "       # Generate a horizon weight tensor from the array\n",
    "       if horizon_weight is not None:\n",
    "           horizon_weight = torch.Tensor(horizon_weight.flatten())\n",
    "       self.horizon_weight = horizon_weight\n",
    "\n",
    "\n",
    "       if \"num_pieces\" not in distribution_kwargs:\n",
    "            num_pieces = 5\n",
    "       else:\n",
    "            num_pieces = distribution_kwargs.pop(\"num_pieces\")\n",
    "\n",
    "       available_distributions = dict(\n",
    "                          Bernoulli=Bernoulli,\n",
    "                          Normal=Normal,\n",
    "                          Poisson=Poisson,\n",
    "                          StudentT=StudentT,\n",
    "                          NegativeBinomial=NegativeBinomial,\n",
    "                          Tweedie=Tweedie,\n",
    "                          ISQF=ISQF)\n",
    "       scale_decouples = dict(\n",
    "                          Bernoulli=bernoulli_scale_decouple,\n",
    "                          Normal=normal_scale_decouple,\n",
    "                          Poisson=poisson_scale_decouple,\n",
    "                          StudentT=student_scale_decouple,\n",
    "                          NegativeBinomial=nbinomial_scale_decouple,\n",
    "                          Tweedie=tweedie_scale_decouple,\n",
    "                          ISQF=isqf_scale_decouple)\n",
    "       param_names = dict(Bernoulli=[\"-logits\"],\n",
    "                          Normal=[\"-loc\", \"-scale\"],\n",
    "                          Poisson=[\"-loc\"],\n",
    "                          StudentT=[\"-df\", \"-loc\", \"-scale\"],\n",
    "                          NegativeBinomial=[\"-total_count\", \"-logits\"],\n",
    "                          Tweedie=[\"-log_mu\"],\n",
    "                          ISQF=[f\"-spline_knot_{i + 1}\" for i in range((num_qk - 1) * num_pieces)] + \\\n",
    "                               [f\"-spline_height_{i + 1}\" for i in range((num_qk - 1) * num_pieces)] + \\\n",
    "                               [\"-beta_l\", \"-beta_r\"] + \\\n",
    "                               [f\"-quantile_knot_{i + 1}\" for i in range(num_qk)],\n",
    "                          )\n",
    "       assert (distribution in available_distributions.keys()), f'{distribution} not available'\n",
    "       if distribution == 'ISQF':\n",
    "            quantiles = torch.sort(qs).values\n",
    "            self.domain_map = partial(isqf_domain_map, \n",
    "                                       quantiles=quantiles, \n",
    "                                       num_pieces=num_pieces)\n",
    "            if return_params:\n",
    "               raise Exception(\"ISQF does not support 'return_params=True'\")                 \n",
    "       elif distribution == 'Tweedie':\n",
    "            rho = distribution_kwargs.pop(\"rho\")\n",
    "            self.domain_map = partial(tweedie_domain_map,\n",
    "                                      rho=rho)\n",
    "            if return_params:\n",
    "               raise Exception(\"Tweedie does not support 'return_params=True'\")                 \n",
    "       else:\n",
    "            self.domain_map = self._domain_map\n",
    "\n",
    "       self.distribution = distribution\n",
    "       self._base_distribution = available_distributions[distribution]\n",
    "       self.scale_decouple = scale_decouples[distribution]\n",
    "       self.distribution_kwargs = distribution_kwargs\n",
    "       self.num_samples = num_samples      \n",
    "       self.param_names = param_names[distribution]\n",
    "\n",
    "       # If True, predict_step will return Distribution's parameters\n",
    "       self.return_params = return_params\n",
    "       if self.return_params:\n",
    "            self.output_names = self.output_names + self.param_names\n",
    "\n",
    "       # Add first output entry for the sample_mean\n",
    "       self.output_names.insert(0, \"\")\n",
    "\n",
    "       self.outputsize_multiplier = len(self.param_names)\n",
    "       self.is_distribution_output = True\n",
    "       self.has_predicted = False\n",
    "\n",
    "    def _domain_map(self, input: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Maps output of neural network to domain of distribution loss\n",
    "\n",
    "        \"\"\"\n",
    "        output = torch.tensor_split(input, self.outputsize_multiplier, dim=2)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def get_distribution(self, distr_args, **distribution_kwargs) -> Distribution:\n",
    "        \"\"\"\n",
    "        Construct the associated Pytorch Distribution, given the collection of\n",
    "        constructor arguments and, optionally, location and scale tensors.\n",
    "\n",
    "        **Parameters**<br>\n",
    "        `distr_args`: Constructor arguments for the underlying Distribution type.<br>\n",
    "\n",
    "        **Returns**<br>\n",
    "        `Distribution`: AffineTransformed distribution.<br>\n",
    "        \"\"\"\n",
    "        distr = self._base_distribution(*distr_args, **distribution_kwargs)\n",
    "        self.distr_mean = distr.mean\n",
    "        \n",
    "        if self.distribution in ('Poisson', 'NegativeBinomial'):\n",
    "              distr.support = constraints.nonnegative\n",
    "        return distr\n",
    "\n",
    "    def sample(self,\n",
    "               distr_args: torch.Tensor,\n",
    "               num_samples: Optional[int] = None):\n",
    "        \"\"\"\n",
    "        Construct the empirical quantiles from the estimated Distribution,\n",
    "        sampling from it `num_samples` independently.\n",
    "\n",
    "        **Parameters**<br>\n",
    "        `distr_args`: Constructor arguments for the underlying Distribution type.<br>\n",
    "        `num_samples`: int, overwrite number of samples for the empirical quantiles.<br>\n",
    "\n",
    "        **Returns**<br>\n",
    "        `samples`: tensor, shape [B,H,`num_samples`].<br>\n",
    "        `quantiles`: tensor, empirical quantiles defined by `levels`.<br>\n",
    "        \"\"\"\n",
    "        if num_samples is None:\n",
    "            num_samples = self.num_samples\n",
    "\n",
    "        # Instantiate Scaled Decoupled Distribution\n",
    "        distr = self.get_distribution(distr_args=distr_args, **self.distribution_kwargs)\n",
    "        samples = distr.sample(sample_shape=(num_samples,))\n",
    "        samples = samples.permute(1, 2, 3, 0)                  # [samples, B, H, N] -> [B, H, N, samples]\n",
    "\n",
    "        sample_mean = torch.mean(samples, dim=-1, keepdim=True) \n",
    "\n",
    "        # Compute quantiles\n",
    "        quantiles_device = self.quantiles.to(distr_args[0].device)\n",
    "        quants = torch.quantile(input=samples, \n",
    "                                q=quantiles_device, \n",
    "                                dim=-1)\n",
    "        quants = quants.permute(1, 2, 3, 0) # [Q, B, H, N] -> [B, H, N, Q]\n",
    "\n",
    "        return samples, sample_mean, quants\n",
    "\n",
    "    def update_quantile(self, q: Optional[List[float]] = None):\n",
    "        if q is not None:\n",
    "          self.quantiles = nn.Parameter(torch.tensor(q, dtype=torch.float32), requires_grad=False)\n",
    "          self.output_names = [\"\"] + [f\"_ql{q_i}\" for q_i in q] + self.return_params * self.param_names\n",
    "          self.has_predicted = True\n",
    "        elif q is None and self.has_predicted:\n",
    "          self.quantiles = nn.Parameter(torch.tensor([0.5], dtype=torch.float32), requires_grad=False)\n",
    "          self.output_names = [\"\", \"-median\"] + self.return_params * self.param_names\n",
    "\n",
    "    def _compute_weights(self, y, mask):\n",
    "        \"\"\"\n",
    "        Compute final weights for each datapoint (based on all weights and all masks)\n",
    "        Set horizon_weight to a ones[H] tensor if not set.\n",
    "        If set, check that it has the same length as the horizon in x.\n",
    "        \"\"\"\n",
    "        if mask is None:\n",
    "            mask = torch.ones_like(y)\n",
    "\n",
    "        if self.horizon_weight is None:\n",
    "            weights = torch.ones_like(mask)\n",
    "        else:\n",
    "            assert mask.shape[1] == len(self.horizon_weight), \\\n",
    "                'horizon_weight must have same length as Y'\n",
    "            weights = self.horizon_weight.clone()\n",
    "            weights = weights[None, :, None].to(mask.device)\n",
    "            weights = torch.ones_like(mask, device=mask.device) * weights\n",
    "        \n",
    "        return weights * mask\n",
    "\n",
    "    def __call__(self,\n",
    "                 y: torch.Tensor,\n",
    "                 distr_args: torch.Tensor,\n",
    "                 mask: Union[torch.Tensor, None] = None):\n",
    "         \"\"\"\n",
    "         Computes the negative log-likelihood objective function. \n",
    "         To estimate the following predictive distribution:\n",
    "\n",
    "         $$\\mathrm{P}(\\mathbf{y}_{\\\\tau}\\,|\\,\\\\theta) \\\\quad \\mathrm{and} \\\\quad -\\log(\\mathrm{P}(\\mathbf{y}_{\\\\tau}\\,|\\,\\\\theta))$$\n",
    "\n",
    "         where $\\\\theta$ represents the distributions parameters. It aditionally \n",
    "         summarizes the objective signal using a weighted average using the `mask` tensor. \n",
    " \n",
    "         **Parameters**<br>\n",
    "         `y`: tensor, Actual values.<br>\n",
    "         `distr_args`: Constructor arguments for the underlying Distribution type.<br>\n",
    "         `loc`: Optional tensor, of the same shape as the batch_shape + event_shape\n",
    "                of the resulting distribution.<br>\n",
    "         `scale`: Optional tensor, of the same shape as the batch_shape+event_shape \n",
    "                of the resulting distribution.<br>\n",
    "         `mask`: tensor, Specifies date stamps per serie to consider in loss.<br>\n",
    "\n",
    "         **Returns**<br>\n",
    "         `loss`: scalar, weighted loss function against which backpropagation will be performed.<br>\n",
    "         \"\"\"\n",
    "         # Instantiate Scaled Decoupled Distribution\n",
    "         distr = self.get_distribution(distr_args=distr_args, **self.distribution_kwargs)\n",
    "         loss_values = -distr.log_prob(y)\n",
    "         loss_weights = self._compute_weights(y=y, mask=mask)\n",
    "         return weighted_average(loss_values, weights=loss_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a462101b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/losses/pytorch.py#L1833){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DistributionLoss.__init__\n",
       "\n",
       ">      DistributionLoss.__init__ (distribution, level=[80, 90], quantiles=None,\n",
       ">                                 num_samples=1000, return_params=False,\n",
       ">                                 horizon_weight=None, **distribution_kwargs)\n",
       "\n",
       "*DistributionLoss\n",
       "\n",
       "This PyTorch module wraps the `torch.distribution` classes allowing it to \n",
       "interact with NeuralForecast models modularly. It shares the negative \n",
       "log-likelihood as the optimization objective and a sample method to \n",
       "generate empirically the quantiles defined by the `level` list.\n",
       "\n",
       "Additionally, it implements a distribution transformation that factorizes the\n",
       "scale-dependent likelihood parameters into a base scale and a multiplier \n",
       "efficiently learnable within the network's non-linearities operating ranges.\n",
       "\n",
       "Available distributions:<br>\n",
       "- Poisson<br>\n",
       "- Normal<br>\n",
       "- StudentT<br>\n",
       "- NegativeBinomial<br>\n",
       "- Tweedie<br>\n",
       "- Bernoulli (Temporal Classifiers)<br>\n",
       "- ISQF (Incremental Spline Quantile Function) \n",
       "\n",
       "**Parameters:**<br>\n",
       "`distribution`: str, identifier of a torch.distributions.Distribution class.<br>\n",
       "`level`: float list [0,100], confidence levels for prediction intervals.<br>\n",
       "`quantiles`: float list [0,1], alternative to level list, target quantiles.<br>\n",
       "`num_samples`: int=500, number of samples for the empirical quantiles.<br>\n",
       "`return_params`: bool=False, wether or not return the Distribution parameters.<br>\n",
       "`horizon_weight`: Tensor of size h, weight for each timestamp of the forecasting window.<br><br>\n",
       "\n",
       "**References:**<br>\n",
       "- [PyTorch Probability Distributions Package: StudentT.](https://pytorch.org/docs/stable/distributions.html#studentt)<br>\n",
       "- [David Salinas, Valentin Flunkert, Jan Gasthaus, Tim Januschowski (2020).\n",
       "   \"DeepAR: Probabilistic forecasting with autoregressive recurrent networks\". International Journal of Forecasting.](https://www.sciencedirect.com/science/article/pii/S0169207019301888)<br>\n",
       "- [Park, Youngsuk, Danielle Maddix, François-Xavier Aubet, Kelvin Kan, Jan Gasthaus, and Yuyang Wang (2022). \"Learning Quantile Functions without Quantile Crossing for Distribution-free Time Series Forecasting\".](https://proceedings.mlr.press/v151/park22a.html)*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/losses/pytorch.py#L1833){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DistributionLoss.__init__\n",
       "\n",
       ">      DistributionLoss.__init__ (distribution, level=[80, 90], quantiles=None,\n",
       ">                                 num_samples=1000, return_params=False,\n",
       ">                                 horizon_weight=None, **distribution_kwargs)\n",
       "\n",
       "*DistributionLoss\n",
       "\n",
       "This PyTorch module wraps the `torch.distribution` classes allowing it to \n",
       "interact with NeuralForecast models modularly. It shares the negative \n",
       "log-likelihood as the optimization objective and a sample method to \n",
       "generate empirically the quantiles defined by the `level` list.\n",
       "\n",
       "Additionally, it implements a distribution transformation that factorizes the\n",
       "scale-dependent likelihood parameters into a base scale and a multiplier \n",
       "efficiently learnable within the network's non-linearities operating ranges.\n",
       "\n",
       "Available distributions:<br>\n",
       "- Poisson<br>\n",
       "- Normal<br>\n",
       "- StudentT<br>\n",
       "- NegativeBinomial<br>\n",
       "- Tweedie<br>\n",
       "- Bernoulli (Temporal Classifiers)<br>\n",
       "- ISQF (Incremental Spline Quantile Function) \n",
       "\n",
       "**Parameters:**<br>\n",
       "`distribution`: str, identifier of a torch.distributions.Distribution class.<br>\n",
       "`level`: float list [0,100], confidence levels for prediction intervals.<br>\n",
       "`quantiles`: float list [0,1], alternative to level list, target quantiles.<br>\n",
       "`num_samples`: int=500, number of samples for the empirical quantiles.<br>\n",
       "`return_params`: bool=False, wether or not return the Distribution parameters.<br>\n",
       "`horizon_weight`: Tensor of size h, weight for each timestamp of the forecasting window.<br><br>\n",
       "\n",
       "**References:**<br>\n",
       "- [PyTorch Probability Distributions Package: StudentT.](https://pytorch.org/docs/stable/distributions.html#studentt)<br>\n",
       "- [David Salinas, Valentin Flunkert, Jan Gasthaus, Tim Januschowski (2020).\n",
       "   \"DeepAR: Probabilistic forecasting with autoregressive recurrent networks\". International Journal of Forecasting.](https://www.sciencedirect.com/science/article/pii/S0169207019301888)<br>\n",
       "- [Park, Youngsuk, Danielle Maddix, François-Xavier Aubet, Kelvin Kan, Jan Gasthaus, and Yuyang Wang (2022). \"Learning Quantile Functions without Quantile Crossing for Distribution-free Time Series Forecasting\".](https://proceedings.mlr.press/v151/park22a.html)*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DistributionLoss, name='DistributionLoss.__init__', title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c367f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/losses/pytorch.py#L1975){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DistributionLoss.sample\n",
       "\n",
       ">      DistributionLoss.sample (distr_args:torch.Tensor,\n",
       ">                               num_samples:Optional[int]=None)\n",
       "\n",
       "*Construct the empirical quantiles from the estimated Distribution,\n",
       "sampling from it `num_samples` independently.\n",
       "\n",
       "**Parameters**<br>\n",
       "`distr_args`: Constructor arguments for the underlying Distribution type.<br>\n",
       "`num_samples`: int, overwrite number of samples for the empirical quantiles.<br>\n",
       "\n",
       "**Returns**<br>\n",
       "`samples`: tensor, shape [B,H,`num_samples`].<br>\n",
       "`quantiles`: tensor, empirical quantiles defined by `levels`.<br>*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/losses/pytorch.py#L1975){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DistributionLoss.sample\n",
       "\n",
       ">      DistributionLoss.sample (distr_args:torch.Tensor,\n",
       ">                               num_samples:Optional[int]=None)\n",
       "\n",
       "*Construct the empirical quantiles from the estimated Distribution,\n",
       "sampling from it `num_samples` independently.\n",
       "\n",
       "**Parameters**<br>\n",
       "`distr_args`: Constructor arguments for the underlying Distribution type.<br>\n",
       "`num_samples`: int, overwrite number of samples for the empirical quantiles.<br>\n",
       "\n",
       "**Returns**<br>\n",
       "`samples`: tensor, shape [B,H,`num_samples`].<br>\n",
       "`quantiles`: tensor, empirical quantiles defined by `levels`.<br>*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DistributionLoss.sample, name='DistributionLoss.sample', title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e32679",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/losses/pytorch.py#L2014){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DistributionLoss.__call__\n",
       "\n",
       ">      DistributionLoss.__call__ (y:torch.Tensor, distr_args:torch.Tensor,\n",
       ">                                 mask:Optional[torch.Tensor]=None)\n",
       "\n",
       "*Computes the negative log-likelihood objective function. \n",
       "To estimate the following predictive distribution:\n",
       "\n",
       "$$\\mathrm{P}(\\mathbf{y}_{\\tau}\\,|\\,\\theta) \\quad \\mathrm{and} \\quad -\\log(\\mathrm{P}(\\mathbf{y}_{\\tau}\\,|\\,\\theta))$$\n",
       "\n",
       "where $\\theta$ represents the distributions parameters. It aditionally \n",
       "summarizes the objective signal using a weighted average using the `mask` tensor. \n",
       "\n",
       "**Parameters**<br>\n",
       "`y`: tensor, Actual values.<br>\n",
       "`distr_args`: Constructor arguments for the underlying Distribution type.<br>\n",
       "`loc`: Optional tensor, of the same shape as the batch_shape + event_shape\n",
       "       of the resulting distribution.<br>\n",
       "`scale`: Optional tensor, of the same shape as the batch_shape+event_shape \n",
       "       of the resulting distribution.<br>\n",
       "`mask`: tensor, Specifies date stamps per serie to consider in loss.<br>\n",
       "\n",
       "**Returns**<br>\n",
       "`loss`: scalar, weighted loss function against which backpropagation will be performed.<br>*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/losses/pytorch.py#L2014){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DistributionLoss.__call__\n",
       "\n",
       ">      DistributionLoss.__call__ (y:torch.Tensor, distr_args:torch.Tensor,\n",
       ">                                 mask:Optional[torch.Tensor]=None)\n",
       "\n",
       "*Computes the negative log-likelihood objective function. \n",
       "To estimate the following predictive distribution:\n",
       "\n",
       "$$\\mathrm{P}(\\mathbf{y}_{\\tau}\\,|\\,\\theta) \\quad \\mathrm{and} \\quad -\\log(\\mathrm{P}(\\mathbf{y}_{\\tau}\\,|\\,\\theta))$$\n",
       "\n",
       "where $\\theta$ represents the distributions parameters. It aditionally \n",
       "summarizes the objective signal using a weighted average using the `mask` tensor. \n",
       "\n",
       "**Parameters**<br>\n",
       "`y`: tensor, Actual values.<br>\n",
       "`distr_args`: Constructor arguments for the underlying Distribution type.<br>\n",
       "`loc`: Optional tensor, of the same shape as the batch_shape + event_shape\n",
       "       of the resulting distribution.<br>\n",
       "`scale`: Optional tensor, of the same shape as the batch_shape+event_shape \n",
       "       of the resulting distribution.<br>\n",
       "`mask`: tensor, Specifies date stamps per serie to consider in loss.<br>\n",
       "\n",
       "**Returns**<br>\n",
       "`loss`: scalar, weighted loss function against which backpropagation will be performed.<br>*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DistributionLoss.__call__, name='DistributionLoss.__call__', title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a7e381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '-lo-98.0', '-lo-80.0', '-median', '-hi-80.0', '-hi-98.0']\n",
      "Parameter containing:\n",
      "tensor([0.0100, 0.1000, 0.5000, 0.9000, 0.9900])\n"
     ]
    }
   ],
   "source": [
    "# | hide\n",
    "# Unit tests to check DistributionLoss' stored quantiles\n",
    "# attribute is correctly instantiated\n",
    "check = DistributionLoss(distribution='Normal', level=[80, 90])\n",
    "test_eq(len(check.quantiles), 5)\n",
    "\n",
    "check = DistributionLoss(distribution='Normal', \n",
    "                         quantiles=[0.0100, 0.1000, 0.5, 0.9000, 0.9900])\n",
    "print(check.output_names)\n",
    "print(check.quantiles)\n",
    "test_eq(len(check.quantiles), 5)\n",
    "\n",
    "check = DistributionLoss(distribution='Normal',\n",
    "                         quantiles=[0.0100, 0.1000, 0.9000, 0.9900])\n",
    "test_eq(len(check.quantiles), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb1b8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "# Unit tests to check DistributionLoss' horizon weight\n",
    "batch_size, horizon, n_outputs = 10, 3, 2\n",
    "y_hat = torch.rand(batch_size, horizon, n_outputs).chunk(2, dim=-1)\n",
    "y = torch.rand(batch_size, horizon, 1)\n",
    "y_loc = torch.rand(batch_size, 1, 1)\n",
    "y_scale = torch.rand(batch_size, 1, 1)\n",
    "\n",
    "loss = DistributionLoss(distribution='Normal', level=[80, 90])\n",
    "loss_with_hweights = DistributionLoss(distribution='Normal', level=[80, 90], horizon_weight = torch.ones(horizon))\n",
    "\n",
    "distr_args = loss.scale_decouple(y_hat, y_loc, y_scale)\n",
    "distr_args_weighted = loss_with_hweights.scale_decouple(y_hat, y_loc, y_scale)\n",
    "\n",
    "test_eq(loss(y, distr_args), loss_with_hweights(y, distr_args_weighted))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "07f459b8",
   "metadata": {},
   "source": [
    "## Poisson Mixture Mesh (PMM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ec688f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class PMM(torch.nn.Module):\n",
    "    \"\"\" Poisson Mixture Mesh\n",
    "\n",
    "    This Poisson Mixture statistical model assumes independence across groups of \n",
    "    data $\\mathcal{G}=\\{[g_{i}]\\}$, and estimates relationships within the group.\n",
    "\n",
    "    $$ \\mathrm{P}\\\\left(\\mathbf{y}_{[b][t+1:t+H]}\\\\right) = \n",
    "    \\prod_{ [g_{i}] \\in \\mathcal{G}} \\mathrm{P} \\\\left(\\mathbf{y}_{[g_{i}][\\\\tau]} \\\\right) =\n",
    "    \\prod_{\\\\beta\\in[g_{i}]} \n",
    "    \\\\left(\\sum_{k=1}^{K} w_k \\prod_{(\\\\beta,\\\\tau) \\in [g_i][t+1:t+H]} \\mathrm{Poisson}(y_{\\\\beta,\\\\tau}, \\hat{\\\\lambda}_{\\\\beta,\\\\tau,k}) \\\\right)$$\n",
    "\n",
    "    **Parameters:**<br>\n",
    "    `n_components`: int=10, the number of mixture components.<br>\n",
    "    `level`: float list [0,100], confidence levels for prediction intervals.<br>\n",
    "    `quantiles`: float list [0,1], alternative to level list, target quantiles.<br>\n",
    "    `return_params`: bool=False, wether or not return the Distribution parameters.<br>\n",
    "    `batch_correlation`: bool=False, wether or not model batch correlations.<br>\n",
    "    `horizon_correlation`: bool=False, wether or not model horizon correlations.<br>\n",
    "\n",
    "    **References:**<br>\n",
    "    [Kin G. Olivares, O. Nganba Meetei, Ruijun Ma, Rohan Reddy, Mengfei Cao, Lee Dicker. \n",
    "    Probabilistic Hierarchical Forecasting with Deep Poisson Mixtures. Submitted to the International \n",
    "    Journal Forecasting, Working paper available at arxiv.](https://arxiv.org/pdf/2110.13179.pdf)\n",
    "    \"\"\"\n",
    "    def __init__(self, n_components=10, level=[80, 90], quantiles=None,\n",
    "                 num_samples=1000, return_params=False,\n",
    "                 batch_correlation=False, horizon_correlation=False, \n",
    "                 weighted=False):\n",
    "        super(PMM, self).__init__()\n",
    "        # Transform level to MQLoss parameters\n",
    "        qs, self.output_names = level_to_outputs(level)\n",
    "        qs = torch.Tensor(qs)\n",
    "\n",
    "        # Transform quantiles to homogeneus output names\n",
    "        if quantiles is not None:\n",
    "            _, self.output_names = quantiles_to_outputs(quantiles)\n",
    "            qs = torch.Tensor(quantiles)\n",
    "        self.quantiles = torch.nn.Parameter(qs, requires_grad=False)\n",
    "        self.num_samples = num_samples\n",
    "        self.batch_correlation = batch_correlation\n",
    "        self.horizon_correlation = horizon_correlation\n",
    "        self.weighted = weighted   \n",
    "\n",
    "        # If True, predict_step will return Distribution's parameters\n",
    "        self.return_params = return_params\n",
    "\n",
    "        lambda_names = [f\"-lambda-{i}\" for i in range(1, n_components + 1)]\n",
    "        if weighted:\n",
    "            weight_names = [f\"-weight-{i}\" for i in range(1, n_components + 1)]\n",
    "            self.param_names = [i for j in zip(lambda_names, weight_names) for i in j]\n",
    "        else:\n",
    "            self.param_names = lambda_names\n",
    "\n",
    "        if self.return_params:           \n",
    "            self.output_names = self.output_names + self.param_names\n",
    "\n",
    "        # Add first output entry for the sample_mean\n",
    "        self.output_names.insert(0, \"\")\n",
    "\n",
    "        self.n_outputs = 1 + weighted\n",
    "        self.n_components = n_components\n",
    "        self.outputsize_multiplier = self.n_outputs * n_components\n",
    "        self.is_distribution_output = True\n",
    "        self.has_predicted = False\n",
    "\n",
    "    def domain_map(self, output: torch.Tensor):\n",
    "        output = output.reshape(output.shape[0],\n",
    "                                output.shape[1],\n",
    "                               -1,\n",
    "                               self.outputsize_multiplier)\n",
    "        \n",
    "        return torch.tensor_split(output, self.n_outputs, dim=-1)\n",
    "        \n",
    "    def scale_decouple(self, \n",
    "                       output,\n",
    "                       loc: Optional[torch.Tensor] = None,\n",
    "                       scale: Optional[torch.Tensor] = None):\n",
    "        \"\"\" Scale Decouple\n",
    "\n",
    "        Stabilizes model's output optimization, by learning residual\n",
    "        variance and residual location based on anchoring `loc`, `scale`.\n",
    "        Also adds domain protection to the distribution parameters.\n",
    "        \"\"\"\n",
    "        if self.weighted:\n",
    "            lambdas, weights = output\n",
    "            weights = F.softmax(weights, dim=-1)\n",
    "        else:\n",
    "            lambdas = output[0]\n",
    "\n",
    "        if (loc is not None) and (scale is not None):\n",
    "            if loc.ndim == 3:\n",
    "                loc = loc.unsqueeze(-1)\n",
    "                scale = scale.unsqueeze(-1)\n",
    "            lambdas = (lambdas * scale) + loc\n",
    "\n",
    "        lambdas = F.softplus(lambdas) + 1e-3\n",
    "        \n",
    "        if self.weighted:\n",
    "            return (lambdas, weights)\n",
    "        else:\n",
    "            return (lambdas, )\n",
    "    \n",
    "    def get_distribution(self, distr_args) -> Distribution:\n",
    "        \"\"\"\n",
    "        Construct the associated Pytorch Distribution, given the collection of\n",
    "        constructor arguments and, optionally, location and scale tensors.\n",
    "\n",
    "        **Parameters**<br>\n",
    "        `distr_args`: Constructor arguments for the underlying Distribution type.<br>\n",
    "\n",
    "        **Returns**<br>\n",
    "        `Distribution`: AffineTransformed distribution.<br>\n",
    "        \"\"\"\n",
    "        if self.weighted:\n",
    "            lambdas, weights = distr_args\n",
    "        else:\n",
    "            lambdas = distr_args[0]\n",
    "            weights = torch.full_like(lambdas, fill_value=1 / self.n_components)\n",
    "\n",
    "        mix = Categorical(weights)\n",
    "        components = Poisson(rate=lambdas)\n",
    "        components.support = constraints.nonnegative\n",
    "        distr = MixtureSameFamily(mixture_distribution=mix,\n",
    "                                      component_distribution=components)    \n",
    "\n",
    "        self.distr_mean = distr.mean\n",
    "        \n",
    "        return distr\n",
    "\n",
    "    def sample(self,\n",
    "               distr_args: torch.Tensor,\n",
    "               num_samples: Optional[int] = None):\n",
    "        \"\"\"\n",
    "        Construct the empirical quantiles from the estimated Distribution,\n",
    "        sampling from it `num_samples` independently.\n",
    "\n",
    "        **Parameters**<br>\n",
    "        `distr_args`: Constructor arguments for the underlying Distribution type.<br>\n",
    "        `num_samples`: int, overwrite number of samples for the empirical quantiles.<br>\n",
    "\n",
    "        **Returns**<br>\n",
    "        `samples`: tensor, shape [B,H,`num_samples`].<br>\n",
    "        `quantiles`: tensor, empirical quantiles defined by `levels`.<br>\n",
    "        \"\"\"\n",
    "        if num_samples is None:\n",
    "            num_samples = self.num_samples\n",
    "\n",
    "        # Instantiate Scaled Decoupled Distribution\n",
    "        distr = self.get_distribution(distr_args=distr_args)\n",
    "        samples = distr.sample(sample_shape=(num_samples,))\n",
    "        samples = samples.permute(1, 2, 3, 0)                  # [samples, B, H, N] -> [B, H, N, samples]\n",
    "\n",
    "        sample_mean = torch.mean(samples, dim=-1, keepdim=True) \n",
    "\n",
    "        # Compute quantiles\n",
    "        quantiles_device = self.quantiles.to(distr_args[0].device)\n",
    "        quants = torch.quantile(input=samples, \n",
    "                                q=quantiles_device, \n",
    "                                dim=-1)\n",
    "        quants = quants.permute(1, 2, 3, 0) # [Q, B, H, N] -> [B, H, N, Q]\n",
    "\n",
    "        return samples, sample_mean, quants\n",
    "    \n",
    "    def update_quantile(self, q: Optional[List[float]] = None):\n",
    "        if q is not None:\n",
    "          self.quantiles = nn.Parameter(torch.tensor(q, dtype=torch.float32), requires_grad=False)\n",
    "          self.output_names = [\"\"] + [f\"_ql{q_i}\" for q_i in q] + self.return_params * self.param_names\n",
    "          self.has_predicted = True\n",
    "        elif q is None and self.has_predicted:\n",
    "          self.quantiles = nn.Parameter(torch.tensor([0.5], dtype=torch.float32), requires_grad=False)         \n",
    "          self.output_names = [\"\", \"-median\"] + self.return_params * self.param_names\n",
    "\n",
    "    def __call__(self,\n",
    "                 y: torch.Tensor,\n",
    "                 distr_args: torch.Tensor,\n",
    "                 mask: Union[torch.Tensor, None] = None):\n",
    "        \"\"\"\n",
    "        Computes the negative log-likelihood objective function. \n",
    "        To estimate the following predictive distribution:\n",
    "\n",
    "        $$\\mathrm{P}(\\mathbf{y}_{\\\\tau}\\,|\\,\\\\theta) \\\\quad \\mathrm{and} \\\\quad -\\log(\\mathrm{P}(\\mathbf{y}_{\\\\tau}\\,|\\,\\\\theta))$$\n",
    "\n",
    "        where $\\\\theta$ represents the distributions parameters. It aditionally \n",
    "        summarizes the objective signal using a weighted average using the `mask` tensor. \n",
    "\n",
    "        **Parameters**<br>\n",
    "        `y`: tensor, Actual values.<br>\n",
    "        `distr_args`: Constructor arguments for the underlying Distribution type.<br>\n",
    "        `mask`: tensor, Specifies date stamps per serie to consider in loss.<br>\n",
    "\n",
    "        **Returns**<br>\n",
    "        `loss`: scalar, weighted loss function against which backpropagation will be performed.<br>\n",
    "        \"\"\"\n",
    "        # Instantiate Scaled Decoupled Distribution\n",
    "        distr = self.get_distribution(distr_args=distr_args)\n",
    "        x = distr._pad(y)\n",
    "        log_prob_x = distr.component_distribution.log_prob(x)\n",
    "        log_mix_prob = torch.log_softmax(distr.mixture_distribution.logits, dim=-1)\n",
    "        if self.batch_correlation:\n",
    "                log_prob_x = torch.sum(log_prob_x, dim=0, keepdim=True)\n",
    "        if self.horizon_correlation:\n",
    "                log_prob_x = torch.sum(log_prob_x, dim=1, keepdim=True)\n",
    "        \n",
    "        loss_values = -torch.logsumexp(log_prob_x + log_mix_prob, dim=-1)  \n",
    "       \n",
    "        return weighted_average(loss_values, weights=mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d7daba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/losses/pytorch.py#L2048){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### PMM.__init__\n",
       "\n",
       ">      PMM.__init__ (n_components=10, level=[80, 90], quantiles=None,\n",
       ">                    num_samples=1000, return_params=False,\n",
       ">                    batch_correlation=False, horizon_correlation=False,\n",
       ">                    weighted=False)\n",
       "\n",
       "*Poisson Mixture Mesh\n",
       "\n",
       "This Poisson Mixture statistical model assumes independence across groups of \n",
       "data $\\mathcal{G}=\\{[g_{i}]\\}$, and estimates relationships within the group.\n",
       "\n",
       "$$ \\mathrm{P}\\left(\\mathbf{y}_{[b][t+1:t+H]}\\right) = \n",
       "\\prod_{ [g_{i}] \\in \\mathcal{G}} \\mathrm{P} \\left(\\mathbf{y}_{[g_{i}][\\tau]} \\right) =\n",
       "\\prod_{\\beta\\in[g_{i}]} \n",
       "\\left(\\sum_{k=1}^{K} w_k \\prod_{(\\beta,\\tau) \\in [g_i][t+1:t+H]} \\mathrm{Poisson}(y_{\\beta,\\tau}, \\hat{\\lambda}_{\\beta,\\tau,k}) \\right)$$\n",
       "\n",
       "**Parameters:**<br>\n",
       "`n_components`: int=10, the number of mixture components.<br>\n",
       "`level`: float list [0,100], confidence levels for prediction intervals.<br>\n",
       "`quantiles`: float list [0,1], alternative to level list, target quantiles.<br>\n",
       "`return_params`: bool=False, wether or not return the Distribution parameters.<br>\n",
       "`batch_correlation`: bool=False, wether or not model batch correlations.<br>\n",
       "`horizon_correlation`: bool=False, wether or not model horizon correlations.<br>\n",
       "\n",
       "**References:**<br>\n",
       "[Kin G. Olivares, O. Nganba Meetei, Ruijun Ma, Rohan Reddy, Mengfei Cao, Lee Dicker. \n",
       "Probabilistic Hierarchical Forecasting with Deep Poisson Mixtures. Submitted to the International \n",
       "Journal Forecasting, Working paper available at arxiv.](https://arxiv.org/pdf/2110.13179.pdf)*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/losses/pytorch.py#L2048){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### PMM.__init__\n",
       "\n",
       ">      PMM.__init__ (n_components=10, level=[80, 90], quantiles=None,\n",
       ">                    num_samples=1000, return_params=False,\n",
       ">                    batch_correlation=False, horizon_correlation=False,\n",
       ">                    weighted=False)\n",
       "\n",
       "*Poisson Mixture Mesh\n",
       "\n",
       "This Poisson Mixture statistical model assumes independence across groups of \n",
       "data $\\mathcal{G}=\\{[g_{i}]\\}$, and estimates relationships within the group.\n",
       "\n",
       "$$ \\mathrm{P}\\left(\\mathbf{y}_{[b][t+1:t+H]}\\right) = \n",
       "\\prod_{ [g_{i}] \\in \\mathcal{G}} \\mathrm{P} \\left(\\mathbf{y}_{[g_{i}][\\tau]} \\right) =\n",
       "\\prod_{\\beta\\in[g_{i}]} \n",
       "\\left(\\sum_{k=1}^{K} w_k \\prod_{(\\beta,\\tau) \\in [g_i][t+1:t+H]} \\mathrm{Poisson}(y_{\\beta,\\tau}, \\hat{\\lambda}_{\\beta,\\tau,k}) \\right)$$\n",
       "\n",
       "**Parameters:**<br>\n",
       "`n_components`: int=10, the number of mixture components.<br>\n",
       "`level`: float list [0,100], confidence levels for prediction intervals.<br>\n",
       "`quantiles`: float list [0,1], alternative to level list, target quantiles.<br>\n",
       "`return_params`: bool=False, wether or not return the Distribution parameters.<br>\n",
       "`batch_correlation`: bool=False, wether or not model batch correlations.<br>\n",
       "`horizon_correlation`: bool=False, wether or not model horizon correlations.<br>\n",
       "\n",
       "**References:**<br>\n",
       "[Kin G. Olivares, O. Nganba Meetei, Ruijun Ma, Rohan Reddy, Mengfei Cao, Lee Dicker. \n",
       "Probabilistic Hierarchical Forecasting with Deep Poisson Mixtures. Submitted to the International \n",
       "Journal Forecasting, Working paper available at arxiv.](https://arxiv.org/pdf/2110.13179.pdf)*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(PMM, name='PMM.__init__', title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8da65c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/losses/pytorch.py#L2132){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### PMM.sample\n",
       "\n",
       ">      PMM.sample (distr_args:torch.Tensor, num_samples:Optional[int]=None)\n",
       "\n",
       "*Construct the empirical quantiles from the estimated Distribution,\n",
       "sampling from it `num_samples` independently.\n",
       "\n",
       "**Parameters**<br>\n",
       "`distr_args`: Constructor arguments for the underlying Distribution type.<br>\n",
       "`num_samples`: int, overwrite number of samples for the empirical quantiles.<br>\n",
       "\n",
       "**Returns**<br>\n",
       "`samples`: tensor, shape [B,H,`num_samples`].<br>\n",
       "`quantiles`: tensor, empirical quantiles defined by `levels`.<br>*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/losses/pytorch.py#L2132){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### PMM.sample\n",
       "\n",
       ">      PMM.sample (distr_args:torch.Tensor, num_samples:Optional[int]=None)\n",
       "\n",
       "*Construct the empirical quantiles from the estimated Distribution,\n",
       "sampling from it `num_samples` independently.\n",
       "\n",
       "**Parameters**<br>\n",
       "`distr_args`: Constructor arguments for the underlying Distribution type.<br>\n",
       "`num_samples`: int, overwrite number of samples for the empirical quantiles.<br>\n",
       "\n",
       "**Returns**<br>\n",
       "`samples`: tensor, shape [B,H,`num_samples`].<br>\n",
       "`quantiles`: tensor, empirical quantiles defined by `levels`.<br>*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(PMM.sample, name='PMM.sample', title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba75717c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/losses/pytorch.py#L2238){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### PMM.__call__\n",
       "\n",
       ">      PMM.__call__ (y:torch.Tensor, distr_args:torch.Tensor,\n",
       ">                    mask:Optional[torch.Tensor]=None)\n",
       "\n",
       "*Computes the negative log-likelihood objective function. \n",
       "To estimate the following predictive distribution:\n",
       "\n",
       "$$\\mathrm{P}(\\mathbf{y}_{\\tau}\\,|\\,\\theta) \\quad \\mathrm{and} \\quad -\\log(\\mathrm{P}(\\mathbf{y}_{\\tau}\\,|\\,\\theta))$$\n",
       "\n",
       "where $\\theta$ represents the distributions parameters. It aditionally \n",
       "summarizes the objective signal using a weighted average using the `mask` tensor. \n",
       "\n",
       "**Parameters**<br>\n",
       "`y`: tensor, Actual values.<br>\n",
       "`distr_args`: Constructor arguments for the underlying Distribution type.<br>\n",
       "`mask`: tensor, Specifies date stamps per serie to consider in loss.<br>\n",
       "\n",
       "**Returns**<br>\n",
       "`loss`: scalar, weighted loss function against which backpropagation will be performed.<br>*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/losses/pytorch.py#L2238){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### PMM.__call__\n",
       "\n",
       ">      PMM.__call__ (y:torch.Tensor, distr_args:torch.Tensor,\n",
       ">                    mask:Optional[torch.Tensor]=None)\n",
       "\n",
       "*Computes the negative log-likelihood objective function. \n",
       "To estimate the following predictive distribution:\n",
       "\n",
       "$$\\mathrm{P}(\\mathbf{y}_{\\tau}\\,|\\,\\theta) \\quad \\mathrm{and} \\quad -\\log(\\mathrm{P}(\\mathbf{y}_{\\tau}\\,|\\,\\theta))$$\n",
       "\n",
       "where $\\theta$ represents the distributions parameters. It aditionally \n",
       "summarizes the objective signal using a weighted average using the `mask` tensor. \n",
       "\n",
       "**Parameters**<br>\n",
       "`y`: tensor, Actual values.<br>\n",
       "`distr_args`: Constructor arguments for the underlying Distribution type.<br>\n",
       "`mask`: tensor, Specifies date stamps per serie to consider in loss.<br>\n",
       "\n",
       "**Returns**<br>\n",
       "`loss`: scalar, weighted loss function against which backpropagation will be performed.<br>*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(PMM.__call__, name='PMM.__call__', title_level=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f7518450",
   "metadata": {},
   "source": [
    "![](imgs_losses/pmm.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a20e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '-lo-98.0', '-lo-80.0', '-median', '-hi-80.0', '-hi-98.0']\n",
      "Parameter containing:\n",
      "tensor([0.0100, 0.1000, 0.5000, 0.9000, 0.9900])\n"
     ]
    }
   ],
   "source": [
    "# | hide\n",
    "# Unit tests to check PMM's stored quantiles\n",
    "# attribute is correctly instantiated\n",
    "check = PMM(n_components=2, level=[80, 90])\n",
    "test_eq(len(check.quantiles), 5)\n",
    "\n",
    "check = PMM(n_components=2, \n",
    "            quantiles=[0.0100, 0.1000, 0.5, 0.9000, 0.9900])\n",
    "print(check.output_names)\n",
    "print(check.quantiles)\n",
    "test_eq(len(check.quantiles), 5)\n",
    "\n",
    "check = PMM(n_components=2,\n",
    "            quantiles=[0.0100, 0.1000, 0.9000, 0.9900])\n",
    "test_eq(len(check.quantiles), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56a2fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights.shape (N,H,1,K) \t torch.Size([2, 1, 1, 3])\n",
      "lambdas.shape (N,H,1, K) \t torch.Size([2, 2, 1, 3])\n",
      "samples.shape (N,H,1,num_samples)  torch.Size([2, 2, 1, 1000])\n",
      "sample_mean.shape (N,H,1,1)  torch.Size([2, 2, 1, 1])\n",
      "quants.shape  (N,H,1,Q) \t\t torch.Size([2, 2, 1, 5])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 370x290 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 370x290 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| hide\n",
    "# Create single mixture and broadcast to N,H,1,K\n",
    "weights = torch.ones((1,3))[None, :, :].unsqueeze(2)\n",
    "lambdas = torch.Tensor([[5,10,15], [10,20,30]])[None, :, :].unsqueeze(2)\n",
    "\n",
    "# Create repetitions for the batch dimension N.\n",
    "N=2\n",
    "weights = torch.repeat_interleave(input=weights, repeats=N, dim=0)\n",
    "lambdas = torch.repeat_interleave(input=lambdas, repeats=N, dim=0)\n",
    "\n",
    "print('weights.shape (N,H,1,K) \\t', weights.shape)\n",
    "print('lambdas.shape (N,H,1, K) \\t', lambdas.shape)\n",
    "\n",
    "distr = PMM(quantiles=[0.1, 0.40, 0.5, 0.60, 0.9], weighted=True)\n",
    "weights = torch.ones_like(lambdas)\n",
    "distr_args = (lambdas, weights)\n",
    "samples, sample_mean, quants = distr.sample(distr_args)\n",
    "\n",
    "print('samples.shape (N,H,1,num_samples) ', samples.shape)\n",
    "print('sample_mean.shape (N,H,1,1) ', sample_mean.shape)\n",
    "print('quants.shape  (N,H,1,Q) \\t\\t', quants.shape)\n",
    "\n",
    "# Plot synthethic data\n",
    "x_plot = range(quants.shape[1]) # H length\n",
    "y_plot_hat = quants[0,:,0,:]  # Filter N,G,T -> H,Q\n",
    "samples_hat = samples[0,:,0,:]  # Filter N,G,T -> H,num_samples\n",
    "\n",
    "# Kernel density plot for single forecast horizon \\tau = t+1\n",
    "fig, ax = plt.subplots(figsize=(3.7, 2.9))\n",
    "\n",
    "ax.hist(samples_hat[0,:], alpha=0.5, label=r'Horizon $\\tau+1$')\n",
    "ax.hist(samples_hat[1,:], alpha=0.5, label=r'Horizon $\\tau+2$')\n",
    "ax.set(xlabel='Y values', ylabel='Probability')\n",
    "plt.title('Single horizon Distributions')\n",
    "plt.legend(bbox_to_anchor=(1, 1), loc='upper left', ncol=1)\n",
    "plt.grid()\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Plot simulated trajectory\n",
    "fig, ax = plt.subplots(figsize=(3.7, 2.9))\n",
    "plt.plot(x_plot, y_plot_hat[:,2], color='black', label='median [q50]')\n",
    "plt.fill_between(x_plot,\n",
    "                 y1=y_plot_hat[:,1], y2=y_plot_hat[:,3],\n",
    "                 facecolor='blue', alpha=0.4, label='[p25-p75]')\n",
    "plt.fill_between(x_plot,\n",
    "                 y1=y_plot_hat[:,0], y2=y_plot_hat[:,4],\n",
    "                 facecolor='blue', alpha=0.2, label='[p1-p99]')\n",
    "ax.set(xlabel='Horizon', ylabel='Y values')\n",
    "plt.title('PMM Probabilistic Predictions')\n",
    "plt.legend(bbox_to_anchor=(1, 1), loc='upper left', ncol=1)\n",
    "plt.grid()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e84e0dd4",
   "metadata": {},
   "source": [
    "## Gaussian Mixture Mesh (GMM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6928b0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class GMM(torch.nn.Module):\n",
    "    \"\"\" Gaussian Mixture Mesh\n",
    "\n",
    "    This Gaussian Mixture statistical model assumes independence across groups of \n",
    "    data $\\mathcal{G}=\\{[g_{i}]\\}$, and estimates relationships within the group.\n",
    "\n",
    "    $$ \\mathrm{P}\\\\left(\\mathbf{y}_{[b][t+1:t+H]}\\\\right) = \n",
    "    \\prod_{ [g_{i}] \\in \\mathcal{G}} \\mathrm{P}\\left(\\mathbf{y}_{[g_{i}][\\\\tau]}\\\\right)=\n",
    "    \\prod_{\\\\beta\\in[g_{i}]}\n",
    "    \\\\left(\\sum_{k=1}^{K} w_k \\prod_{(\\\\beta,\\\\tau) \\in [g_i][t+1:t+H]} \n",
    "    \\mathrm{Gaussian}(y_{\\\\beta,\\\\tau}, \\hat{\\mu}_{\\\\beta,\\\\tau,k}, \\sigma_{\\\\beta,\\\\tau,k})\\\\right)$$\n",
    "\n",
    "    **Parameters:**<br>\n",
    "    `n_components`: int=10, the number of mixture components.<br>\n",
    "    `level`: float list [0,100], confidence levels for prediction intervals.<br>\n",
    "    `quantiles`: float list [0,1], alternative to level list, target quantiles.<br>\n",
    "    `return_params`: bool=False, wether or not return the Distribution parameters.<br>\n",
    "    `batch_correlation`: bool=False, wether or not model batch correlations.<br>\n",
    "    `horizon_correlation`: bool=False, wether or not model horizon correlations.<br><br>\n",
    "\n",
    "    **References:**<br>\n",
    "    [Kin G. Olivares, O. Nganba Meetei, Ruijun Ma, Rohan Reddy, Mengfei Cao, Lee Dicker. \n",
    "    Probabilistic Hierarchical Forecasting with Deep Poisson Mixtures. Submitted to the International \n",
    "    Journal Forecasting, Working paper available at arxiv.](https://arxiv.org/pdf/2110.13179.pdf)\n",
    "    \"\"\"\n",
    "    def __init__(self, n_components=1, level=[80, 90], quantiles=None, \n",
    "                 num_samples=1000, return_params=False,\n",
    "                 batch_correlation=False, horizon_correlation=False,\n",
    "                 weighted=False):\n",
    "        super(GMM, self).__init__()\n",
    "        # Transform level to MQLoss parameters\n",
    "        qs, self.output_names = level_to_outputs(level)\n",
    "        qs = torch.Tensor(qs)\n",
    "\n",
    "        # Transform quantiles to homogeneus output names\n",
    "        if quantiles is not None:\n",
    "            _, self.output_names = quantiles_to_outputs(quantiles)\n",
    "            qs = torch.Tensor(quantiles)\n",
    "        self.quantiles = torch.nn.Parameter(qs, requires_grad=False)\n",
    "        self.num_samples = num_samples\n",
    "        self.batch_correlation = batch_correlation\n",
    "        self.horizon_correlation = horizon_correlation     \n",
    "        self.weighted = weighted   \n",
    "\n",
    "        # If True, predict_step will return Distribution's parameters\n",
    "        self.return_params = return_params\n",
    "\n",
    "        mu_names = [f\"-mu-{i}\" for i in range(1, n_components + 1)]\n",
    "        std_names = [f\"-std-{i}\" for i in range(1, n_components + 1)]\n",
    "        if weighted:\n",
    "            weight_names = [f\"-weight-{i}\" for i in range(1, n_components + 1)]\n",
    "            self.param_names = [\n",
    "            i for j in zip(mu_names, std_names, weight_names) for i in j\n",
    "        ]\n",
    "        else:\n",
    "            self.param_names = [i for j in zip(mu_names, std_names) for i in j]\n",
    "\n",
    "        if self.return_params:\n",
    "            self.output_names = self.output_names + self.param_names\n",
    "\n",
    "        # Add first output entry for the sample_mean\n",
    "        self.output_names.insert(0, \"\")\n",
    "\n",
    "        self.n_outputs = 2 + weighted\n",
    "        self.n_components = n_components\n",
    "        self.outputsize_multiplier = self.n_outputs * n_components\n",
    "        self.is_distribution_output = True\n",
    "        self.has_predicted = False\n",
    "\n",
    "    def domain_map(self, output: torch.Tensor):\n",
    "        output = output.reshape(output.shape[0],\n",
    "                                output.shape[1],\n",
    "                               -1,\n",
    "                               self.outputsize_multiplier)\n",
    "        \n",
    "        return torch.tensor_split(output, self.n_outputs, dim=-1)\n",
    "\n",
    "    def scale_decouple(self, \n",
    "                       output,\n",
    "                       loc: Optional[torch.Tensor] = None,\n",
    "                       scale: Optional[torch.Tensor] = None,\n",
    "                       eps: float=0.2):\n",
    "        \"\"\" Scale Decouple\n",
    "\n",
    "        Stabilizes model's output optimization, by learning residual\n",
    "        variance and residual location based on anchoring `loc`, `scale`.\n",
    "        Also adds domain protection to the distribution parameters.\n",
    "        \"\"\"\n",
    "        if self.weighted:\n",
    "            means, stds, weights = output\n",
    "            weights = F.softmax(weights, dim=-1)\n",
    "        else:\n",
    "            means, stds = output\n",
    "            \n",
    "        stds = F.softplus(stds)\n",
    "        if (loc is not None) and (scale is not None):\n",
    "            if loc.ndim == 3:\n",
    "                loc = loc.unsqueeze(-1)\n",
    "                scale = scale.unsqueeze(-1)\n",
    "            means = (means * scale) + loc\n",
    "            stds = (stds + eps) * scale\n",
    "        \n",
    "        if self.weighted:\n",
    "            return (means, stds, weights)\n",
    "        else:\n",
    "            return (means, stds)\n",
    "\n",
    "    def get_distribution(self, distr_args) -> Distribution:\n",
    "        \"\"\"\n",
    "        Construct the associated Pytorch Distribution, given the collection of\n",
    "        constructor arguments and, optionally, location and scale tensors.\n",
    "\n",
    "        **Parameters**<br>\n",
    "        `distr_args`: Constructor arguments for the underlying Distribution type.<br>\n",
    "\n",
    "        **Returns**<br>\n",
    "        `Distribution`: AffineTransformed distribution.<br>\n",
    "        \"\"\"\n",
    "        if self.weighted:\n",
    "            means, stds, weights = distr_args\n",
    "        else:\n",
    "            means, stds = distr_args\n",
    "            weights = torch.full_like(means, fill_value=1 / self.n_components)\n",
    "            \n",
    "        mix = Categorical(weights)\n",
    "        components = Normal(loc=means, scale=stds)\n",
    "        distr = MixtureSameFamily(mixture_distribution=mix,\n",
    "                                      component_distribution=components)    \n",
    "\n",
    "        self.distr_mean = distr.mean\n",
    "        \n",
    "        return distr\n",
    "\n",
    "    def sample(self,\n",
    "               distr_args: torch.Tensor,\n",
    "               num_samples: Optional[int] = None):\n",
    "        \"\"\"\n",
    "        Construct the empirical quantiles from the estimated Distribution,\n",
    "        sampling from it `num_samples` independently.\n",
    "\n",
    "        **Parameters**<br>\n",
    "        `distr_args`: Constructor arguments for the underlying Distribution type.<br>\n",
    "        `num_samples`: int, overwrite number of samples for the empirical quantiles.<br>\n",
    "\n",
    "        **Returns**<br>\n",
    "        `samples`: tensor, shape [B,H,`num_samples`].<br>\n",
    "        `quantiles`: tensor, empirical quantiles defined by `levels`.<br>\n",
    "        \"\"\"\n",
    "        if num_samples is None:\n",
    "            num_samples = self.num_samples\n",
    "\n",
    "        # Instantiate Scaled Decoupled Distribution\n",
    "        distr = self.get_distribution(distr_args=distr_args)\n",
    "        samples = distr.sample(sample_shape=(num_samples,))\n",
    "        samples = samples.permute(1, 2, 3, 0)                  # [samples, B, H, N] -> [B, H, N, samples]\n",
    "\n",
    "        sample_mean = torch.mean(samples, dim=-1, keepdim=True) \n",
    "\n",
    "        # Compute quantiles\n",
    "        quantiles_device = self.quantiles.to(distr_args[0].device)\n",
    "        quants = torch.quantile(input=samples, \n",
    "                                q=quantiles_device, \n",
    "                                dim=-1)\n",
    "        quants = quants.permute(1, 2, 3, 0) # [Q, B, H, N] -> [B, H, N, Q]\n",
    "\n",
    "        return samples, sample_mean, quants\n",
    "    \n",
    "    def update_quantile(self, q: Optional[List[float]] = None):\n",
    "        if q is not None:\n",
    "          self.quantiles = nn.Parameter(torch.tensor(q, dtype=torch.float32), requires_grad=False)\n",
    "          self.output_names = [\"\"] + [f\"_ql{q_i}\" for q_i in q] + self.return_params * self.param_names\n",
    "          self.has_predicted = True\n",
    "        elif q is None and self.has_predicted:\n",
    "          self.quantiles = nn.Parameter(torch.tensor([0.5], dtype=torch.float32), requires_grad=False)          \n",
    "          self.output_names = [\"\", \"-median\"] + self.return_params * self.param_names\n",
    "\n",
    "    def __call__(self,\n",
    "                 y: torch.Tensor,\n",
    "                 distr_args: torch.Tensor,\n",
    "                 mask: Union[torch.Tensor, None] = None):\n",
    "        \"\"\"\n",
    "        Computes the negative log-likelihood objective function. \n",
    "        To estimate the following predictive distribution:\n",
    "\n",
    "        $$\\mathrm{P}(\\mathbf{y}_{\\\\tau}\\,|\\,\\\\theta) \\\\quad \\mathrm{and} \\\\quad -\\log(\\mathrm{P}(\\mathbf{y}_{\\\\tau}\\,|\\,\\\\theta))$$\n",
    "\n",
    "        where $\\\\theta$ represents the distributions parameters. It aditionally \n",
    "        summarizes the objective signal using a weighted average using the `mask` tensor. \n",
    "\n",
    "        **Parameters**<br>\n",
    "        `y`: tensor, Actual values.<br>\n",
    "        `distr_args`: Constructor arguments for the underlying Distribution type.<br>\n",
    "        `mask`: tensor, Specifies date stamps per serie to consider in loss.<br>\n",
    "\n",
    "        **Returns**<br>\n",
    "        `loss`: scalar, weighted loss function against which backpropagation will be performed.<br>\n",
    "        \"\"\"\n",
    "        # Instantiate Scaled Decoupled Distribution\n",
    "        distr = self.get_distribution(distr_args=distr_args)\n",
    "        x = distr._pad(y)\n",
    "        log_prob_x = distr.component_distribution.log_prob(x)\n",
    "        log_mix_prob = torch.log_softmax(distr.mixture_distribution.logits, dim=-1)\n",
    "        if self.batch_correlation:\n",
    "                log_prob_x = torch.sum(log_prob_x, dim=0, keepdim=True)\n",
    "        if self.horizon_correlation:\n",
    "                log_prob_x = torch.sum(log_prob_x, dim=1, keepdim=True)\n",
    "        loss_values = -torch.logsumexp(log_prob_x + log_mix_prob, dim=-1)  \n",
    "       \n",
    "        return weighted_average(loss_values, weights=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4ebf3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/losses/pytorch.py#L2248){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### GMM.__init__\n",
       "\n",
       ">      GMM.__init__ (n_components=1, level=[80, 90], quantiles=None,\n",
       ">                    num_samples=1000, return_params=False,\n",
       ">                    batch_correlation=False, horizon_correlation=False,\n",
       ">                    weighted=False)\n",
       "\n",
       "*Gaussian Mixture Mesh\n",
       "\n",
       "This Gaussian Mixture statistical model assumes independence across groups of \n",
       "data $\\mathcal{G}=\\{[g_{i}]\\}$, and estimates relationships within the group.\n",
       "\n",
       "$$ \\mathrm{P}\\left(\\mathbf{y}_{[b][t+1:t+H]}\\right) = \n",
       "\\prod_{ [g_{i}] \\in \\mathcal{G}} \\mathrm{P}\\left(\\mathbf{y}_{[g_{i}][\\tau]}\\right)=\n",
       "\\prod_{\\beta\\in[g_{i}]}\n",
       "\\left(\\sum_{k=1}^{K} w_k \\prod_{(\\beta,\\tau) \\in [g_i][t+1:t+H]} \n",
       "\\mathrm{Gaussian}(y_{\\beta,\\tau}, \\hat{\\mu}_{\\beta,\\tau,k}, \\sigma_{\\beta,\\tau,k})\\right)$$\n",
       "\n",
       "**Parameters:**<br>\n",
       "`n_components`: int=10, the number of mixture components.<br>\n",
       "`level`: float list [0,100], confidence levels for prediction intervals.<br>\n",
       "`quantiles`: float list [0,1], alternative to level list, target quantiles.<br>\n",
       "`return_params`: bool=False, wether or not return the Distribution parameters.<br>\n",
       "`batch_correlation`: bool=False, wether or not model batch correlations.<br>\n",
       "`horizon_correlation`: bool=False, wether or not model horizon correlations.<br><br>\n",
       "\n",
       "**References:**<br>\n",
       "[Kin G. Olivares, O. Nganba Meetei, Ruijun Ma, Rohan Reddy, Mengfei Cao, Lee Dicker. \n",
       "Probabilistic Hierarchical Forecasting with Deep Poisson Mixtures. Submitted to the International \n",
       "Journal Forecasting, Working paper available at arxiv.](https://arxiv.org/pdf/2110.13179.pdf)*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/losses/pytorch.py#L2248){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### GMM.__init__\n",
       "\n",
       ">      GMM.__init__ (n_components=1, level=[80, 90], quantiles=None,\n",
       ">                    num_samples=1000, return_params=False,\n",
       ">                    batch_correlation=False, horizon_correlation=False,\n",
       ">                    weighted=False)\n",
       "\n",
       "*Gaussian Mixture Mesh\n",
       "\n",
       "This Gaussian Mixture statistical model assumes independence across groups of \n",
       "data $\\mathcal{G}=\\{[g_{i}]\\}$, and estimates relationships within the group.\n",
       "\n",
       "$$ \\mathrm{P}\\left(\\mathbf{y}_{[b][t+1:t+H]}\\right) = \n",
       "\\prod_{ [g_{i}] \\in \\mathcal{G}} \\mathrm{P}\\left(\\mathbf{y}_{[g_{i}][\\tau]}\\right)=\n",
       "\\prod_{\\beta\\in[g_{i}]}\n",
       "\\left(\\sum_{k=1}^{K} w_k \\prod_{(\\beta,\\tau) \\in [g_i][t+1:t+H]} \n",
       "\\mathrm{Gaussian}(y_{\\beta,\\tau}, \\hat{\\mu}_{\\beta,\\tau,k}, \\sigma_{\\beta,\\tau,k})\\right)$$\n",
       "\n",
       "**Parameters:**<br>\n",
       "`n_components`: int=10, the number of mixture components.<br>\n",
       "`level`: float list [0,100], confidence levels for prediction intervals.<br>\n",
       "`quantiles`: float list [0,1], alternative to level list, target quantiles.<br>\n",
       "`return_params`: bool=False, wether or not return the Distribution parameters.<br>\n",
       "`batch_correlation`: bool=False, wether or not model batch correlations.<br>\n",
       "`horizon_correlation`: bool=False, wether or not model horizon correlations.<br><br>\n",
       "\n",
       "**References:**<br>\n",
       "[Kin G. Olivares, O. Nganba Meetei, Ruijun Ma, Rohan Reddy, Mengfei Cao, Lee Dicker. \n",
       "Probabilistic Hierarchical Forecasting with Deep Poisson Mixtures. Submitted to the International \n",
       "Journal Forecasting, Working paper available at arxiv.](https://arxiv.org/pdf/2110.13179.pdf)*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(GMM, name='GMM.__init__', title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea56d8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/losses/pytorch.py#L2338){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### GMM.sample\n",
       "\n",
       ">      GMM.sample (distr_args:torch.Tensor, num_samples:Optional[int]=None)\n",
       "\n",
       "*Construct the empirical quantiles from the estimated Distribution,\n",
       "sampling from it `num_samples` independently.\n",
       "\n",
       "**Parameters**<br>\n",
       "`distr_args`: Constructor arguments for the underlying Distribution type.<br>\n",
       "`num_samples`: int, overwrite number of samples for the empirical quantiles.<br>\n",
       "\n",
       "**Returns**<br>\n",
       "`samples`: tensor, shape [B,H,`num_samples`].<br>\n",
       "`quantiles`: tensor, empirical quantiles defined by `levels`.<br>*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/losses/pytorch.py#L2338){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### GMM.sample\n",
       "\n",
       ">      GMM.sample (distr_args:torch.Tensor, num_samples:Optional[int]=None)\n",
       "\n",
       "*Construct the empirical quantiles from the estimated Distribution,\n",
       "sampling from it `num_samples` independently.\n",
       "\n",
       "**Parameters**<br>\n",
       "`distr_args`: Constructor arguments for the underlying Distribution type.<br>\n",
       "`num_samples`: int, overwrite number of samples for the empirical quantiles.<br>\n",
       "\n",
       "**Returns**<br>\n",
       "`samples`: tensor, shape [B,H,`num_samples`].<br>\n",
       "`quantiles`: tensor, empirical quantiles defined by `levels`.<br>*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(GMM.sample, name='GMM.sample', title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f16e4f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/losses/pytorch.py#L2445){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### GMM.__call__\n",
       "\n",
       ">      GMM.__call__ (y:torch.Tensor, distr_args:torch.Tensor,\n",
       ">                    mask:Optional[torch.Tensor]=None)\n",
       "\n",
       "*Computes the negative log-likelihood objective function. \n",
       "To estimate the following predictive distribution:\n",
       "\n",
       "$$\\mathrm{P}(\\mathbf{y}_{\\tau}\\,|\\,\\theta) \\quad \\mathrm{and} \\quad -\\log(\\mathrm{P}(\\mathbf{y}_{\\tau}\\,|\\,\\theta))$$\n",
       "\n",
       "where $\\theta$ represents the distributions parameters. It aditionally \n",
       "summarizes the objective signal using a weighted average using the `mask` tensor. \n",
       "\n",
       "**Parameters**<br>\n",
       "`y`: tensor, Actual values.<br>\n",
       "`distr_args`: Constructor arguments for the underlying Distribution type.<br>\n",
       "`mask`: tensor, Specifies date stamps per serie to consider in loss.<br>\n",
       "\n",
       "**Returns**<br>\n",
       "`loss`: scalar, weighted loss function against which backpropagation will be performed.<br>*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/losses/pytorch.py#L2445){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### GMM.__call__\n",
       "\n",
       ">      GMM.__call__ (y:torch.Tensor, distr_args:torch.Tensor,\n",
       ">                    mask:Optional[torch.Tensor]=None)\n",
       "\n",
       "*Computes the negative log-likelihood objective function. \n",
       "To estimate the following predictive distribution:\n",
       "\n",
       "$$\\mathrm{P}(\\mathbf{y}_{\\tau}\\,|\\,\\theta) \\quad \\mathrm{and} \\quad -\\log(\\mathrm{P}(\\mathbf{y}_{\\tau}\\,|\\,\\theta))$$\n",
       "\n",
       "where $\\theta$ represents the distributions parameters. It aditionally \n",
       "summarizes the objective signal using a weighted average using the `mask` tensor. \n",
       "\n",
       "**Parameters**<br>\n",
       "`y`: tensor, Actual values.<br>\n",
       "`distr_args`: Constructor arguments for the underlying Distribution type.<br>\n",
       "`mask`: tensor, Specifies date stamps per serie to consider in loss.<br>\n",
       "\n",
       "**Returns**<br>\n",
       "`loss`: scalar, weighted loss function against which backpropagation will be performed.<br>*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(GMM.__call__, name='GMM.__call__', title_level=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aed232a4",
   "metadata": {},
   "source": [
    "![](imgs_losses/gmm.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebe4250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '-lo-98.0', '-lo-80.0', '-median', '-hi-80.0', '-hi-98.0']\n",
      "Parameter containing:\n",
      "tensor([0.0100, 0.1000, 0.5000, 0.9000, 0.9900])\n"
     ]
    }
   ],
   "source": [
    "# | hide\n",
    "# Unit tests to check PMM's stored quantiles\n",
    "# attribute is correctly instantiated\n",
    "check = GMM(n_components=2, level=[80, 90])\n",
    "test_eq(len(check.quantiles), 5)\n",
    "\n",
    "check = GMM(n_components=2, \n",
    "            quantiles=[0.0100, 0.1000, 0.5, 0.9000, 0.9900])\n",
    "print(check.output_names)\n",
    "print(check.quantiles)\n",
    "test_eq(len(check.quantiles), 5)\n",
    "\n",
    "check = GMM(n_components=2,\n",
    "            quantiles=[0.0100, 0.1000, 0.9000, 0.9900])\n",
    "test_eq(len(check.quantiles), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684d2382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights.shape (N,H,1,K) \t torch.Size([2, 2, 1, 3])\n",
      "means.shape (N,H,1,K) \t torch.Size([2, 2, 1, 3])\n",
      "stds.shape (N,H,1,K) \t torch.Size([2, 2, 1, 3])\n",
      "samples.shape (N,H,1,num_samples)  torch.Size([2, 2, 1, 1000])\n",
      "sample_mean.shape (N,H,1,1)  torch.Size([2, 2, 1, 1])\n",
      "quants.shape  (N,H,1, Q) \t\t torch.Size([2, 2, 1, 5])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 370x290 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 370x290 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| hide\n",
    "# Create single mixture and broadcast to N,H,1,K\n",
    "means   = torch.Tensor([[5,10,15], [10,20,30]])[None, :, :].unsqueeze(2)\n",
    "\n",
    "# # Create repetitions for the batch dimension N.\n",
    "N=2\n",
    "means = torch.repeat_interleave(input=means, repeats=N, dim=0)\n",
    "weights = torch.ones_like(means)\n",
    "stds  = torch.ones_like(means)\n",
    "\n",
    "print('weights.shape (N,H,1,K) \\t', weights.shape)\n",
    "print('means.shape (N,H,1,K) \\t', means.shape)\n",
    "print('stds.shape (N,H,1,K) \\t', stds.shape)\n",
    "\n",
    "distr = GMM(quantiles=[0.1, 0.40, 0.5, 0.60, 0.9], weighted=True)\n",
    "distr_args = (means, stds, weights)\n",
    "samples, sample_mean, quants = distr.sample(distr_args)\n",
    "\n",
    "print('samples.shape (N,H,1,num_samples) ', samples.shape)\n",
    "print('sample_mean.shape (N,H,1,1) ', sample_mean.shape)\n",
    "print('quants.shape  (N,H,1, Q) \\t\\t', quants.shape)\n",
    "\n",
    "# Plot synthethic data\n",
    "x_plot = range(quants.shape[1]) # H length\n",
    "y_plot_hat = quants[0,:,0,:]  # Filter N,G,T -> H,Q\n",
    "samples_hat = samples[0,:,0,:]  # Filter N,G,T -> H,num_samples\n",
    "\n",
    "# Kernel density plot for single forecast horizon \\tau = t+1\n",
    "fig, ax = plt.subplots(figsize=(3.7, 2.9))\n",
    "\n",
    "ax.hist(samples_hat[0,:], alpha=0.5, bins=50,\n",
    "        label=r'Horizon $\\tau+1$')\n",
    "ax.hist(samples_hat[1,:], alpha=0.5, bins=50,\n",
    "        label=r'Horizon $\\tau+2$')\n",
    "ax.set(xlabel='Y values', ylabel='Probability')\n",
    "plt.title('Single horizon Distributions')\n",
    "plt.legend(bbox_to_anchor=(1, 1), loc='upper left', ncol=1)\n",
    "plt.grid()\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Plot simulated trajectory\n",
    "fig, ax = plt.subplots(figsize=(3.7, 2.9))\n",
    "plt.plot(x_plot, y_plot_hat[:,2], color='black', label='median [q50]')\n",
    "plt.fill_between(x_plot,\n",
    "                 y1=y_plot_hat[:,1], y2=y_plot_hat[:,3],\n",
    "                 facecolor='blue', alpha=0.4, label='[p25-p75]')\n",
    "plt.fill_between(x_plot,\n",
    "                 y1=y_plot_hat[:,0], y2=y_plot_hat[:,4],\n",
    "                 facecolor='blue', alpha=0.2, label='[p1-p99]')\n",
    "ax.set(xlabel='Horizon', ylabel='Y values')\n",
    "plt.title('GMM Probabilistic Predictions')\n",
    "plt.legend(bbox_to_anchor=(1, 1), loc='upper left', ncol=1)\n",
    "plt.grid()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "694a2afe",
   "metadata": {},
   "source": [
    "## Negative Binomial Mixture Mesh (NBMM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdbe5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class NBMM(torch.nn.Module):\n",
    "    \"\"\" Negative Binomial Mixture Mesh\n",
    "\n",
    "    This N. Binomial Mixture statistical model assumes independence across groups of \n",
    "    data $\\mathcal{G}=\\{[g_{i}]\\}$, and estimates relationships within the group.\n",
    "\n",
    "    $$ \\mathrm{P}\\\\left(\\mathbf{y}_{[b][t+1:t+H]}\\\\right) = \n",
    "    \\prod_{ [g_{i}] \\in \\mathcal{G}} \\mathrm{P}\\left(\\mathbf{y}_{[g_{i}][\\\\tau]}\\\\right)=\n",
    "    \\prod_{\\\\beta\\in[g_{i}]}\n",
    "    \\\\left(\\sum_{k=1}^{K} w_k \\prod_{(\\\\beta,\\\\tau) \\in [g_i][t+1:t+H]} \n",
    "    \\mathrm{NBinomial}(y_{\\\\beta,\\\\tau}, \\hat{r}_{\\\\beta,\\\\tau,k}, \\hat{p}_{\\\\beta,\\\\tau,k})\\\\right)$$\n",
    "\n",
    "    **Parameters:**<br>\n",
    "    `n_components`: int=10, the number of mixture components.<br>\n",
    "    `level`: float list [0,100], confidence levels for prediction intervals.<br>\n",
    "    `quantiles`: float list [0,1], alternative to level list, target quantiles.<br>\n",
    "    `return_params`: bool=False, wether or not return the Distribution parameters.<br><br>\n",
    "\n",
    "    **References:**<br>\n",
    "    [Kin G. Olivares, O. Nganba Meetei, Ruijun Ma, Rohan Reddy, Mengfei Cao, Lee Dicker. \n",
    "    Probabilistic Hierarchical Forecasting with Deep Poisson Mixtures. Submitted to the International \n",
    "    Journal Forecasting, Working paper available at arxiv.](https://arxiv.org/pdf/2110.13179.pdf)\n",
    "    \"\"\"\n",
    "    def __init__(self, n_components=1, level=[80, 90], quantiles=None, \n",
    "                 num_samples=1000, return_params=False, weighted=False):\n",
    "        super(NBMM, self).__init__()\n",
    "        # Transform level to MQLoss parameters\n",
    "        qs, self.output_names = level_to_outputs(level)\n",
    "        qs = torch.Tensor(qs)\n",
    "\n",
    "        # Transform quantiles to homogeneus output names\n",
    "        if quantiles is not None:\n",
    "            _, self.output_names = quantiles_to_outputs(quantiles)\n",
    "            qs = torch.Tensor(quantiles)\n",
    "        self.quantiles = torch.nn.Parameter(qs, requires_grad=False)\n",
    "        self.num_samples = num_samples\n",
    "        self.weighted = weighted   \n",
    "\n",
    "        # If True, predict_step will return Distribution's parameters\n",
    "        self.return_params = return_params\n",
    "\n",
    "        total_count_names = [f\"-total_count-{i}\" for i in range(1, n_components + 1)]\n",
    "        probs_names = [f\"-probs-{i}\" for i in range(1, n_components + 1)]\n",
    "        if weighted:\n",
    "            weight_names = [f\"-weight-{i}\" for i in range(1, n_components + 1)]\n",
    "            self.param_names = [\n",
    "            i for j in zip(total_count_names, probs_names, weight_names) for i in j\n",
    "        ]\n",
    "        else:\n",
    "            self.param_names = [i for j in zip(total_count_names, probs_names) for i in j]\n",
    "\n",
    "        if self.return_params:\n",
    "            self.output_names = self.output_names + self.param_names\n",
    "\n",
    "        # Add first output entry for the sample_mean\n",
    "        self.output_names.insert(0, \"\")            \n",
    "\n",
    "        self.n_outputs = 2 + weighted\n",
    "        self.n_components = n_components\n",
    "        self.outputsize_multiplier = self.n_outputs * n_components\n",
    "        self.is_distribution_output = True\n",
    "        self.has_predicted = False\n",
    "\n",
    "    def domain_map(self, output: torch.Tensor):\n",
    "        output = output.reshape(output.shape[0],\n",
    "                                output.shape[1],\n",
    "                               -1,\n",
    "                               self.outputsize_multiplier)\n",
    "        \n",
    "        return torch.tensor_split(output, self.n_outputs, dim=-1)\n",
    "\n",
    "    def scale_decouple(self, \n",
    "                       output,\n",
    "                       loc: Optional[torch.Tensor] = None,\n",
    "                       scale: Optional[torch.Tensor] = None,\n",
    "                       eps: float=0.2):\n",
    "        \"\"\" Scale Decouple\n",
    "\n",
    "        Stabilizes model's output optimization, by learning residual\n",
    "        variance and residual location based on anchoring `loc`, `scale`.\n",
    "        Also adds domain protection to the distribution parameters.\n",
    "        \"\"\"\n",
    "        # Efficient NBinomial parametrization\n",
    "        if self.weighted:\n",
    "            mu, alpha, weights = output\n",
    "            weights = F.softmax(weights, dim=-1)\n",
    "        else:\n",
    "            mu, alpha = output\n",
    "\n",
    "        mu = F.softplus(mu) + 1e-8\n",
    "        alpha = F.softplus(alpha) + 1e-8    # alpha = 1/total_counts\n",
    "        if (loc is not None) and (scale is not None):\n",
    "            if loc.ndim == 3:\n",
    "                loc = loc.unsqueeze(-1)\n",
    "                scale = scale.unsqueeze(-1)           \n",
    "            mu *= loc\n",
    "            alpha /= (loc + 1.)\n",
    "\n",
    "        # mu = total_count * (probs/(1-probs))\n",
    "        # => probs = mu / (total_count + mu)\n",
    "        # => probs = mu / [total_count * (1 + mu * (1/total_count))]\n",
    "        total_count = 1.0 / alpha\n",
    "        probs = (mu * alpha / (1.0 + mu * alpha)) + 1e-8 \n",
    "        if self.weighted:\n",
    "            return (total_count, probs, weights)\n",
    "        else:\n",
    "            return (total_count, probs)\n",
    "\n",
    "    def get_distribution(self, distr_args) -> Distribution:\n",
    "        \"\"\"\n",
    "        Construct the associated Pytorch Distribution, given the collection of\n",
    "        constructor arguments and, optionally, location and scale tensors.\n",
    "\n",
    "        **Parameters**<br>\n",
    "        `distr_args`: Constructor arguments for the underlying Distribution type.<br>\n",
    "\n",
    "        **Returns**<br>\n",
    "        `Distribution`: AffineTransformed distribution.<br>\n",
    "        \"\"\"\n",
    "        if self.weighted:\n",
    "            total_count, probs, weights = distr_args\n",
    "        else:\n",
    "            total_count, probs = distr_args\n",
    "            weights = torch.full_like(total_count, fill_value=1 / self.n_components)\n",
    "\n",
    "        mix = Categorical(weights)\n",
    "        components = NegativeBinomial(total_count, probs)\n",
    "        components.support = constraints.nonnegative\n",
    "        distr = MixtureSameFamily(mixture_distribution=mix,\n",
    "                                      component_distribution=components)    \n",
    "\n",
    "        self.distr_mean = distr.mean\n",
    "        \n",
    "        return distr\n",
    "\n",
    "    def sample(self,\n",
    "               distr_args: torch.Tensor,\n",
    "               num_samples: Optional[int] = None):\n",
    "        \"\"\"\n",
    "        Construct the empirical quantiles from the estimated Distribution,\n",
    "        sampling from it `num_samples` independently.\n",
    "\n",
    "        **Parameters**<br>\n",
    "        `distr_args`: Constructor arguments for the underlying Distribution type.<br>\n",
    "        `num_samples`: int, overwrite number of samples for the empirical quantiles.<br>\n",
    "\n",
    "        **Returns**<br>\n",
    "        `samples`: tensor, shape [B,H,`num_samples`].<br>\n",
    "        `quantiles`: tensor, empirical quantiles defined by `levels`.<br>\n",
    "        \"\"\"\n",
    "        if num_samples is None:\n",
    "            num_samples = self.num_samples\n",
    "\n",
    "        # Instantiate Scaled Decoupled Distribution\n",
    "        distr = self.get_distribution(distr_args=distr_args)\n",
    "        samples = distr.sample(sample_shape=(num_samples,))\n",
    "        samples = samples.permute(1, 2, 3, 0)                  # [samples, B, H, N] -> [B, H, N, samples]\n",
    "\n",
    "        sample_mean = torch.mean(samples, dim=-1, keepdim=True) \n",
    "\n",
    "        # Compute quantiles\n",
    "        quantiles_device = self.quantiles.to(distr_args[0].device)\n",
    "        quants = torch.quantile(input=samples, \n",
    "                                q=quantiles_device, \n",
    "                                dim=-1)\n",
    "        quants = quants.permute(1, 2, 3, 0) # [Q, B, H, N] -> [B, H, N, Q]\n",
    "\n",
    "        return samples, sample_mean, quants\n",
    "\n",
    "    def update_quantile(self, q: Optional[List[float]] = None):\n",
    "        if q is not None:\n",
    "          self.quantiles = nn.Parameter(torch.tensor(q, dtype=torch.float32), requires_grad=False)\n",
    "          self.output_names = [\"\"] + [f\"_ql{q_i}\" for q_i in q] + self.return_params * self.param_names\n",
    "          self.has_predicted = True\n",
    "        elif q is None and self.has_predicted:\n",
    "          self.quantiles = nn.Parameter(torch.tensor([0.5], dtype=torch.float32), requires_grad=False)\n",
    "          self.output_names = [\"\", \"-median\"] + self.return_params * self.param_names\n",
    "\n",
    "    def __call__(self,\n",
    "                 y: torch.Tensor,\n",
    "                 distr_args: torch.Tensor,\n",
    "                 mask: Union[torch.Tensor, None] = None):\n",
    "        \"\"\"\n",
    "        Computes the negative log-likelihood objective function. \n",
    "        To estimate the following predictive distribution:\n",
    "\n",
    "        $$\\mathrm{P}(\\mathbf{y}_{\\\\tau}\\,|\\,\\\\theta) \\\\quad \\mathrm{and} \\\\quad -\\log(\\mathrm{P}(\\mathbf{y}_{\\\\tau}\\,|\\,\\\\theta))$$\n",
    "\n",
    "        where $\\\\theta$ represents the distributions parameters. It aditionally \n",
    "        summarizes the objective signal using a weighted average using the `mask` tensor. \n",
    "\n",
    "        **Parameters**<br>\n",
    "        `y`: tensor, Actual values.<br>\n",
    "        `distr_args`: Constructor arguments for the underlying Distribution type.<br>\n",
    "        `mask`: tensor, Specifies date stamps per serie to consider in loss.<br>\n",
    "\n",
    "        **Returns**<br>\n",
    "        `loss`: scalar, weighted loss function against which backpropagation will be performed.<br>\n",
    "        \"\"\"\n",
    "        # Instantiate Scaled Decoupled Distribution\n",
    "        distr = self.get_distribution(distr_args=distr_args)\n",
    "        loss_values = -distr.log_prob(y)\n",
    "        loss_weights = mask\n",
    "       \n",
    "        return weighted_average(loss_values, weights=loss_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed5e73c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/losses/pytorch.py#L2455){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### NBMM.__init__\n",
       "\n",
       ">      NBMM.__init__ (n_components=1, level=[80, 90], quantiles=None,\n",
       ">                     num_samples=1000, return_params=False, weighted=False)\n",
       "\n",
       "*Negative Binomial Mixture Mesh\n",
       "\n",
       "This N. Binomial Mixture statistical model assumes independence across groups of \n",
       "data $\\mathcal{G}=\\{[g_{i}]\\}$, and estimates relationships within the group.\n",
       "\n",
       "$$ \\mathrm{P}\\left(\\mathbf{y}_{[b][t+1:t+H]}\\right) = \n",
       "\\prod_{ [g_{i}] \\in \\mathcal{G}} \\mathrm{P}\\left(\\mathbf{y}_{[g_{i}][\\tau]}\\right)=\n",
       "\\prod_{\\beta\\in[g_{i}]}\n",
       "\\left(\\sum_{k=1}^{K} w_k \\prod_{(\\beta,\\tau) \\in [g_i][t+1:t+H]} \n",
       "\\mathrm{NBinomial}(y_{\\beta,\\tau}, \\hat{r}_{\\beta,\\tau,k}, \\hat{p}_{\\beta,\\tau,k})\\right)$$\n",
       "\n",
       "**Parameters:**<br>\n",
       "`n_components`: int=10, the number of mixture components.<br>\n",
       "`level`: float list [0,100], confidence levels for prediction intervals.<br>\n",
       "`quantiles`: float list [0,1], alternative to level list, target quantiles.<br>\n",
       "`return_params`: bool=False, wether or not return the Distribution parameters.<br><br>\n",
       "\n",
       "**References:**<br>\n",
       "[Kin G. Olivares, O. Nganba Meetei, Ruijun Ma, Rohan Reddy, Mengfei Cao, Lee Dicker. \n",
       "Probabilistic Hierarchical Forecasting with Deep Poisson Mixtures. Submitted to the International \n",
       "Journal Forecasting, Working paper available at arxiv.](https://arxiv.org/pdf/2110.13179.pdf)*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/losses/pytorch.py#L2455){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### NBMM.__init__\n",
       "\n",
       ">      NBMM.__init__ (n_components=1, level=[80, 90], quantiles=None,\n",
       ">                     num_samples=1000, return_params=False, weighted=False)\n",
       "\n",
       "*Negative Binomial Mixture Mesh\n",
       "\n",
       "This N. Binomial Mixture statistical model assumes independence across groups of \n",
       "data $\\mathcal{G}=\\{[g_{i}]\\}$, and estimates relationships within the group.\n",
       "\n",
       "$$ \\mathrm{P}\\left(\\mathbf{y}_{[b][t+1:t+H]}\\right) = \n",
       "\\prod_{ [g_{i}] \\in \\mathcal{G}} \\mathrm{P}\\left(\\mathbf{y}_{[g_{i}][\\tau]}\\right)=\n",
       "\\prod_{\\beta\\in[g_{i}]}\n",
       "\\left(\\sum_{k=1}^{K} w_k \\prod_{(\\beta,\\tau) \\in [g_i][t+1:t+H]} \n",
       "\\mathrm{NBinomial}(y_{\\beta,\\tau}, \\hat{r}_{\\beta,\\tau,k}, \\hat{p}_{\\beta,\\tau,k})\\right)$$\n",
       "\n",
       "**Parameters:**<br>\n",
       "`n_components`: int=10, the number of mixture components.<br>\n",
       "`level`: float list [0,100], confidence levels for prediction intervals.<br>\n",
       "`quantiles`: float list [0,1], alternative to level list, target quantiles.<br>\n",
       "`return_params`: bool=False, wether or not return the Distribution parameters.<br><br>\n",
       "\n",
       "**References:**<br>\n",
       "[Kin G. Olivares, O. Nganba Meetei, Ruijun Ma, Rohan Reddy, Mengfei Cao, Lee Dicker. \n",
       "Probabilistic Hierarchical Forecasting with Deep Poisson Mixtures. Submitted to the International \n",
       "Journal Forecasting, Working paper available at arxiv.](https://arxiv.org/pdf/2110.13179.pdf)*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(NBMM, name='NBMM.__init__', title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ea98ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/losses/pytorch.py#L2548){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### NBMM.sample\n",
       "\n",
       ">      NBMM.sample (distr_args:torch.Tensor, num_samples:Optional[int]=None)\n",
       "\n",
       "*Construct the empirical quantiles from the estimated Distribution,\n",
       "sampling from it `num_samples` independently.\n",
       "\n",
       "**Parameters**<br>\n",
       "`distr_args`: Constructor arguments for the underlying Distribution type.<br>\n",
       "`num_samples`: int, overwrite number of samples for the empirical quantiles.<br>\n",
       "\n",
       "**Returns**<br>\n",
       "`samples`: tensor, shape [B,H,`num_samples`].<br>\n",
       "`quantiles`: tensor, empirical quantiles defined by `levels`.<br>*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/losses/pytorch.py#L2548){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### NBMM.sample\n",
       "\n",
       ">      NBMM.sample (distr_args:torch.Tensor, num_samples:Optional[int]=None)\n",
       "\n",
       "*Construct the empirical quantiles from the estimated Distribution,\n",
       "sampling from it `num_samples` independently.\n",
       "\n",
       "**Parameters**<br>\n",
       "`distr_args`: Constructor arguments for the underlying Distribution type.<br>\n",
       "`num_samples`: int, overwrite number of samples for the empirical quantiles.<br>\n",
       "\n",
       "**Returns**<br>\n",
       "`samples`: tensor, shape [B,H,`num_samples`].<br>\n",
       "`quantiles`: tensor, empirical quantiles defined by `levels`.<br>*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(NBMM.sample, name='NBMM.sample', title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7189c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/losses/pytorch.py#L2659){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### NBMM.__call__\n",
       "\n",
       ">      NBMM.__call__ (y:torch.Tensor, distr_args:torch.Tensor,\n",
       ">                     mask:Optional[torch.Tensor]=None)\n",
       "\n",
       "*Computes the negative log-likelihood objective function. \n",
       "To estimate the following predictive distribution:\n",
       "\n",
       "$$\\mathrm{P}(\\mathbf{y}_{\\tau}\\,|\\,\\theta) \\quad \\mathrm{and} \\quad -\\log(\\mathrm{P}(\\mathbf{y}_{\\tau}\\,|\\,\\theta))$$\n",
       "\n",
       "where $\\theta$ represents the distributions parameters. It aditionally \n",
       "summarizes the objective signal using a weighted average using the `mask` tensor. \n",
       "\n",
       "**Parameters**<br>\n",
       "`y`: tensor, Actual values.<br>\n",
       "`distr_args`: Constructor arguments for the underlying Distribution type.<br>\n",
       "`mask`: tensor, Specifies date stamps per serie to consider in loss.<br>\n",
       "\n",
       "**Returns**<br>\n",
       "`loss`: scalar, weighted loss function against which backpropagation will be performed.<br>*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/losses/pytorch.py#L2659){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### NBMM.__call__\n",
       "\n",
       ">      NBMM.__call__ (y:torch.Tensor, distr_args:torch.Tensor,\n",
       ">                     mask:Optional[torch.Tensor]=None)\n",
       "\n",
       "*Computes the negative log-likelihood objective function. \n",
       "To estimate the following predictive distribution:\n",
       "\n",
       "$$\\mathrm{P}(\\mathbf{y}_{\\tau}\\,|\\,\\theta) \\quad \\mathrm{and} \\quad -\\log(\\mathrm{P}(\\mathbf{y}_{\\tau}\\,|\\,\\theta))$$\n",
       "\n",
       "where $\\theta$ represents the distributions parameters. It aditionally \n",
       "summarizes the objective signal using a weighted average using the `mask` tensor. \n",
       "\n",
       "**Parameters**<br>\n",
       "`y`: tensor, Actual values.<br>\n",
       "`distr_args`: Constructor arguments for the underlying Distribution type.<br>\n",
       "`mask`: tensor, Specifies date stamps per serie to consider in loss.<br>\n",
       "\n",
       "**Returns**<br>\n",
       "`loss`: scalar, weighted loss function against which backpropagation will be performed.<br>*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(NBMM.__call__, name='NBMM.__call__', title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67e2931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights.shape (N,H,1,K) \t torch.Size([2, 2, 1, 3])\n",
      "counts.shape (N,H,1,K) \t torch.Size([2, 2, 1, 3])\n",
      "probs.shape (N,H,1,K) \t torch.Size([2, 2, 1, 3])\n",
      "samples.shape (N,H,1,num_samples)  torch.Size([2, 2, 1, 2000])\n",
      "sample_mean.shape (N,H,1,1)  torch.Size([2, 2, 1, 1])\n",
      "quants.shape  (N,H,1,Q) \t\t torch.Size([2, 2, 1, 5])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 370x290 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfgAAAEyCAYAAAAWW8KtAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAWnJJREFUeJzt3XdYk1f7B/BvAiEQpuwgEBUVFUeduNFXcdVZW622KtVaW63VWm3rqrirVmvbt9rhgKpUa6197c9JW3FUrYpYUVxVEJSN7Jlxfn/EPBASIEEgg/tzXbk0T06enEMgd54z7sNjjDEQQgghxKzwDV0BQgghhNQ9CvCEEEKIGaIATwghhJghCvCEEEKIGaIATwghhJghCvCEEEKIGaIATwghhJghCvCEEEKIGaIATwghhJghCvDPKSwsDDweD9bW1nj06JHG4wMGDED79u3VjjVr1gw8Ho+7WVtbo2XLlliwYAEyMzPVyoaGhoLH44HP5+Phw4ca5y8sLISDgwN4PB5CQkJqrG/l17azs0NgYCB++OEH/RpeA1W9K7fneYSEhMDOzk6nss2aNVP7eSQkJIDH4yEsLIw7pnrvEhIS9KrHunXr8Ouvv2ocj4qKAo/HQ1RUlF7n00ZVX9WNz+fDxcUFI0aMwMWLF5/7/LoICQlBs2bN1I7xeDyEhobqdZ7k5GSEhobi+vXrGo+pfk8IIXWPAnwdKS0txbJly3Qu36dPH1y8eBEXL17E8ePHMWvWLHz77bcYNmyY1vJ2dnbYvXu3xvGDBw9CKpVCIBDU6rVVQW7atGnYvn27zucwdocPH8by5curLfPiiy/i4sWLEIvFep27qgDfpUsXXLx4EV26dNHrfNWZO3cuLl68iHPnzmH9+vX4559/MHDgQMTExNTZa+jj4sWLePPNN/V6TnJyMlauXKk1wL/55psN9oWFkMbG0tAVMBfDhg1DREQEFi5ciE6dOtVY3snJCT179uTuDxw4EPn5+Vi9ejXu3buH1q1bq5WfOHEiwsPDsXLlSvD55d/Ldu7ciXHjxuHIkSM617Xyaw8ePBgSiQRbtmzBO++8o/U5crkcMpkMQqFQ59cxpM6dO9dYxs3NDW5ubnX2mg4ODmo/17rg6+vLnbNPnz5o2bIlBg0ahG3btuH777/X+pzi4mJYW1vXy5VxXbfP29sb3t7edXpOQogSXcHXkQ8//BAuLi746KOPan0OR0dHANB6NT59+nQkJSUhMjKSO3bv3j2cP38e06dPr/VrAsqA7+/vzw0xqLqHN27ciDVr1qB58+YQCoU4ffo0AODIkSPo1asXRCIR7O3tERwcXOVVWFJSEl566SU4ODjA0dERr7/+OjIyMtTKHDhwAEOGDIFYLIaNjQ3atm2Ljz/+GIWFhVrPeevWLQwaNAi2trZwc3PDu+++i6KiIrUylbvotdHWRR8TE4ORI0fC3d0dQqEQXl5eePHFF/H48WMAyi7qwsJChIeHc93nAwYMAFB1F/3ff/+NUaNGwcXFBdbW1vDz88P8+fOrrVtVVAFW9V6p2nDq1ClMnz4dbm5uEIlEKC0tBaD82fbq1Qu2traws7PD0KFDtV79h4WFwd/fH0KhEG3btq1yyEZbF/2TJ0/w1ltvwcfHB1ZWVvDy8sLLL7+MtLQ0REVFoXv37gCAN954g/uZqc6hrYteoVBg48aNaNOmDYRCIdzd3TF16lTuPVBRDX9duXIF/fr1g0gkQosWLfDpp59CoVConW/NmjXw9/eHjY0NnJyc0LFjR3zxxRc6/tQJMU0U4OuIvb09li1bhpMnT+LPP/+ssTxjDDKZDDKZDAUFBTh9+jS2bt2KPn36oHnz5hrlW7VqhX79+mHXrl3csV27dqFZs2YYNGjQc9VdKpXi0aNHGlezX375Jf7880989tlnOH78ONq0aYOIiAiMGTMGDg4O+PHHH7Fz505kZ2djwIABOH/+vMa5x40bh5YtW+Lnn39GaGgofv31VwwdOhRSqZQrc//+fYwYMQI7d+7EiRMnMH/+fPz0008YNWqU1rqOGDECgwYNwq+//op3330X3377LSZOnPhcPwNAOZ8hODgYaWlp+PrrrxEZGYmtW7fC19cX+fn5AJRd1DY2NtxY+MWLF7Ft27Yqz3ny5En069cPiYmJ2LJlC44fP45ly5YhLS2tVnX8999/AUDjvZo+fToEAgH27NmDn3/+GQKBAOvWrcOkSZPQrl07/PTTT9izZw/y8/PRr18/xMXFcc8NCwvDG2+8gbZt2+LQoUNYtmwZVq9erdPv8ZMnT9C9e3ccPnwYCxYswPHjx7F161Y4OjoiOzsbXbp04YaWli1bxv3Mquvmf+edd/DRRx8hODgYR44cwerVq3HixAn07t1bY05HamoqXnvtNbz++us4cuQIhg8fjsWLF2Pv3r1cmY0bNyI0NBSTJk3C0aNHceDAAcyYMQM5OTk1to8Qk8bIc9m9ezcDwK5cucJKS0tZixYtWLdu3ZhCoWCMMRYUFMQCAgLUniORSBgAjVuPHj1YSkqKWtkVK1YwACwjI4Pt3r2bCYVClpWVxWQyGROLxSw0NJQxxpitrS2bNm1ajfWVSCRsxIgRTCqVMqlUyuLj49m0adMYALZo0SLGGGPx8fEMAPPz82NlZWXcc+VyOfPy8mIdOnRgcrmcO56fn8/c3d1Z7969Ner9/vvvq73+vn37GAC2d+9erfVTKBRMKpWyM2fOMADsn3/+4R5T1fOLL75Qe87atWsZAHb+/Hm1dlb8eajatHv3bu6Y6r2Lj49njDF29epVBoD9+uuv1f4Mq/pZnz59mgFgp0+f5o75+fkxPz8/VlxcXO05K1PVd8OGDUwqlbKSkhIWHR3NunfvzgCwo0ePqrVh6tSpas9PTExklpaWbO7cuWrH8/PzmaenJ5swYQJjrPw97dKlC/c7yxhjCQkJTCAQMIlEovZ8AGzFihXc/enTpzOBQMDi4uKqbMuVK1c0fvYqqt8Tldu3bzMAbPbs2Wrl/v77bwaALVmyhDsWFBTEALC///5brWy7du3Y0KFDufsjR45kL7zwQpX1I8Rc0RV8HbKyssKaNWtw9epV/PTTT9WW7du3L65cuYIrV67gr7/+ws6dO5GRkYH//Oc/Vc48f+WVV2BlZYV9+/bh2LFjSE1N1WnmfGXHjh2DQCCAQCBA8+bN8dNPP2Hu3LlYs2aNWrnRo0erDRfcvXsXycnJmDJlito8ADs7O4wfPx6XLl3S6Cp/7bXX1O5PmDABlpaWXHc/ADx8+BCTJ0+Gp6cnLCwsIBAIEBQUBAC4ffu2Rv0rn3Py5MkAoHbO2mjZsiWaNGmCjz76CN98843aVW5t3Lt3Dw8ePMCMGTNgbW1dq3N89NFHEAgEsLa2RteuXZGYmIhvv/0WI0aMUCs3fvx4tfsnT56ETCbD1KlTuZ4imUwGa2trBAUFccMIqvd08uTJal3lEokEvXv3rrF+x48fx8CBA9G2bdtata8y1XtY+fe6R48eaNu2Lf744w+1456enujRo4fasY4dO6qtaOnRowf++ecfzJ49GydPnkReXl6d1JUQY0eT7OrYq6++is8++wxLly7FSy+9VGU5R0dHdOvWjbvfu3dvtGvXDr169cLmzZuxfv16jefY2tpi4sSJ2LVrFyQSCTc5Tl99+/bF559/Dh6PB5FIBD8/P1hZWWmUqzy7PCsrS+txAPDy8oJCoUB2djZEIhF33NPTU62cpaUlXFxcuHMVFBSgX79+sLa2xpo1a9C6dWuIRCJu7L64uFjr8ytSvYbqnLXl6OiIM2fOYO3atViyZAmys7MhFosxc+ZMLFu2TK+VCgC4uQbPM4ls3rx5eP3118Hn8+Hk5ITmzZtrnTxX+T1RDQGoxr8rU31BU/3MKr9PqmM1LSHMyMio00lyNf2OVV6KWvl3AQCEQqHa783ixYtha2uLvXv34ptvvoGFhQX69++PDRs2qP0NEmJuKMDXMR6Phw0bNiA4OBjfffedXs/t2LEjAOCff/6pssz06dOxY8cO3LhxA/v27atVHSt/uahK5UCi+jBNSUnRKJucnAw+n48mTZqoHU9NTUXTpk25+zKZDFlZWdy5/vzzTyQnJyMqKoq7agdQ5fho5eerXqNi/Z5Hhw4dsH//fjDGcOPGDYSFhWHVqlWwsbHBxx9/rNe5VOPklSeH6cPb27tW75WrqysA4Oeff672S6DqZ6b6GVak7Vhlbm5uz9W+quqTkpKi8cUhOTmZa5c+LC0tsWDBAixYsAA5OTn4/fffsWTJEgwdOhRJSUlqX0gJMSfURV8PBg8ejODgYKxatQoFBQU6P0+1Ttjd3b3KMr169cL06dMxbtw4jBs37nmrqhd/f380bdoUERERYIxxxwsLC3Ho0CFuZn1Flb+E/PTTT5DJZNzMc1Vgqrz87ttvv62yHpXPGRERAQDcOesCj8dDp06d8Pnnn8PJyQnXrl3jHqt8hViV1q1bw8/PD7t27eJmtTeUoUOHwtLSEg8ePEC3bt203gDleyoWi/Hjjz+qvaePHj3ChQsXanyd4cOH4/Tp07h7926VZVTvrS4/s//85z8AoDZJDgCuXLmC27dvP/eEUicnJ7z88suYM2cOnj59qneSI0JMCV3B15MNGzaga9euSE9PR0BAgMbjOTk5uHTpEgDlzPDbt29j3bp1EAqFmDNnTrXn3rlzZ73UuSZ8Ph8bN27Ea6+9hpEjR2LWrFkoLS3Fpk2bkJOTg08//VTjOb/88gssLS0RHByMW7duYfny5ejUqRMmTJgAQDk00aRJE7z99ttYsWIFBAIB9u3bV2UvhpWVFTZv3oyCggJ0794dFy5cwJo1azB8+HD07dv3udr3f//3f9i2bRvGjh2LFi1agDGGX375BTk5OQgODubKdejQAVFRUfjtt98gFothb28Pf39/ref8+uuvMWrUKPTs2RPvv/8+fH19kZiYiJMnT9a6B0YXzZo1w6pVq7B06VI8fPgQw4YNQ5MmTZCWlobLly/D1taWy6mwevVqvPnmmxg3bhxmzpyJnJwchIaGau22r2zVqlU4fvw4+vfvjyVLlqBDhw7IycnBiRMnsGDBArRp0wZ+fn6wsbHBvn370LZtW9jZ2cHLywteXl4a5/P398dbb72Fr776Cnw+H8OHD0dCQgKWL18OHx8fvP/++3r/LEaNGoX27dujW7ducHNzw6NHj7B161ZIJBK0atVK7/MRYjIMO8fP9FWcRV/Z5MmTGYAaZ9FbWFgwX19f9vLLL7OYmBi1shVn0VdHn1n0L774YrVlVDO4N23apPXxX3/9lQUGBjJra2tma2vLBg0axP766y+t9Y6OjmajRo1idnZ2zN7enk2aNImlpaWplb1w4QLr1asXE4lEzM3Njb355pvs2rVrGjOvp02bxmxtbdmNGzfYgAEDmI2NDXN2dmbvvPMOKygo0GinvrPo79y5wyZNmsT8/PyYjY0Nc3R0ZD169GBhYWFq575+/Trr06cPE4lEDAALCgpijGmfRc8YYxcvXmTDhw9njo6OTCgUMj8/P43VBZXV9B5UboO23z/GlO/VwIEDmYODAxMKhUwikbCXX36Z/f7772rlduzYwVq1asWsrKxY69at2a5du9i0adNqnEXPGGNJSUls+vTpzNPTkwkEAubl5cUmTJig9j7/+OOPrE2bNkwgEKido/IsesaUM/s3bNjAWrduzQQCAXN1dWWvv/46S0pKUiunbYUKY0yj3ps3b2a9e/dmrq6uzMrKivn6+rIZM2awhIQErT8zQswFj7EK/XKEEEIIMQs0Bk8IIYSYIQrwhBBCiBmiAE8IIYSYIQrwhBBCiBmiAE8IIYSYIbNfB69QKJCcnAx7e/t62R+bEEK0YYwhPz8fXl5eans3ENJQzD7AJycnw8fHx9DVIIQ0UklJSXWar58QXZl9gLe3tweg/CNzcHDQ6TlSqRSnTp3CkCFD9N5gxBhRe4ybubUHML821aY9eXl58PHx4T6DCGloZh/gVd3yDg4OegV4kUgEBwcHs/lwovYYL3NrD2B+bXqe9tDQIDEUGhgihBBCzBAFeEIIIcQMUYAnhBBCzJDZj8ETQogxk8vlkEqlhq4GMRECgQAWFhY6lTVogN++fTu2b9+OhIQEAEBAQAA++eQTDB8+HAAQEhKC8PBwtecEBgZy+6gTQoipYowhNTUVOTk5hq4KMTFOTk7w9PSscQKnQQO8t7c3Pv30U7Rs2RIAEB4ejjFjxiAmJgYBAQEAgGHDhmH37t3cc6ysrAxSV0IIqUuq4O7u7g6RSESz7UmNGGMoKipCeno6AEAsFldb3qABftSoUWr3165di+3bt+PSpUtcgBcKhfD09DRE9QghpF7I5XIuuLu4uBi6OsSE2NjYAADS09Ph7u5ebXe90YzBy+VyHDx4EIWFhejVqxd3PCoqCu7u7nByckJQUBDWrl0Ld3f3Ks9TWlqK0tJS7n5eXh4A5TpWXce5VOXMZVyM2mPczK09gHm1iTGgoED/9lRXVvWYSCR6vsqRRkn1eyOVSqsN8DzGGGuoSmkTGxuLXr16oaSkBHZ2doiIiMCIESMAAAcOHICdnR0kEgni4+OxfPlyyGQyREdHQygUaj1faGgoVq5cqXE8IiKC/pgIIQ2mqKgIkydPRm5urkaSrZKSEsTHx6N58+awtrY2UA2JqdL198fgAb6srAyJiYnIycnBoUOHsGPHDpw5cwbt2rXTKJuSkgKJRIL9+/fjpZde0no+bVfwPj4+yMzM1CuTXWRkJIKDg80mCxe1x3iZW3sA025TQQGQkQHk5Smv3gHAxkaKBw/0a09eXh5cXV0pwJM6p+vvj8G76K2srLhJdt26dcOVK1fwxRdf4Ntvv9UoKxaLIZFIcP/+/SrPJxQKtV7dCwQCvT9oavMcY0btMW7m1h7AdNoklwNZWcrAXlKiPMbjKW8AoOoF1ac9ptBuYt6MLtENY0ztCryirKwsJCUl1ThzkBBCdFFUBDx6BNy4ASQllQd3YngDBgzA/PnzufvNmjXD1q1b6/U1o6KiwOPxwOPxMHbs2Hp9raqoXt/Jyem5z2XQAL9kyRKcO3cOCQkJiI2NxdKlSxEVFYXXXnsNBQUFWLhwIS5evIiEhARERUVh1KhRcHV1xbhx4wxZbUKICWNMebV+5w5w+zaQmQkoFIauFanJlStX8NZbbzXIa929exdhYWF6PSckJIQLzqpbz5491cqUlpZi7ty5cHV1ha2tLUaPHo3Hjx+rlUlJSamzLzIG7aJPS0vDlClTkJKSAkdHR3Ts2BEnTpxAcHAwiouLERsbix9++AE5OTkQi8UYOHAgDhw4QNsvEkL0Vlqq7ILPygJkMkPXhujLzc2twV5LtXJLXzXlbZk/fz5+++037N+/Hy4uLvjggw8wcuRIREdHc7PhPT094ejo+Fz1VzHoFfzOnTuRkJCA0tJSpKen4/fff0dwcDAA5Vq/kydPIj09HWVlZXj06BHCwsLg4+NjyCoTQkxMbi7w77/AzZtAWppxBnfGGAoLCw1y02ee9YABAzB37lzMnz8fTZo0gYeHB7777jsUFhbijTfegL29Pfz8/HD8+HG158XFxWHEiBGws7ODh4cHpkyZgszMTO7xwsJCTJ06FXZ2dhCLxdi8ebPGa1fuot+yZQs6dOgAW1tb+Pj4YPbs2SgoKOAeDwsLg5OTE06ePIm2bdvCzs4Ow4YNQ0pKih7vTNX1qzyEAJTnbVHdnJ2ducdyc3Oxc+dObN68GYMHD0bnzp2xd+9exMbG4vfff9e7TrowujF4Qgh5XjIZkJoKxMYqg3turqFrVL2ioiLY2dkZ5FZUVKRXXcPDw+Hq6orLly9j7ty5eOedd/DKK6+gd+/euHbtGoYOHYopU6Zw501JSUFQUBBeeOEFXL16FSdOnEBaWhomTJjAnXPRokU4ffo0Dh8+jFOnTiEqKgrR0dHV1oPP5+PLL7/EzZs3ER4ejj///BMffvihxs/1s88+w549e3D27FkkJiZi4cKFerVXn/qp8ra0bt0aM2fO5DLOAUB0dDSkUimGDBnCHfPy8kL79u1x4cIFveukC4PPoieEkLqiWuKWnV2+xI3UrU6dOmHZsmUAgMWLF+PTTz+Fq6srZs6cCQD45JNPsH37dty4cQM9e/bE9u3b0aVLF6xbt447x65du+Dj44N79+7By8sLO3fuxA8//MD14IaHh8Pb27vaelS8em7evDlWr16Nd955B9u2beOOS6VSfPPNN/Dz8wMAvPvuu1i1apVe7S0oKNCpfsOHD8crr7yilrflP//5D5e3JTU1FVZWVmjSpIna8zw8PJCamqpXnXRFAZ4QYtIUivIlbsXFhq5N7YhEIrXu5YZ+bX107NiR+7+FhQVcXFzQoUMH7piHhwcAcFev0dHROH36NOzs7DTO9eDBAxQXF6OsrEwtg6mzszP8/f2rrcfp06exbt06xMXFIS8vDzKZDCUlJSgsLIStrS3XNlVwB5RLrSteVeviwYMHOtVv4sSJ3P/bt2+Pbt26QSKR4OjRo1XmbQGUwzP1tQ8BBXhCiEkqKSmfNCeXG7o2z4fH43FBydhVXt/P4/HUjqmCleLZ0gSFQoFRo0Zhw4YNGucSi8XV5jWpyqNHjzBixAi8/fbbWL16NZydnXH+/HnMmDFDLUWwtrrqm9uttrngKudt8fT0RFlZGbKzs9Wu4tPT09G7d+9avUZNaAyeEGIyGFN2v9+7B9y6BaSnm35wN3ddunTBrVu30KxZM7Rs2VLtZmtri5YtW0IgEKhtA56dnY179+5Vec6rV69CJpNh8+bN6NmzJ1q3bo3k5OR6qX9t6gdo5m3p2rUrBAIBIiMjuTIpKSm4efMmBXhCSOMllQLJycpJcw8fAvn5hq4R0dWcOXPw9OlTTJo0CZcvX8bDhw9x6tQpTJ8+HXK5HHZ2dpgxYwYWLVqEP/74Azdv3kRISAj4/KrDk5+fH2QyGb766is8fPgQe/bswTfffFMv9delfrrkbXF0dMSMGTPwwQcf4I8//kBMTAxef/11dOjQAYMHD66XulMXPSHEaOXlKbvhc3Np0pyp8vLywl9//YWPPvoIQ4cORWlpKSQSCYYNG8YFyU2bNqGgoACjR4+Gvb09PvjgA+RWs/ThhRdewJYtW7BhwwYsXrwY/fv3x/r16zF16tR6aUNN9bOwsNApb8vnn38OS0tLTJgwAcXFxRg0aBDCwsKq3RHueVCAJ4QYFW154YlxiIqK0jiWkJCgcazyuHWrVq3wyy+/VHleOzs77NmzB3v27OGOLVq0qNrXef/99/H++++rHZsyZQr3/5CQEISEhKg9Pnbs2FqNqWur39GjR7n/q/K21MTa2hpfffUVvvrqK73rUBsU4AkhRqGoSBnUnz6l1LHEsLy9vTFq1Cj8+OOPDf7adnZ2kMlkdbLLIAV4QojBKBTKSXMZGUBhoaFrQxq7wMBAbta7tmV9DeH69esAUCfd9hTgCSENjvLCE2NkY2PDbV9eE23DFXVB19fXBQV4QkiDyc1VLm3LyzN0TQgxfxTgCSH1SiZTbsmakQGUlRm6NoQ0HhTgCSH15tEj5dU6LXEjpOFRgCeE1BlVXnjV3hnZ2UA1+UoIIfWIAjwh5LmVlCjH1p8+Va5jp2VuhBgeBXhCSK0wBuTkKMfWKXUsIcaHAjwhRC9lZcpJc5mZyhzxpO58913Dvt5bb+lXfsCAAThz5gwAICYmBi+88ELdV8pIqXbJc3R0RE5OjmEroyMaHSOE6CQvD3jwALh5E0hJoeDeWM2cORMpKSlo3769TuWjoqIwZswYiMVi2Nra4oUXXsC+ffs0yvB4PI3bnTt3nru+2s7L4/GwadMmrsyAAQM0Hn/11VfVzpOSkoKtW7c+d30aEl3BE0KqJJeXX61TXngCACKRCJ6enjqXv3DhAjp27IiPPvoIHh4eOHr0KKZOnQoHBweMGjVKrezdu3fh4ODA3Xdzc3vu+qakpKjdP378OGbMmIHx48erHZ85cyZWrVrF3bexsVF73NPTE46Ojs9dn4ZEAZ4QooHywhNdREVFYeDAgfi///s/LFmyBHfv3kWnTp2wY8cOdOjQAQCwZMkStee89957OHnyJA4fPqwR4N3d3eHk5KTz6w8YMIDrSdi7dy8sLCzwzjvvYPXq1VyXeuUvI//73/8wcOBAtGjRQu24vl9cTIFBu+i3b9+Ojh07wsHBAQ4ODujVqxeOHz/OPc4YQ2hoKLy8vGBjY4MBAwbg1q1bBqwxIeZLtcTtzh3g9m3lVTsFd6KLRYsW4bPPPsOVK1fg7u6O0aNHQ1rNGE5ubi6cnZ01jnfu3BlisRiDBg3C6dOndXrt8PBwWFpa4u+//8aXX36Jzz//HDt27NBaNi0tDUePHsWMGTM0Htu3bx9cXV0REBCAhQsXIt8MZo4a9Are29sbn376KZd7Nzw8HGPGjEFMTAwCAgKwceNGbNmyBWFhYWjdujXWrFmD4OBg3L17V22PXUJI7VFeePK8VqxYgeDgYADKz3Fvb28cPnwYEyZM0Cj7888/48qVK/j222+5Y2KxGN999x26du2K0tJS7NmzB4MGDUJUVBT69+9f7Wv7+Pjg888/B4/Hg7+/P2JjY/H5559j5syZGmXDw8Nhb2+Pl156Se34a6+9hubNm8PT0xM3b97E4sWL8c8//yAyMrI2Pw6jYdAAX7l7Zu3atdi+fTsuXbqEdu3aYevWrVi6dCn3ZoSHh8PDwwMRERGYNWuW1nOWlpaitLSUu5/3LOm1VCqt9htlRapyupY3dtQe42ao9uTmKq/S6+NCRaGQqv1r6uRy/d8jc/n91EWvXr24/zs7O8Pf3x+3b9/WKBcVFYWQkBB8//33CAgI4I77+/vD399f7XxJSUn47LPP0L9/f5w7dw7Dhw/nHv/222/x2muvAQB69uzJdcernrt582bI5XKNHdl27dqF1157TWMr1opfBtq3b49WrVqhW7duuHbtGrp06aLvj8NoGM0YvFwux8GDB1FYWIhevXohPj4eqampGDJkCFdGKBQiKCgIFy5cqDLAr1+/HitXrtQ4furUKYhEIr3qZOrf3iqj9hg3c2sPAKSmmleb9HmPioqK6rEmxq9i0AWAM2fOYNSoUdiyZQumTp1a4/N79uyJvXv3AgC6devGbaMKAB4eHnrX59y5c7h79y4OHDhQY9kuXbpAIBDg/v37FOCfR2xsLHr16oWSkhLY2dnh8OHDaNeuHS5cuABA84308PDAo0ePqjzf4sWLsWDBAu5+Xl4efHx8MGTIELXZmdWRSqWIjIxEcHAwBAJBLVplXKg9xq0h2lNYqLxaz8lpmLzwCoUUqamR8PQMBp9v+u+RjY0UDx7o9x7lNaIt8y5dugRfX18AQHZ2Nu7du4c2bdpwj0dFRWHkyJHYsGED3tJx8X1MTAzEYjGA6rdxvXTpksb9Vq1aaVy979y5E127dkWnTp1qfO1bt25BKpVyr2+qDB7g/f39cf36deTk5ODQoUOYNm0al0gB0PwWyBjTOFaRUCiEUCjUOC4QCPT+8KzNc4wZtce41XV7VJPmMjKA4mLlMR5PeWsofL7ALAK8Klbo8x6Z0+9mTVatWgUXFxd4eHhg6dKlcHV1xdixYwEog/uLL76IefPmYfz48Uh9tlGBlZUVN9Fu69ataNasGQICAlBWVoa9e/fi0KFDOHToUI2vnZSUhAULFmDWrFm4du0avvrqK2zevFmtTF5eHg4ePKhxHAAePHiAffv2YcSIEXB1dUVcXBw++OADdO7cGX369HnOn4xhGTzAW1lZcd/MunXrhitXruCLL77ARx99BABITU1V+xaVnp5eq+4ZQhqL4uLyJW5yuaFrQ/Shb2Y5Y/Hpp59i3rx5uH//Pjp16oQjR47AysoKABAWFoaioiKsX78e69ev554TFBSEqKgoAEBZWRkWLlyIJ0+ewMbGBgEBATh69ChGjBhR42tPnToVxcXF6NGjBywsLDB37lyNXoL9+/eDMYZJkyZpPN/Kygp//PEHvvjiCxQUFMDHxwcvvvgiVqxYodELYGoMHuArY4yhtLSUm9EYGRmJzp07A1D+Epw5cwYbNmwwcC0JMS6qvPDp6UBBgaFrQxqbvn374ubNm1ofCwsLQ1hYWLXP//DDD/Hhhx/W6rUFAgG2bt2K7du3V1nmrbfeqnJowMfHR63X2JwYNMAvWbIEw4cPh4+PD/Lz87F//35ERUXhxIkT4PF4mD9/PtatW4dWrVqhVatWWLduHUQiESZPnmzIahNiNCgvPGlo27Ztw44dO3Dx4kVDV6VB2dnZQSaTaczAN2YGDfBpaWmYMmUKUlJS4OjoiI4dO+LEiRPcesoPP/wQxcXFmD17NrKzsxEYGIhTp07RGnjS6OXlKbvhc3MbZtIcIYAyGUzxswkdvr6+3GToxkA1i9+Uuu0NGuB37txZ7eM8Hg+hoaEIDQ1tmAoRYsRUeeEzMpTJaQhpaE2bNlW7P2DAADADfsNUjeE3hKpm8RszoxuDJ4SoKypSjq1nZ1PqWEKI7ijAE2KEFAplQM/IUK5hJ4QQfVGAJ8SIUF54QkhdoQBPiIGphjAfPKCrdUJI3aEAT4iByGTKq/X0dOX9/HyAb9ANnAkh5oQCPCENrKBAGdizs5VX7zRxjhBSHyjAE9IAtOWFJ6Sy6OiGfb2uXfUrP2DAAC7rW0xMDF544YW6r5QBNGvWjNvELDs7G05OToatUB2hDkFC6lFxMZCYCNy4ofyXgjsxdTNnzkRKSgrat2+vU/mSkhKEhISgQ4cOsLS05DahqQv5+fmYP38+JBIJbGxs0Lt3b1y5ckWtTFpaGkJCQuDl5QWRSIRhw4bh/v37amWuXLmi08Y2poYCPCF1jDHlRi937wJxccqrdtr0hZgLkUgET09PWFrq1gEsl8thY2OD9957D4MHD67Turz55puIjIzEnj17EBsbiyFDhmDw4MF48uQJAOXeJmPHjsXDhw/xv//9DzExMZBIJBg8eDAKK8xodXNz43a2MycU4AmpI2VlQHIyEBsLxMfTpi/E/EVFRYHH4+Ho0aPo1KkTrK2tERgYiNjYWK6Mra0ttm/fjpkzZ8LT01Pnc4eEhGDs2LFYuXIl3N3d4eDggFmzZqGsrAwAUFxcjEOHDmHjxo3o378/WrZsidDQUDRv3pzbeOb+/fu4dOkStm/fju7du8Pf3x/btm1DQUEBfvzxx7r9YRghCvCEPKe8POUSt5s3gZQU2vSFND6LFi3CZ599hitXrsDd3R2jR4+GtA7+EP744w/cvn0bp0+fxo8//ojDhw9j5cqVAACZTAa5XK6x+YuNjQ3Onz8PACh9ltO5YhkLCwtYWVlxZcwZBXhCakEuB9LSlEH9/n3lVq206QtprFasWIHg4GB06NAB4eHhSEtLw+HDh5/7vFZWVti1axcCAgLw4osvYtWqVfjyyy+hUChgb2+PXr16YfXq1UhOToZcLsfevXvx999/IyUlBQDQpk0bSCQSLF68GNnZ2SgrK8Onn36K1NRUrow5owBPiB6KioCEBOWkucePadMXQgCgV69e3P+dnZ3h7++P27dv6/TcxMRE2NnZcbd169Zxj3Xq1AkikUjtdQoKCpCUlAQA2LNnDxhjaNq0KYRCIb788ktMnjyZ2/FNIBDg0KFDuHfvHpydnSESiRAVFYXhw4eb1K5wtUXL5AipgSovfHq6MsATQmrG4/F0Kufl5cVtxQpAp8luqnP7+fnhzJkzKCwsRF5eHsRiMSZOnIjmzZtzZbt27Yrr168jNzcXZWVlcHNzQ2BgILp166Zfg0wQBXhCqkB54QnRzaVLl+Dr6wtAuY783r17aNOmjU7PtbS0rHIr1n/++QfFxcWwsbHhXsfOzg7e3t5q5WxtbWFra4vs7GycPHkSGzdu1DiXo6MjAOXEu6tXr2L16tU6t89UUYAnpALGgNxcZWDPyzN0bQgxDatWrYKLiws8PDywdOlSuLq6qq13j4uLQ1lZGZ4+fYr8/Hzuir2mRDllZWWYMWMGli1bhkePHmHFihV49913wX+W0/nkyZNgjMHf3x///vsvFi1aBH9/f7zxxhvcOQ4ePAg3Nzf4+voiNjYW8+bNw9ixYzFkyJC6/jEYHQrwhEA58z0zU3l7tgqHkAanb2Y5Y/Hpp59i3rx5uH//Pjp16oQjR47AysqKe3zEiBFcpjgA6Ny5MwDlOvXqDBo0CK1atUL//v1RWlqKV199FaGhodzjubm5WLx4MR4/fgxnZ2eMHz8ea9euhUAg4MqkpKRgwYIFSEtLg1gsxtSpU7F8+fI6arlxowBPGrWCAuXYOs2CJ6T2+vbti5s3b1b5eEJCQq3PvXLlSm5pXGUTJkzAhAkTqn3+e++9h/fee6/Wr2/KaBY9aXTkcmUXfFycMtucatMXQkjNtm3bBjs7O7VkNqYuICAAw4cPN3Q16hxdwZNGo7hYGdifPqXUsYTUxr59+1D8bEMFX19fXLhwwcA1qhvHjh3jEvM4ODgYuDZ1x6ABfv369fjll19w584dbqOADRs2wN/fnysTEhKC8PBwtecFBgbi0qVLDV1dYoIYU16hZ2RQ6lhCnlfTpk3V7g8YMKDGcfTaCgsLq5fzaiORSBrstRqSQQP8mTNnMGfOHHTv3h0ymQxLly7FkCFDEBcXB1tbW67csGHDsHv3bu5+xckbhGhTVla+xI1SxxJCGiODBvgTJ06o3d+9ezfc3d0RHR2N/v37c8eFQqFemxSQxisvTxnYc3NpXJ0YP4VCYegqEBOk6++NUY3B5+bmAtDMZBQVFQV3d3c4OTkhKCgIa9euhbu7u9ZzlJaWchsMAEDes8XMUqlU580PVOXqYrMEY2Du7ZHJlOPqWVmmmTpWoZCq/WsOzK1Ncrn+f0PVlbWysgKfz0dycjLc3NxgZWWlc+Y30ngxxlBWVoaMjAzw+fwae7N5rL4GUPTEGMOYMWOQnZ2Nc+fOcccPHDgAOzs7SCQSxMfHY/ny5ZDJZIiOjoZQKNQ4T2hoqNYlFREREWo5jQkhpD4VFRVh8uTJyM3N1Tpxq6ysDCkpKSii/MdETyKRCGKx2HQC/Jw5c3D06FGcP39eIw1hRSkpKZBIJNi/fz9eeukljce1XcH7+PggMzNT59mRUqkUkZGRCA4OVkuYYKrMqT0KBZCRIUV0dCQ8PYPB55t2ewDlVW5qqvm0BzC/NtnYSPHggX5/Q3l5eXB1da0ywAPKCxvVtqeE6MLCwgKWlpY69fgYRRf93LlzceTIEZw9e7ba4A4AYrEYEokE9+/f1/q4UCjUemUvEAj0Dm61eY4xM+X2qPLCZ2aWT5rj8wVmETxUzK09gPm0SbXxmD5/Q7qU4/F4Jv13SYybQQM8Ywxz587F4cOHERUVpbYDUFWysrKQlJQEsVjcADUkhkR54QkhpPYMGuDnzJmDiIgI/O9//4O9vT1SU1MBKHf9sbGxQUFBAUJDQzF+/HiIxWIkJCRgyZIlcHV1xbhx4wxZdVKPKC88IYQ8P4MG+O3btwNQJkuoaPfu3QgJCYGFhQViY2Pxww8/ICcnB2KxGAMHDsSBAwdgb29vgBqT+kR54QkhpO4YvIu+OjY2Njh58mQD1YYYglyuXOKWkaFMJUsIIaRuGMUkO9L4qPLCZ2UpZ8YTQgipWxTgSYOhvPCEENJwKMCTekd54QkhpOFRgCf1hvLCE0KI4Tx3gJfL5YiNjYVEIkGTJk3qok7EhMlkyiv1jAzTzAtPCCHmgq/vE+bPn4+dO3cCUAb3oKAgdOnSBT4+PoiKiqrr+hETUVgIJCQAsbHA48cU3AkhxND0DvA///wzOnXqBAD47bffEB8fjzt37mD+/PlYunRpnVeQGC+FQpmM5vZt4M4dmhFPCCHGRO8An5mZye3NfuzYMbzyyito3bo1ZsyYgdjY2DqvIDE+paXKq/QbN4BHjwDaDIsQQoyP3gHew8MDcXFxkMvlOHHiBAYPHgxAuTWihWpHBmJ2GFNmmLt/H7h5E0hLUyapIYQQYpz0nmT3xhtvYMKECRCLxeDxeAgODgYA/P3332jTpk2dV5AYFuWFJ4QQ06R3gA8NDUX79u2RlJSEV155hdua1cLCAh9//HGdV5AYRn6+ciY85YUnhBDTVKtlci+//DIAoKSkhDs2bdq0uqkRMRjKC08IIeZD7zF4uVyO1atXo2nTprCzs8PDhw8BAMuXL+eWzxHTUlwMJCYqJ80lJlJwJwQASkqUfw9XrwKXLxu6NoToT+8Av3btWoSFhWHjxo2wsrLijnfo0AE7duyo08qR+sOY8mr97l0gLk551U5L3EhjJpMBqanKL7q//w4cO6YM7omJlNeBmCa9u+h/+OEHfPfddxg0aBDefvtt7njHjh1x586dOq0cqXuqvPCZmcoPNEIaK9XmR+npylUh2dn0JZeYF70D/JMnT9CyZUuN4wqFAlLaScSoPXxIu7iRxq2gQBnMMzKUN/rIIuZM7wAfEBCAc+fOQSKRqB0/ePAgOnfuXGcVI89PlRc+LU15Py8P4Os9KEOI6SotVV6hp6crAzolZSKNid4BfsWKFZgyZQqePHkChUKBX375BXfv3sUPP/yA//u//6uPOhI9FRYqP8xUXY7U7UgaC9WXWlVQz801dI0IMRy9A/yoUaNw4MABrFu3DjweD5988gm6dOmC3377jUt6QxqeQlG+xI2uUkhjocqwqArotB8CIeVqtQ5+6NChGDp0aF3XhdRCSYkyqGdlUepY0jgUFCh/51Xd7pRhkRDtnns/eNLwGFN2PWZkKMfVCTFnZWXlwTwtjXqoCNGV3lOu+Hw+LCwsqrzpY/369ejevTvs7e3h7u6OsWPH4u7du2plGGMIDQ2Fl5cXbGxsMGDAANy6dUvfapsFqRRISVFu9vLgAQV3Yp7kcmVAv3kT+PNP4OhRZaKZ+HgK7oToQ+8r+MOHD6vdl0qliImJQXh4OFauXKnXuc6cOYM5c+age/fukMlkWLp0KYYMGYK4uDjY2toCADZu3IgtW7YgLCwMrVu3xpo1axAcHIy7d+/C3t5e3+qbJMoLT8xd5XF0Gm4i5PnpHeDHjBmjcezll19GQEAADhw4gBkzZuh8rhMnTqjd3717N9zd3REdHY3+/fuDMYatW7di6dKleOmllwAA4eHh8PDwQEREBGbNmqVv9U2GXK78oMvIUI6zE2JOiorKA3p6Oo2jE1If6mwMPjAwEDNnznyuc+Q+W9Pi7OwMAIiPj0dqaiqGDBnClREKhQgKCsKFCxe0BvjS0lKUVsgrmfesH1sqleqciEdVzhCJe0pKlFnmnj6tu9nACoVU7V9TR+0xfpXbVHHb4YwMzYRLxp6fgcfT/zOBEn8RQ6uTAF9cXIyvvvoK3t7etT4HYwwLFixA37590b59ewBAamoqAMDDw0OtrIeHBx49eqT1POvXr9c6VHDq1CmIRCK96hQZGalXeWOXmkrtMWbm1h5As00uLsqbqdLnM6GIJgwQA9M7wDdp0gQ8Ho+7zxhDfn4+RCIR9u7dW+uKvPvuu7hx4wbOnz+v8VjF11O9ZuVjKosXL8aCBQu4+3l5efDx8cGQIUPg4OCgU12kUikiIyMRHBwMgUCgRyv0U1am7IbPyqrfvPAKhRSpqZHw9AwGn19/7Wko1B7jlJenvEJPTweys6Xo1CkS164FQ6Ew3Tap+PpK4e6u32dCHs2CJQamd4D//PPP1YIrn8+Hm5sbAgMD0aRJk1pVYu7cuThy5AjOnj2r1gvg6ekJQHklLxaLuePp6ekaV/UqQqEQQqFQ47hAINA7WNfmObpQLXGrmGWrIboo+XyBSQeQyqg9hlVSoly2plrCVnGuiOr3WaEQmEWAV01u1eczoT4vDgjRhd4BPiQkpM5enDGGuXPn4vDhw4iKikLz5s3VHm/evDk8PT0RGRnJ5bkvKyvDmTNnsGHDhjqrR0NQpdDMyKCtJ4lpksnKE8ykpytXdxBCjJdOAf7GjRs6n7Bjx446l50zZw4iIiLwv//9D/b29tyYu6OjI2xsbMDj8TB//nysW7cOrVq1QqtWrbBu3TqIRCJMnjxZ59cxpMp54QkxFRXTHyu73Rvv77BCIUc+faMhJkanAP/CCy+Ax+OB1bAIm8fjQa7HAtbt27cDAAYMGKB2fPfu3VxPwYcffoji4mLMnj0b2dnZCAwMxKlTp4x6DTzlhSemKj+/fDvVzMzGu52qXC5Fauo1JCWdRWLiGTx5ch6dOrXFxIkTDV01QnSmU4CPj4+vlxev6QsDoPzSEBoaitDQ0HqpQ12ivPDE1Kh+Z1VBvbjY0DUyDJmsBMnJl5GYqAroFyCVqn87j4+P1+kzixBjoVOAr7z3OymnygtPY5LEFMhk5TPd09Mbb7rjsrJCPHly4VlAP4vk5L8hl6tPjrG2bgIfn37w9e2PHj36oFu35CpX7xBijGq9Dj4uLg6JiYkoq5SCavTo0c9dKVNQMXEHZeEixoox5di5KqDXZQIlU1JSkoOkpPPPutzPIjU1GgqF+tpUW1sP+Pj0h69vf/j6BsHNLQA8nnI5gEQihYVFmiGqTkit6R3gHz58iHHjxiE2NlZtXF71zVafMXhTRHnhibErKCgP6BkZjXMcvbAwA0lJ55CYeAZJSWeRlvYPAPU/WAcHH/j6BnFB3dm5NV2hE7Oid4CfN28emjdvjt9//x0tWrTA5cuXkZWVhQ8++ACfffZZfdTR4CgvPDFmpaXqy9ca48TO/Pwn3Ph5YuJZZGXd1ijTpElL+PoGwde3P3x8+sPJqVnDV5SQBqR3gL948SL+/PNPuLm5gc/ng8/no2/fvli/fj3ee+89xMTE1Ec9DaKkBEhObrzdmsQ4yeXlOd3T05W9SY0JYww5OfFcd3ti4hnk5DzUKOfqGsB1t/v49IO9vZcBakuI4egd4OVyOezs7AAArq6uSE5Ohr+/PyQSicZe7qbuzh3j3wSDmD/VRM6Ky9ca0xdOxhiysu5y3e2JiWeRn/9YrQyPx4eHxwvPutuD4OPTFyKRq4FqTIhx0DvAt2/fHjdu3ECLFi0QGBiIjRs3wsrKCt999x1atGhRH3UkpNFRJUhSBfXGNJGTMQXS02O5q/OkpLMoKspQK8PnW0Is7s51t3t794G1taOBakyIcdI7wC9btgyFhYUAgDVr1mDkyJHo168fXFxccODAgTqvICGNRXJyebf7sz+xRkEulyItLYYL6I8fn0dJSY5aGUtLa3h59eQCetOmPWFlZWuYChNiIvQO8EOHDuX+36JFC8TFxeHp06cau8wRQqqmUCgnbqalKf/18wOuXm0cXe8yWSmSky9z3e2PH/8FqVT9G41AYAtv7z7cpDixuDssLTU3kSKEVE3vAB8eHo6XX34Ztrbl356dnZ3rtFKEmKOcnPJu94rZDs19nocyqcxFJCaeRVLSWTx5cklLUhmnZ0lllMvWPD07g8+vdZoOQghqEeAXLlyI2bNnY9SoUXj99dcxbNgwWFrSHyIhlRUXl4+hp6c3nl0ES0pykZx8BrduhePy5fVISdFMKiMSuXPd7RJJENzc2nNJZQghdUPvyJySkoITJ07gxx9/xKuvvgobGxu88soreP3119G7d+/6qCMhJkEqLQ/mGRmNJ3VxUVHms6QyyjH09PR/wJj6WIO9vbfaGnQXF38a0iOknukd4C0tLTFy5EiMHDkSRUVFOHz4MCIiIjBw4EB4e3vjwYMH9VFPQoyOatdAVYKZ7OzGkd0wPz+Z625PTDyDzMw4jTJNmvihS5dmEIkmwcfnP3B0bEYBnZAG9lx96yKRCEOHDkV2djYePXqE27c1s0cRYk7y8soDemamcvMWc8YYQ25uQoWAfhbZ2f9qlHN1bcetQff17QdHR3d063YMV6+OgEIhMEDNCSG1CvCqK/d9+/bh999/h4+PDyZNmoSDBw/Wdf0IMaiSEvVxdHNPVcwYw9On97iUr0lJZ5GXl1SpFA8eHi9wWeK8vfvC1tatUplGmACfECOjd4CfNGkSfvvtN4hEIrzyyiuIioqisXdiNlTbqaqCurlvp8qYAhkZN9XyuBcVpauVUSaV6cZtyqJMKuNkmAoTQnSmd4Dn8Xg4cOAAhg4dSrPnicljTDmOrlq+lp1t3mvRFQoZUlNjuPHzpKTzKCnJVitjYSFE06Y9uYDetGkvSipDiAnSO0JHRETURz0IaTD5+eUz3c19O1WZrBQpKVe47vbHj/9CWVmBWhllUpne3Bp0L6/usLS0NlCNCSF1hS7BidkrKVFfvmbO26lKpUV4/PgiNyEuOfkSZDL1iQNCoSOXVMbXtz88PDrDwoImwlVkZQU4OZXfmjYFnjwxcKUI0RMFeGJ2ZDJlprj0dGW3uzmPo5eU5OLJkwvc+HlKyhUtSWXcuO52X19lUhk+38JANTY+1tbqwdzJCRCJ1MtUvk+IKdA5wD9+/Bje3t71WRdCaoUxZRpY1cS4rCzzHUdXJpU5z+2ylpZ2XUtSmaZcd7uvb3+4uLShNejPiESawdyaRiOImdI5wLdv3x5fffUVpkyZUmcvfvbsWWzatAnR0dFISUnB4cOHMXbsWO7xkJAQhIeHqz0nMDAQly5dqrM6ENOVkFC+Ht1ct1MtKEh5NsNdlVTmlkYZJ6cWalninJyaN/qAzuMBdnbKAO7oWB7MrawMXDFCGpDOAX7dunWYM2cOfv31V3z33XdwcXF57hcvLCxEp06d8MYbb2D8+PFaywwbNgy7d+/m7lvRX2ijVFpaPo6emQm0awfcuGF+V+o5OQnc+HlS0lk8fXpfo4yLS9sKAb0fHBwad88an18ezCveaJEPaex0/hOYPXs2hg8fjhkzZiAgIADfffcdRo8e/VwvPnz4cAwfPrzaMkKhEJ6enjqfs7S0FKUVdvXIezYAK5VKIdVxurSqnEJhHtOrVe0wpfbI5eXL1zIzgdzc8jSwfL5U7V9TpUwqcx9JSVE4d+4grl+fi9xcbUllOsLXtx8kkn7w8ekDW1v3SmWM7+dQX+8Rn6+8Iq94s7cHLLRMKajLL39yubIdun6G6FuWkPqg13fc5s2b488//8R///tfjB8/Hm3bttVYC3/t2rU6rWBUVBTc3d3h5OSEoKAgrF27Fu7ulT/gyq1fvx4rV67UOH7q1CmI9Jwpk5oaqXd9jZkptkd1NaZNly6m1R6FQoHExETExcXh5s2biIuLQ05OjloZPp+Pli1bol27dggICEDbtm1hZ2dXocTVBq3z86rv96ioqGFXRURG6t6eInNerkFMAo8x/bbHePToEUJCQhAXF4e33npLI8CvWLGidhXh8TTG4A8cOAA7OztIJBLEx8dj+fLlkMlkiI6OhlAo1HoebVfwPj4+yMzMhIODg051kUqliIyMhKdnMPh8018+pFBIkZpqfO0pLi5fi56Rofs4Op8vRZcukbh2Ldio85wrk8r8g8RE1U5rf1WRVKY7AgPFsLKagqZN+8LKyq6KM5oOfd8jKyvAwaF8vNzREbC1VY6lGwMbGykePIhEcHAwBALdfufy8vLg6uqK3NxcnT97CKlLel3Bf//99/jggw8wePBg3Lx5E25ulfNP162JEydy/2/fvj26desGiUSCo0eP4qWXXtL6HKFQqDX4CwQCnf8wVfh8gVEFxOdl6PZIpeVr0dPSgMLC5zufQiEwqgCvTCpzlRtDVyaVUd8zViAQoWnT3twYupdXD1hZWTzbmGXYszYZqAH1QNt7JBRqjpfbGnmiPNUQgD6fI/p+3hBS13QO8MOGDcPly5fx3//+F1OnTq3POlVJLBZDIpHg/n3NiUfE+CgU5evRMzLMbztVqbQIT55c4ibEPXlysYqkMn25ZWuenl20JJUx37FakUh5ZU7L0ghpeDoHeLlcjhs3bhh0LXxWVhaSkpIgFosNVgdSvdxc9e1U5XJD16julJbm4fHjC9wa9OTkKxoTF21sXLnlahJJENzcOjSapDK2tuVB3NFRmXBo8GDlxDhCSMPTOcDrM7lEVwUFBfj33/K9pePj43H9+nU4OzvD2dkZoaGhGD9+PMRiMRISErBkyRK4urpi3LhxdV4XUjvFxeUBPT1duZzNXBQVZeHx4/Nclri0tBiNpDJ2dl5qa9BdXdua/Rp0Hk85c73iVbmjI1CxR1qhAJKTDVRBQggAA6eqvXr1KgYOHMjdX7BgAQBg2rRp2L59O2JjY/HDDz8gJycHYrEYAwcOxIEDB2Bvb2+oKjd6Mln5evT0dOXGLeaioCCV625PTDyDjIybGmWcnJo/yxCnDOpOTi3MOqDz+Zpd7A4OtMacEFNg0D/TAQMGoLpJ/CdPnmzA2hBtFAr17VRzcswnuUxu7qMKAf0snj69p1HGxaUN192uTCrjY4CaNgwLC/Wsb6pgTl3shJgm+h5ONOTlqY+jy2Q1P8fYMcaQnf0v192elHQWubmPKpXiwd29I9fd7uPTD3Z2Hgapb30TCDTTuNrbG8+yNELI86MAT1BSUh7QMzKU4+qmjjEFMjLiuAlxiYlnUViYqlaGx7OAWNyV25TF27svbGyaGKjG9afy1qdOTsrUroQQ80YBvhGSyZRX5qqgbg7bqSoUMqSl/cONnyclnUNx8VO1MhYWVhCLe3Dj502b9oJQaF7zOWxsNIO5jY1h60QIMQwK8I2AaprDvXvKgP70qemPo0ulUiQlXcSjRxeQlHQWSUnnNZLKWFrawNu7N7cG3curBwQC84l2FZelqW5VJHgkhDRCFODNVEFB+RV6VhbQqRNw547pBnaptBhPnlx6FszPYOPGv1BWKbetUOgAb+++8PVVznJXJpUx/d0HK259WvFGidIIIdWhAG8mVNuppqUp/624z4UpzoIuLc3HkycXuElxycmXtSSVceHGz319+8PdvZPJJ5Xh87WvMadlaYQQfdHHhomSycrTwKanKzPImbLi4qdISjrPjaGnpsaAMfU0eHZ2Yvj6BkEi6YPhwxnS0t4CY6bbJ61altbk2by+oCDlfVP8QkYIMT4U4E0EY8o16BW73U21ux0ACgrSuNntSUlnkZ4eC0A9J4KjYzOuu93Hpz+aNPEDj8cDny+Fr+8xpKfzTSa3vaWl9jXmPF551jcK7oSQukQB3ogVFqovX9N1O1VjlJeXxHW3K5PK3NUo4+zsXyGg94Ojo68Bavr8aFkaIcQYUIA3ImVl5cE8Pf35t1M1FGVSmQdcd3ti4lnk5iZolHN378iNofv49DfJpDLW1prBXCQybJ0IIQSgAG9QcrlyyZpqYlxOjmlup8qYApmZt59dnSsTyxQUpKiV4fEs4OnZpUKWuL6wsXE2UI1rRyTSDOa09SkhxFhRgG9gOTnls92zskxzO1WFQo709H+47vakpLMoLs5SK8PnC+DlVTGpTG+TSSpTeVmaauzcyvRX3BFCGhEK8PWsqEh9HN0Ut1OVy6VISbnKTYp7/Pg8SkvV099ZWtqgadNeXED38go0iaQyfL72Nea0LI0QYuroY6yOSaXq26kWFBi6RvqTSouRnHyZ625/8uQipNIitTJWVvbw8enLbZ0qFnc1+qQyfL72mewWpr10nhBCtKIA/5xU26mqAnp2tumNoyuTylzkJsSlpFyGXK4+Zd/GxlltQpyHRyfw+cb766NtWZq9PS1DI4Q0Hsb7CW3EcnPLr9IzMkxvHL24OBuPH5/nJsWlpl7TSCpja+vJdbf7+PSHm1s78HjGGR2trDSDuZ0dbX1KCGncKMDroKSkfKZ7erryvikpKEjDhQsXcPp0JB49Oo/09BvQTCoj4RLK+Pr2R5MmLcEzwggpFJZnfuvWTfl/W1vD1okQQowRBfhq3Lxpmtup5uU95rrbk5LOIivrjkYZZ+fWlZLKSAxQ0+pVtSxNlfnNy4u63AkhpCoU4Kvx8KHxp4NljCEn56HaGvScnHiNchKJBG5uI+DtPQC+vv1hZ+dpgNpWzc5Os5udtj4lhJDaowBvYhhjyMy8rZYlrqAgWa0Mj8eHp2eXCjutBaJ//79x9eoIKBSG3WOUx9O+WxptfUoIIXWLAryRUyaVucF1tycmnkVxcaZaGWVSme7cGLq3d28IhQ4VHpdWPm2D4POVy9AqL0ujNeaEEFL/DPpRe/bsWWzatAnR0dFISUnB4cOHMXbsWO5xxhhWrlyJ7777DtnZ2QgMDMTXX3+NgIAAw1W6nsnlUqSmRnMBPSnpPEpL1feCtbS05pLK+Pj0R9OmgRAIDJsAXbX1aeVgTmPkhBBiGAYN8IWFhejUqRPeeOMNjB8/XuPxjRs3YsuWLQgLC0Pr1q2xZs0aBAcH4+7du7C3N420pzWRyUq4pDKJiWfx5MkFrUllvL37cJPixOJuBk0qIxBopnG1t6dlaYQQYkwMGuCHDx+O4cOHa32MMYatW7di6dKleOmllwAA4eHh8PDwQEREBGbNmqX1eaWlpSitkA8279kUeKlUCqlUt65qVbn66NouKyvA48eX8OjRWSQmnkdyclVJZfrA17cfJJL+8PDoqCWpjO51U7WjNu1RrTFXBXJHR+3L0hhruAQ/CoVU7V9TZ27tAcyvTXK5sh26foboW5aQ+sBjzDjyrvF4PLUu+ocPH8LPzw/Xrl1D586duXJjxoyBk5MTwsPDtZ4nNDQUK1eu1DgeEREBkQH28SwoKMDt27dx69Yt3Lp1Cw8ePICi0tR8JycnBAQEICAgAO3atYOvry/41LdNiEkrKirC5MmTkZubCwcHh5qfQEgdM9rpTqmpqQAADw/1PcI9PDzw6NGjKp+3ePFiLFiwgLufl5cHHx8fDBkyROc/MqlUisjISFy7Fqz3rPPCwnQkJp5/djuHtDTNpDIODr6QSPrB17cffH37wtm5FZdUJjNTeatLfL4UXbqot8fWtvzKXHUzlWVpCoUUqamR8PQMBp9v+tPvza09gPm1ycZGigcPIhEcHAyBjks+8kwtgQYxO0Yb4FUqZ1NjjFWbYU0oFEKoJVIJBAKd/zBVFApBjQE+P/8JN36emHgWWVm3Nco4O7fiNmXx9e2vkVSmPru3ebzyLvW2bQVwchLAyck8lqXx+QKzCB4q5tYewHzapNqQSJ/PEX0/bwipa0Yb4D09lYlYUlNTIRaLuePp6ekaV/UNRZlUJp5LKJOYeBY5OQ81yrm5ta+wBr0/7OzEWs5W9/h87WvM+Xxl5jc/P5rVTgghjYXRBvjmzZvD09MTkZGR3Bh8WVkZzpw5gw0bNjRIHVRJZSpmicvPf6JWhsfjw8OjM7cpi49PP4hELvVeN32WpRl7Nj5CCCF1z6ABvqCgAP/++y93Pz4+HtevX4ezszN8fX0xf/58rFu3Dq1atUKrVq2wbt06iEQiTJ48ud7qlJ+fjx07duDgwYP455+ZKCrKUHtclVRGdYXetGlvWFs71lt9AGV3esVZ7KpgTsvSCCGEVMWgAf7q1asYOHAgd181OW7atGkICwvDhx9+iOLiYsyePZtLdHPq1Kl6XQNvYWGBjz76iFviYmlpDS+vntz4edOmPes1qYyVleYGK3Z29fZyhBBCzJRBA/yAAQNQ3So9Ho+H0NBQhIaGNlidRCIRZs+ejczMTFhazoSHR09YWtbP9HJra81gboCVfIQQQsyQ0Y7BG9KmTZtw7NgxXL3au842Z6lq61NCCCGkPlCAr2M8nrJLvfJMdivDZZYlhBDSCFGAfw58vmYwd3Ki3dIIIYQYHoUiHfH52pelqRJgEEIIIcaEAnw1WrRQ3y2NksQQ0vjweNQrR0wT/dpWo317CuqEmDseTzlHRihU/qu6qe4LBIBMBsTFGbqmhOiHAjwhxKxZWGgP3KobpYwn5ooCPCHEpFlaVh28hUKaJ0MaLwrwhBCjJhBoBu6KwZyG0QjRjgI8IcRgVOPf1XWh054LhNQOBXhCSL3h85UZG6sK3pQAipD6QwGeEFJrqglslQO3hQWQnAx07EiT2AgxFArwhJAqCQTVd6FXNYHt2WaMhBADogBPSCPF46kHcG3d5zSBjRDTRQGeEDNVcQKbtuVjAgFNYCPEnFGAJ8RE8flVZ1+jBC6EEArwhBipiglctF2FU350Qkh16COCEANRjX9bWipnnHt7AyJReTCn8W9CyPOgAE9IPVBNYKuuC101/i2VArduAa6u1K1OCKk7FOAJqQU+v+YNTGgCGyHEkIw6wIeGhmLlypVqxzw8PJCammqgGpHGouIOZNquwmn8mxBi7Iz+YyogIAC///47d9+CtoYidcDSsvoNTOjXjBBi6ow+wFtaWsLT09PQ1SAmpqYNTGgCGyHE3Bl9gL9//z68vLwgFAoRGBiIdevWoUWLFlWWLy0tRWlpKXc/Ly8PACCVSiHVMX+mqpxCYR75NlXtMJf2MKZsh42NFDY25UlbdB3/lsuVN2Oh+n3T9ffTFJhbm2rTHnNpOzFdPMYYM3QlqnL8+HEUFRWhdevWSEtLw5o1a3Dnzh3cunULLi4uWp+jbdweACIiIiASieq7yoQQAgAoKirC5MmTkZubCwcHB0NXhzRCRh3gKyssLISfnx8+/PBDLFiwQGsZbVfwPj4+yMzM1PmPTCqVIjIyEp6eweDzTX/dkkIhRWqq8bTH0lLzilvVhS4Q1DyBTfX+BAcHQ2AG68rMrT2A+bWpNu3Jy8uDq6srBXhiMEbfRV+Rra0tOnTogPv371dZRigUQigUahwXCAR6f9Dw+QKjCIh1paHaU9MGJnU1ga0276kxM7f2AObXJn3aY07tJqbJpAJ8aWkpbt++jX79+hm6Ko1WxR3IqkriQuu/CSHE8Iw6wC9cuBCjRo2Cr68v0tPTsWbNGuTl5WHatGmGrprZqpzARdsGJhTACSHE+Bl1gH/8+DEmTZqEzMxMuLm5oWfPnrh06RIkEomhq2ayKiZwqSoDGyGEENNn1AF+//79hq6CyVHtQFYxcPP5ys1MOnQArK0NXUNCCCENwagDPNFU0wYm2hK4qJbjUnY2QghpPCjAGxEer/ruc5rARgghRFcU4BtQxQls2q7CafybEEJIXaEAX4csLKrfwIR2ICOEENJQKOTooWICF21X4TTGTQghxFhQgK+Gjw8gEtEOZIQQQkwPBfhquLjQuDghhBDTRNekhBBCiBmiAE8IIYSYIQrwhBBCiBmiAE8IIYSYIQrwhBBCiBmiAE8IIYSYIQrwhBBCiBmiAE8IIYSYIbNPdMMYAwDk5eXp/BypVIqioiLk5eVBYAaZbqg9xs3c2gOYX5tq0x7VZ47qM4iQhmb2AT4/Px8A4OPjY+CaEEIao/z8fDg6Ohq6GqQR4jEz/3qpUCiQnJwMe3t78HTcTD0vLw8+Pj5ISkqCg4NDPdew/lF7jJu5tQcwvzbVpj2MMeTn58PLywt82siCGIDZX8Hz+Xx4e3vX6rkODg5m8eGkQu0xbubWHsD82qRve+jKnRgSfa0khBBCzBAFeEIIIcQMUYDXQigUYsWKFRAKhYauSp2g9hg3c2sPYH5tMrf2kMbB7CfZEUIIIY0RXcETQgghZogCPCGEEGKGKMATQgghZogCPCGEEGKGGm2A37ZtG5o3bw5ra2t07doV586dq7b8mTNn0LVrV1hbW6NFixb45ptvGqimutGnPb/88guCg4Ph5uYGBwcH9OrVCydPnmzA2tZM3/dH5a+//oKlpSVeeOGF+q2gnvRtT2lpKZYuXQqJRAKhUAg/Pz/s2rWrgWpbM33bs2/fPnTq1AkikQhisRhvvPEGsrKyGqi21Tt79ixGjRoFLy8v8Hg8/PrrrzU+x9g/DwgBALBGaP/+/UwgELDvv/+excXFsXnz5jFbW1v26NEjreUfPnzIRCIRmzdvHouLi2Pff/89EwgE7Oeff27gmmunb3vmzZvHNmzYwC5fvszu3bvHFi9ezAQCAbt27VoD11w7fdujkpOTw1q0aMGGDBnCOnXq1DCV1UFt2jN69GgWGBjIIiMjWXx8PPv777/ZX3/91YC1rpq+7Tl37hzj8/nsiy++YA8fPmTnzp1jAQEBbOzYsQ1cc+2OHTvGli5dyg4dOsQAsMOHD1db3tg/DwhRaZQBvkePHuztt99WO9amTRv28ccfay3/4YcfsjZt2qgdmzVrFuvZs2e91VEf+rZHm3bt2rGVK1fWddVqpbbtmThxIlu2bBlbsWKFUQV4fdtz/Phx5ujoyLKyshqienrTtz2bNm1iLVq0UDv25ZdfMm9v73qrY23pEuCN/fOAEJVG10VfVlaG6OhoDBkyRO34kCFDcOHCBa3PuXjxokb5oUOH4urVq5BKpfVWV13Upj2VKRQK5Ofnw9nZuT6qqJfatmf37t148OABVqxYUd9V1Ett2nPkyBF069YNGzduRNOmTdG6dWssXLgQxcXFDVHlatWmPb1798bjx49x7NgxMMaQlpaGn3/+GS+++GJDVLnOGfPnASEVmf1mM5VlZmZCLpfDw8ND7biHhwdSU1O1Pic1NVVreZlMhszMTIjF4nqrb01q057KNm/ejMLCQkyYMKE+qqiX2rTn/v37+Pjjj3Hu3DlYWhrXr3Rt2vPw4UOcP38e1tbWOHz4MDIzMzF79mw8ffrU4OPwtWlP7969sW/fPkycOBElJSWQyWQYPXo0vvrqq4aocp0z5s8DQipqdFfwKpW3jmWMVbudrLby2o4bir7tUfnxxx8RGhqKAwcOwN3dvb6qpzdd2yOXyzF58mSsXLkSrVu3bqjq6U2f90ehUIDH42Hfvn3o0aMHRowYgS1btiAsLMworuIB/doTFxeH9957D5988gmio6Nx4sQJxMfH4+23326IqtYLY/88IARohFfwrq6usLCw0LjaSE9P1/hWruLp6am1vKWlJVxcXOqtrrqoTXtUDhw4gBkzZuDgwYMYPHhwfVZTZ/q2Jz8/H1evXkVMTAzeffddAMoAyRiDpaUlTp06hf/85z8NUndtavP+iMViNG3aVG2r0bZt24IxhsePH6NVq1b1Wufq1KY969evR58+fbBo0SIAQMeOHWFra4t+/fphzZo1JnfFa8yfB4RU1Oiu4K2srNC1a1dERkaqHY+MjETv3r21PqdXr14a5U+dOoVu3bpBIBDUW111UZv2AMor95CQEERERBjVWKi+7XFwcEBsbCyuX7/O3d5++234+/vj+vXrCAwMbKiqa1Wb96dPnz5ITk5GQUEBd+zevXvg8/nw9vau1/rWpDbtKSoqAp+v/lFjYWEBoPzK15QY8+cBIWoMNLnPoFTLfHbu3Mni4uLY/Pnzma2tLUtISGCMMfbxxx+zKVOmcOVVy2Lef/99FhcXx3bu3GlUy2L0bU9ERASztLRkX3/9NUtJSeFuOTk5hmqCGn3bU5mxzaLXtz35+fnM29ubvfzyy+zWrVvszJkzrFWrVuzNN980VBPU6Nue3bt3M0tLS7Zt2zb24MEDdv78edatWzfWo0cPQzVBTX5+PouJiWExMTEMANuyZQuLiYnhlv2Z2ucBISqNMsAzxtjXX3/NJBIJs7KyYl26dGFnzpzhHps2bRoLCgpSKx8VFcU6d+7MrKysWLNmzdj27dsbuMbV06c9QUFBDIDGbdq0aQ1f8Sro+/5UZGwBnjH923P79m02ePBgZmNjw7y9vdmCBQtYUVFRA9e6avq258svv2Tt2rVjNjY2TCwWs9dee409fvy4gWut3enTp6v9ezDFzwNCGGOMtoslhBBCzFCjG4MnhBBCGgMK8IQQQogZogBPCCGEmCEK8IQQQogZogBPCCGEmCEK8IQQQogZogBPCCGEmCEK8IQQQogZogBPiBYJCQng8Xi4fv26oatCCCG1QgGemKyQkBCMHTtW43hUVBR4PB5ycnJqfW4fHx+kpKSgffv2ta8gIYQYUKPbLpaQmpSVlcHKygqenp6GrgohhNQaXcETs3fo0CEEBARAKBSiWbNm2Lx5s9rjzZo1w5o1axASEgJHR0fMnDlTo4s+JCQEPB5P4xYVFQUAyM7OxtSpU9GkSROIRCIMHz4c9+/f514jLCwMTk5OOHnyJNq2bQs7OzsMGzYMKSkpDfVjIIQ0MhTgiVmLjo7GhAkT8OqrryI2NhahoaFYvnw5wsLC1Mpt2rQJ7du3R3R0NJYvX65xni+++AIpKSncbd68eXB3d0ebNm0AKL8AXL16FUeOHMHFixfBGMOIESMglUq5cxQVFeGzzz7Dnj17cPbsWSQmJmLhwoX12n5CSCNm4N3sCKm1adOmMQsLC2Zra6t2s7a2ZgBYdnY2mzx5MgsODlZ73qJFi1i7du24+xKJhI0dO1atTHx8PAPAYmJiNF730KFDTCgUsnPnzjHGGLt37x4DwP766y+uTGZmJrOxsWE//fQTY0y5JzoA9u+//3Jlvv76a+bh4fHcPwdCCNGGruCJSRs4cCCuX7+udtuxYwf3+O3bt9GnTx+15/Tp0wf379+HXC7njnXr1k2n14uJicHUqVPx9ddfo2/fvtxrWFpaIjAwkCvn4uICf39/3L59mzsmEong5+fH3ReLxUhPT9evwYQQoiOaZEdMmq2tLVq2bKl27PHjx9z/GWPg8XhqjzPGtJ6nJqmpqRg9ejRmzJiBGTNmVHs+ba8tEAjUHufxeFU+lxBCnhddwROz1q5dO5w/f17t2IULF9C6dWtYWFjofJ6SkhKMGTMGbdq0wZYtWzReQyaT4e+//+aOZWVl4d69e2jbtu3zNYAQQmqJruCJWfvggw/QvXt3rF69GhMnTsTFixfx3//+F9u2bdPrPLNmzUJSUhL++OMPZGRkcMednZ3RqlUrjBkzBjNnzsS3334Le3t7fPzxx2jatCnGjBlT100ihBCd0BU8MWtdunTBTz/9hP3796N9+/b45JNPsGrVKoSEhOh1njNnziAlJQXt2rWDWCzmbhcuXAAA7N69G127dsXIkSPRq1cvMMZw7NgxjW55QghpKDxGg4CEEEKI2aEreEIIIcQMUYAnhBBCzBAFeEIIIcQMUYAnhBBCzBAFeEIIIcQMUYAnhBBCzBAFeEIIIcQMUYAnhBBCzBAFeEIIIcQMUYAnhBBCzBAFeEIIIcQM/T9OuudhoQszYgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 370x290 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| hide\n",
    "# Create single mixture and broadcast to N,H,1,K\n",
    "counts   = torch.Tensor([[5,10,15], [10,20,30]])[None, :, :].unsqueeze(2)\n",
    "\n",
    "# # Create repetitions for the batch dimension N.\n",
    "N=2\n",
    "counts = torch.repeat_interleave(input=counts, repeats=N, dim=0)\n",
    "weights = torch.ones_like(counts)\n",
    "probs  = torch.ones_like(counts) * 0.5\n",
    "\n",
    "print('weights.shape (N,H,1,K) \\t', weights.shape)\n",
    "print('counts.shape (N,H,1,K) \\t', counts.shape)\n",
    "print('probs.shape (N,H,1,K) \\t', probs.shape)\n",
    "\n",
    "model = NBMM(quantiles=[0.1, 0.40, 0.5, 0.60, 0.9], weighted=True)\n",
    "distr_args = (counts, probs, weights)\n",
    "samples, sample_mean, quants = model.sample(distr_args, num_samples=2000)\n",
    "\n",
    "print('samples.shape (N,H,1,num_samples) ', samples.shape)\n",
    "print('sample_mean.shape (N,H,1,1) ', sample_mean.shape)\n",
    "print('quants.shape  (N,H,1,Q) \\t\\t', quants.shape)\n",
    "\n",
    "# Plot synthethic data\n",
    "x_plot = range(quants.shape[1]) # H length\n",
    "y_plot_hat = quants[0,:,0,:]  # Filter N,G,T -> H,Q\n",
    "samples_hat = samples[0,:,0,:]  # Filter N,G,T -> H,num_samples\n",
    "\n",
    "# Kernel density plot for single forecast horizon \\tau = t+1\n",
    "fig, ax = plt.subplots(figsize=(3.7, 2.9))\n",
    "\n",
    "ax.hist(samples_hat[0,:], alpha=0.5, bins=30,\n",
    "        label=r'Horizon $\\tau+1$')\n",
    "ax.hist(samples_hat[1,:], alpha=0.5, bins=30,\n",
    "        label=r'Horizon $\\tau+2$')\n",
    "ax.set(xlabel='Y values', ylabel='Probability')\n",
    "plt.title('Single horizon Distributions')\n",
    "plt.legend(bbox_to_anchor=(1, 1), loc='upper left', ncol=1)\n",
    "plt.grid()\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Plot simulated trajectory\n",
    "fig, ax = plt.subplots(figsize=(3.7, 2.9))\n",
    "plt.plot(x_plot, y_plot_hat[:,2], color='black', label='median [q50]')\n",
    "plt.fill_between(x_plot,\n",
    "                 y1=y_plot_hat[:,1], y2=y_plot_hat[:,3],\n",
    "                 facecolor='blue', alpha=0.4, label='[p25-p75]')\n",
    "plt.fill_between(x_plot,\n",
    "                 y1=y_plot_hat[:,0], y2=y_plot_hat[:,4],\n",
    "                 facecolor='blue', alpha=0.2, label='[p1-p99]')\n",
    "ax.set(xlabel='Horizon', ylabel='Y values')\n",
    "plt.title('NBM Probabilistic Predictions')\n",
    "plt.legend(bbox_to_anchor=(1, 1), loc='upper left', ncol=1)\n",
    "plt.grid()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a6cf4850",
   "metadata": {},
   "source": [
    "# 5. Robustified Errors\n",
    "\n",
    "This type of errors from robust statistic focus on methods resistant to outliers and violations of assumptions, providing reliable estimates and inferences. Robust estimators are used to reduce the impact of outliers, offering more stable results."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7588f6d2",
   "metadata": {},
   "source": [
    "## Huber Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae9f60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class HuberLoss(BasePointLoss):\n",
    "    \"\"\" Huber Loss\n",
    "\n",
    "    The Huber loss, employed in robust regression, is a loss function that \n",
    "    exhibits reduced sensitivity to outliers in data when compared to the \n",
    "    squared error loss. This function is also refered as SmoothL1.\n",
    "\n",
    "    The Huber loss function is quadratic for small errors and linear for large \n",
    "    errors, with equal values and slopes of the different sections at the two \n",
    "    points where $(y_{\\\\tau}-\\hat{y}_{\\\\tau})^{2}$=$|y_{\\\\tau}-\\hat{y}_{\\\\tau}|$.\n",
    "\n",
    "    $$ L_{\\delta}(y_{\\\\tau},\\; \\hat{y}_{\\\\tau})\n",
    "    =\\\\begin{cases}{\\\\frac{1}{2}}(y_{\\\\tau}-\\hat{y}_{\\\\tau})^{2}\\;{\\\\text{for }}|y_{\\\\tau}-\\hat{y}_{\\\\tau}|\\leq \\delta \\\\\\ \n",
    "    \\\\delta \\ \\cdot \\left(|y_{\\\\tau}-\\hat{y}_{\\\\tau}|-{\\\\frac {1}{2}}\\delta \\\\right),\\;{\\\\text{otherwise.}}\\end{cases}$$\n",
    "\n",
    "    where $\\\\delta$ is a threshold parameter that determines the point at which the loss transitions from quadratic to linear,\n",
    "    and can be tuned to control the trade-off between robustness and accuracy in the predictions.\n",
    "\n",
    "    **Parameters:**<br>\n",
    "    `delta`: float=1.0, Specifies the threshold at which to change between delta-scaled L1 and L2 loss.\n",
    "    `horizon_weight`: Tensor of size h, weight for each timestamp of the forecasting window. <br>\n",
    "    \n",
    "    **References:**<br>\n",
    "    [Huber Peter, J (1964). \"Robust Estimation of a Location Parameter\". Annals of Statistics](https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-35/issue-1/Robust-Estimation-of-a-Location-Parameter/10.1214/aoms/1177703732.full)\n",
    "    \"\"\"   \n",
    "    def __init__(self, delta: float=1., horizon_weight=None):\n",
    "        super(HuberLoss, self).__init__(horizon_weight=horizon_weight,\n",
    "                                  outputsize_multiplier=1,\n",
    "                                  output_names=[''])\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self,\n",
    "                 y: torch.Tensor,\n",
    "                 y_hat: torch.Tensor,\n",
    "                 y_insample: torch.Tensor,\n",
    "                 mask: Union[torch.Tensor, None] = None,\n",
    "                 ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        **Parameters:**<br>\n",
    "        `y`: tensor, Actual values.<br>\n",
    "        `y_hat`: tensor, Predicted values.<br>\n",
    "        `mask`: tensor, Specifies date stamps per serie to consider in loss.<br>\n",
    "\n",
    "        **Returns:**<br>\n",
    "        `huber_loss`: tensor (single value).\n",
    "        \"\"\"\n",
    "        losses = F.huber_loss(y, y_hat, reduction='none', delta=self.delta)        \n",
    "        weights = self._compute_weights(y=y, mask=mask)\n",
    "        return _weighted_mean(losses=losses, weights=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccbfa88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/losses/pytorch.py#L2669){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### HuberLoss.__init__\n",
       "\n",
       ">      HuberLoss.__init__ (delta:float=1.0, horizon_weight=None)\n",
       "\n",
       "*Huber Loss\n",
       "\n",
       "The Huber loss, employed in robust regression, is a loss function that \n",
       "exhibits reduced sensitivity to outliers in data when compared to the \n",
       "squared error loss. This function is also refered as SmoothL1.\n",
       "\n",
       "The Huber loss function is quadratic for small errors and linear for large \n",
       "errors, with equal values and slopes of the different sections at the two \n",
       "points where $(y_{\\tau}-\\hat{y}_{\\tau})^{2}$=$|y_{\\tau}-\\hat{y}_{\\tau}|$.\n",
       "\n",
       "$$ L_{\\delta}(y_{\\tau},\\; \\hat{y}_{\\tau})\n",
       "=\\begin{cases}{\\frac{1}{2}}(y_{\\tau}-\\hat{y}_{\\tau})^{2}\\;{\\text{for }}|y_{\\tau}-\\hat{y}_{\\tau}|\\leq \\delta \\\\ \n",
       "\\delta \\ \\cdot \\left(|y_{\\tau}-\\hat{y}_{\\tau}|-{\\frac {1}{2}}\\delta \\right),\\;{\\text{otherwise.}}\\end{cases}$$\n",
       "\n",
       "where $\\delta$ is a threshold parameter that determines the point at which the loss transitions from quadratic to linear,\n",
       "and can be tuned to control the trade-off between robustness and accuracy in the predictions.\n",
       "\n",
       "**Parameters:**<br>\n",
       "`delta`: float=1.0, Specifies the threshold at which to change between delta-scaled L1 and L2 loss.\n",
       "`horizon_weight`: Tensor of size h, weight for each timestamp of the forecasting window. <br>\n",
       "\n",
       "**References:**<br>\n",
       "[Huber Peter, J (1964). \"Robust Estimation of a Location Parameter\". Annals of Statistics](https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-35/issue-1/Robust-Estimation-of-a-Location-Parameter/10.1214/aoms/1177703732.full)*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/losses/pytorch.py#L2669){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### HuberLoss.__init__\n",
       "\n",
       ">      HuberLoss.__init__ (delta:float=1.0, horizon_weight=None)\n",
       "\n",
       "*Huber Loss\n",
       "\n",
       "The Huber loss, employed in robust regression, is a loss function that \n",
       "exhibits reduced sensitivity to outliers in data when compared to the \n",
       "squared error loss. This function is also refered as SmoothL1.\n",
       "\n",
       "The Huber loss function is quadratic for small errors and linear for large \n",
       "errors, with equal values and slopes of the different sections at the two \n",
       "points where $(y_{\\tau}-\\hat{y}_{\\tau})^{2}$=$|y_{\\tau}-\\hat{y}_{\\tau}|$.\n",
       "\n",
       "$$ L_{\\delta}(y_{\\tau},\\; \\hat{y}_{\\tau})\n",
       "=\\begin{cases}{\\frac{1}{2}}(y_{\\tau}-\\hat{y}_{\\tau})^{2}\\;{\\text{for }}|y_{\\tau}-\\hat{y}_{\\tau}|\\leq \\delta \\\\ \n",
       "\\delta \\ \\cdot \\left(|y_{\\tau}-\\hat{y}_{\\tau}|-{\\frac {1}{2}}\\delta \\right),\\;{\\text{otherwise.}}\\end{cases}$$\n",
       "\n",
       "where $\\delta$ is a threshold parameter that determines the point at which the loss transitions from quadratic to linear,\n",
       "and can be tuned to control the trade-off between robustness and accuracy in the predictions.\n",
       "\n",
       "**Parameters:**<br>\n",
       "`delta`: float=1.0, Specifies the threshold at which to change between delta-scaled L1 and L2 loss.\n",
       "`horizon_weight`: Tensor of size h, weight for each timestamp of the forecasting window. <br>\n",
       "\n",
       "**References:**<br>\n",
       "[Huber Peter, J (1964). \"Robust Estimation of a Location Parameter\". Annals of Statistics](https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-35/issue-1/Robust-Estimation-of-a-Location-Parameter/10.1214/aoms/1177703732.full)*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(HuberLoss, name='HuberLoss.__init__', title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6226178b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/losses/pytorch.py#L2701){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### HuberLoss.__call__\n",
       "\n",
       ">      HuberLoss.__call__ (y:torch.Tensor, y_hat:torch.Tensor,\n",
       ">                          y_insample:torch.Tensor,\n",
       ">                          mask:Optional[torch.Tensor]=None)\n",
       "\n",
       "***Parameters:**<br>\n",
       "`y`: tensor, Actual values.<br>\n",
       "`y_hat`: tensor, Predicted values.<br>\n",
       "`mask`: tensor, Specifies date stamps per serie to consider in loss.<br>\n",
       "\n",
       "**Returns:**<br>\n",
       "`huber_loss`: tensor (single value).*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/losses/pytorch.py#L2701){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### HuberLoss.__call__\n",
       "\n",
       ">      HuberLoss.__call__ (y:torch.Tensor, y_hat:torch.Tensor,\n",
       ">                          y_insample:torch.Tensor,\n",
       ">                          mask:Optional[torch.Tensor]=None)\n",
       "\n",
       "***Parameters:**<br>\n",
       "`y`: tensor, Actual values.<br>\n",
       "`y_hat`: tensor, Predicted values.<br>\n",
       "`mask`: tensor, Specifies date stamps per serie to consider in loss.<br>\n",
       "\n",
       "**Returns:**<br>\n",
       "`huber_loss`: tensor (single value).*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(HuberLoss.__call__, name='HuberLoss.__call__', title_level=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "06aad81b",
   "metadata": {},
   "source": [
    "![](imgs_losses/huber_loss.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7f835621",
   "metadata": {},
   "source": [
    "## Tukey Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ea3109",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TukeyLoss(BasePointLoss):\n",
    "    \"\"\" Tukey Loss\n",
    "\n",
    "    The Tukey loss function, also known as Tukey's biweight function, is a \n",
    "    robust statistical loss function used in robust statistics. Tukey's loss exhibits\n",
    "    quadratic behavior near the origin, like the Huber loss; however, it is even more\n",
    "    robust to outliers as the loss for large residuals remains constant instead of \n",
    "    scaling linearly.\n",
    "\n",
    "    The parameter $c$ in Tukey's loss determines the ''saturation'' point\n",
    "    of the function: Higher values of $c$ enhance sensitivity, while lower values \n",
    "    increase resistance to outliers.\n",
    "\n",
    "    $$ L_{c}(y_{\\\\tau},\\; \\hat{y}_{\\\\tau})\n",
    "    =\\\\begin{cases}{\n",
    "    \\\\frac{c^{2}}{6}} \\\\left[1-(\\\\frac{y_{\\\\tau}-\\hat{y}_{\\\\tau}}{c})^{2} \\\\right]^{3}    \\;\\\\text{for } |y_{\\\\tau}-\\hat{y}_{\\\\tau}|\\leq c \\\\\\ \n",
    "    \\\\frac{c^{2}}{6} \\qquad \\\\text{otherwise.}  \\end{cases}$$\n",
    "\n",
    "    Please note that the Tukey loss function assumes the data to be stationary or\n",
    "    normalized beforehand. If the error values are excessively large, the algorithm\n",
    "    may need help to converge during optimization. It is advisable to employ small learning rates.\n",
    "\n",
    "    **Parameters:**<br>\n",
    "    `c`: float=4.685, Specifies the Tukey loss' threshold on which residuals are no longer considered.<br>\n",
    "    `normalize`: bool=True, Wether normalization is performed within Tukey loss' computation.<br>\n",
    "\n",
    "    **References:**<br>\n",
    "    [Beaton, A. E., and Tukey, J. W. (1974). \"The Fitting of Power Series, Meaning Polynomials, Illustrated on Band-Spectroscopic Data.\"](https://www.jstor.org/stable/1267936)\n",
    "    \"\"\"\n",
    "    def __init__(self, c: float=4.685, normalize: bool=True):\n",
    "        super(TukeyLoss, self).__init__()\n",
    "        self.outputsize_multiplier = 1\n",
    "        self.c = c\n",
    "        self.normalize = normalize\n",
    "        self.output_names = ['']\n",
    "        self.is_distribution_output = False\n",
    "\n",
    "    def domain_map(self, y_hat: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "        Univariate: [B, H, 1]\n",
    "        Multivariate: [B, H, N]\n",
    "\n",
    "        Output: [B, H, N]\n",
    "        \"\"\"\n",
    "\n",
    "        return y_hat\n",
    "\n",
    "    def masked_mean(self, x, mask, dim):\n",
    "        x_nan = x.masked_fill(mask < 1, float(\"nan\"))\n",
    "        x_mean = x_nan.nanmean(dim=dim, keepdim=True)\n",
    "        x_mean = torch.nan_to_num(x_mean, nan=0.0)\n",
    "        return x_mean\n",
    "\n",
    "    def __call__(self,\n",
    "                 y: torch.Tensor,\n",
    "                 y_hat: torch.Tensor,\n",
    "                 y_insample: torch.Tensor,\n",
    "                 mask: Union[torch.Tensor, None] = None,\n",
    "                 ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        **Parameters:**<br>\n",
    "        `y`: tensor, Actual values.<br>\n",
    "        `y_hat`: tensor, Predicted values.<br>\n",
    "        `mask`: tensor, Specifies date stamps per serie to consider in loss.<br>\n",
    "\n",
    "        **Returns:**<br>\n",
    "        `tukey_loss`: tensor (single value).\n",
    "        \"\"\"\n",
    "        if mask is None:\n",
    "            mask = torch.ones_like(y_hat)\n",
    "\n",
    "        # We normalize the Tukey loss, to satisfy 4.685 normal outlier bounds\n",
    "        if self.normalize:\n",
    "            y_mean = self.masked_mean(x=y, mask=mask, dim=-1)\n",
    "            y_std = torch.sqrt(self.masked_mean(x=(y - y_mean) ** 2, mask=mask, dim=-1)) + 1e-2\n",
    "        else:\n",
    "            y_std = 1.\n",
    "        delta_y = torch.abs(y - y_hat) / y_std\n",
    "\n",
    "        tukey_mask = torch.greater_equal(self.c * torch.ones_like(delta_y), delta_y)\n",
    "        tukey_loss = tukey_mask * mask * (1-(delta_y/(self.c))**2)**3 + (1-(tukey_mask * 1))\n",
    "        tukey_loss = (self.c**2 / 6) * torch.mean(tukey_loss)\n",
    "        return tukey_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4653e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/losses/pytorch.py#L2721){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TukeyLoss.__init__\n",
       "\n",
       ">      TukeyLoss.__init__ (c:float=4.685, normalize:bool=True)\n",
       "\n",
       "*Tukey Loss\n",
       "\n",
       "The Tukey loss function, also known as Tukey's biweight function, is a \n",
       "robust statistical loss function used in robust statistics. Tukey's loss exhibits\n",
       "quadratic behavior near the origin, like the Huber loss; however, it is even more\n",
       "robust to outliers as the loss for large residuals remains constant instead of \n",
       "scaling linearly.\n",
       "\n",
       "The parameter $c$ in Tukey's loss determines the ''saturation'' point\n",
       "of the function: Higher values of $c$ enhance sensitivity, while lower values \n",
       "increase resistance to outliers.\n",
       "\n",
       "$$ L_{c}(y_{\\tau},\\; \\hat{y}_{\\tau})\n",
       "=\\begin{cases}{\n",
       "\\frac{c^{2}}{6}} \\left[1-(\\frac{y_{\\tau}-\\hat{y}_{\\tau}}{c})^{2} \\right]^{3}    \\;\\text{for } |y_{\\tau}-\\hat{y}_{\\tau}|\\leq c \\\\ \n",
       "\\frac{c^{2}}{6} \\qquad \\text{otherwise.}  \\end{cases}$$\n",
       "\n",
       "Please note that the Tukey loss function assumes the data to be stationary or\n",
       "normalized beforehand. If the error values are excessively large, the algorithm\n",
       "may need help to converge during optimization. It is advisable to employ small learning rates.\n",
       "\n",
       "**Parameters:**<br>\n",
       "`c`: float=4.685, Specifies the Tukey loss' threshold on which residuals are no longer considered.<br>\n",
       "`normalize`: bool=True, Wether normalization is performed within Tukey loss' computation.<br>\n",
       "\n",
       "**References:**<br>\n",
       "[Beaton, A. E., and Tukey, J. W. (1974). \"The Fitting of Power Series, Meaning Polynomials, Illustrated on Band-Spectroscopic Data.\"](https://www.jstor.org/stable/1267936)*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/losses/pytorch.py#L2721){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TukeyLoss.__init__\n",
       "\n",
       ">      TukeyLoss.__init__ (c:float=4.685, normalize:bool=True)\n",
       "\n",
       "*Tukey Loss\n",
       "\n",
       "The Tukey loss function, also known as Tukey's biweight function, is a \n",
       "robust statistical loss function used in robust statistics. Tukey's loss exhibits\n",
       "quadratic behavior near the origin, like the Huber loss; however, it is even more\n",
       "robust to outliers as the loss for large residuals remains constant instead of \n",
       "scaling linearly.\n",
       "\n",
       "The parameter $c$ in Tukey's loss determines the ''saturation'' point\n",
       "of the function: Higher values of $c$ enhance sensitivity, while lower values \n",
       "increase resistance to outliers.\n",
       "\n",
       "$$ L_{c}(y_{\\tau},\\; \\hat{y}_{\\tau})\n",
       "=\\begin{cases}{\n",
       "\\frac{c^{2}}{6}} \\left[1-(\\frac{y_{\\tau}-\\hat{y}_{\\tau}}{c})^{2} \\right]^{3}    \\;\\text{for } |y_{\\tau}-\\hat{y}_{\\tau}|\\leq c \\\\ \n",
       "\\frac{c^{2}}{6} \\qquad \\text{otherwise.}  \\end{cases}$$\n",
       "\n",
       "Please note that the Tukey loss function assumes the data to be stationary or\n",
       "normalized beforehand. If the error values are excessively large, the algorithm\n",
       "may need help to converge during optimization. It is advisable to employ small learning rates.\n",
       "\n",
       "**Parameters:**<br>\n",
       "`c`: float=4.685, Specifies the Tukey loss' threshold on which residuals are no longer considered.<br>\n",
       "`normalize`: bool=True, Wether normalization is performed within Tukey loss' computation.<br>\n",
       "\n",
       "**References:**<br>\n",
       "[Beaton, A. E., and Tukey, J. W. (1974). \"The Fitting of Power Series, Meaning Polynomials, Illustrated on Band-Spectroscopic Data.\"](https://www.jstor.org/stable/1267936)*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(TukeyLoss, name='TukeyLoss.__init__', title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7686462",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/losses/pytorch.py#L2772){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TukeyLoss.__call__\n",
       "\n",
       ">      TukeyLoss.__call__ (y:torch.Tensor, y_hat:torch.Tensor,\n",
       ">                          y_insample:torch.Tensor,\n",
       ">                          mask:Optional[torch.Tensor]=None)\n",
       "\n",
       "***Parameters:**<br>\n",
       "`y`: tensor, Actual values.<br>\n",
       "`y_hat`: tensor, Predicted values.<br>\n",
       "`mask`: tensor, Specifies date stamps per serie to consider in loss.<br>\n",
       "\n",
       "**Returns:**<br>\n",
       "`tukey_loss`: tensor (single value).*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/losses/pytorch.py#L2772){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TukeyLoss.__call__\n",
       "\n",
       ">      TukeyLoss.__call__ (y:torch.Tensor, y_hat:torch.Tensor,\n",
       ">                          y_insample:torch.Tensor,\n",
       ">                          mask:Optional[torch.Tensor]=None)\n",
       "\n",
       "***Parameters:**<br>\n",
       "`y`: tensor, Actual values.<br>\n",
       "`y_hat`: tensor, Predicted values.<br>\n",
       "`mask`: tensor, Specifies date stamps per serie to consider in loss.<br>\n",
       "\n",
       "**Returns:**<br>\n",
       "`tukey_loss`: tensor (single value).*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(TukeyLoss.__call__, name='TukeyLoss.__call__', title_level=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8ae50f25",
   "metadata": {},
   "source": [
    "![](imgs_losses/tukey_loss.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a8a28d9c",
   "metadata": {},
   "source": [
    "## Huberized Quantile Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549e6bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class HuberQLoss(BasePointLoss):\n",
    "    \"\"\" Huberized Quantile Loss\n",
    "\n",
    "    The Huberized quantile loss is a modified version of the quantile loss function that\n",
    "    combines the advantages of the quantile loss and the Huber loss. It is commonly used\n",
    "    in regression tasks, especially when dealing with data that contains outliers or heavy tails.\n",
    "\n",
    "    The Huberized quantile loss between `y` and `y_hat` measure the Huber Loss in a non-symmetric way.\n",
    "    The loss pays more attention to under/over-estimation depending on the quantile parameter $q$; \n",
    "    and controls the trade-off between robustness and accuracy in the predictions with the parameter $delta$.\n",
    "\n",
    "    $$ \\mathrm{HuberQL}(\\\\mathbf{y}_{\\\\tau}, \\\\mathbf{\\hat{y}}^{(q)}_{\\\\tau}) = \n",
    "    (1-q)\\, L_{\\delta}(y_{\\\\tau},\\; \\hat{y}^{(q)}_{\\\\tau}) \\mathbb{1}\\{ \\hat{y}^{(q)}_{\\\\tau} \\geq y_{\\\\tau} \\} + \n",
    "    q\\, L_{\\delta}(y_{\\\\tau},\\; \\hat{y}^{(q)}_{\\\\tau}) \\mathbb{1}\\{ \\hat{y}^{(q)}_{\\\\tau} < y_{\\\\tau} \\} $$\n",
    "\n",
    "    **Parameters:**<br>\n",
    "    `delta`: float=1.0, Specifies the threshold at which to change between delta-scaled L1 and L2 loss.<br>\n",
    "    `q`: float, between 0 and 1. The slope of the quantile loss, in the context of quantile regression, the q determines the conditional quantile level.<br>\n",
    "    `horizon_weight`: Tensor of size h, weight for each timestamp of the forecasting window. <br>\n",
    "\n",
    "    **References:**<br>\n",
    "    [Huber Peter, J (1964). \"Robust Estimation of a Location Parameter\". Annals of Statistics](https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-35/issue-1/Robust-Estimation-of-a-Location-Parameter/10.1214/aoms/1177703732.full)<br>\n",
    "    [Roger Koenker and Gilbert Bassett, Jr., \"Regression Quantiles\".](https://www.jstor.org/stable/1913643)\n",
    "    \"\"\"\n",
    "    def __init__(self, q, delta: float=1., horizon_weight=None):\n",
    "        super(HuberQLoss, self).__init__(horizon_weight=horizon_weight,\n",
    "                                           outputsize_multiplier=1,\n",
    "                                           output_names=[f'_q{q}_d{delta}'])\n",
    "        self.q = q\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self,\n",
    "                 y: torch.Tensor,\n",
    "                 y_hat: torch.Tensor,\n",
    "                 y_insample: torch.Tensor,\n",
    "                 mask: Union[torch.Tensor, None] = None,\n",
    "                 ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        **Parameters:**<br>\n",
    "        `y`: tensor, Actual values.<br>\n",
    "        `y_hat`: tensor, Predicted values.<br>\n",
    "        `mask`: tensor, Specifies datapoints to consider in loss.<br>\n",
    "\n",
    "        **Returns:**<br>\n",
    "        `huber_qloss`: tensor (single value).\n",
    "        \"\"\"\n",
    "        \n",
    "        error  = y_hat - y\n",
    "        zero_error = torch.zeros_like(error)\n",
    "        sq     = torch.maximum(-error, zero_error)\n",
    "        s1_q   = torch.maximum(error, zero_error)\n",
    "        losses = self.q * F.huber_loss(sq, zero_error, \n",
    "                                       reduction='none', delta=self.delta) + \\\n",
    "                 (1 - self.q) * F.huber_loss(s1_q, zero_error, \n",
    "                                        reduction='none', delta=self.delta)\n",
    "\n",
    "        weights = self._compute_weights(y=y, mask=mask)\n",
    "        return _weighted_mean(losses=losses, weights=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec830ac0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/losses/pytorch.py#L2809){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### HuberQLoss.__init__\n",
       "\n",
       ">      HuberQLoss.__init__ (q, delta:float=1.0, horizon_weight=None)\n",
       "\n",
       "*Huberized Quantile Loss\n",
       "\n",
       "The Huberized quantile loss is a modified version of the quantile loss function that\n",
       "combines the advantages of the quantile loss and the Huber loss. It is commonly used\n",
       "in regression tasks, especially when dealing with data that contains outliers or heavy tails.\n",
       "\n",
       "The Huberized quantile loss between `y` and `y_hat` measure the Huber Loss in a non-symmetric way.\n",
       "The loss pays more attention to under/over-estimation depending on the quantile parameter $q$; \n",
       "and controls the trade-off between robustness and accuracy in the predictions with the parameter $delta$.\n",
       "\n",
       "$$ \\mathrm{HuberQL}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}^{(q)}_{\\tau}) = \n",
       "(1-q)\\, L_{\\delta}(y_{\\tau},\\; \\hat{y}^{(q)}_{\\tau}) \\mathbb{1}\\{ \\hat{y}^{(q)}_{\\tau} \\geq y_{\\tau} \\} + \n",
       "q\\, L_{\\delta}(y_{\\tau},\\; \\hat{y}^{(q)}_{\\tau}) \\mathbb{1}\\{ \\hat{y}^{(q)}_{\\tau} < y_{\\tau} \\} $$\n",
       "\n",
       "**Parameters:**<br>\n",
       "`delta`: float=1.0, Specifies the threshold at which to change between delta-scaled L1 and L2 loss.<br>\n",
       "`q`: float, between 0 and 1. The slope of the quantile loss, in the context of quantile regression, the q determines the conditional quantile level.<br>\n",
       "`horizon_weight`: Tensor of size h, weight for each timestamp of the forecasting window. <br>\n",
       "\n",
       "**References:**<br>\n",
       "[Huber Peter, J (1964). \"Robust Estimation of a Location Parameter\". Annals of Statistics](https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-35/issue-1/Robust-Estimation-of-a-Location-Parameter/10.1214/aoms/1177703732.full)<br>\n",
       "[Roger Koenker and Gilbert Bassett, Jr., \"Regression Quantiles\".](https://www.jstor.org/stable/1913643)*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/losses/pytorch.py#L2809){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### HuberQLoss.__init__\n",
       "\n",
       ">      HuberQLoss.__init__ (q, delta:float=1.0, horizon_weight=None)\n",
       "\n",
       "*Huberized Quantile Loss\n",
       "\n",
       "The Huberized quantile loss is a modified version of the quantile loss function that\n",
       "combines the advantages of the quantile loss and the Huber loss. It is commonly used\n",
       "in regression tasks, especially when dealing with data that contains outliers or heavy tails.\n",
       "\n",
       "The Huberized quantile loss between `y` and `y_hat` measure the Huber Loss in a non-symmetric way.\n",
       "The loss pays more attention to under/over-estimation depending on the quantile parameter $q$; \n",
       "and controls the trade-off between robustness and accuracy in the predictions with the parameter $delta$.\n",
       "\n",
       "$$ \\mathrm{HuberQL}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}^{(q)}_{\\tau}) = \n",
       "(1-q)\\, L_{\\delta}(y_{\\tau},\\; \\hat{y}^{(q)}_{\\tau}) \\mathbb{1}\\{ \\hat{y}^{(q)}_{\\tau} \\geq y_{\\tau} \\} + \n",
       "q\\, L_{\\delta}(y_{\\tau},\\; \\hat{y}^{(q)}_{\\tau}) \\mathbb{1}\\{ \\hat{y}^{(q)}_{\\tau} < y_{\\tau} \\} $$\n",
       "\n",
       "**Parameters:**<br>\n",
       "`delta`: float=1.0, Specifies the threshold at which to change between delta-scaled L1 and L2 loss.<br>\n",
       "`q`: float, between 0 and 1. The slope of the quantile loss, in the context of quantile regression, the q determines the conditional quantile level.<br>\n",
       "`horizon_weight`: Tensor of size h, weight for each timestamp of the forecasting window. <br>\n",
       "\n",
       "**References:**<br>\n",
       "[Huber Peter, J (1964). \"Robust Estimation of a Location Parameter\". Annals of Statistics](https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-35/issue-1/Robust-Estimation-of-a-Location-Parameter/10.1214/aoms/1177703732.full)<br>\n",
       "[Roger Koenker and Gilbert Bassett, Jr., \"Regression Quantiles\".](https://www.jstor.org/stable/1913643)*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(HuberQLoss, name='HuberQLoss.__init__', title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15409d3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/losses/pytorch.py#L2843){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### HuberQLoss.__call__\n",
       "\n",
       ">      HuberQLoss.__call__ (y:torch.Tensor, y_hat:torch.Tensor,\n",
       ">                           y_insample:torch.Tensor,\n",
       ">                           mask:Optional[torch.Tensor]=None)\n",
       "\n",
       "***Parameters:**<br>\n",
       "`y`: tensor, Actual values.<br>\n",
       "`y_hat`: tensor, Predicted values.<br>\n",
       "`mask`: tensor, Specifies datapoints to consider in loss.<br>\n",
       "\n",
       "**Returns:**<br>\n",
       "`huber_qloss`: tensor (single value).*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/losses/pytorch.py#L2843){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### HuberQLoss.__call__\n",
       "\n",
       ">      HuberQLoss.__call__ (y:torch.Tensor, y_hat:torch.Tensor,\n",
       ">                           y_insample:torch.Tensor,\n",
       ">                           mask:Optional[torch.Tensor]=None)\n",
       "\n",
       "***Parameters:**<br>\n",
       "`y`: tensor, Actual values.<br>\n",
       "`y_hat`: tensor, Predicted values.<br>\n",
       "`mask`: tensor, Specifies datapoints to consider in loss.<br>\n",
       "\n",
       "**Returns:**<br>\n",
       "`huber_qloss`: tensor (single value).*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(HuberQLoss.__call__, name='HuberQLoss.__call__', title_level=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a2d97f31",
   "metadata": {},
   "source": [
    "![](imgs_losses/huber_qloss.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2e7e3143",
   "metadata": {},
   "source": [
    "## Huberized MQLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc992c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class HuberMQLoss(BasePointLoss):\n",
    "    \"\"\"  Huberized Multi-Quantile loss\n",
    "\n",
    "    The Huberized Multi-Quantile loss (HuberMQL) is a modified version of the multi-quantile loss function \n",
    "    that combines the advantages of the quantile loss and the Huber loss. HuberMQL is commonly used in regression \n",
    "    tasks, especially when dealing with data that contains outliers or heavy tails. The loss function pays \n",
    "    more attention to under/over-estimation depending on the quantile list $[q_{1},q_{2},\\dots]$ parameter. \n",
    "    It controls the trade-off between robustness and prediction accuracy with the parameter $\\\\delta$.\n",
    "\n",
    "    $$ \\mathrm{HuberMQL}_{\\delta}(\\\\mathbf{y}_{\\\\tau},[\\\\mathbf{\\hat{y}}^{(q_{1})}_{\\\\tau}, ... ,\\hat{y}^{(q_{n})}_{\\\\tau}]) = \n",
    "    \\\\frac{1}{n} \\\\sum_{q_{i}} \\mathrm{HuberQL}_{\\\\delta}(\\\\mathbf{y}_{\\\\tau}, \\\\mathbf{\\hat{y}}^{(q_{i})}_{\\\\tau}) $$\n",
    "\n",
    "    **Parameters:**<br>\n",
    "    `level`: int list [0,100]. Probability levels for prediction intervals (Defaults median).\n",
    "    `quantiles`: float list [0., 1.]. Alternative to level, quantiles to estimate from y distribution.\n",
    "    `delta`: float=1.0, Specifies the threshold at which to change between delta-scaled L1 and L2 loss.<br>   \n",
    "    `horizon_weight`: Tensor of size h, weight for each timestamp of the forecasting window. <br> \n",
    "\n",
    "    **References:**<br>\n",
    "    [Huber Peter, J (1964). \"Robust Estimation of a Location Parameter\". Annals of Statistics](https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-35/issue-1/Robust-Estimation-of-a-Location-Parameter/10.1214/aoms/1177703732.full)<br>\n",
    "    [Roger Koenker and Gilbert Bassett, Jr., \"Regression Quantiles\".](https://www.jstor.org/stable/1913643)\n",
    "    \"\"\"\n",
    "    def __init__(self, level=[80, 90], quantiles=None, delta: float=1.0, horizon_weight=None):\n",
    "\n",
    "        qs, output_names = level_to_outputs(level)\n",
    "        qs = torch.Tensor(qs)\n",
    "        # Transform quantiles to homogeneus output names\n",
    "        if quantiles is not None:\n",
    "            _, output_names = quantiles_to_outputs(quantiles)\n",
    "            qs = torch.Tensor(quantiles)\n",
    "\n",
    "        super(HuberMQLoss, self).__init__(horizon_weight=horizon_weight,\n",
    "                                     outputsize_multiplier=len(qs),\n",
    "                                     output_names=output_names)\n",
    "        \n",
    "        self.quantiles = torch.nn.Parameter(qs, requires_grad=False)\n",
    "        self.delta = delta\n",
    "\n",
    "    def domain_map(self, y_hat: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "        Univariate: [B, H, 1 * Q]\n",
    "        Multivariate: [B, H, N * Q]\n",
    "\n",
    "        Output: [B, H, N, Q]\n",
    "        \"\"\"\n",
    "        output = y_hat.reshape(y_hat.shape[0],\n",
    "                               y_hat.shape[1],\n",
    "                               -1,\n",
    "                               self.outputsize_multiplier)\n",
    "\n",
    "        return output\n",
    "    \n",
    "    def _compute_weights(self, y, mask):\n",
    "        \"\"\"\n",
    "        Compute final weights for each datapoint (based on all weights and all masks)\n",
    "        Set horizon_weight to a ones[H] tensor if not set.\n",
    "        If set, check that it has the same length as the horizon in x.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.horizon_weight is None:\n",
    "            weights = torch.ones_like(mask)\n",
    "        else:\n",
    "            assert mask.shape[1] == len(self.horizon_weight), \\\n",
    "                'horizon_weight must have same length as Y'       \n",
    "            weights = self.horizon_weight.clone()\n",
    "            weights = weights[None, :, None, None].to(mask.device)\n",
    "            weights = torch.ones_like(mask, device=mask.device) * weights\n",
    "        \n",
    "        return weights * mask\n",
    "\n",
    "    def __call__(self,\n",
    "                 y: torch.Tensor,\n",
    "                 y_hat: torch.Tensor,\n",
    "                 y_insample: torch.Tensor,\n",
    "                 mask: Union[torch.Tensor, None] = None,\n",
    "                 ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        **Parameters:**<br>\n",
    "        `y`: tensor, Actual values.<br>\n",
    "        `y_hat`: tensor, Predicted values.<br>\n",
    "        `mask`: tensor, Specifies date stamps per serie to consider in loss.<br>\n",
    "\n",
    "        **Returns:**<br>\n",
    "        `hmqloss`: tensor (single value).\n",
    "        \"\"\"\n",
    "        # [B, h, N] -> [B, h, N, 1]\n",
    "        if y_hat.ndim == 3:\n",
    "            y_hat = y_hat.unsqueeze(-1)\n",
    "\n",
    "        y = y.unsqueeze(-1)        \n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(-1)\n",
    "        else:\n",
    "            mask = torch.ones_like(y, device=y.device)\n",
    "        \n",
    "        error  = y_hat - y\n",
    "        \n",
    "        zero_error = torch.zeros_like(error)        \n",
    "        sq     = torch.maximum(-error, torch.zeros_like(error))\n",
    "        s1_q   = torch.maximum(error, torch.zeros_like(error))\n",
    "        \n",
    "        quantiles = self.quantiles[None, None, None, :]\n",
    "        losses = F.huber_loss(quantiles * sq, zero_error, \n",
    "                                        reduction='none', delta=self.delta) + \\\n",
    "                  F.huber_loss((1 - quantiles) * s1_q, zero_error, \n",
    "                                reduction='none', delta=self.delta)\n",
    "        losses = (1 / len(quantiles)) * losses\n",
    "\n",
    "        weights = self._compute_weights(y=losses, mask=mask) \n",
    "\n",
    "        return _weighted_mean(losses=losses, weights=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a662632",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/losses/pytorch.py#L2872){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### HuberMQLoss.__init__\n",
       "\n",
       ">      HuberMQLoss.__init__ (level=[80, 90], quantiles=None, delta:float=1.0,\n",
       ">                            horizon_weight=None)\n",
       "\n",
       "*Huberized Multi-Quantile loss\n",
       "\n",
       "The Huberized Multi-Quantile loss (HuberMQL) is a modified version of the multi-quantile loss function \n",
       "that combines the advantages of the quantile loss and the Huber loss. HuberMQL is commonly used in regression \n",
       "tasks, especially when dealing with data that contains outliers or heavy tails. The loss function pays \n",
       "more attention to under/over-estimation depending on the quantile list $[q_{1},q_{2},\\dots]$ parameter. \n",
       "It controls the trade-off between robustness and prediction accuracy with the parameter $\\delta$.\n",
       "\n",
       "$$ \\mathrm{HuberMQL}_{\\delta}(\\mathbf{y}_{\\tau},[\\mathbf{\\hat{y}}^{(q_{1})}_{\\tau}, ... ,\\hat{y}^{(q_{n})}_{\\tau}]) = \n",
       "\\frac{1}{n} \\sum_{q_{i}} \\mathrm{HuberQL}_{\\delta}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}^{(q_{i})}_{\\tau}) $$\n",
       "\n",
       "**Parameters:**<br>\n",
       "`level`: int list [0,100]. Probability levels for prediction intervals (Defaults median).\n",
       "`quantiles`: float list [0., 1.]. Alternative to level, quantiles to estimate from y distribution.\n",
       "`delta`: float=1.0, Specifies the threshold at which to change between delta-scaled L1 and L2 loss.<br>   \n",
       "`horizon_weight`: Tensor of size h, weight for each timestamp of the forecasting window. <br> \n",
       "\n",
       "**References:**<br>\n",
       "[Huber Peter, J (1964). \"Robust Estimation of a Location Parameter\". Annals of Statistics](https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-35/issue-1/Robust-Estimation-of-a-Location-Parameter/10.1214/aoms/1177703732.full)<br>\n",
       "[Roger Koenker and Gilbert Bassett, Jr., \"Regression Quantiles\".](https://www.jstor.org/stable/1913643)*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/losses/pytorch.py#L2872){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### HuberMQLoss.__init__\n",
       "\n",
       ">      HuberMQLoss.__init__ (level=[80, 90], quantiles=None, delta:float=1.0,\n",
       ">                            horizon_weight=None)\n",
       "\n",
       "*Huberized Multi-Quantile loss\n",
       "\n",
       "The Huberized Multi-Quantile loss (HuberMQL) is a modified version of the multi-quantile loss function \n",
       "that combines the advantages of the quantile loss and the Huber loss. HuberMQL is commonly used in regression \n",
       "tasks, especially when dealing with data that contains outliers or heavy tails. The loss function pays \n",
       "more attention to under/over-estimation depending on the quantile list $[q_{1},q_{2},\\dots]$ parameter. \n",
       "It controls the trade-off between robustness and prediction accuracy with the parameter $\\delta$.\n",
       "\n",
       "$$ \\mathrm{HuberMQL}_{\\delta}(\\mathbf{y}_{\\tau},[\\mathbf{\\hat{y}}^{(q_{1})}_{\\tau}, ... ,\\hat{y}^{(q_{n})}_{\\tau}]) = \n",
       "\\frac{1}{n} \\sum_{q_{i}} \\mathrm{HuberQL}_{\\delta}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}^{(q_{i})}_{\\tau}) $$\n",
       "\n",
       "**Parameters:**<br>\n",
       "`level`: int list [0,100]. Probability levels for prediction intervals (Defaults median).\n",
       "`quantiles`: float list [0., 1.]. Alternative to level, quantiles to estimate from y distribution.\n",
       "`delta`: float=1.0, Specifies the threshold at which to change between delta-scaled L1 and L2 loss.<br>   \n",
       "`horizon_weight`: Tensor of size h, weight for each timestamp of the forecasting window. <br> \n",
       "\n",
       "**References:**<br>\n",
       "[Huber Peter, J (1964). \"Robust Estimation of a Location Parameter\". Annals of Statistics](https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-35/issue-1/Robust-Estimation-of-a-Location-Parameter/10.1214/aoms/1177703732.full)<br>\n",
       "[Roger Koenker and Gilbert Bassett, Jr., \"Regression Quantiles\".](https://www.jstor.org/stable/1913643)*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(HuberMQLoss, name='HuberMQLoss.__init__', title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f733ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/losses/pytorch.py#L2943){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### HuberMQLoss.__call__\n",
       "\n",
       ">      HuberMQLoss.__call__ (y:torch.Tensor, y_hat:torch.Tensor,\n",
       ">                            y_insample:torch.Tensor,\n",
       ">                            mask:Optional[torch.Tensor]=None)\n",
       "\n",
       "***Parameters:**<br>\n",
       "`y`: tensor, Actual values.<br>\n",
       "`y_hat`: tensor, Predicted values.<br>\n",
       "`mask`: tensor, Specifies date stamps per serie to consider in loss.<br>\n",
       "\n",
       "**Returns:**<br>\n",
       "`hmqloss`: tensor (single value).*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/losses/pytorch.py#L2943){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### HuberMQLoss.__call__\n",
       "\n",
       ">      HuberMQLoss.__call__ (y:torch.Tensor, y_hat:torch.Tensor,\n",
       ">                            y_insample:torch.Tensor,\n",
       ">                            mask:Optional[torch.Tensor]=None)\n",
       "\n",
       "***Parameters:**<br>\n",
       "`y`: tensor, Actual values.<br>\n",
       "`y_hat`: tensor, Predicted values.<br>\n",
       "`mask`: tensor, Specifies date stamps per serie to consider in loss.<br>\n",
       "\n",
       "**Returns:**<br>\n",
       "`hmqloss`: tensor (single value).*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(HuberMQLoss.__call__, name='HuberMQLoss.__call__', title_level=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "47782e38",
   "metadata": {},
   "source": [
    "![](imgs_losses/hmq_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affe8b2f",
   "metadata": {},
   "source": [
    "## Huberized IQLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e71b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class HuberIQLoss(HuberQLoss):\n",
    "    \"\"\"Implicit Huber Quantile Loss\n",
    "\n",
    "    Computes the huberized quantile loss between `y` and `y_hat`, with the quantile `q` provided as an input to the network. \n",
    "    HuberIQLoss measures the deviation of a huberized quantile forecast.\n",
    "    By weighting the absolute deviation in a non symmetric way, the\n",
    "    loss pays more attention to under or over estimation.\n",
    "\n",
    "    $$ \\mathrm{HuberQL}(\\\\mathbf{y}_{\\\\tau}, \\\\mathbf{\\hat{y}}^{(q)}_{\\\\tau}) = \n",
    "    (1-q)\\, L_{\\delta}(y_{\\\\tau},\\; \\hat{y}^{(q)}_{\\\\tau}) \\mathbb{1}\\{ \\hat{y}^{(q)}_{\\\\tau} \\geq y_{\\\\tau} \\} + \n",
    "    q\\, L_{\\delta}(y_{\\\\tau},\\; \\hat{y}^{(q)}_{\\\\tau}) \\mathbb{1}\\{ \\hat{y}^{(q)}_{\\\\tau} < y_{\\\\tau} \\} $$\n",
    "\n",
    "    **Parameters:**<br>\n",
    "    `quantile_sampling`: str, default='uniform', sampling distribution used to sample the quantiles during training. Choose from ['uniform', 'beta']. <br>\n",
    "    `horizon_weight`: Tensor of size h, weight for each timestamp of the forecasting window. <br>\n",
    "    `delta`: float=1.0, Specifies the threshold at which to change between delta-scaled L1 and L2 loss.<br>\n",
    "\n",
    "    **References:**<br>\n",
    "    [Gouttes, Adèle, Kashif Rasul, Mateusz Koren, Johannes Stephan, and Tofigh Naghibi, \"Probabilistic Time Series Forecasting with Implicit Quantile Networks\".](http://arxiv.org/abs/2107.03743)\n",
    "    [Huber Peter, J (1964). \"Robust Estimation of a Location Parameter\". Annals of Statistics](https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-35/issue-1/Robust-Estimation-of-a-Location-Parameter/10.1214/aoms/1177703732.full)<br>\n",
    "    [Roger Koenker and Gilbert Bassett, Jr., \"Regression Quantiles\".](https://www.jstor.org/stable/1913643)\n",
    "    \"\"\"\n",
    "    def __init__(self, cos_embedding_dim = 64, concentration0 = 1.0, concentration1 = 1.0, delta = 1.0, horizon_weight=None):\n",
    "        self.update_quantile()\n",
    "        super(HuberIQLoss, self).__init__(\n",
    "            q = self.q,\n",
    "            delta = delta,\n",
    "            horizon_weight=horizon_weight\n",
    "        )\n",
    "\n",
    "        self.cos_embedding_dim = cos_embedding_dim\n",
    "        self.concentration0 = concentration0\n",
    "        self.concentration1 = concentration1\n",
    "        self.has_sampled = False\n",
    "        self.has_predicted = False\n",
    "\n",
    "        self.quantile_layer = QuantileLayer(\n",
    "            num_output=1, cos_embedding_dim=self.cos_embedding_dim\n",
    "        )\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Linear(1, 1), nn.PReLU()\n",
    "        )\n",
    "        \n",
    "    def _sample_quantiles(self, sample_size, device):\n",
    "        if not self.has_sampled:\n",
    "            self._init_sampling_distribution(device)\n",
    "\n",
    "        quantiles = self.sampling_distr.sample(sample_size)\n",
    "        self.has_sampled = True        \n",
    "        self.has_predicted = False\n",
    "\n",
    "        return quantiles\n",
    "    \n",
    "    def _init_sampling_distribution(self, device):\n",
    "        concentration0 = torch.tensor([self.concentration0],\n",
    "                                      device=device,\n",
    "                                      dtype=torch.float32)\n",
    "        concentration1 = torch.tensor([self.concentration1],\n",
    "                                      device=device,\n",
    "                                      dtype=torch.float32)        \n",
    "        self.sampling_distr = Beta(concentration0 = concentration0,\n",
    "                                   concentration1 = concentration1)\n",
    "\n",
    "    def update_quantile(self, q: List[float] = [0.5]):\n",
    "        self.q = q[0]\n",
    "        self.output_names = [f\"_ql{q[0]}\"]\n",
    "        self.has_predicted = True\n",
    "\n",
    "    def domain_map(self, y_hat):\n",
    "        \"\"\"\n",
    "        Adds IQN network to output of network\n",
    "\n",
    "        Input shapes to this function:\n",
    "         \n",
    "        Univariate: y_hat = [B, h, 1] \n",
    "        Multivariate: y_hat = [B, h, N]\n",
    "        \"\"\"\n",
    "        if self.eval() and self.has_predicted:\n",
    "            quantiles = torch.full(size=y_hat.shape, \n",
    "                                    fill_value=self.q,\n",
    "                                    device=y_hat.device,\n",
    "                                    dtype=y_hat.dtype) \n",
    "            quantiles = quantiles.unsqueeze(-1)             \n",
    "        else:\n",
    "            quantiles = self._sample_quantiles(sample_size=y_hat.shape,\n",
    "                                        device=y_hat.device)\n",
    "\n",
    "        # Embed the quantiles and add to y_hat\n",
    "        emb_taus = self.quantile_layer(quantiles)\n",
    "        emb_inputs = y_hat.unsqueeze(-1) * (1.0 + emb_taus)\n",
    "        emb_outputs = self.output_layer(emb_inputs)\n",
    "        \n",
    "        # Domain map\n",
    "        y_hat = emb_outputs.squeeze(-1)\n",
    "\n",
    "        return y_hat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccf9024",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### HuberIQLoss.__init__\n",
       "\n",
       ">      HuberIQLoss.__init__ (cos_embedding_dim=64, concentration0=1.0,\n",
       ">                            concentration1=1.0, delta=1.0, horizon_weight=None)\n",
       "\n",
       "*Implicit Huber Quantile Loss\n",
       "\n",
       "Computes the huberized quantile loss between `y` and `y_hat`, with the quantile `q` provided as an input to the network. \n",
       "HuberIQLoss measures the deviation of a huberized quantile forecast.\n",
       "By weighting the absolute deviation in a non symmetric way, the\n",
       "loss pays more attention to under or over estimation.\n",
       "\n",
       "$$ \\mathrm{HuberQL}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}^{(q)}_{\\tau}) = \n",
       "(1-q)\\, L_{\\delta}(y_{\\tau},\\; \\hat{y}^{(q)}_{\\tau}) \\mathbb{1}\\{ \\hat{y}^{(q)}_{\\tau} \\geq y_{\\tau} \\} + \n",
       "q\\, L_{\\delta}(y_{\\tau},\\; \\hat{y}^{(q)}_{\\tau}) \\mathbb{1}\\{ \\hat{y}^{(q)}_{\\tau} < y_{\\tau} \\} $$\n",
       "\n",
       "**Parameters:**<br>\n",
       "`quantile_sampling`: str, default='uniform', sampling distribution used to sample the quantiles during training. Choose from ['uniform', 'beta']. <br>\n",
       "`horizon_weight`: Tensor of size h, weight for each timestamp of the forecasting window. <br>\n",
       "`delta`: float=1.0, Specifies the threshold at which to change between delta-scaled L1 and L2 loss.<br>\n",
       "\n",
       "**References:**<br>\n",
       "[Gouttes, Adèle, Kashif Rasul, Mateusz Koren, Johannes Stephan, and Tofigh Naghibi, \"Probabilistic Time Series Forecasting with Implicit Quantile Networks\".](http://arxiv.org/abs/2107.03743)\n",
       "[Huber Peter, J (1964). \"Robust Estimation of a Location Parameter\". Annals of Statistics](https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-35/issue-1/Robust-Estimation-of-a-Location-Parameter/10.1214/aoms/1177703732.full)<br>\n",
       "[Roger Koenker and Gilbert Bassett, Jr., \"Regression Quantiles\".](https://www.jstor.org/stable/1913643)*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### HuberIQLoss.__init__\n",
       "\n",
       ">      HuberIQLoss.__init__ (cos_embedding_dim=64, concentration0=1.0,\n",
       ">                            concentration1=1.0, delta=1.0, horizon_weight=None)\n",
       "\n",
       "*Implicit Huber Quantile Loss\n",
       "\n",
       "Computes the huberized quantile loss between `y` and `y_hat`, with the quantile `q` provided as an input to the network. \n",
       "HuberIQLoss measures the deviation of a huberized quantile forecast.\n",
       "By weighting the absolute deviation in a non symmetric way, the\n",
       "loss pays more attention to under or over estimation.\n",
       "\n",
       "$$ \\mathrm{HuberQL}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}^{(q)}_{\\tau}) = \n",
       "(1-q)\\, L_{\\delta}(y_{\\tau},\\; \\hat{y}^{(q)}_{\\tau}) \\mathbb{1}\\{ \\hat{y}^{(q)}_{\\tau} \\geq y_{\\tau} \\} + \n",
       "q\\, L_{\\delta}(y_{\\tau},\\; \\hat{y}^{(q)}_{\\tau}) \\mathbb{1}\\{ \\hat{y}^{(q)}_{\\tau} < y_{\\tau} \\} $$\n",
       "\n",
       "**Parameters:**<br>\n",
       "`quantile_sampling`: str, default='uniform', sampling distribution used to sample the quantiles during training. Choose from ['uniform', 'beta']. <br>\n",
       "`horizon_weight`: Tensor of size h, weight for each timestamp of the forecasting window. <br>\n",
       "`delta`: float=1.0, Specifies the threshold at which to change between delta-scaled L1 and L2 loss.<br>\n",
       "\n",
       "**References:**<br>\n",
       "[Gouttes, Adèle, Kashif Rasul, Mateusz Koren, Johannes Stephan, and Tofigh Naghibi, \"Probabilistic Time Series Forecasting with Implicit Quantile Networks\".](http://arxiv.org/abs/2107.03743)\n",
       "[Huber Peter, J (1964). \"Robust Estimation of a Location Parameter\". Annals of Statistics](https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-35/issue-1/Robust-Estimation-of-a-Location-Parameter/10.1214/aoms/1177703732.full)<br>\n",
       "[Roger Koenker and Gilbert Bassett, Jr., \"Regression Quantiles\".](https://www.jstor.org/stable/1913643)*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(HuberIQLoss, name='HuberIQLoss.__init__', title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a84e21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/losses/pytorch.py#L2843){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### HuberIQLoss.__call__\n",
       "\n",
       ">      HuberIQLoss.__call__ (y:torch.Tensor, y_hat:torch.Tensor,\n",
       ">                            y_insample:torch.Tensor,\n",
       ">                            mask:Optional[torch.Tensor]=None)\n",
       "\n",
       "***Parameters:**<br>\n",
       "`y`: tensor, Actual values.<br>\n",
       "`y_hat`: tensor, Predicted values.<br>\n",
       "`mask`: tensor, Specifies datapoints to consider in loss.<br>\n",
       "\n",
       "**Returns:**<br>\n",
       "`huber_qloss`: tensor (single value).*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/losses/pytorch.py#L2843){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### HuberIQLoss.__call__\n",
       "\n",
       ">      HuberIQLoss.__call__ (y:torch.Tensor, y_hat:torch.Tensor,\n",
       ">                            y_insample:torch.Tensor,\n",
       ">                            mask:Optional[torch.Tensor]=None)\n",
       "\n",
       "***Parameters:**<br>\n",
       "`y`: tensor, Actual values.<br>\n",
       "`y_hat`: tensor, Predicted values.<br>\n",
       "`mask`: tensor, Specifies datapoints to consider in loss.<br>\n",
       "\n",
       "**Returns:**<br>\n",
       "`huber_qloss`: tensor (single value).*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(HuberIQLoss.__call__, name='HuberIQLoss.__call__', title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4a68dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "# Unit tests\n",
    "# Check that default quantile is set to 0.5 at initialization\n",
    "check = HuberIQLoss()\n",
    "test_eq(check.q, 0.5)\n",
    "\n",
    "# Check that quantiles are correctly updated - prediction\n",
    "check = HuberIQLoss()\n",
    "check.update_quantile([0.7])\n",
    "test_eq(check.q, 0.7)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eb99f88b",
   "metadata": {},
   "source": [
    "# 6. Others"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "013d1502",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fda0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Accuracy(BasePointLoss):\n",
    "    \"\"\" Accuracy\n",
    "\n",
    "    Computes the accuracy between categorical `y` and `y_hat`.\n",
    "    This evaluation metric is only meant for evalution, as it\n",
    "    is not differentiable.\n",
    "\n",
    "    $$ \\mathrm{Accuracy}(\\\\mathbf{y}_{\\\\tau}, \\\\mathbf{\\hat{y}}_{\\\\tau}) = \\\\frac{1}{H} \\\\sum^{t+H}_{\\\\tau=t+1} \\mathrm{1}\\{\\\\mathbf{y}_{\\\\tau}==\\\\mathbf{\\hat{y}}_{\\\\tau}\\} $$\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self,):\n",
    "        super(Accuracy, self).__init__()\n",
    "        self.is_distribution_output = False\n",
    "        self.outputsize_multiplier = 1\n",
    "\n",
    "    def domain_map(self, y_hat: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "        Univariate: [B, H, 1]\n",
    "        Multivariate: [B, H, N]\n",
    "\n",
    "        Output: [B, H, N]\n",
    "        \"\"\"\n",
    "\n",
    "        return y_hat\n",
    "    \n",
    "    def __call__(self,\n",
    "                 y: torch.Tensor,\n",
    "                 y_hat: torch.Tensor,\n",
    "                 y_insample: torch.Tensor,\n",
    "                 mask: Union[torch.Tensor, None] = None,\n",
    "                 ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        **Parameters:**<br>\n",
    "        `y`: tensor, Actual values.<br>\n",
    "        `y_hat`: tensor, Predicted values.<br>\n",
    "        `mask`: tensor, Specifies date stamps per serie to consider in loss.<br>\n",
    "\n",
    "        **Returns:**<br>\n",
    "        `accuracy`: tensor (single value).\n",
    "        \"\"\"\n",
    "\n",
    "        if mask is None:\n",
    "            mask = torch.ones_like(y_hat)\n",
    "\n",
    "        measure = (y == y_hat) * mask\n",
    "        accuracy = torch.mean(measure)\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eeb2d06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/losses/pytorch.py#L2986){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Accuracy.__init__\n",
       "\n",
       ">      Accuracy.__init__ ()\n",
       "\n",
       "*Accuracy\n",
       "\n",
       "Computes the accuracy between categorical `y` and `y_hat`.\n",
       "This evaluation metric is only meant for evalution, as it\n",
       "is not differentiable.\n",
       "\n",
       "$$ \\mathrm{Accuracy}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}_{\\tau}) = \\frac{1}{H} \\sum^{t+H}_{\\tau=t+1} \\mathrm{1}\\{\\mathbf{y}_{\\tau}==\\mathbf{\\hat{y}}_{\\tau}\\} $$*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/losses/pytorch.py#L2986){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Accuracy.__init__\n",
       "\n",
       ">      Accuracy.__init__ ()\n",
       "\n",
       "*Accuracy\n",
       "\n",
       "Computes the accuracy between categorical `y` and `y_hat`.\n",
       "This evaluation metric is only meant for evalution, as it\n",
       "is not differentiable.\n",
       "\n",
       "$$ \\mathrm{Accuracy}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}_{\\tau}) = \\frac{1}{H} \\sum^{t+H}_{\\tau=t+1} \\mathrm{1}\\{\\mathbf{y}_{\\tau}==\\mathbf{\\hat{y}}_{\\tau}\\} $$*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(Accuracy, name='Accuracy.__init__', title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2111646c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/losses/pytorch.py#L3010){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Accuracy.__call__\n",
       "\n",
       ">      Accuracy.__call__ (y:torch.Tensor, y_hat:torch.Tensor,\n",
       ">                         y_insample:torch.Tensor,\n",
       ">                         mask:Optional[torch.Tensor]=None)\n",
       "\n",
       "***Parameters:**<br>\n",
       "`y`: tensor, Actual values.<br>\n",
       "`y_hat`: tensor, Predicted values.<br>\n",
       "`mask`: tensor, Specifies date stamps per serie to consider in loss.<br>\n",
       "\n",
       "**Returns:**<br>\n",
       "`accuracy`: tensor (single value).*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/losses/pytorch.py#L3010){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Accuracy.__call__\n",
       "\n",
       ">      Accuracy.__call__ (y:torch.Tensor, y_hat:torch.Tensor,\n",
       ">                         y_insample:torch.Tensor,\n",
       ">                         mask:Optional[torch.Tensor]=None)\n",
       "\n",
       "***Parameters:**<br>\n",
       "`y`: tensor, Actual values.<br>\n",
       "`y_hat`: tensor, Predicted values.<br>\n",
       "`mask`: tensor, Specifies date stamps per serie to consider in loss.<br>\n",
       "\n",
       "**Returns:**<br>\n",
       "`accuracy`: tensor (single value).*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(Accuracy.__call__, name='Accuracy.__call__', title_level=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3742e6be",
   "metadata": {},
   "source": [
    "## Scaled Continuous Ranked Probability Score (sCRPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d210a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class sCRPS(BasePointLoss):\n",
    "    \"\"\"Scaled Continues Ranked Probability Score\n",
    "\n",
    "    Calculates a scaled variation of the CRPS, as proposed by Rangapuram (2021),\n",
    "    to measure the accuracy of predicted quantiles `y_hat` compared to the observation `y`.\n",
    "\n",
    "    This metric averages percentual weighted absolute deviations as \n",
    "    defined by the quantile losses.\n",
    "\n",
    "    $$ \\mathrm{sCRPS}(\\\\mathbf{\\hat{y}}^{(q)}_{\\\\tau}, \\mathbf{y}_{\\\\tau}) = \\\\frac{2}{N} \\sum_{i}\n",
    "    \\int^{1}_{0}\n",
    "    \\\\frac{\\mathrm{QL}(\\\\mathbf{\\hat{y}}^{(q}_{\\\\tau} y_{i,\\\\tau})_{q}}{\\sum_{i} | y_{i,\\\\tau} |} dq $$\n",
    "\n",
    "    where $\\\\mathbf{\\hat{y}}^{(q}_{\\\\tau}$ is the estimated quantile, and $y_{i,\\\\tau}$\n",
    "    are the target variable realizations.\n",
    "\n",
    "    **Parameters:**<br>\n",
    "    `level`: int list [0,100]. Probability levels for prediction intervals (Defaults median).\n",
    "    `quantiles`: float list [0., 1.]. Alternative to level, quantiles to estimate from y distribution.\n",
    "\n",
    "    **References:**<br>\n",
    "    - [Gneiting, Tilmann. (2011). \\\"Quantiles as optimal point forecasts\\\". \n",
    "    International Journal of Forecasting.](https://www.sciencedirect.com/science/article/pii/S0169207010000063)<br>\n",
    "    - [Spyros Makridakis, Evangelos Spiliotis, Vassilios Assimakopoulos, Zhi Chen, Anil Gaba, Ilia Tsetlin, Robert L. Winkler. (2022). \n",
    "    \\\"The M5 uncertainty competition: Results, findings and conclusions\\\". \n",
    "    International Journal of Forecasting.](https://www.sciencedirect.com/science/article/pii/S0169207021001722)<br>\n",
    "    - [Syama Sundar Rangapuram, Lucien D Werner, Konstantinos Benidis, Pedro Mercado, Jan Gasthaus, Tim Januschowski. (2021). \n",
    "    \\\"End-to-End Learning of Coherent Probabilistic Forecasts for Hierarchical Time Series\\\". \n",
    "    Proceedings of the 38th International Conference on Machine Learning (ICML).](https://proceedings.mlr.press/v139/rangapuram21a.html)\n",
    "    \"\"\"\n",
    "    def __init__(self, level=[80, 90], quantiles=None):\n",
    "        super(sCRPS, self).__init__()\n",
    "        self.mql = MQLoss(level=level, quantiles=quantiles)\n",
    "        self.is_distribution_output = False\n",
    "    \n",
    "    def __call__(self,\n",
    "                 y: torch.Tensor,\n",
    "                 y_hat: torch.Tensor,\n",
    "                 y_insample: torch.Tensor,\n",
    "                 mask: Union[torch.Tensor, None] = None,\n",
    "                 ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        **Parameters:**<br>\n",
    "        `y`: tensor, Actual values.<br>\n",
    "        `y_hat`: tensor, Predicted values.<br>\n",
    "        `mask`: tensor, Specifies date stamps per series to consider in loss.<br>\n",
    "\n",
    "        **Returns:**<br>\n",
    "        `scrps`: tensor (single value).\n",
    "        \"\"\"\n",
    "        mql = self.mql(y=y, y_hat=y_hat, mask=mask, y_insample=y_insample)\n",
    "        norm = torch.sum(torch.abs(y))\n",
    "        unmean = torch.sum(mask)\n",
    "        scrps = 2 * mql * unmean / (norm + 1e-5)\n",
    "        return scrps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53770648",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/losses/pytorch.py#L3033){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### sCRPS.__init__\n",
       "\n",
       ">      sCRPS.__init__ (level=[80, 90], quantiles=None)\n",
       "\n",
       "*Scaled Continues Ranked Probability Score\n",
       "\n",
       "Calculates a scaled variation of the CRPS, as proposed by Rangapuram (2021),\n",
       "to measure the accuracy of predicted quantiles `y_hat` compared to the observation `y`.\n",
       "\n",
       "This metric averages percentual weighted absolute deviations as \n",
       "defined by the quantile losses.\n",
       "\n",
       "$$ \\mathrm{sCRPS}(\\mathbf{\\hat{y}}^{(q)}_{\\tau}, \\mathbf{y}_{\\tau}) = \\frac{2}{N} \\sum_{i}\n",
       "\\int^{1}_{0}\n",
       "\\frac{\\mathrm{QL}(\\mathbf{\\hat{y}}^{(q}_{\\tau} y_{i,\\tau})_{q}}{\\sum_{i} | y_{i,\\tau} |} dq $$\n",
       "\n",
       "where $\\mathbf{\\hat{y}}^{(q}_{\\tau}$ is the estimated quantile, and $y_{i,\\tau}$\n",
       "are the target variable realizations.\n",
       "\n",
       "**Parameters:**<br>\n",
       "`level`: int list [0,100]. Probability levels for prediction intervals (Defaults median).\n",
       "`quantiles`: float list [0., 1.]. Alternative to level, quantiles to estimate from y distribution.\n",
       "\n",
       "**References:**<br>\n",
       "- [Gneiting, Tilmann. (2011). \"Quantiles as optimal point forecasts\". \n",
       "International Journal of Forecasting.](https://www.sciencedirect.com/science/article/pii/S0169207010000063)<br>\n",
       "- [Spyros Makridakis, Evangelos Spiliotis, Vassilios Assimakopoulos, Zhi Chen, Anil Gaba, Ilia Tsetlin, Robert L. Winkler. (2022). \n",
       "\"The M5 uncertainty competition: Results, findings and conclusions\". \n",
       "International Journal of Forecasting.](https://www.sciencedirect.com/science/article/pii/S0169207021001722)<br>\n",
       "- [Syama Sundar Rangapuram, Lucien D Werner, Konstantinos Benidis, Pedro Mercado, Jan Gasthaus, Tim Januschowski. (2021). \n",
       "\"End-to-End Learning of Coherent Probabilistic Forecasts for Hierarchical Time Series\". \n",
       "Proceedings of the 38th International Conference on Machine Learning (ICML).](https://proceedings.mlr.press/v139/rangapuram21a.html)*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/losses/pytorch.py#L3033){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### sCRPS.__init__\n",
       "\n",
       ">      sCRPS.__init__ (level=[80, 90], quantiles=None)\n",
       "\n",
       "*Scaled Continues Ranked Probability Score\n",
       "\n",
       "Calculates a scaled variation of the CRPS, as proposed by Rangapuram (2021),\n",
       "to measure the accuracy of predicted quantiles `y_hat` compared to the observation `y`.\n",
       "\n",
       "This metric averages percentual weighted absolute deviations as \n",
       "defined by the quantile losses.\n",
       "\n",
       "$$ \\mathrm{sCRPS}(\\mathbf{\\hat{y}}^{(q)}_{\\tau}, \\mathbf{y}_{\\tau}) = \\frac{2}{N} \\sum_{i}\n",
       "\\int^{1}_{0}\n",
       "\\frac{\\mathrm{QL}(\\mathbf{\\hat{y}}^{(q}_{\\tau} y_{i,\\tau})_{q}}{\\sum_{i} | y_{i,\\tau} |} dq $$\n",
       "\n",
       "where $\\mathbf{\\hat{y}}^{(q}_{\\tau}$ is the estimated quantile, and $y_{i,\\tau}$\n",
       "are the target variable realizations.\n",
       "\n",
       "**Parameters:**<br>\n",
       "`level`: int list [0,100]. Probability levels for prediction intervals (Defaults median).\n",
       "`quantiles`: float list [0., 1.]. Alternative to level, quantiles to estimate from y distribution.\n",
       "\n",
       "**References:**<br>\n",
       "- [Gneiting, Tilmann. (2011). \"Quantiles as optimal point forecasts\". \n",
       "International Journal of Forecasting.](https://www.sciencedirect.com/science/article/pii/S0169207010000063)<br>\n",
       "- [Spyros Makridakis, Evangelos Spiliotis, Vassilios Assimakopoulos, Zhi Chen, Anil Gaba, Ilia Tsetlin, Robert L. Winkler. (2022). \n",
       "\"The M5 uncertainty competition: Results, findings and conclusions\". \n",
       "International Journal of Forecasting.](https://www.sciencedirect.com/science/article/pii/S0169207021001722)<br>\n",
       "- [Syama Sundar Rangapuram, Lucien D Werner, Konstantinos Benidis, Pedro Mercado, Jan Gasthaus, Tim Januschowski. (2021). \n",
       "\"End-to-End Learning of Coherent Probabilistic Forecasts for Hierarchical Time Series\". \n",
       "Proceedings of the 38th International Conference on Machine Learning (ICML).](https://proceedings.mlr.press/v139/rangapuram21a.html)*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(sCRPS, name='sCRPS.__init__', title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3646250f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/losses/pytorch.py#L3069){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### sCRPS.__call__\n",
       "\n",
       ">      sCRPS.__call__ (y:torch.Tensor, y_hat:torch.Tensor,\n",
       ">                      y_insample:torch.Tensor,\n",
       ">                      mask:Optional[torch.Tensor]=None)\n",
       "\n",
       "***Parameters:**<br>\n",
       "`y`: tensor, Actual values.<br>\n",
       "`y_hat`: tensor, Predicted values.<br>\n",
       "`mask`: tensor, Specifies date stamps per series to consider in loss.<br>\n",
       "\n",
       "**Returns:**<br>\n",
       "`scrps`: tensor (single value).*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/losses/pytorch.py#L3069){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### sCRPS.__call__\n",
       "\n",
       ">      sCRPS.__call__ (y:torch.Tensor, y_hat:torch.Tensor,\n",
       ">                      y_insample:torch.Tensor,\n",
       ">                      mask:Optional[torch.Tensor]=None)\n",
       "\n",
       "***Parameters:**<br>\n",
       "`y`: tensor, Actual values.<br>\n",
       "`y_hat`: tensor, Predicted values.<br>\n",
       "`mask`: tensor, Specifies date stamps per series to consider in loss.<br>\n",
       "\n",
       "**Returns:**<br>\n",
       "`scrps`: tensor (single value).*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(sCRPS.__call__, name='sCRPS.__call__', title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdfa174",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# Each 1 is an error, there are 6 datapoints.\n",
    "y = torch.Tensor([[0,0,0],[0,0,0]]).unsqueeze(-1)\n",
    "y_hat = torch.Tensor([[0,0,1],[1,0,1]]).unsqueeze(-1)\n",
    "\n",
    "# Complete mask and horizon_weight\n",
    "mask = torch.Tensor([[1,1,1],[1,1,1]]).unsqueeze(-1)\n",
    "horizon_weight = torch.Tensor([1,1,1])\n",
    "\n",
    "mae = MAE(horizon_weight=horizon_weight)\n",
    "loss = mae(y=y, y_hat=y_hat, mask=mask)\n",
    "assert loss==(3/6), 'Should be 3/6'\n",
    "\n",
    "# Incomplete mask and complete horizon_weight\n",
    "mask = torch.Tensor([[1,1,1],[0,1,1]]).unsqueeze(-1) # Only 1 error and points is masked.\n",
    "horizon_weight = torch.Tensor([1,1,1])\n",
    "mae = MAE(horizon_weight=horizon_weight)\n",
    "loss = mae(y=y, y_hat=y_hat, mask=mask)\n",
    "assert loss==(2/5), 'Should be 2/5'\n",
    "\n",
    "# Complete mask and incomplete horizon_weight\n",
    "mask = torch.Tensor([[1,1,1],[1,1,1]]).unsqueeze(-1)\n",
    "horizon_weight = torch.Tensor([1,1,0]) # 2 errors and points are masked.\n",
    "mae = MAE(horizon_weight=horizon_weight)\n",
    "loss = mae(y=y, y_hat=y_hat, mask=mask)\n",
    "assert loss==(1/4), 'Should be 1/4'\n",
    "\n",
    "# Incomplete mask and incomplete horizon_weight\n",
    "mask = torch.Tensor([[0,1,1],[1,1,1]]).unsqueeze(-1)\n",
    "horizon_weight = torch.Tensor([1,1,0]) # 2 errors are masked, and 3 points.\n",
    "mae = MAE(horizon_weight=horizon_weight)\n",
    "loss = mae(y=y, y_hat=y_hat, mask=mask)\n",
    "assert loss==(1/3), 'Should be 1/3'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
