{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14ce00e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57869174",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1167bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from os import cpu_count\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from ray import tune\n",
    "from ray.tune.search.basic_variant import BasicVariantGenerator\n",
    "\n",
    "from neuralforecast.common._base_auto import BaseAuto\n",
    "\n",
    "from neuralforecast.models.rnn import RNN\n",
    "from neuralforecast.models.gru import GRU\n",
    "from neuralforecast.models.tcn import TCN\n",
    "from neuralforecast.models.lstm import LSTM\n",
    "from neuralforecast.models.dilated_rnn import DilatedRNN\n",
    "\n",
    "from neuralforecast.models.mlp import MLP\n",
    "from neuralforecast.models.nbeats import NBEATS\n",
    "from neuralforecast.models.nbeatsx import NBEATSx\n",
    "from neuralforecast.models.nhits import NHITS\n",
    "\n",
    "from neuralforecast.models.tft import TFT\n",
    "from neuralforecast.models.vanillatransformer import VanillaTransformer\n",
    "from neuralforecast.models.informer import Informer\n",
    "from neuralforecast.models.autoformer import Autoformer\n",
    "from neuralforecast.models.patchtst import PatchTST\n",
    "\n",
    "from neuralforecast.models.stemgnn import StemGNN\n",
    "from neuralforecast.models.hint import HINT\n",
    "\n",
    "from neuralforecast.losses.pytorch import MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "495dd890",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from fastcore.test import test_eq\n",
    "from nbdev.showdoc import show_doc\n",
    "\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "from neuralforecast.losses.pytorch import MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "490b7416",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "logging.getLogger(\"pytorch_lightning\").setLevel(logging.ERROR)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "plt.rcParams[\"axes.grid\"]=True\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "plt.rcParams[\"figure.figsize\"] = (6,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae65ca7",
   "metadata": {},
   "source": [
    "# <span style=\"color:DarkOrange\"> Models </span>\n",
    "\n",
    "> NeuralForecast contains user-friendly implementations of neural forecasting models that allow for easy transition of computing capabilities (GPU/CPU), computation parallelization, and hyperparameter tuning.<br><br> All the NeuralForecast models are \"global\" because we train them with all the series from the input pd.DataFrame data `Y_df`, yet the optimization objective is, momentarily, \"univariate\" as it does not consider the interaction between the output predictions across time series. Like the StatsForecast library, `core.NeuralForecast` allows you to explore collections of models efficiently and contains functions for convenient wrangling of input and output pd.DataFrames predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8183a9",
   "metadata": {},
   "source": [
    "# <span style=\"color:DarkBlue\"> 1. Automatic Forecasting </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858e6a1b",
   "metadata": {},
   "source": [
    "## <span style=\"color:DarkBlue\"> A. RNN-Based </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60458256",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class AutoRNN(BaseAuto):\n",
    "    \n",
    "    default_config = {\n",
    "        \"input_size_multiplier\": [-1, 4, 16, 64],\n",
    "        \"inference_input_size_multiplier\": [-1],\n",
    "        \"h\": None,\n",
    "        \"encoder_hidden_size\": tune.choice([50, 100, 200, 300]),\n",
    "        \"encoder_n_layers\": tune.randint(1, 4),\n",
    "        \"context_size\": tune.choice([5, 10, 50]),\n",
    "        \"decoder_hidden_size\": tune.choice([64, 128, 256, 512]),\n",
    "        \"learning_rate\": tune.loguniform(1e-4, 1e-1),\n",
    "        \"max_steps\": tune.choice([500, 1000]),\n",
    "        \"batch_size\": tune.choice([16, 32]),\n",
    "        \"loss\": None,\n",
    "        \"random_seed\": tune.randint(1, 20)\n",
    "    }\n",
    "\n",
    "    def __init__(self,\n",
    "                 h,\n",
    "                 loss=MAE(),\n",
    "                 valid_loss=None,\n",
    "                 config=None, \n",
    "                 search_alg=BasicVariantGenerator(random_state=1),\n",
    "                 num_samples=10,\n",
    "                 refit_with_val=False,\n",
    "                 cpus=cpu_count(),\n",
    "                 gpus=torch.cuda.device_count(),\n",
    "                 verbose=False):\n",
    "        \"\"\" Auto RNN\n",
    "        \n",
    "        **Parameters:**<br>\n",
    "        \n",
    "        \"\"\"\n",
    "        # Define search space, input/output sizes\n",
    "        if config is None:\n",
    "            config = self.default_config.copy()        \n",
    "            config['input_size'] = tune.choice([h*x \\\n",
    "                         for x in self.default_config[\"input_size_multiplier\"]])\n",
    "            config['inference_input_size'] = tune.choice([h*x \\\n",
    "                         for x in self.default_config[\"inference_input_size_multiplier\"]])\n",
    "            del config[\"input_size_multiplier\"]\n",
    "\n",
    "        super(AutoRNN, self).__init__(\n",
    "              cls_model=RNN, \n",
    "              h=h,\n",
    "              loss=loss,\n",
    "              valid_loss=valid_loss,\n",
    "              config=config, \n",
    "              search_alg=search_alg,\n",
    "              num_samples=num_samples, \n",
    "              refit_with_val=refit_with_val,\n",
    "              cpus=cpus,\n",
    "              gpus=gpus,\n",
    "              verbose=verbose\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "142f7028",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/auto.py#L41){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### AutoRNN\n",
       "\n",
       ">      AutoRNN (h, loss=MAE(), valid_loss=None, config=None,\n",
       ">               search_alg=<ray.tune.search.basic_variant.BasicVariantGenerator\n",
       ">               object at 0x7f994033c6a0>, num_samples=10, refit_with_val=False,\n",
       ">               cpus=4, gpus=0, verbose=False)\n",
       "\n",
       "Class for Automatic Hyperparameter Optimization, it builds on top of `ray` to\n",
       "give access to a wide variety of hyperparameter optimization tools ranging\n",
       "from classic grid search, to Bayesian optimization and HyperBand algorithm.\n",
       "\n",
       "The validation loss to be optimized is defined by the `config['loss']` dictionary\n",
       "value, the config also contains the rest of the hyperparameter search space.\n",
       "\n",
       "It is important to note that the success of this hyperparameter optimization\n",
       "heavily relies on a strong correlation between the validation and test periods.\n",
       "\n",
       "**Parameters:**<br>\n",
       "`cls_model`: PyTorch/PyTorchLightning model, see `neuralforecast.models` [collection here](https://nixtla.github.io/neuralforecast/models.html).<br>\n",
       "`h`: int, forecast horizon.<br>\n",
       "`loss`: PyTorch module, instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
       "`valid_loss`: PyTorch module=`loss`, instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
       "`config`: dict, dictionary with ray.tune defined search space.<br>\n",
       "`search_alg`: ray.tune.search variant, BasicVariantGenerator, HyperOptSearch, DragonflySearch, TuneBOHB for details\n",
       "    see [tune.search](https://docs.ray.io/en/latest/tune/api_docs/suggestion.html#).<br>\n",
       "`num_samples`: int, number of hyperparameter optimization steps/samples.<br>\n",
       "`cpus`: int, number of cpus to use during optimization, default all available.<br>\n",
       "`gpus`: int, number of gpus to use during optimization, default all available.<br>\n",
       "`refit_wo_val`: bool, number of gpus to use during optimization, default all available.<br>\n",
       "`verbose`: bool, wether print partial outputs.<br>\n",
       "`alias`: str, optional,  Custom name of the model.<br>"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/auto.py#L41){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### AutoRNN\n",
       "\n",
       ">      AutoRNN (h, loss=MAE(), valid_loss=None, config=None,\n",
       ">               search_alg=<ray.tune.search.basic_variant.BasicVariantGenerator\n",
       ">               object at 0x7f994033c6a0>, num_samples=10, refit_with_val=False,\n",
       ">               cpus=4, gpus=0, verbose=False)\n",
       "\n",
       "Class for Automatic Hyperparameter Optimization, it builds on top of `ray` to\n",
       "give access to a wide variety of hyperparameter optimization tools ranging\n",
       "from classic grid search, to Bayesian optimization and HyperBand algorithm.\n",
       "\n",
       "The validation loss to be optimized is defined by the `config['loss']` dictionary\n",
       "value, the config also contains the rest of the hyperparameter search space.\n",
       "\n",
       "It is important to note that the success of this hyperparameter optimization\n",
       "heavily relies on a strong correlation between the validation and test periods.\n",
       "\n",
       "**Parameters:**<br>\n",
       "`cls_model`: PyTorch/PyTorchLightning model, see `neuralforecast.models` [collection here](https://nixtla.github.io/neuralforecast/models.html).<br>\n",
       "`h`: int, forecast horizon.<br>\n",
       "`loss`: PyTorch module, instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
       "`valid_loss`: PyTorch module=`loss`, instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
       "`config`: dict, dictionary with ray.tune defined search space.<br>\n",
       "`search_alg`: ray.tune.search variant, BasicVariantGenerator, HyperOptSearch, DragonflySearch, TuneBOHB for details\n",
       "    see [tune.search](https://docs.ray.io/en/latest/tune/api_docs/suggestion.html#).<br>\n",
       "`num_samples`: int, number of hyperparameter optimization steps/samples.<br>\n",
       "`cpus`: int, number of cpus to use during optimization, default all available.<br>\n",
       "`gpus`: int, number of gpus to use during optimization, default all available.<br>\n",
       "`refit_wo_val`: bool, number of gpus to use during optimization, default all available.<br>\n",
       "`verbose`: bool, wether print partial outputs.<br>\n",
       "`alias`: str, optional,  Custom name of the model.<br>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(AutoRNN, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b53800a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from neuralforecast.tsdataset import TimeSeriesDataset\n",
    "from neuralforecast.utils import AirPassengersDF as Y_df\n",
    "\n",
    "# Split train/test and declare time series dataset\n",
    "Y_train_df = Y_df[Y_df.ds<='1959-12-31'] # 132 train\n",
    "Y_test_df = Y_df[Y_df.ds>'1959-12-31']   # 12 test\n",
    "dataset, *_ = TimeSeriesDataset.from_df(Y_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce08be9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 1\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "# Use your own config or AutoRNN.default_config\n",
    "config = dict(max_steps=2, val_check_steps=1, input_size=-1, encoder_hidden_size=8)\n",
    "model = AutoRNN(h=12, config=config, num_samples=1, cpus=1)\n",
    "\n",
    "model.fit(dataset=dataset)\n",
    "y_hat = model.predict(dataset=dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a274792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting predictions\n",
    "# Y_plot_df = Y_test_df[['unique_id', 'ds', 'y']].copy()\n",
    "# Y_plot_df['AutoRNN'] = y_hat\n",
    "\n",
    "# pd.concat([Y_train_df, Y_plot_df]).drop('unique_id', axis=1).set_index('ds').plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "821e7999",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class AutoLSTM(BaseAuto):\n",
    "\n",
    "    default_config = {\n",
    "        \"input_size_multiplier\": [-1, 4, 16, 64],\n",
    "        \"inference_input_size_multiplier\": [-1],\n",
    "        \"h\": None,\n",
    "        \"encoder_hidden_size\": tune.choice([50, 100, 200, 300]),\n",
    "        \"encoder_n_layers\": tune.randint(1, 4),\n",
    "        \"context_size\": tune.choice([5, 10, 50]),\n",
    "        \"decoder_hidden_size\": tune.choice([64, 128, 256, 512]),\n",
    "        \"learning_rate\": tune.loguniform(1e-4, 1e-1),\n",
    "        \"max_steps\": tune.choice([500, 1000]),\n",
    "        \"batch_size\": tune.choice([16, 32]),\n",
    "        \"loss\": None,\n",
    "        \"random_seed\": tune.randint(1, 20)\n",
    "    }\n",
    "\n",
    "    def __init__(self,\n",
    "                 h,\n",
    "                 loss=MAE(),\n",
    "                 valid_loss=None,\n",
    "                 config=None,\n",
    "                 search_alg=BasicVariantGenerator(random_state=1),\n",
    "                 num_samples=10,\n",
    "                 refit_with_val=False,\n",
    "                 cpus=cpu_count(),\n",
    "                 gpus=torch.cuda.device_count(),\n",
    "                 verbose=False):\n",
    "\n",
    "        # Define search space, input/output sizes\n",
    "        if config is None:\n",
    "            config = self.default_config.copy()        \n",
    "            config['input_size'] = tune.choice([h*x \\\n",
    "                         for x in self.default_config[\"input_size_multiplier\"]])\n",
    "            config['inference_input_size'] = tune.choice([h*x \\\n",
    "                         for x in self.default_config[\"inference_input_size_multiplier\"]])\n",
    "            del config[\"input_size_multiplier\"]\n",
    "\n",
    "        super(AutoLSTM, self).__init__(\n",
    "              cls_model=LSTM,\n",
    "              h=h,\n",
    "              loss=loss,\n",
    "              valid_loss=valid_loss,\n",
    "              config=config,\n",
    "              search_alg=search_alg,\n",
    "              num_samples=num_samples,\n",
    "              refit_with_val=refit_with_val,\n",
    "              cpus=cpus,\n",
    "              gpus=gpus,\n",
    "              verbose=verbose\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1d3965f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/auto.py#L101){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### AutoLSTM\n",
       "\n",
       ">      AutoLSTM (h, loss=MAE(), valid_loss=None, config=None,\n",
       ">                search_alg=<ray.tune.search.basic_variant.BasicVariantGenerator\n",
       ">                object at 0x7f994142c6d0>, num_samples=10,\n",
       ">                refit_with_val=False, cpus=4, gpus=0, verbose=False)\n",
       "\n",
       "Class for Automatic Hyperparameter Optimization, it builds on top of `ray` to\n",
       "give access to a wide variety of hyperparameter optimization tools ranging\n",
       "from classic grid search, to Bayesian optimization and HyperBand algorithm.\n",
       "\n",
       "The validation loss to be optimized is defined by the `config['loss']` dictionary\n",
       "value, the config also contains the rest of the hyperparameter search space.\n",
       "\n",
       "It is important to note that the success of this hyperparameter optimization\n",
       "heavily relies on a strong correlation between the validation and test periods.\n",
       "\n",
       "**Parameters:**<br>\n",
       "`cls_model`: PyTorch/PyTorchLightning model, see `neuralforecast.models` [collection here](https://nixtla.github.io/neuralforecast/models.html).<br>\n",
       "`h`: int, forecast horizon.<br>\n",
       "`loss`: PyTorch module, instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
       "`valid_loss`: PyTorch module=`loss`, instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
       "`config`: dict, dictionary with ray.tune defined search space.<br>\n",
       "`search_alg`: ray.tune.search variant, BasicVariantGenerator, HyperOptSearch, DragonflySearch, TuneBOHB for details\n",
       "    see [tune.search](https://docs.ray.io/en/latest/tune/api_docs/suggestion.html#).<br>\n",
       "`num_samples`: int, number of hyperparameter optimization steps/samples.<br>\n",
       "`cpus`: int, number of cpus to use during optimization, default all available.<br>\n",
       "`gpus`: int, number of gpus to use during optimization, default all available.<br>\n",
       "`refit_wo_val`: bool, number of gpus to use during optimization, default all available.<br>\n",
       "`verbose`: bool, wether print partial outputs.<br>\n",
       "`alias`: str, optional,  Custom name of the model.<br>"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/auto.py#L101){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### AutoLSTM\n",
       "\n",
       ">      AutoLSTM (h, loss=MAE(), valid_loss=None, config=None,\n",
       ">                search_alg=<ray.tune.search.basic_variant.BasicVariantGenerator\n",
       ">                object at 0x7f994142c6d0>, num_samples=10,\n",
       ">                refit_with_val=False, cpus=4, gpus=0, verbose=False)\n",
       "\n",
       "Class for Automatic Hyperparameter Optimization, it builds on top of `ray` to\n",
       "give access to a wide variety of hyperparameter optimization tools ranging\n",
       "from classic grid search, to Bayesian optimization and HyperBand algorithm.\n",
       "\n",
       "The validation loss to be optimized is defined by the `config['loss']` dictionary\n",
       "value, the config also contains the rest of the hyperparameter search space.\n",
       "\n",
       "It is important to note that the success of this hyperparameter optimization\n",
       "heavily relies on a strong correlation between the validation and test periods.\n",
       "\n",
       "**Parameters:**<br>\n",
       "`cls_model`: PyTorch/PyTorchLightning model, see `neuralforecast.models` [collection here](https://nixtla.github.io/neuralforecast/models.html).<br>\n",
       "`h`: int, forecast horizon.<br>\n",
       "`loss`: PyTorch module, instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
       "`valid_loss`: PyTorch module=`loss`, instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
       "`config`: dict, dictionary with ray.tune defined search space.<br>\n",
       "`search_alg`: ray.tune.search variant, BasicVariantGenerator, HyperOptSearch, DragonflySearch, TuneBOHB for details\n",
       "    see [tune.search](https://docs.ray.io/en/latest/tune/api_docs/suggestion.html#).<br>\n",
       "`num_samples`: int, number of hyperparameter optimization steps/samples.<br>\n",
       "`cpus`: int, number of cpus to use during optimization, default all available.<br>\n",
       "`gpus`: int, number of gpus to use during optimization, default all available.<br>\n",
       "`refit_wo_val`: bool, number of gpus to use during optimization, default all available.<br>\n",
       "`verbose`: bool, wether print partial outputs.<br>\n",
       "`alias`: str, optional,  Custom name of the model.<br>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(AutoLSTM, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c68f5a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 1\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "# Use your own config or AutoLSTM.default_config\n",
    "config = dict(max_steps=2, val_check_steps=1, input_size=-1, encoder_hidden_size=8)\n",
    "model = AutoLSTM(h=12, config=config, num_samples=1, cpus=1)\n",
    "\n",
    "# Fit and predict\n",
    "model.fit(dataset=dataset)\n",
    "y_hat = model.predict(dataset=dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "901129ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# Y_plot_df = Y_test_df[['unique_id', 'ds', 'y']].copy()\n",
    "# Y_plot_df['AutoLSTM'] = y_hat\n",
    "\n",
    "# pd.concat([Y_train_df, Y_plot_df]).drop('unique_id', axis=1).set_index('ds').plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1dcaf5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class AutoGRU(BaseAuto):\n",
    "\n",
    "    default_config = {\n",
    "        \"input_size_multiplier\": [-1, 4, 16, 64],\n",
    "        \"inference_input_size_multiplier\": [-1],\n",
    "        \"h\": None,\n",
    "        \"encoder_hidden_size\": tune.choice([50, 100, 200, 300]),\n",
    "        \"encoder_n_layers\": tune.randint(1, 4),\n",
    "        \"context_size\": tune.choice([5, 10, 50]),\n",
    "        \"decoder_hidden_size\": tune.choice([64, 128, 256, 512]),\n",
    "        \"learning_rate\": tune.loguniform(1e-4, 1e-1),\n",
    "        \"max_steps\": tune.choice([500, 1000]),\n",
    "        \"batch_size\": tune.choice([16, 32]),\n",
    "        \"loss\": None,\n",
    "        \"random_seed\": tune.randint(1, 20)\n",
    "    }\n",
    "\n",
    "    def __init__(self,\n",
    "                 h,\n",
    "                 loss=MAE(),\n",
    "                 valid_loss=None,\n",
    "                 config=None,\n",
    "                 search_alg=BasicVariantGenerator(random_state=1),\n",
    "                 num_samples=10,\n",
    "                 refit_with_val=False,\n",
    "                 cpus=cpu_count(),\n",
    "                 gpus=torch.cuda.device_count(),\n",
    "                 verbose=False,\n",
    "                 alias=None):\n",
    "        \n",
    "        # Define search space, input/output sizes\n",
    "        if config is None:\n",
    "            config = self.default_config.copy()        \n",
    "            config['input_size'] = tune.choice([h*x \\\n",
    "                         for x in self.default_config[\"input_size_multiplier\"]])\n",
    "            config['inference_input_size'] = tune.choice([h*x \\\n",
    "                         for x in self.default_config[\"inference_input_size_multiplier\"]])\n",
    "            del config[\"input_size_multiplier\"]\n",
    "\n",
    "        super(AutoGRU, self).__init__(\n",
    "              cls_model=GRU,\n",
    "              h=h,\n",
    "              loss=loss,\n",
    "              valid_loss=valid_loss,\n",
    "              config=config, \n",
    "              search_alg=search_alg,\n",
    "              num_samples=num_samples,\n",
    "              refit_with_val=refit_with_val,\n",
    "              cpus=cpus,\n",
    "              gpus=gpus,\n",
    "              verbose=verbose,\n",
    "              alias=alias,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b078228a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/auto.py#L156){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### AutoGRU\n",
       "\n",
       ">      AutoGRU (h, loss=MAE(), valid_loss=None, config=None,\n",
       ">               search_alg=<ray.tune.search.basic_variant.BasicVariantGenerator\n",
       ">               object at 0x7f994148d2b0>, num_samples=10, refit_with_val=False,\n",
       ">               cpus=4, gpus=0, verbose=False, alias=None)\n",
       "\n",
       "Class for Automatic Hyperparameter Optimization, it builds on top of `ray` to\n",
       "give access to a wide variety of hyperparameter optimization tools ranging\n",
       "from classic grid search, to Bayesian optimization and HyperBand algorithm.\n",
       "\n",
       "The validation loss to be optimized is defined by the `config['loss']` dictionary\n",
       "value, the config also contains the rest of the hyperparameter search space.\n",
       "\n",
       "It is important to note that the success of this hyperparameter optimization\n",
       "heavily relies on a strong correlation between the validation and test periods.\n",
       "\n",
       "**Parameters:**<br>\n",
       "`cls_model`: PyTorch/PyTorchLightning model, see `neuralforecast.models` [collection here](https://nixtla.github.io/neuralforecast/models.html).<br>\n",
       "`h`: int, forecast horizon.<br>\n",
       "`loss`: PyTorch module, instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
       "`valid_loss`: PyTorch module=`loss`, instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
       "`config`: dict, dictionary with ray.tune defined search space.<br>\n",
       "`search_alg`: ray.tune.search variant, BasicVariantGenerator, HyperOptSearch, DragonflySearch, TuneBOHB for details\n",
       "    see [tune.search](https://docs.ray.io/en/latest/tune/api_docs/suggestion.html#).<br>\n",
       "`num_samples`: int, number of hyperparameter optimization steps/samples.<br>\n",
       "`cpus`: int, number of cpus to use during optimization, default all available.<br>\n",
       "`gpus`: int, number of gpus to use during optimization, default all available.<br>\n",
       "`refit_wo_val`: bool, number of gpus to use during optimization, default all available.<br>\n",
       "`verbose`: bool, wether print partial outputs.<br>\n",
       "`alias`: str, optional,  Custom name of the model.<br>"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/auto.py#L156){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### AutoGRU\n",
       "\n",
       ">      AutoGRU (h, loss=MAE(), valid_loss=None, config=None,\n",
       ">               search_alg=<ray.tune.search.basic_variant.BasicVariantGenerator\n",
       ">               object at 0x7f994148d2b0>, num_samples=10, refit_with_val=False,\n",
       ">               cpus=4, gpus=0, verbose=False, alias=None)\n",
       "\n",
       "Class for Automatic Hyperparameter Optimization, it builds on top of `ray` to\n",
       "give access to a wide variety of hyperparameter optimization tools ranging\n",
       "from classic grid search, to Bayesian optimization and HyperBand algorithm.\n",
       "\n",
       "The validation loss to be optimized is defined by the `config['loss']` dictionary\n",
       "value, the config also contains the rest of the hyperparameter search space.\n",
       "\n",
       "It is important to note that the success of this hyperparameter optimization\n",
       "heavily relies on a strong correlation between the validation and test periods.\n",
       "\n",
       "**Parameters:**<br>\n",
       "`cls_model`: PyTorch/PyTorchLightning model, see `neuralforecast.models` [collection here](https://nixtla.github.io/neuralforecast/models.html).<br>\n",
       "`h`: int, forecast horizon.<br>\n",
       "`loss`: PyTorch module, instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
       "`valid_loss`: PyTorch module=`loss`, instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
       "`config`: dict, dictionary with ray.tune defined search space.<br>\n",
       "`search_alg`: ray.tune.search variant, BasicVariantGenerator, HyperOptSearch, DragonflySearch, TuneBOHB for details\n",
       "    see [tune.search](https://docs.ray.io/en/latest/tune/api_docs/suggestion.html#).<br>\n",
       "`num_samples`: int, number of hyperparameter optimization steps/samples.<br>\n",
       "`cpus`: int, number of cpus to use during optimization, default all available.<br>\n",
       "`gpus`: int, number of gpus to use during optimization, default all available.<br>\n",
       "`refit_wo_val`: bool, number of gpus to use during optimization, default all available.<br>\n",
       "`verbose`: bool, wether print partial outputs.<br>\n",
       "`alias`: str, optional,  Custom name of the model.<br>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(AutoGRU, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "caf08e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 1\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "# Use your own config or AutoGRU.default_config\n",
    "config = dict(max_steps=2, val_check_steps=1, input_size=-1, encoder_hidden_size=8)\n",
    "model = AutoGRU(h=12, config=config, num_samples=1, cpus=1)\n",
    "\n",
    "# Fit and predict\n",
    "model.fit(dataset=dataset)\n",
    "y_hat = model.predict(dataset=dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c258778e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# Y_plot_df = Y_test_df[['unique_id', 'ds', 'y']].copy()\n",
    "# Y_plot_df['AutoGRU'] = y_hat\n",
    "\n",
    "# pd.concat([Y_train_df, Y_plot_df]).drop('unique_id', axis=1).set_index('ds').plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c5d5ed05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class AutoTCN(BaseAuto):\n",
    "\n",
    "    default_config = {\n",
    "        \"input_size_multiplier\": [-1, 4, 16, 64],\n",
    "        \"inference_input_size_multiplier\": [-1],\n",
    "        \"h\": None,\n",
    "        \"encoder_hidden_size\": tune.choice([50, 100, 200, 300]),\n",
    "        \"context_size\": tune.choice([5, 10, 50]),\n",
    "        \"decoder_hidden_size\": tune.choice([64, 128]),\n",
    "        \"learning_rate\": tune.loguniform(1e-4, 1e-1),\n",
    "        \"max_steps\": tune.choice([500, 1000]),\n",
    "        \"batch_size\": tune.choice([16, 32]),\n",
    "        \"loss\": None,\n",
    "        \"random_seed\": tune.randint(1, 20)\n",
    "    }\n",
    "\n",
    "    def __init__(self,\n",
    "                 h,\n",
    "                 loss=MAE(),\n",
    "                 valid_loss=None,\n",
    "                 config=None,\n",
    "                 search_alg=BasicVariantGenerator(random_state=1),\n",
    "                 num_samples=10,\n",
    "                 refit_with_val=False,\n",
    "                 cpus=cpu_count(),\n",
    "                 gpus=torch.cuda.device_count(),\n",
    "                 verbose=False,\n",
    "                 alias=None):\n",
    "        \n",
    "        # Define search space, input/output sizes\n",
    "        if config is None:\n",
    "            config = self.default_config.copy()        \n",
    "            config['input_size'] = tune.choice([h*x \\\n",
    "                         for x in self.default_config[\"input_size_multiplier\"]])\n",
    "            config['inference_input_size'] = tune.choice([h*x \\\n",
    "                         for x in self.default_config[\"inference_input_size_multiplier\"]])\n",
    "            del config[\"input_size_multiplier\"]\n",
    "\n",
    "        super(AutoTCN, self).__init__(\n",
    "              cls_model=TCN,\n",
    "              h=h,\n",
    "              loss=loss,\n",
    "              valid_loss=valid_loss,\n",
    "              config=config,\n",
    "              search_alg=search_alg,\n",
    "              num_samples=num_samples,\n",
    "              refit_with_val=refit_with_val,\n",
    "              cpus=cpus,\n",
    "              gpus=gpus,\n",
    "              verbose=verbose,\n",
    "              alias=alias,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3fc5d09e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/auto.py#L213){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### AutoTCN\n",
       "\n",
       ">      AutoTCN (h, loss=MAE(), valid_loss=None, config=None,\n",
       ">               search_alg=<ray.tune.search.basic_variant.BasicVariantGenerator\n",
       ">               object at 0x7f994103cc40>, num_samples=10, refit_with_val=False,\n",
       ">               cpus=4, gpus=0, verbose=False, alias=None)\n",
       "\n",
       "Class for Automatic Hyperparameter Optimization, it builds on top of `ray` to\n",
       "give access to a wide variety of hyperparameter optimization tools ranging\n",
       "from classic grid search, to Bayesian optimization and HyperBand algorithm.\n",
       "\n",
       "The validation loss to be optimized is defined by the `config['loss']` dictionary\n",
       "value, the config also contains the rest of the hyperparameter search space.\n",
       "\n",
       "It is important to note that the success of this hyperparameter optimization\n",
       "heavily relies on a strong correlation between the validation and test periods.\n",
       "\n",
       "**Parameters:**<br>\n",
       "`cls_model`: PyTorch/PyTorchLightning model, see `neuralforecast.models` [collection here](https://nixtla.github.io/neuralforecast/models.html).<br>\n",
       "`h`: int, forecast horizon.<br>\n",
       "`loss`: PyTorch module, instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
       "`valid_loss`: PyTorch module=`loss`, instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
       "`config`: dict, dictionary with ray.tune defined search space.<br>\n",
       "`search_alg`: ray.tune.search variant, BasicVariantGenerator, HyperOptSearch, DragonflySearch, TuneBOHB for details\n",
       "    see [tune.search](https://docs.ray.io/en/latest/tune/api_docs/suggestion.html#).<br>\n",
       "`num_samples`: int, number of hyperparameter optimization steps/samples.<br>\n",
       "`cpus`: int, number of cpus to use during optimization, default all available.<br>\n",
       "`gpus`: int, number of gpus to use during optimization, default all available.<br>\n",
       "`refit_wo_val`: bool, number of gpus to use during optimization, default all available.<br>\n",
       "`verbose`: bool, wether print partial outputs.<br>\n",
       "`alias`: str, optional,  Custom name of the model.<br>"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/auto.py#L213){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### AutoTCN\n",
       "\n",
       ">      AutoTCN (h, loss=MAE(), valid_loss=None, config=None,\n",
       ">               search_alg=<ray.tune.search.basic_variant.BasicVariantGenerator\n",
       ">               object at 0x7f994103cc40>, num_samples=10, refit_with_val=False,\n",
       ">               cpus=4, gpus=0, verbose=False, alias=None)\n",
       "\n",
       "Class for Automatic Hyperparameter Optimization, it builds on top of `ray` to\n",
       "give access to a wide variety of hyperparameter optimization tools ranging\n",
       "from classic grid search, to Bayesian optimization and HyperBand algorithm.\n",
       "\n",
       "The validation loss to be optimized is defined by the `config['loss']` dictionary\n",
       "value, the config also contains the rest of the hyperparameter search space.\n",
       "\n",
       "It is important to note that the success of this hyperparameter optimization\n",
       "heavily relies on a strong correlation between the validation and test periods.\n",
       "\n",
       "**Parameters:**<br>\n",
       "`cls_model`: PyTorch/PyTorchLightning model, see `neuralforecast.models` [collection here](https://nixtla.github.io/neuralforecast/models.html).<br>\n",
       "`h`: int, forecast horizon.<br>\n",
       "`loss`: PyTorch module, instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
       "`valid_loss`: PyTorch module=`loss`, instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
       "`config`: dict, dictionary with ray.tune defined search space.<br>\n",
       "`search_alg`: ray.tune.search variant, BasicVariantGenerator, HyperOptSearch, DragonflySearch, TuneBOHB for details\n",
       "    see [tune.search](https://docs.ray.io/en/latest/tune/api_docs/suggestion.html#).<br>\n",
       "`num_samples`: int, number of hyperparameter optimization steps/samples.<br>\n",
       "`cpus`: int, number of cpus to use during optimization, default all available.<br>\n",
       "`gpus`: int, number of gpus to use during optimization, default all available.<br>\n",
       "`refit_wo_val`: bool, number of gpus to use during optimization, default all available.<br>\n",
       "`verbose`: bool, wether print partial outputs.<br>\n",
       "`alias`: str, optional,  Custom name of the model.<br>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(AutoTCN, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fb7032bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 1\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "# Use your own config or AutoTCN.default_config\n",
    "config = dict(max_steps=2, val_check_steps=1, input_size=-1, encoder_hidden_size=8)\n",
    "model = AutoTCN(h=12, config=config, num_samples=1, cpus=1)\n",
    "\n",
    "# Fit and predict\n",
    "model.fit(dataset=dataset)\n",
    "y_hat = model.predict(dataset=dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dc65650b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# Y_plot_df = Y_test_df[['unique_id', 'ds', 'y']].copy()\n",
    "# Y_plot_df['AutoTCN'] = y_hat\n",
    "\n",
    "# pd.concat([Y_train_df, Y_plot_df]).drop('unique_id', axis=1).set_index('ds').plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "42b60f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class AutoDilatedRNN(BaseAuto):\n",
    "\n",
    "    default_config = {\n",
    "        \"input_size_multiplier\": [-1, 4, 16, 64],\n",
    "        \"inference_input_size_multiplier\": [-1],\n",
    "        \"h\": None,\n",
    "        \"cell_type\": tune.choice(['LSTM', 'GRU']),\n",
    "        \"encoder_hidden_size\": tune.choice([50, 100, 200, 300]),\n",
    "        \"dilations\": tune.choice([ [[1, 2], [4, 8]], [[1, 2, 4, 8]] ]),\n",
    "        \"context_size\": tune.choice([5, 10, 50]),\n",
    "        \"decoder_hidden_size\": tune.choice([64, 128, 256, 512]),\n",
    "        \"learning_rate\": tune.loguniform(1e-4, 1e-1),\n",
    "        \"max_steps\": tune.choice([500, 1000]),\n",
    "        \"batch_size\": tune.choice([16, 32]),\n",
    "        \"loss\": None,\n",
    "        \"random_seed\": tune.randint(1, 20)\n",
    "    }\n",
    "\n",
    "    def __init__(self,\n",
    "                 h,\n",
    "                 loss=MAE(),\n",
    "                 valid_loss=None,\n",
    "                 config=None,\n",
    "                 search_alg=BasicVariantGenerator(random_state=1),\n",
    "                 num_samples=10,\n",
    "                 refit_with_val=False,\n",
    "                 cpus=cpu_count(),\n",
    "                 gpus=torch.cuda.device_count(),\n",
    "                 verbose=False,\n",
    "                 alias=None):\n",
    "        \n",
    "        # Define search space, input/output sizes\n",
    "        if config is None:\n",
    "            config = self.default_config.copy()        \n",
    "            config['input_size'] = tune.choice([h*x \\\n",
    "                         for x in self.default_config[\"input_size_multiplier\"]])\n",
    "            config['inference_input_size'] = tune.choice([h*x \\\n",
    "                         for x in self.default_config[\"inference_input_size_multiplier\"]])\n",
    "            del config[\"input_size_multiplier\"]\n",
    "\n",
    "        super(AutoDilatedRNN, self).__init__(\n",
    "              cls_model=DilatedRNN,\n",
    "              h=h,\n",
    "              loss=loss,\n",
    "              valid_loss=valid_loss,\n",
    "              config=config,\n",
    "              search_alg=search_alg,\n",
    "              num_samples=num_samples, \n",
    "              refit_with_val=refit_with_val,\n",
    "              cpus=cpus,\n",
    "              gpus=gpus,\n",
    "              verbose=verbose,\n",
    "              alias=alias,\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d132351d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/auto.py#L269){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### AutoDilatedRNN\n",
       "\n",
       ">      AutoDilatedRNN (h, loss=MAE(), valid_loss=None, config=None,\n",
       ">                      search_alg=<ray.tune.search.basic_variant.BasicVariantGen\n",
       ">                      erator object at 0x7f9942d4cd30>, num_samples=10,\n",
       ">                      refit_with_val=False, cpus=4, gpus=0, verbose=False,\n",
       ">                      alias=None)\n",
       "\n",
       "Class for Automatic Hyperparameter Optimization, it builds on top of `ray` to\n",
       "give access to a wide variety of hyperparameter optimization tools ranging\n",
       "from classic grid search, to Bayesian optimization and HyperBand algorithm.\n",
       "\n",
       "The validation loss to be optimized is defined by the `config['loss']` dictionary\n",
       "value, the config also contains the rest of the hyperparameter search space.\n",
       "\n",
       "It is important to note that the success of this hyperparameter optimization\n",
       "heavily relies on a strong correlation between the validation and test periods.\n",
       "\n",
       "**Parameters:**<br>\n",
       "`cls_model`: PyTorch/PyTorchLightning model, see `neuralforecast.models` [collection here](https://nixtla.github.io/neuralforecast/models.html).<br>\n",
       "`h`: int, forecast horizon.<br>\n",
       "`loss`: PyTorch module, instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
       "`valid_loss`: PyTorch module=`loss`, instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
       "`config`: dict, dictionary with ray.tune defined search space.<br>\n",
       "`search_alg`: ray.tune.search variant, BasicVariantGenerator, HyperOptSearch, DragonflySearch, TuneBOHB for details\n",
       "    see [tune.search](https://docs.ray.io/en/latest/tune/api_docs/suggestion.html#).<br>\n",
       "`num_samples`: int, number of hyperparameter optimization steps/samples.<br>\n",
       "`cpus`: int, number of cpus to use during optimization, default all available.<br>\n",
       "`gpus`: int, number of gpus to use during optimization, default all available.<br>\n",
       "`refit_wo_val`: bool, number of gpus to use during optimization, default all available.<br>\n",
       "`verbose`: bool, wether print partial outputs.<br>\n",
       "`alias`: str, optional,  Custom name of the model.<br>"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/auto.py#L269){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### AutoDilatedRNN\n",
       "\n",
       ">      AutoDilatedRNN (h, loss=MAE(), valid_loss=None, config=None,\n",
       ">                      search_alg=<ray.tune.search.basic_variant.BasicVariantGen\n",
       ">                      erator object at 0x7f9942d4cd30>, num_samples=10,\n",
       ">                      refit_with_val=False, cpus=4, gpus=0, verbose=False,\n",
       ">                      alias=None)\n",
       "\n",
       "Class for Automatic Hyperparameter Optimization, it builds on top of `ray` to\n",
       "give access to a wide variety of hyperparameter optimization tools ranging\n",
       "from classic grid search, to Bayesian optimization and HyperBand algorithm.\n",
       "\n",
       "The validation loss to be optimized is defined by the `config['loss']` dictionary\n",
       "value, the config also contains the rest of the hyperparameter search space.\n",
       "\n",
       "It is important to note that the success of this hyperparameter optimization\n",
       "heavily relies on a strong correlation between the validation and test periods.\n",
       "\n",
       "**Parameters:**<br>\n",
       "`cls_model`: PyTorch/PyTorchLightning model, see `neuralforecast.models` [collection here](https://nixtla.github.io/neuralforecast/models.html).<br>\n",
       "`h`: int, forecast horizon.<br>\n",
       "`loss`: PyTorch module, instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
       "`valid_loss`: PyTorch module=`loss`, instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
       "`config`: dict, dictionary with ray.tune defined search space.<br>\n",
       "`search_alg`: ray.tune.search variant, BasicVariantGenerator, HyperOptSearch, DragonflySearch, TuneBOHB for details\n",
       "    see [tune.search](https://docs.ray.io/en/latest/tune/api_docs/suggestion.html#).<br>\n",
       "`num_samples`: int, number of hyperparameter optimization steps/samples.<br>\n",
       "`cpus`: int, number of cpus to use during optimization, default all available.<br>\n",
       "`gpus`: int, number of gpus to use during optimization, default all available.<br>\n",
       "`refit_wo_val`: bool, number of gpus to use during optimization, default all available.<br>\n",
       "`verbose`: bool, wether print partial outputs.<br>\n",
       "`alias`: str, optional,  Custom name of the model.<br>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(AutoDilatedRNN, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "533eb1da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 1\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "# Use your own config or AutoDilatedRNN.default_config\n",
    "config = dict(max_steps=2, val_check_steps=1, input_size=-1, encoder_hidden_size=8)\n",
    "model = AutoDilatedRNN(h=12, config=config, num_samples=1, cpus=1)\n",
    "\n",
    "# Fit and predict\n",
    "model.fit(dataset=dataset)\n",
    "y_hat = model.predict(dataset=dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f62d7979",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# Y_plot_df = Y_test_df[['unique_id', 'ds', 'y']].copy()\n",
    "# Y_plot_df['AutoDilatedRNN'] = y_hat\n",
    "\n",
    "# pd.concat([Y_train_df, Y_plot_df]).drop('unique_id', axis=1).set_index('ds').plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7feff99a",
   "metadata": {},
   "source": [
    "## <span style=\"color:DarkBlue\"> B. MLP-Based </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fa01d378",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class AutoMLP(BaseAuto):\n",
    "\n",
    "    default_config = {\n",
    "        \"input_size_multiplier\": [1, 2, 3, 4, 5],\n",
    "        \"h\": None,\n",
    "        \"hidden_size\": tune.choice( [256, 512, 1024] ),\n",
    "        \"num_layers\": tune.randint(2, 6),\n",
    "        \"learning_rate\": tune.loguniform(1e-4, 1e-1),\n",
    "        \"scaler_type\": tune.choice([None, 'robust', 'standard']),\n",
    "        \"max_steps\": tune.choice([500, 1000]),\n",
    "        \"batch_size\": tune.choice([32, 64, 128, 256]),\n",
    "        \"windows_batch_size\": tune.choice([128, 256, 512, 1024]),\n",
    "        \"loss\": None,\n",
    "        \"random_seed\": tune.randint(1, 20),\n",
    "    }\n",
    "\n",
    "    def __init__(self,\n",
    "                 h,\n",
    "                 loss=MAE(),\n",
    "                 valid_loss=None,     \n",
    "                 config=None,\n",
    "                 search_alg=BasicVariantGenerator(random_state=1),\n",
    "                 num_samples=10,\n",
    "                 refit_with_val=False,\n",
    "                 cpus=cpu_count(),\n",
    "                 gpus=torch.cuda.device_count(),\n",
    "                 verbose=False,\n",
    "                 alias=None):\n",
    "\n",
    "        # Define search space, input/output sizes\n",
    "        if config is None:\n",
    "            config = self.default_config.copy()        \n",
    "            config['input_size'] = tune.choice([h*x \\\n",
    "                         for x in self.default_config[\"input_size_multiplier\"]])\n",
    "\n",
    "            # Rolling windows with step_size=1 or step_size=h\n",
    "            # See `BaseWindows` and `BaseRNN`'s create_windows\n",
    "            config['step_size'] = tune.choice([1, h])\n",
    "            del config[\"input_size_multiplier\"]\n",
    "\n",
    "        super(AutoMLP, self).__init__(\n",
    "              cls_model=MLP,\n",
    "              h=h,\n",
    "              loss=loss,\n",
    "              valid_loss=valid_loss,\n",
    "              config=config, \n",
    "              search_alg=search_alg,\n",
    "              num_samples=num_samples, \n",
    "              refit_with_val=refit_with_val,\n",
    "              cpus=cpus,\n",
    "              gpus=gpus,\n",
    "              verbose=verbose,\n",
    "              alias=alias,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a4c8ecf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/auto.py#L327){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### AutoMLP\n",
       "\n",
       ">      AutoMLP (h, loss=MAE(), valid_loss=None, config=None,\n",
       ">               search_alg=<ray.tune.search.basic_variant.BasicVariantGenerator\n",
       ">               object at 0x7f9942d518b0>, num_samples=10, refit_with_val=False,\n",
       ">               cpus=4, gpus=0, verbose=False, alias=None)\n",
       "\n",
       "Class for Automatic Hyperparameter Optimization, it builds on top of `ray` to\n",
       "give access to a wide variety of hyperparameter optimization tools ranging\n",
       "from classic grid search, to Bayesian optimization and HyperBand algorithm.\n",
       "\n",
       "The validation loss to be optimized is defined by the `config['loss']` dictionary\n",
       "value, the config also contains the rest of the hyperparameter search space.\n",
       "\n",
       "It is important to note that the success of this hyperparameter optimization\n",
       "heavily relies on a strong correlation between the validation and test periods.\n",
       "\n",
       "**Parameters:**<br>\n",
       "`cls_model`: PyTorch/PyTorchLightning model, see `neuralforecast.models` [collection here](https://nixtla.github.io/neuralforecast/models.html).<br>\n",
       "`h`: int, forecast horizon.<br>\n",
       "`loss`: PyTorch module, instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
       "`valid_loss`: PyTorch module=`loss`, instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
       "`config`: dict, dictionary with ray.tune defined search space.<br>\n",
       "`search_alg`: ray.tune.search variant, BasicVariantGenerator, HyperOptSearch, DragonflySearch, TuneBOHB for details\n",
       "    see [tune.search](https://docs.ray.io/en/latest/tune/api_docs/suggestion.html#).<br>\n",
       "`num_samples`: int, number of hyperparameter optimization steps/samples.<br>\n",
       "`cpus`: int, number of cpus to use during optimization, default all available.<br>\n",
       "`gpus`: int, number of gpus to use during optimization, default all available.<br>\n",
       "`refit_wo_val`: bool, number of gpus to use during optimization, default all available.<br>\n",
       "`verbose`: bool, wether print partial outputs.<br>\n",
       "`alias`: str, optional,  Custom name of the model.<br>"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/auto.py#L327){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### AutoMLP\n",
       "\n",
       ">      AutoMLP (h, loss=MAE(), valid_loss=None, config=None,\n",
       ">               search_alg=<ray.tune.search.basic_variant.BasicVariantGenerator\n",
       ">               object at 0x7f9942d518b0>, num_samples=10, refit_with_val=False,\n",
       ">               cpus=4, gpus=0, verbose=False, alias=None)\n",
       "\n",
       "Class for Automatic Hyperparameter Optimization, it builds on top of `ray` to\n",
       "give access to a wide variety of hyperparameter optimization tools ranging\n",
       "from classic grid search, to Bayesian optimization and HyperBand algorithm.\n",
       "\n",
       "The validation loss to be optimized is defined by the `config['loss']` dictionary\n",
       "value, the config also contains the rest of the hyperparameter search space.\n",
       "\n",
       "It is important to note that the success of this hyperparameter optimization\n",
       "heavily relies on a strong correlation between the validation and test periods.\n",
       "\n",
       "**Parameters:**<br>\n",
       "`cls_model`: PyTorch/PyTorchLightning model, see `neuralforecast.models` [collection here](https://nixtla.github.io/neuralforecast/models.html).<br>\n",
       "`h`: int, forecast horizon.<br>\n",
       "`loss`: PyTorch module, instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
       "`valid_loss`: PyTorch module=`loss`, instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
       "`config`: dict, dictionary with ray.tune defined search space.<br>\n",
       "`search_alg`: ray.tune.search variant, BasicVariantGenerator, HyperOptSearch, DragonflySearch, TuneBOHB for details\n",
       "    see [tune.search](https://docs.ray.io/en/latest/tune/api_docs/suggestion.html#).<br>\n",
       "`num_samples`: int, number of hyperparameter optimization steps/samples.<br>\n",
       "`cpus`: int, number of cpus to use during optimization, default all available.<br>\n",
       "`gpus`: int, number of gpus to use during optimization, default all available.<br>\n",
       "`refit_wo_val`: bool, number of gpus to use during optimization, default all available.<br>\n",
       "`verbose`: bool, wether print partial outputs.<br>\n",
       "`alias`: str, optional,  Custom name of the model.<br>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(AutoMLP, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c637d57f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 1\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "# Use your own config or AutoMLP.default_config\n",
    "config = dict(max_steps=2, val_check_steps=1, input_size=12, hidden_size=8)\n",
    "model = AutoMLP(h=12, config=config, num_samples=1, cpus=1)\n",
    "\n",
    "# Fit and predict\n",
    "model.fit(dataset=dataset)\n",
    "y_hat = model.predict(dataset=dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3f739c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# Y_plot_df = Y_test_df[['unique_id', 'ds', 'y']].copy()\n",
    "# Y_plot_df['AutoMLP'] = y_hat\n",
    "\n",
    "# pd.concat([Y_train_df, Y_plot_df]).drop('unique_id', axis=1).set_index('ds').plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1ed4c88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class AutoNBEATS(BaseAuto):\n",
    "\n",
    "    default_config = {\n",
    "        \"input_size_multiplier\": [1, 2, 3, 4, 5],\n",
    "        \"h\": None,\n",
    "        \"learning_rate\": tune.loguniform(1e-4, 1e-1),\n",
    "        \"scaler_type\": tune.choice([None, 'robust', 'standard']),\n",
    "        \"max_steps\": tune.choice([500, 1000]),\n",
    "        \"batch_size\": tune.choice([32, 64, 128, 256]),\n",
    "        \"windows_batch_size\": tune.choice([128, 256, 512, 1024]),\n",
    "        \"loss\": None,\n",
    "        \"random_seed\": tune.randint(1, 20),\n",
    "    }\n",
    "\n",
    "    def __init__(self,\n",
    "                 h,\n",
    "                 loss=MAE(),\n",
    "                 valid_loss=None,\n",
    "                 config=None, \n",
    "                 search_alg=BasicVariantGenerator(random_state=1),\n",
    "                 num_samples=10,\n",
    "                 refit_with_val=False,\n",
    "                 cpus=cpu_count(),\n",
    "                 gpus=torch.cuda.device_count(),\n",
    "                 verbose=False,\n",
    "                 alias=None):\n",
    "        \n",
    "        # Define search space, input/output sizes\n",
    "        if config is None:\n",
    "            config = self.default_config.copy()        \n",
    "            config['input_size'] = tune.choice([h*x \\\n",
    "                         for x in self.default_config[\"input_size_multiplier\"]])\n",
    "\n",
    "            # Rolling windows with step_size=1 or step_size=h\n",
    "            # See `BaseWindows` and `BaseRNN`'s create_windows\n",
    "            config['step_size'] = tune.choice([1, h])\n",
    "            del config[\"input_size_multiplier\"]\n",
    "\n",
    "        super(AutoNBEATS, self).__init__(\n",
    "              cls_model=NBEATS, \n",
    "              h=h,\n",
    "              loss=loss,\n",
    "              valid_loss=valid_loss,\n",
    "              config=config,\n",
    "              search_alg=search_alg,\n",
    "              num_samples=num_samples, \n",
    "              refit_with_val=refit_with_val,\n",
    "              cpus=cpus,\n",
    "              gpus=gpus,\n",
    "              verbose=verbose,\n",
    "              alias=alias,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ef824625",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/auto.py#L384){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### AutoNBEATS\n",
       "\n",
       ">      AutoNBEATS (h, loss=MAE(), valid_loss=None, config=None,\n",
       ">                  search_alg=<ray.tune.search.basic_variant.BasicVariantGenerat\n",
       ">                  or object at 0x7f9942d967f0>, num_samples=10,\n",
       ">                  refit_with_val=False, cpus=4, gpus=0, verbose=False,\n",
       ">                  alias=None)\n",
       "\n",
       "Class for Automatic Hyperparameter Optimization, it builds on top of `ray` to\n",
       "give access to a wide variety of hyperparameter optimization tools ranging\n",
       "from classic grid search, to Bayesian optimization and HyperBand algorithm.\n",
       "\n",
       "The validation loss to be optimized is defined by the `config['loss']` dictionary\n",
       "value, the config also contains the rest of the hyperparameter search space.\n",
       "\n",
       "It is important to note that the success of this hyperparameter optimization\n",
       "heavily relies on a strong correlation between the validation and test periods.\n",
       "\n",
       "**Parameters:**<br>\n",
       "`cls_model`: PyTorch/PyTorchLightning model, see `neuralforecast.models` [collection here](https://nixtla.github.io/neuralforecast/models.html).<br>\n",
       "`h`: int, forecast horizon.<br>\n",
       "`loss`: PyTorch module, instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
       "`valid_loss`: PyTorch module=`loss`, instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
       "`config`: dict, dictionary with ray.tune defined search space.<br>\n",
       "`search_alg`: ray.tune.search variant, BasicVariantGenerator, HyperOptSearch, DragonflySearch, TuneBOHB for details\n",
       "    see [tune.search](https://docs.ray.io/en/latest/tune/api_docs/suggestion.html#).<br>\n",
       "`num_samples`: int, number of hyperparameter optimization steps/samples.<br>\n",
       "`cpus`: int, number of cpus to use during optimization, default all available.<br>\n",
       "`gpus`: int, number of gpus to use during optimization, default all available.<br>\n",
       "`refit_wo_val`: bool, number of gpus to use during optimization, default all available.<br>\n",
       "`verbose`: bool, wether print partial outputs.<br>\n",
       "`alias`: str, optional,  Custom name of the model.<br>"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/auto.py#L384){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### AutoNBEATS\n",
       "\n",
       ">      AutoNBEATS (h, loss=MAE(), valid_loss=None, config=None,\n",
       ">                  search_alg=<ray.tune.search.basic_variant.BasicVariantGenerat\n",
       ">                  or object at 0x7f9942d967f0>, num_samples=10,\n",
       ">                  refit_with_val=False, cpus=4, gpus=0, verbose=False,\n",
       ">                  alias=None)\n",
       "\n",
       "Class for Automatic Hyperparameter Optimization, it builds on top of `ray` to\n",
       "give access to a wide variety of hyperparameter optimization tools ranging\n",
       "from classic grid search, to Bayesian optimization and HyperBand algorithm.\n",
       "\n",
       "The validation loss to be optimized is defined by the `config['loss']` dictionary\n",
       "value, the config also contains the rest of the hyperparameter search space.\n",
       "\n",
       "It is important to note that the success of this hyperparameter optimization\n",
       "heavily relies on a strong correlation between the validation and test periods.\n",
       "\n",
       "**Parameters:**<br>\n",
       "`cls_model`: PyTorch/PyTorchLightning model, see `neuralforecast.models` [collection here](https://nixtla.github.io/neuralforecast/models.html).<br>\n",
       "`h`: int, forecast horizon.<br>\n",
       "`loss`: PyTorch module, instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
       "`valid_loss`: PyTorch module=`loss`, instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
       "`config`: dict, dictionary with ray.tune defined search space.<br>\n",
       "`search_alg`: ray.tune.search variant, BasicVariantGenerator, HyperOptSearch, DragonflySearch, TuneBOHB for details\n",
       "    see [tune.search](https://docs.ray.io/en/latest/tune/api_docs/suggestion.html#).<br>\n",
       "`num_samples`: int, number of hyperparameter optimization steps/samples.<br>\n",
       "`cpus`: int, number of cpus to use during optimization, default all available.<br>\n",
       "`gpus`: int, number of gpus to use during optimization, default all available.<br>\n",
       "`refit_wo_val`: bool, number of gpus to use during optimization, default all available.<br>\n",
       "`verbose`: bool, wether print partial outputs.<br>\n",
       "`alias`: str, optional,  Custom name of the model.<br>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(AutoNBEATS, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c0f0fe9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 1\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "# Use your own config or AutoNBEATS.default_config\n",
    "config = dict(max_steps=2, val_check_steps=1, input_size=12,\n",
    "              mlp_units=3*[[8, 8]])\n",
    "model = AutoNBEATS(h=12, config=config, num_samples=1, cpus=1)\n",
    "\n",
    "# Fit and predict\n",
    "model.fit(dataset=dataset)\n",
    "y_hat = model.predict(dataset=dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2fa7d6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# Y_plot_df = Y_test_df[['unique_id', 'ds', 'y']].copy()\n",
    "# Y_plot_df['AutoNBEATS'] = y_hat\n",
    "\n",
    "# pd.concat([Y_train_df, Y_plot_df]).drop('unique_id', axis=1).set_index('ds').plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "753c5300",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class AutoNBEATSx(BaseAuto):\n",
    "\n",
    "    default_config = {\n",
    "        \"input_size_multiplier\": [1, 2, 3, 4, 5],\n",
    "        \"h\": None,\n",
    "        \"learning_rate\": tune.loguniform(1e-4, 1e-1),\n",
    "        \"scaler_type\": tune.choice([None, 'robust', 'standard']),\n",
    "        \"max_steps\": tune.choice([500, 1000]),\n",
    "        \"batch_size\": tune.choice([32, 64, 128, 256]),\n",
    "        \"windows_batch_size\": tune.choice([128, 256, 512, 1024]),\n",
    "        \"loss\": None,\n",
    "        \"random_seed\": tune.randint(1, 20),\n",
    "    }\n",
    "\n",
    "    def __init__(self,\n",
    "                 h,\n",
    "                 loss=MAE(),\n",
    "                 valid_loss=None,\n",
    "                 config=None, \n",
    "                 search_alg=BasicVariantGenerator(random_state=1),\n",
    "                 num_samples=10,\n",
    "                 refit_with_val=False,\n",
    "                 cpus=cpu_count(),\n",
    "                 gpus=torch.cuda.device_count(),\n",
    "                 verbose=False,\n",
    "                 alias=None):\n",
    "        \n",
    "        # Define search space, input/output sizes\n",
    "        if config is None:\n",
    "            config = self.default_config.copy()        \n",
    "            config['input_size'] = tune.choice([h*x \\\n",
    "                         for x in self.default_config[\"input_size_multiplier\"]])\n",
    "\n",
    "            # Rolling windows with step_size=1 or step_size=h\n",
    "            # See `BaseWindows` and `BaseRNN`'s create_windows\n",
    "            config['step_size'] = tune.choice([1, h])\n",
    "            del config[\"input_size_multiplier\"]\n",
    "\n",
    "        super(AutoNBEATSx, self).__init__(\n",
    "              cls_model=NBEATSx,\n",
    "              h=h,\n",
    "              loss=loss,\n",
    "              valid_loss=valid_loss,\n",
    "              config=config,\n",
    "              search_alg=search_alg,\n",
    "              num_samples=num_samples, \n",
    "              refit_with_val=refit_with_val,\n",
    "              cpus=cpus,\n",
    "              gpus=gpus,\n",
    "              verbose=verbose,\n",
    "              alias=alias,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a2829115",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/auto.py#L439){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### AutoNBEATSx\n",
       "\n",
       ">      AutoNBEATSx (h, loss=MAE(), valid_loss=None, config=None,\n",
       ">                   search_alg=<ray.tune.search.basic_variant.BasicVariantGenera\n",
       ">                   tor object at 0x7f994148dca0>, num_samples=10,\n",
       ">                   refit_with_val=False, cpus=4, gpus=0, verbose=False,\n",
       ">                   alias=None)\n",
       "\n",
       "Class for Automatic Hyperparameter Optimization, it builds on top of `ray` to\n",
       "give access to a wide variety of hyperparameter optimization tools ranging\n",
       "from classic grid search, to Bayesian optimization and HyperBand algorithm.\n",
       "\n",
       "The validation loss to be optimized is defined by the `config['loss']` dictionary\n",
       "value, the config also contains the rest of the hyperparameter search space.\n",
       "\n",
       "It is important to note that the success of this hyperparameter optimization\n",
       "heavily relies on a strong correlation between the validation and test periods.\n",
       "\n",
       "**Parameters:**<br>\n",
       "`cls_model`: PyTorch/PyTorchLightning model, see `neuralforecast.models` [collection here](https://nixtla.github.io/neuralforecast/models.html).<br>\n",
       "`h`: int, forecast horizon.<br>\n",
       "`loss`: PyTorch module, instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
       "`valid_loss`: PyTorch module=`loss`, instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
       "`config`: dict, dictionary with ray.tune defined search space.<br>\n",
       "`search_alg`: ray.tune.search variant, BasicVariantGenerator, HyperOptSearch, DragonflySearch, TuneBOHB for details\n",
       "    see [tune.search](https://docs.ray.io/en/latest/tune/api_docs/suggestion.html#).<br>\n",
       "`num_samples`: int, number of hyperparameter optimization steps/samples.<br>\n",
       "`cpus`: int, number of cpus to use during optimization, default all available.<br>\n",
       "`gpus`: int, number of gpus to use during optimization, default all available.<br>\n",
       "`refit_wo_val`: bool, number of gpus to use during optimization, default all available.<br>\n",
       "`verbose`: bool, wether print partial outputs.<br>\n",
       "`alias`: str, optional,  Custom name of the model.<br>"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/auto.py#L439){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### AutoNBEATSx\n",
       "\n",
       ">      AutoNBEATSx (h, loss=MAE(), valid_loss=None, config=None,\n",
       ">                   search_alg=<ray.tune.search.basic_variant.BasicVariantGenera\n",
       ">                   tor object at 0x7f994148dca0>, num_samples=10,\n",
       ">                   refit_with_val=False, cpus=4, gpus=0, verbose=False,\n",
       ">                   alias=None)\n",
       "\n",
       "Class for Automatic Hyperparameter Optimization, it builds on top of `ray` to\n",
       "give access to a wide variety of hyperparameter optimization tools ranging\n",
       "from classic grid search, to Bayesian optimization and HyperBand algorithm.\n",
       "\n",
       "The validation loss to be optimized is defined by the `config['loss']` dictionary\n",
       "value, the config also contains the rest of the hyperparameter search space.\n",
       "\n",
       "It is important to note that the success of this hyperparameter optimization\n",
       "heavily relies on a strong correlation between the validation and test periods.\n",
       "\n",
       "**Parameters:**<br>\n",
       "`cls_model`: PyTorch/PyTorchLightning model, see `neuralforecast.models` [collection here](https://nixtla.github.io/neuralforecast/models.html).<br>\n",
       "`h`: int, forecast horizon.<br>\n",
       "`loss`: PyTorch module, instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
       "`valid_loss`: PyTorch module=`loss`, instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
       "`config`: dict, dictionary with ray.tune defined search space.<br>\n",
       "`search_alg`: ray.tune.search variant, BasicVariantGenerator, HyperOptSearch, DragonflySearch, TuneBOHB for details\n",
       "    see [tune.search](https://docs.ray.io/en/latest/tune/api_docs/suggestion.html#).<br>\n",
       "`num_samples`: int, number of hyperparameter optimization steps/samples.<br>\n",
       "`cpus`: int, number of cpus to use during optimization, default all available.<br>\n",
       "`gpus`: int, number of gpus to use during optimization, default all available.<br>\n",
       "`refit_wo_val`: bool, number of gpus to use during optimization, default all available.<br>\n",
       "`verbose`: bool, wether print partial outputs.<br>\n",
       "`alias`: str, optional,  Custom name of the model.<br>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(AutoNBEATSx, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dca94b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 1\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "# Use your own config or AutoNBEATS.default_config\n",
    "config = dict(max_steps=2, val_check_steps=1, input_size=12,\n",
    "              mlp_units=3*[[8, 8]])\n",
    "model = AutoNBEATSx(h=12, config=config, num_samples=1, cpus=1)\n",
    "\n",
    "# Fit and predict\n",
    "model.fit(dataset=dataset)\n",
    "y_hat = model.predict(dataset=dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f19ebaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# Y_plot_df = Y_test_df[['unique_id', 'ds', 'y']].copy()\n",
    "# Y_plot_df['AutoNBEATSx'] = y_hat\n",
    "\n",
    "# pd.concat([Y_train_df, Y_plot_df]).drop('unique_id', axis=1).set_index('ds').plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "79a2ede0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class AutoNHITS(BaseAuto):\n",
    "\n",
    "    default_config = {\n",
    "       \"input_size_multiplier\": [1, 2, 3, 4, 5],\n",
    "       \"h\": None,\n",
    "       \"n_pool_kernel_size\": tune.choice([[2, 2, 1], 3*[1], 3*[2], 3*[4], \n",
    "                                         [8, 4, 1], [16, 8, 1]]),\n",
    "       \"n_freq_downsample\": tune.choice([[168, 24, 1], [24, 12, 1], \n",
    "                                         [180, 60, 1], [60, 8, 1], \n",
    "                                         [40, 20, 1], [1, 1, 1]]),\n",
    "       \"learning_rate\": tune.loguniform(1e-4, 1e-1),\n",
    "       \"scaler_type\": tune.choice([None, 'robust', 'standard']),\n",
    "       \"max_steps\": tune.quniform(lower=500, upper=1500, q=100),\n",
    "       \"batch_size\": tune.qloguniform(lower=5, upper=9, base=2, q=1), #[32, 64, 128, 256]\n",
    "       \"windows_batch_size\": tune.qloguniform(lower=7, upper=10, base=2, q=1), #[128, 256, 512, 1024]\n",
    "       \"loss\": None,\n",
    "       \"random_seed\": tune.randint(lower=1, upper=20),\n",
    "    }\n",
    "\n",
    "    def __init__(self,\n",
    "                 h,\n",
    "                 loss=MAE(),\n",
    "                 valid_loss=None,\n",
    "                 config=None, \n",
    "                 search_alg=BasicVariantGenerator(random_state=1),\n",
    "                 num_samples=10,\n",
    "                 refit_with_val=False,\n",
    "                 cpus=cpu_count(),\n",
    "                 gpus=torch.cuda.device_count(),\n",
    "                 verbose=False,\n",
    "                 alias=None):\n",
    "\n",
    "        # Define search space, input/output sizes\n",
    "        if config is None:\n",
    "            config = self.default_config.copy()        \n",
    "            config['input_size'] = tune.choice([h*x \\\n",
    "                         for x in self.default_config[\"input_size_multiplier\"]])\n",
    "            \n",
    "            # Rolling windows with step_size=1 or step_size=h\n",
    "            # See `BaseWindows` and `BaseRNN`'s create_windows\n",
    "            config['step_size'] = tune.choice([1, h])\n",
    "            del config[\"input_size_multiplier\"]\n",
    "\n",
    "        super(AutoNHITS, self).__init__(\n",
    "              cls_model=NHITS, \n",
    "              h=h,\n",
    "              loss=loss,\n",
    "              valid_loss=valid_loss,\n",
    "              config=config,\n",
    "              search_alg=search_alg,\n",
    "              num_samples=num_samples,\n",
    "              refit_with_val=refit_with_val,\n",
    "              cpus=cpus,\n",
    "              gpus=gpus,\n",
    "              verbose=verbose,\n",
    "              alias=alias,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fd1236fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/auto.py#L494){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### AutoNHITS\n",
       "\n",
       ">      AutoNHITS (h, loss=MAE(), valid_loss=None, config=None,\n",
       ">                 search_alg=<ray.tune.search.basic_variant.BasicVariantGenerato\n",
       ">                 r object at 0x7f994269d340>, num_samples=10,\n",
       ">                 refit_with_val=False, cpus=4, gpus=0, verbose=False,\n",
       ">                 alias=None)\n",
       "\n",
       "Class for Automatic Hyperparameter Optimization, it builds on top of `ray` to\n",
       "give access to a wide variety of hyperparameter optimization tools ranging\n",
       "from classic grid search, to Bayesian optimization and HyperBand algorithm.\n",
       "\n",
       "The validation loss to be optimized is defined by the `config['loss']` dictionary\n",
       "value, the config also contains the rest of the hyperparameter search space.\n",
       "\n",
       "It is important to note that the success of this hyperparameter optimization\n",
       "heavily relies on a strong correlation between the validation and test periods.\n",
       "\n",
       "**Parameters:**<br>\n",
       "`cls_model`: PyTorch/PyTorchLightning model, see `neuralforecast.models` [collection here](https://nixtla.github.io/neuralforecast/models.html).<br>\n",
       "`h`: int, forecast horizon.<br>\n",
       "`loss`: PyTorch module, instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
       "`valid_loss`: PyTorch module=`loss`, instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
       "`config`: dict, dictionary with ray.tune defined search space.<br>\n",
       "`search_alg`: ray.tune.search variant, BasicVariantGenerator, HyperOptSearch, DragonflySearch, TuneBOHB for details\n",
       "    see [tune.search](https://docs.ray.io/en/latest/tune/api_docs/suggestion.html#).<br>\n",
       "`num_samples`: int, number of hyperparameter optimization steps/samples.<br>\n",
       "`cpus`: int, number of cpus to use during optimization, default all available.<br>\n",
       "`gpus`: int, number of gpus to use during optimization, default all available.<br>\n",
       "`refit_wo_val`: bool, number of gpus to use during optimization, default all available.<br>\n",
       "`verbose`: bool, wether print partial outputs.<br>\n",
       "`alias`: str, optional,  Custom name of the model.<br>"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/auto.py#L494){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### AutoNHITS\n",
       "\n",
       ">      AutoNHITS (h, loss=MAE(), valid_loss=None, config=None,\n",
       ">                 search_alg=<ray.tune.search.basic_variant.BasicVariantGenerato\n",
       ">                 r object at 0x7f994269d340>, num_samples=10,\n",
       ">                 refit_with_val=False, cpus=4, gpus=0, verbose=False,\n",
       ">                 alias=None)\n",
       "\n",
       "Class for Automatic Hyperparameter Optimization, it builds on top of `ray` to\n",
       "give access to a wide variety of hyperparameter optimization tools ranging\n",
       "from classic grid search, to Bayesian optimization and HyperBand algorithm.\n",
       "\n",
       "The validation loss to be optimized is defined by the `config['loss']` dictionary\n",
       "value, the config also contains the rest of the hyperparameter search space.\n",
       "\n",
       "It is important to note that the success of this hyperparameter optimization\n",
       "heavily relies on a strong correlation between the validation and test periods.\n",
       "\n",
       "**Parameters:**<br>\n",
       "`cls_model`: PyTorch/PyTorchLightning model, see `neuralforecast.models` [collection here](https://nixtla.github.io/neuralforecast/models.html).<br>\n",
       "`h`: int, forecast horizon.<br>\n",
       "`loss`: PyTorch module, instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
       "`valid_loss`: PyTorch module=`loss`, instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
       "`config`: dict, dictionary with ray.tune defined search space.<br>\n",
       "`search_alg`: ray.tune.search variant, BasicVariantGenerator, HyperOptSearch, DragonflySearch, TuneBOHB for details\n",
       "    see [tune.search](https://docs.ray.io/en/latest/tune/api_docs/suggestion.html#).<br>\n",
       "`num_samples`: int, number of hyperparameter optimization steps/samples.<br>\n",
       "`cpus`: int, number of cpus to use during optimization, default all available.<br>\n",
       "`gpus`: int, number of gpus to use during optimization, default all available.<br>\n",
       "`refit_wo_val`: bool, number of gpus to use during optimization, default all available.<br>\n",
       "`verbose`: bool, wether print partial outputs.<br>\n",
       "`alias`: str, optional,  Custom name of the model.<br>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(AutoNHITS, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1c3d2af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 1\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "# Use your own config or AutoNHITS.default_config\n",
    "config = dict(max_steps=2, val_check_steps=1, input_size=12, \n",
    "              mlp_units=3 * [[8, 8]])\n",
    "model = AutoNHITS(h=12, config=config, num_samples=1, cpus=1)\n",
    "\n",
    "# Fit and predict\n",
    "model.fit(dataset=dataset)\n",
    "y_hat = model.predict(dataset=dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b20df13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# Y_plot_df = Y_test_df[['unique_id', 'ds', 'y']].copy()\n",
    "# Y_plot_df['AutoNHITS'] = y_hat\n",
    "\n",
    "# pd.concat([Y_train_df, Y_plot_df]).drop('unique_id', axis=1).set_index('ds').plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd705a56",
   "metadata": {},
   "source": [
    "## <span style=\"color:DarkBlue\"> C. Transformer-Based </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "91cd9b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class AutoTFT(BaseAuto):\n",
    "\n",
    "    default_config = {\n",
    "        \"input_size_multiplier\": [1, 2, 3, 4, 5],\n",
    "        \"h\": None,\n",
    "        \"hidden_size\": tune.choice([64, 128, 256]),\n",
    "        \"n_head\": tune.choice([4, 8]),\n",
    "        \"learning_rate\": tune.loguniform(1e-4, 1e-1),\n",
    "        \"scaler_type\": tune.choice([None, 'robust', 'standard']),\n",
    "        \"max_steps\": tune.choice([500, 1000, 2000]),\n",
    "        \"batch_size\": tune.choice([32, 64, 128, 256]),\n",
    "        \"windows_batch_size\": tune.choice([128, 256, 512, 1024]),\n",
    "        \"loss\": None,\n",
    "        \"random_seed\": tune.randint(1, 20),\n",
    "    }\n",
    "\n",
    "    def __init__(self,\n",
    "                 h,\n",
    "                 loss=MAE(),\n",
    "                 valid_loss=None,\n",
    "                 config=None, \n",
    "                 search_alg=BasicVariantGenerator(random_state=1),\n",
    "                 num_samples=10,\n",
    "                 refit_with_val=False,\n",
    "                 cpus=cpu_count(),\n",
    "                 gpus=torch.cuda.device_count(),\n",
    "                 verbose=False,\n",
    "                 alias=None):\n",
    "        \n",
    "        # Define search space, input/output sizes\n",
    "        if config is None:\n",
    "            config = self.default_config.copy()        \n",
    "            config['input_size'] = tune.choice([h*x \\\n",
    "                         for x in self.default_config[\"input_size_multiplier\"]])\n",
    "\n",
    "            # Rolling windows with step_size=1 or step_size=h\n",
    "            # See `BaseWindows` and `BaseRNN`'s create_windows\n",
    "            config['step_size'] = tune.choice([1, h])\n",
    "            del config[\"input_size_multiplier\"]\n",
    "\n",
    "        super(AutoTFT, self).__init__(\n",
    "              cls_model=TFT, \n",
    "              h=h,\n",
    "              loss=loss,\n",
    "              valid_loss=valid_loss,\n",
    "              config=config,\n",
    "              search_alg=search_alg,\n",
    "              num_samples=num_samples, \n",
    "              refit_with_val=refit_with_val,\n",
    "              cpus=cpus,\n",
    "              gpus=gpus,\n",
    "              verbose=verbose,\n",
    "              alias=alias,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c1ab1077",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/auto.py#L564){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### AutoTFT\n",
       "\n",
       ">      AutoTFT (h, loss=MAE(), valid_loss=None, config=None,\n",
       ">               search_alg=<ray.tune.search.basic_variant.BasicVariantGenerator\n",
       ">               object at 0x7f9942706d60>, num_samples=10, refit_with_val=False,\n",
       ">               cpus=4, gpus=0, verbose=False, alias=None)\n",
       "\n",
       "Class for Automatic Hyperparameter Optimization, it builds on top of `ray` to\n",
       "give access to a wide variety of hyperparameter optimization tools ranging\n",
       "from classic grid search, to Bayesian optimization and HyperBand algorithm.\n",
       "\n",
       "The validation loss to be optimized is defined by the `config['loss']` dictionary\n",
       "value, the config also contains the rest of the hyperparameter search space.\n",
       "\n",
       "It is important to note that the success of this hyperparameter optimization\n",
       "heavily relies on a strong correlation between the validation and test periods.\n",
       "\n",
       "**Parameters:**<br>\n",
       "`cls_model`: PyTorch/PyTorchLightning model, see `neuralforecast.models` [collection here](https://nixtla.github.io/neuralforecast/models.html).<br>\n",
       "`h`: int, forecast horizon.<br>\n",
       "`loss`: PyTorch module, instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
       "`valid_loss`: PyTorch module=`loss`, instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
       "`config`: dict, dictionary with ray.tune defined search space.<br>\n",
       "`search_alg`: ray.tune.search variant, BasicVariantGenerator, HyperOptSearch, DragonflySearch, TuneBOHB for details\n",
       "    see [tune.search](https://docs.ray.io/en/latest/tune/api_docs/suggestion.html#).<br>\n",
       "`num_samples`: int, number of hyperparameter optimization steps/samples.<br>\n",
       "`cpus`: int, number of cpus to use during optimization, default all available.<br>\n",
       "`gpus`: int, number of gpus to use during optimization, default all available.<br>\n",
       "`refit_wo_val`: bool, number of gpus to use during optimization, default all available.<br>\n",
       "`verbose`: bool, wether print partial outputs.<br>\n",
       "`alias`: str, optional,  Custom name of the model.<br>"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/auto.py#L564){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### AutoTFT\n",
       "\n",
       ">      AutoTFT (h, loss=MAE(), valid_loss=None, config=None,\n",
       ">               search_alg=<ray.tune.search.basic_variant.BasicVariantGenerator\n",
       ">               object at 0x7f9942706d60>, num_samples=10, refit_with_val=False,\n",
       ">               cpus=4, gpus=0, verbose=False, alias=None)\n",
       "\n",
       "Class for Automatic Hyperparameter Optimization, it builds on top of `ray` to\n",
       "give access to a wide variety of hyperparameter optimization tools ranging\n",
       "from classic grid search, to Bayesian optimization and HyperBand algorithm.\n",
       "\n",
       "The validation loss to be optimized is defined by the `config['loss']` dictionary\n",
       "value, the config also contains the rest of the hyperparameter search space.\n",
       "\n",
       "It is important to note that the success of this hyperparameter optimization\n",
       "heavily relies on a strong correlation between the validation and test periods.\n",
       "\n",
       "**Parameters:**<br>\n",
       "`cls_model`: PyTorch/PyTorchLightning model, see `neuralforecast.models` [collection here](https://nixtla.github.io/neuralforecast/models.html).<br>\n",
       "`h`: int, forecast horizon.<br>\n",
       "`loss`: PyTorch module, instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
       "`valid_loss`: PyTorch module=`loss`, instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
       "`config`: dict, dictionary with ray.tune defined search space.<br>\n",
       "`search_alg`: ray.tune.search variant, BasicVariantGenerator, HyperOptSearch, DragonflySearch, TuneBOHB for details\n",
       "    see [tune.search](https://docs.ray.io/en/latest/tune/api_docs/suggestion.html#).<br>\n",
       "`num_samples`: int, number of hyperparameter optimization steps/samples.<br>\n",
       "`cpus`: int, number of cpus to use during optimization, default all available.<br>\n",
       "`gpus`: int, number of gpus to use during optimization, default all available.<br>\n",
       "`refit_wo_val`: bool, number of gpus to use during optimization, default all available.<br>\n",
       "`verbose`: bool, wether print partial outputs.<br>\n",
       "`alias`: str, optional,  Custom name of the model.<br>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(AutoTFT, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e283e96f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 1\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "# Use your own config or AutoNHITS.default_config\n",
    "config = dict(max_steps=1, val_check_steps=1, input_size=12, hidden_size=8)\n",
    "model = AutoTFT(h=12, config=config, num_samples=1, cpus=1)\n",
    "\n",
    "# Fit and predict\n",
    "model.fit(dataset=dataset)\n",
    "y_hat = model.predict(dataset=dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f12a74e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# Y_plot_df = Y_test_df[['unique_id', 'ds', 'y']].copy()\n",
    "# Y_plot_df['AutoTFT'] = y_hat\n",
    "\n",
    "# pd.concat([Y_train_df, Y_plot_df]).drop('unique_id', axis=1).set_index('ds').plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "28b5ea6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class AutoVanillaTransformer(BaseAuto):\n",
    "\n",
    "    default_config = {\n",
    "        \"input_size_multiplier\": [1, 2, 3, 4, 5],\n",
    "        \"h\": None,\n",
    "        \"hidden_size\": tune.choice([64, 128, 256]),\n",
    "        \"n_head\": tune.choice([4, 8]),\n",
    "        \"learning_rate\": tune.loguniform(1e-4, 1e-1),\n",
    "        \"scaler_type\": tune.choice([None, 'robust', 'standard']),\n",
    "        \"max_steps\": tune.choice([500, 1000, 2000]),\n",
    "        \"batch_size\": tune.choice([32, 64, 128, 256]),\n",
    "        \"windows_batch_size\": tune.choice([128, 256, 512, 1024]),\n",
    "        \"loss\": None,\n",
    "        \"random_seed\": tune.randint(1, 20),\n",
    "    }\n",
    "\n",
    "    def __init__(self,\n",
    "                 h,\n",
    "                 loss=MAE(),\n",
    "                 valid_loss=None,\n",
    "                 config=None, \n",
    "                 search_alg=BasicVariantGenerator(random_state=1),\n",
    "                 num_samples=10,\n",
    "                 refit_with_val=False,\n",
    "                 cpus=cpu_count(),\n",
    "                 gpus=torch.cuda.device_count(),\n",
    "                 verbose=False,\n",
    "                 alias=None):\n",
    "        \n",
    "        # Define search space, input/output sizes\n",
    "        if config is None:\n",
    "            config = self.default_config.copy()        \n",
    "            config['input_size'] = tune.choice([h*x \\\n",
    "                         for x in self.default_config[\"input_size_multiplier\"]])\n",
    "\n",
    "            # Rolling windows with step_size=1 or step_size=h\n",
    "            # See `BaseWindows` and `BaseRNN`'s create_windows\n",
    "            config['step_size'] = tune.choice([1, h])\n",
    "            del config[\"input_size_multiplier\"]\n",
    "\n",
    "        super(AutoVanillaTransformer, self).__init__(\n",
    "              cls_model=VanillaTransformer, \n",
    "              h=h,\n",
    "              loss=loss,\n",
    "              valid_loss=valid_loss,\n",
    "              config=config,\n",
    "              search_alg=search_alg,\n",
    "              num_samples=num_samples, \n",
    "              refit_with_val=refit_with_val,\n",
    "              cpus=cpus,\n",
    "              gpus=gpus,\n",
    "              verbose=verbose,\n",
    "              alias=alias,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "265a8197",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/auto.py#L621){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### AutoVanillaTransformer\n",
       "\n",
       ">      AutoVanillaTransformer (h, loss=MAE(), valid_loss=None, config=None,\n",
       ">                              search_alg=<ray.tune.search.basic_variant.BasicVa\n",
       ">                              riantGenerator object at 0x7f9942e7d700>,\n",
       ">                              num_samples=10, refit_with_val=False, cpus=4,\n",
       ">                              gpus=0, verbose=False, alias=None)\n",
       "\n",
       "Class for Automatic Hyperparameter Optimization, it builds on top of `ray` to\n",
       "give access to a wide variety of hyperparameter optimization tools ranging\n",
       "from classic grid search, to Bayesian optimization and HyperBand algorithm.\n",
       "\n",
       "The validation loss to be optimized is defined by the `config['loss']` dictionary\n",
       "value, the config also contains the rest of the hyperparameter search space.\n",
       "\n",
       "It is important to note that the success of this hyperparameter optimization\n",
       "heavily relies on a strong correlation between the validation and test periods.\n",
       "\n",
       "**Parameters:**<br>\n",
       "`cls_model`: PyTorch/PyTorchLightning model, see `neuralforecast.models` [collection here](https://nixtla.github.io/neuralforecast/models.html).<br>\n",
       "`h`: int, forecast horizon.<br>\n",
       "`loss`: PyTorch module, instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
       "`valid_loss`: PyTorch module=`loss`, instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
       "`config`: dict, dictionary with ray.tune defined search space.<br>\n",
       "`search_alg`: ray.tune.search variant, BasicVariantGenerator, HyperOptSearch, DragonflySearch, TuneBOHB for details\n",
       "    see [tune.search](https://docs.ray.io/en/latest/tune/api_docs/suggestion.html#).<br>\n",
       "`num_samples`: int, number of hyperparameter optimization steps/samples.<br>\n",
       "`cpus`: int, number of cpus to use during optimization, default all available.<br>\n",
       "`gpus`: int, number of gpus to use during optimization, default all available.<br>\n",
       "`refit_wo_val`: bool, number of gpus to use during optimization, default all available.<br>\n",
       "`verbose`: bool, wether print partial outputs.<br>\n",
       "`alias`: str, optional,  Custom name of the model.<br>"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/auto.py#L621){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### AutoVanillaTransformer\n",
       "\n",
       ">      AutoVanillaTransformer (h, loss=MAE(), valid_loss=None, config=None,\n",
       ">                              search_alg=<ray.tune.search.basic_variant.BasicVa\n",
       ">                              riantGenerator object at 0x7f9942e7d700>,\n",
       ">                              num_samples=10, refit_with_val=False, cpus=4,\n",
       ">                              gpus=0, verbose=False, alias=None)\n",
       "\n",
       "Class for Automatic Hyperparameter Optimization, it builds on top of `ray` to\n",
       "give access to a wide variety of hyperparameter optimization tools ranging\n",
       "from classic grid search, to Bayesian optimization and HyperBand algorithm.\n",
       "\n",
       "The validation loss to be optimized is defined by the `config['loss']` dictionary\n",
       "value, the config also contains the rest of the hyperparameter search space.\n",
       "\n",
       "It is important to note that the success of this hyperparameter optimization\n",
       "heavily relies on a strong correlation between the validation and test periods.\n",
       "\n",
       "**Parameters:**<br>\n",
       "`cls_model`: PyTorch/PyTorchLightning model, see `neuralforecast.models` [collection here](https://nixtla.github.io/neuralforecast/models.html).<br>\n",
       "`h`: int, forecast horizon.<br>\n",
       "`loss`: PyTorch module, instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
       "`valid_loss`: PyTorch module=`loss`, instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
       "`config`: dict, dictionary with ray.tune defined search space.<br>\n",
       "`search_alg`: ray.tune.search variant, BasicVariantGenerator, HyperOptSearch, DragonflySearch, TuneBOHB for details\n",
       "    see [tune.search](https://docs.ray.io/en/latest/tune/api_docs/suggestion.html#).<br>\n",
       "`num_samples`: int, number of hyperparameter optimization steps/samples.<br>\n",
       "`cpus`: int, number of cpus to use during optimization, default all available.<br>\n",
       "`gpus`: int, number of gpus to use during optimization, default all available.<br>\n",
       "`refit_wo_val`: bool, number of gpus to use during optimization, default all available.<br>\n",
       "`verbose`: bool, wether print partial outputs.<br>\n",
       "`alias`: str, optional,  Custom name of the model.<br>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(AutoVanillaTransformer, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b3f3e429",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 1\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "# Use your own config or AutoNHITS.default_config\n",
    "config = dict(max_steps=1, val_check_steps=1, input_size=12, hidden_size=8)\n",
    "model = AutoVanillaTransformer(h=12, config=config, num_samples=1, cpus=1)\n",
    "\n",
    "# Fit and predict\n",
    "model.fit(dataset=dataset)\n",
    "y_hat = model.predict(dataset=dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "02ad28d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# Y_plot_df = Y_test_df[['unique_id', 'ds', 'y']].copy()\n",
    "# Y_plot_df['AutoVanillaTransformer'] = y_hat\n",
    "\n",
    "# pd.concat([Y_train_df, Y_plot_df]).drop('unique_id', axis=1).set_index('ds').plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e9d509ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class AutoInformer(BaseAuto):\n",
    "\n",
    "    default_config = {\n",
    "        \"input_size_multiplier\": [1, 2, 3, 4, 5],\n",
    "        \"h\": None,\n",
    "        \"hidden_size\": tune.choice([64, 128, 256]),\n",
    "        \"n_head\": tune.choice([4, 8]),\n",
    "        \"learning_rate\": tune.loguniform(1e-4, 1e-1),\n",
    "        \"scaler_type\": tune.choice([None, 'robust', 'standard']),\n",
    "        \"max_steps\": tune.choice([500, 1000, 2000]),\n",
    "        \"batch_size\": tune.choice([32, 64, 128, 256]),\n",
    "        \"windows_batch_size\": tune.choice([128, 256, 512, 1024]),\n",
    "        \"loss\": None,\n",
    "        \"random_seed\": tune.randint(1, 20),\n",
    "    }\n",
    "\n",
    "    def __init__(self,\n",
    "                 h,\n",
    "                 loss=MAE(),\n",
    "                 valid_loss=None,\n",
    "                 config=None, \n",
    "                 search_alg=BasicVariantGenerator(random_state=1),\n",
    "                 num_samples=10,\n",
    "                 refit_with_val=False,\n",
    "                 cpus=cpu_count(),\n",
    "                 gpus=torch.cuda.device_count(),\n",
    "                 verbose=False,\n",
    "                 alias=None):\n",
    "        \n",
    "        # Define search space, input/output sizes\n",
    "        if config is None:\n",
    "            config = self.default_config.copy()        \n",
    "            config['input_size'] = tune.choice([h*x \\\n",
    "                         for x in self.default_config[\"input_size_multiplier\"]])\n",
    "\n",
    "            # Rolling windows with step_size=1 or step_size=h\n",
    "            # See `BaseWindows` and `BaseRNN`'s create_windows\n",
    "            config['step_size'] = tune.choice([1, h])\n",
    "            del config[\"input_size_multiplier\"]\n",
    "\n",
    "        super(AutoInformer, self).__init__(\n",
    "              cls_model=Informer, \n",
    "              h=h,\n",
    "              loss=loss,\n",
    "              valid_loss=valid_loss,\n",
    "              config=config,\n",
    "              search_alg=search_alg,\n",
    "              num_samples=num_samples, \n",
    "              refit_with_val=refit_with_val,\n",
    "              cpus=cpus,\n",
    "              gpus=gpus,\n",
    "              verbose=verbose,\n",
    "              alias=alias,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f3c43869",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/auto.py#L678){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### AutoInformer\n",
       "\n",
       ">      AutoInformer (h, loss=MAE(), valid_loss=None, config=None,\n",
       ">                    search_alg=<ray.tune.search.basic_variant.BasicVariantGener\n",
       ">                    ator object at 0x7f9942e9aca0>, num_samples=10,\n",
       ">                    refit_with_val=False, cpus=4, gpus=0, verbose=False,\n",
       ">                    alias=None)\n",
       "\n",
       "Class for Automatic Hyperparameter Optimization, it builds on top of `ray` to\n",
       "give access to a wide variety of hyperparameter optimization tools ranging\n",
       "from classic grid search, to Bayesian optimization and HyperBand algorithm.\n",
       "\n",
       "The validation loss to be optimized is defined by the `config['loss']` dictionary\n",
       "value, the config also contains the rest of the hyperparameter search space.\n",
       "\n",
       "It is important to note that the success of this hyperparameter optimization\n",
       "heavily relies on a strong correlation between the validation and test periods.\n",
       "\n",
       "**Parameters:**<br>\n",
       "`cls_model`: PyTorch/PyTorchLightning model, see `neuralforecast.models` [collection here](https://nixtla.github.io/neuralforecast/models.html).<br>\n",
       "`h`: int, forecast horizon.<br>\n",
       "`loss`: PyTorch module, instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
       "`valid_loss`: PyTorch module=`loss`, instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
       "`config`: dict, dictionary with ray.tune defined search space.<br>\n",
       "`search_alg`: ray.tune.search variant, BasicVariantGenerator, HyperOptSearch, DragonflySearch, TuneBOHB for details\n",
       "    see [tune.search](https://docs.ray.io/en/latest/tune/api_docs/suggestion.html#).<br>\n",
       "`num_samples`: int, number of hyperparameter optimization steps/samples.<br>\n",
       "`cpus`: int, number of cpus to use during optimization, default all available.<br>\n",
       "`gpus`: int, number of gpus to use during optimization, default all available.<br>\n",
       "`refit_wo_val`: bool, number of gpus to use during optimization, default all available.<br>\n",
       "`verbose`: bool, wether print partial outputs.<br>\n",
       "`alias`: str, optional,  Custom name of the model.<br>"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/auto.py#L678){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### AutoInformer\n",
       "\n",
       ">      AutoInformer (h, loss=MAE(), valid_loss=None, config=None,\n",
       ">                    search_alg=<ray.tune.search.basic_variant.BasicVariantGener\n",
       ">                    ator object at 0x7f9942e9aca0>, num_samples=10,\n",
       ">                    refit_with_val=False, cpus=4, gpus=0, verbose=False,\n",
       ">                    alias=None)\n",
       "\n",
       "Class for Automatic Hyperparameter Optimization, it builds on top of `ray` to\n",
       "give access to a wide variety of hyperparameter optimization tools ranging\n",
       "from classic grid search, to Bayesian optimization and HyperBand algorithm.\n",
       "\n",
       "The validation loss to be optimized is defined by the `config['loss']` dictionary\n",
       "value, the config also contains the rest of the hyperparameter search space.\n",
       "\n",
       "It is important to note that the success of this hyperparameter optimization\n",
       "heavily relies on a strong correlation between the validation and test periods.\n",
       "\n",
       "**Parameters:**<br>\n",
       "`cls_model`: PyTorch/PyTorchLightning model, see `neuralforecast.models` [collection here](https://nixtla.github.io/neuralforecast/models.html).<br>\n",
       "`h`: int, forecast horizon.<br>\n",
       "`loss`: PyTorch module, instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
       "`valid_loss`: PyTorch module=`loss`, instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
       "`config`: dict, dictionary with ray.tune defined search space.<br>\n",
       "`search_alg`: ray.tune.search variant, BasicVariantGenerator, HyperOptSearch, DragonflySearch, TuneBOHB for details\n",
       "    see [tune.search](https://docs.ray.io/en/latest/tune/api_docs/suggestion.html#).<br>\n",
       "`num_samples`: int, number of hyperparameter optimization steps/samples.<br>\n",
       "`cpus`: int, number of cpus to use during optimization, default all available.<br>\n",
       "`gpus`: int, number of gpus to use during optimization, default all available.<br>\n",
       "`refit_wo_val`: bool, number of gpus to use during optimization, default all available.<br>\n",
       "`verbose`: bool, wether print partial outputs.<br>\n",
       "`alias`: str, optional,  Custom name of the model.<br>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(AutoInformer, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1648c77b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 1\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "# Use your own config or AutoNHITS.default_config\n",
    "config = dict(max_steps=1, val_check_steps=1, input_size=12, hidden_size=8)\n",
    "model = AutoInformer(h=12, config=config, num_samples=1, cpus=1)\n",
    "\n",
    "# Fit and predict\n",
    "model.fit(dataset=dataset)\n",
    "y_hat = model.predict(dataset=dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2f90be13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# Y_plot_df = Y_test_df[['unique_id', 'ds', 'y']].copy()\n",
    "# Y_plot_df['AutoInformer'] = y_hat\n",
    "\n",
    "# pd.concat([Y_train_df, Y_plot_df]).drop('unique_id', axis=1).set_index('ds').plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d409d246",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class AutoAutoformer(BaseAuto):\n",
    "\n",
    "    default_config = {\n",
    "        \"input_size_multiplier\": [1, 2, 3, 4, 5],\n",
    "        \"h\": None,\n",
    "        \"hidden_size\": tune.choice([64, 128, 256]),\n",
    "        \"n_head\": tune.choice([4, 8]),\n",
    "        \"learning_rate\": tune.loguniform(1e-4, 1e-1),\n",
    "        \"scaler_type\": tune.choice([None, 'robust', 'standard']),\n",
    "        \"max_steps\": tune.choice([500, 1000, 2000]),\n",
    "        \"batch_size\": tune.choice([32, 64, 128, 256]),\n",
    "        \"windows_batch_size\": tune.choice([128, 256, 512, 1024]),\n",
    "        \"loss\": None,\n",
    "        \"random_seed\": tune.randint(1, 20),\n",
    "    }\n",
    "\n",
    "    def __init__(self,\n",
    "                 h,\n",
    "                 loss=MAE(),\n",
    "                 valid_loss=None,\n",
    "                 config=None, \n",
    "                 search_alg=BasicVariantGenerator(random_state=1),\n",
    "                 num_samples=10,\n",
    "                 refit_with_val=False,\n",
    "                 cpus=cpu_count(),\n",
    "                 gpus=torch.cuda.device_count(),\n",
    "                 verbose=False,\n",
    "                 alias=None):\n",
    "        \n",
    "        # Define search space, input/output sizes\n",
    "        if config is None:\n",
    "            config = self.default_config.copy()        \n",
    "            config['input_size'] = tune.choice([h*x \\\n",
    "                         for x in self.default_config[\"input_size_multiplier\"]])\n",
    "\n",
    "            # Rolling windows with step_size=1 or step_size=h\n",
    "            # See `BaseWindows` and `BaseRNN`'s create_windows\n",
    "            config['step_size'] = tune.choice([1, h])\n",
    "            del config[\"input_size_multiplier\"]\n",
    "\n",
    "        super(AutoAutoformer, self).__init__(\n",
    "              cls_model=Autoformer, \n",
    "              h=h,\n",
    "              loss=loss,\n",
    "              valid_loss=valid_loss,\n",
    "              config=config,\n",
    "              search_alg=search_alg,\n",
    "              num_samples=num_samples, \n",
    "              refit_with_val=refit_with_val,\n",
    "              cpus=cpus,\n",
    "              gpus=gpus,\n",
    "              verbose=verbose,\n",
    "              alias=alias,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "38e34860",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/auto.py#L735){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### AutoAutoformer\n",
       "\n",
       ">      AutoAutoformer (h, loss=MAE(), valid_loss=None, config=None,\n",
       ">                      search_alg=<ray.tune.search.basic_variant.BasicVariantGen\n",
       ">                      erator object at 0x7f9943113790>, num_samples=10,\n",
       ">                      refit_with_val=False, cpus=4, gpus=0, verbose=False,\n",
       ">                      alias=None)\n",
       "\n",
       "Class for Automatic Hyperparameter Optimization, it builds on top of `ray` to\n",
       "give access to a wide variety of hyperparameter optimization tools ranging\n",
       "from classic grid search, to Bayesian optimization and HyperBand algorithm.\n",
       "\n",
       "The validation loss to be optimized is defined by the `config['loss']` dictionary\n",
       "value, the config also contains the rest of the hyperparameter search space.\n",
       "\n",
       "It is important to note that the success of this hyperparameter optimization\n",
       "heavily relies on a strong correlation between the validation and test periods.\n",
       "\n",
       "**Parameters:**<br>\n",
       "`cls_model`: PyTorch/PyTorchLightning model, see `neuralforecast.models` [collection here](https://nixtla.github.io/neuralforecast/models.html).<br>\n",
       "`h`: int, forecast horizon.<br>\n",
       "`loss`: PyTorch module, instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
       "`valid_loss`: PyTorch module=`loss`, instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
       "`config`: dict, dictionary with ray.tune defined search space.<br>\n",
       "`search_alg`: ray.tune.search variant, BasicVariantGenerator, HyperOptSearch, DragonflySearch, TuneBOHB for details\n",
       "    see [tune.search](https://docs.ray.io/en/latest/tune/api_docs/suggestion.html#).<br>\n",
       "`num_samples`: int, number of hyperparameter optimization steps/samples.<br>\n",
       "`cpus`: int, number of cpus to use during optimization, default all available.<br>\n",
       "`gpus`: int, number of gpus to use during optimization, default all available.<br>\n",
       "`refit_wo_val`: bool, number of gpus to use during optimization, default all available.<br>\n",
       "`verbose`: bool, wether print partial outputs.<br>\n",
       "`alias`: str, optional,  Custom name of the model.<br>"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/auto.py#L735){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### AutoAutoformer\n",
       "\n",
       ">      AutoAutoformer (h, loss=MAE(), valid_loss=None, config=None,\n",
       ">                      search_alg=<ray.tune.search.basic_variant.BasicVariantGen\n",
       ">                      erator object at 0x7f9943113790>, num_samples=10,\n",
       ">                      refit_with_val=False, cpus=4, gpus=0, verbose=False,\n",
       ">                      alias=None)\n",
       "\n",
       "Class for Automatic Hyperparameter Optimization, it builds on top of `ray` to\n",
       "give access to a wide variety of hyperparameter optimization tools ranging\n",
       "from classic grid search, to Bayesian optimization and HyperBand algorithm.\n",
       "\n",
       "The validation loss to be optimized is defined by the `config['loss']` dictionary\n",
       "value, the config also contains the rest of the hyperparameter search space.\n",
       "\n",
       "It is important to note that the success of this hyperparameter optimization\n",
       "heavily relies on a strong correlation between the validation and test periods.\n",
       "\n",
       "**Parameters:**<br>\n",
       "`cls_model`: PyTorch/PyTorchLightning model, see `neuralforecast.models` [collection here](https://nixtla.github.io/neuralforecast/models.html).<br>\n",
       "`h`: int, forecast horizon.<br>\n",
       "`loss`: PyTorch module, instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
       "`valid_loss`: PyTorch module=`loss`, instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
       "`config`: dict, dictionary with ray.tune defined search space.<br>\n",
       "`search_alg`: ray.tune.search variant, BasicVariantGenerator, HyperOptSearch, DragonflySearch, TuneBOHB for details\n",
       "    see [tune.search](https://docs.ray.io/en/latest/tune/api_docs/suggestion.html#).<br>\n",
       "`num_samples`: int, number of hyperparameter optimization steps/samples.<br>\n",
       "`cpus`: int, number of cpus to use during optimization, default all available.<br>\n",
       "`gpus`: int, number of gpus to use during optimization, default all available.<br>\n",
       "`refit_wo_val`: bool, number of gpus to use during optimization, default all available.<br>\n",
       "`verbose`: bool, wether print partial outputs.<br>\n",
       "`alias`: str, optional,  Custom name of the model.<br>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(AutoAutoformer, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "38d4b216",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 1\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "# Use your own config or AutoNHITS.default_config\n",
    "config = dict(max_steps=1, val_check_steps=1, input_size=12, hidden_size=8)\n",
    "model = AutoAutoformer(h=12, config=config, num_samples=1, cpus=1)\n",
    "\n",
    "# Fit and predict\n",
    "model.fit(dataset=dataset)\n",
    "y_hat = model.predict(dataset=dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b5811961",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# Y_plot_df = Y_test_df[['unique_id', 'ds', 'y']].copy()\n",
    "# Y_plot_df['AutoAutoformer'] = y_hat\n",
    "\n",
    "# pd.concat([Y_train_df, Y_plot_df]).drop('unique_id', axis=1).set_index('ds').plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ddb3cde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class AutoPatchTST(BaseAuto):\n",
    "\n",
    "    default_config = {\n",
    "        \"input_size_multiplier\": [1, 2, 3],\n",
    "        \"h\": None,\n",
    "        \"hidden_size\": tune.choice([16, 128, 256]),\n",
    "        \"n_head\": tune.choice([4, 16]),\n",
    "        \"patch_len\": tune.choice([16, 24]),\n",
    "        \"learning_rate\": tune.loguniform(1e-4, 1e-1),\n",
    "        \"scaler_type\": tune.choice([None, 'robust', 'standard']),\n",
    "        \"revin\": tune.choice([False, True]),\n",
    "        \"max_steps\": tune.choice([500, 1000, 5000]),\n",
    "        \"batch_size\": tune.choice([32, 64, 128, 256]),\n",
    "        \"windows_batch_size\": tune.choice([128, 256, 512, 1024]),\n",
    "        \"loss\": None,\n",
    "        \"random_seed\": tune.randint(1, 20),\n",
    "    }\n",
    "\n",
    "    def __init__(self,\n",
    "                 h,\n",
    "                 loss=MAE(),\n",
    "                 valid_loss=None,\n",
    "                 config=None, \n",
    "                 search_alg=BasicVariantGenerator(random_state=1),\n",
    "                 num_samples=10,\n",
    "                 refit_with_val=False,\n",
    "                 cpus=cpu_count(),\n",
    "                 gpus=torch.cuda.device_count(),\n",
    "                 verbose=False,\n",
    "                 alias=None):\n",
    "        \n",
    "        # Define search space, input/output sizes\n",
    "        if config is None:\n",
    "            config = self.default_config.copy()        \n",
    "            config['input_size'] = tune.choice([h*x \\\n",
    "                         for x in self.default_config[\"input_size_multiplier\"]])\n",
    "\n",
    "            # Rolling windows with step_size=1 or step_size=h\n",
    "            # See `BaseWindows` and `BaseRNN`'s create_windows\n",
    "            config['step_size'] = tune.choice([1, h])\n",
    "            del config[\"input_size_multiplier\"]\n",
    "\n",
    "        super(AutoPatchTST, self).__init__(\n",
    "              cls_model=PatchTST, \n",
    "              h=h,\n",
    "              loss=loss,\n",
    "              valid_loss=valid_loss,\n",
    "              config=config,\n",
    "              search_alg=search_alg,\n",
    "              num_samples=num_samples, \n",
    "              refit_with_val=refit_with_val,\n",
    "              cpus=cpus,\n",
    "              gpus=gpus,\n",
    "              verbose=verbose,\n",
    "              alias=alias,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9d807209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/auto.py#L792){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### AutoPatchTST\n",
       "\n",
       ">      AutoPatchTST (h, loss=MAE(), valid_loss=None, config=None,\n",
       ">                    search_alg=<ray.tune.search.basic_variant.BasicVariantGener\n",
       ">                    ator object at 0x7f9943119070>, num_samples=10,\n",
       ">                    refit_with_val=False, cpus=4, gpus=0, verbose=False,\n",
       ">                    alias=None)\n",
       "\n",
       "Class for Automatic Hyperparameter Optimization, it builds on top of `ray` to\n",
       "give access to a wide variety of hyperparameter optimization tools ranging\n",
       "from classic grid search, to Bayesian optimization and HyperBand algorithm.\n",
       "\n",
       "The validation loss to be optimized is defined by the `config['loss']` dictionary\n",
       "value, the config also contains the rest of the hyperparameter search space.\n",
       "\n",
       "It is important to note that the success of this hyperparameter optimization\n",
       "heavily relies on a strong correlation between the validation and test periods.\n",
       "\n",
       "**Parameters:**<br>\n",
       "`cls_model`: PyTorch/PyTorchLightning model, see `neuralforecast.models` [collection here](https://nixtla.github.io/neuralforecast/models.html).<br>\n",
       "`h`: int, forecast horizon.<br>\n",
       "`loss`: PyTorch module, instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
       "`valid_loss`: PyTorch module=`loss`, instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
       "`config`: dict, dictionary with ray.tune defined search space.<br>\n",
       "`search_alg`: ray.tune.search variant, BasicVariantGenerator, HyperOptSearch, DragonflySearch, TuneBOHB for details\n",
       "    see [tune.search](https://docs.ray.io/en/latest/tune/api_docs/suggestion.html#).<br>\n",
       "`num_samples`: int, number of hyperparameter optimization steps/samples.<br>\n",
       "`cpus`: int, number of cpus to use during optimization, default all available.<br>\n",
       "`gpus`: int, number of gpus to use during optimization, default all available.<br>\n",
       "`refit_wo_val`: bool, number of gpus to use during optimization, default all available.<br>\n",
       "`verbose`: bool, wether print partial outputs.<br>\n",
       "`alias`: str, optional,  Custom name of the model.<br>"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/auto.py#L792){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### AutoPatchTST\n",
       "\n",
       ">      AutoPatchTST (h, loss=MAE(), valid_loss=None, config=None,\n",
       ">                    search_alg=<ray.tune.search.basic_variant.BasicVariantGener\n",
       ">                    ator object at 0x7f9943119070>, num_samples=10,\n",
       ">                    refit_with_val=False, cpus=4, gpus=0, verbose=False,\n",
       ">                    alias=None)\n",
       "\n",
       "Class for Automatic Hyperparameter Optimization, it builds on top of `ray` to\n",
       "give access to a wide variety of hyperparameter optimization tools ranging\n",
       "from classic grid search, to Bayesian optimization and HyperBand algorithm.\n",
       "\n",
       "The validation loss to be optimized is defined by the `config['loss']` dictionary\n",
       "value, the config also contains the rest of the hyperparameter search space.\n",
       "\n",
       "It is important to note that the success of this hyperparameter optimization\n",
       "heavily relies on a strong correlation between the validation and test periods.\n",
       "\n",
       "**Parameters:**<br>\n",
       "`cls_model`: PyTorch/PyTorchLightning model, see `neuralforecast.models` [collection here](https://nixtla.github.io/neuralforecast/models.html).<br>\n",
       "`h`: int, forecast horizon.<br>\n",
       "`loss`: PyTorch module, instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
       "`valid_loss`: PyTorch module=`loss`, instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
       "`config`: dict, dictionary with ray.tune defined search space.<br>\n",
       "`search_alg`: ray.tune.search variant, BasicVariantGenerator, HyperOptSearch, DragonflySearch, TuneBOHB for details\n",
       "    see [tune.search](https://docs.ray.io/en/latest/tune/api_docs/suggestion.html#).<br>\n",
       "`num_samples`: int, number of hyperparameter optimization steps/samples.<br>\n",
       "`cpus`: int, number of cpus to use during optimization, default all available.<br>\n",
       "`gpus`: int, number of gpus to use during optimization, default all available.<br>\n",
       "`refit_wo_val`: bool, number of gpus to use during optimization, default all available.<br>\n",
       "`verbose`: bool, wether print partial outputs.<br>\n",
       "`alias`: str, optional,  Custom name of the model.<br>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(AutoPatchTST, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "13878573",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 1\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "# Use your own config or AutoNHITS.default_config\n",
    "config = dict(max_steps=1, val_check_steps=1, input_size=12, hidden_size=16)\n",
    "model = AutoPatchTST(h=12, config=config, num_samples=1, cpus=1)\n",
    "\n",
    "# Fit and predict\n",
    "model.fit(dataset=dataset)\n",
    "y_hat = model.predict(dataset=dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9c4c0195",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# Y_plot_df = Y_test_df[['unique_id', 'ds', 'y']].copy()\n",
    "# Y_plot_df['AutoPatchTST'] = y_hat\n",
    "\n",
    "# pd.concat([Y_train_df, Y_plot_df]).drop('unique_id', axis=1).set_index('ds').plot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e6fd22c7",
   "metadata": {},
   "source": [
    "## <span style=\"color:DarkBlue\"> D. Multivariate </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b6784e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class AutoStemGNN(BaseAuto):\n",
    "\n",
    "    default_config = {\n",
    "        \"input_size_multiplier\": [1, 2, 3, 4],\n",
    "        \"h\": None,\n",
    "        \"n_series\": None,\n",
    "        \"n_stacks\": tune.choice([2, 3]),\n",
    "        \"multi_layer\": tune.choice([3, 5, 7]),\n",
    "        \"learning_rate\": tune.loguniform(1e-4, 1e-1),\n",
    "        \"scaler_type\": tune.choice([None, 'robust', 'standard']),\n",
    "        \"max_steps\": tune.choice([500, 1000, 2000]),\n",
    "        \"batch_size\": tune.choice([32, 64, 128, 256]),\n",
    "        \"loss\": None,\n",
    "        \"random_seed\": tune.randint(1, 20),\n",
    "    }\n",
    "\n",
    "    def __init__(self,\n",
    "                 h,\n",
    "                 n_series,\n",
    "                 loss=MAE(),\n",
    "                 valid_loss=None,\n",
    "                 config=None, \n",
    "                 search_alg=BasicVariantGenerator(random_state=1),\n",
    "                 num_samples=10,\n",
    "                 refit_with_val=False,\n",
    "                 cpus=cpu_count(),\n",
    "                 gpus=torch.cuda.device_count(),\n",
    "                 verbose=False,\n",
    "                 alias=None):\n",
    "        \n",
    "        # Define search space, input/output sizes\n",
    "        if config is None:\n",
    "            config = self.default_config.copy()        \n",
    "            config['input_size'] = tune.choice([h*x \\\n",
    "                         for x in self.default_config[\"input_size_multiplier\"]])\n",
    "\n",
    "            # Rolling windows with step_size=1 or step_size=h\n",
    "            # See `BaseWindows` and `BaseRNN`'s create_windows\n",
    "            config['step_size'] = tune.choice([1, h])\n",
    "            del config[\"input_size_multiplier\"]\n",
    "\n",
    "        # Always use n_series from parameters\n",
    "        config['n_series'] = n_series\n",
    "\n",
    "        super(AutoStemGNN, self).__init__(\n",
    "              cls_model=StemGNN, \n",
    "              h=h,\n",
    "              loss=loss,\n",
    "              valid_loss=valid_loss,\n",
    "              config=config,\n",
    "              search_alg=search_alg,\n",
    "              num_samples=num_samples, \n",
    "              refit_with_val=refit_with_val,\n",
    "              cpus=cpus,\n",
    "              gpus=gpus,\n",
    "              verbose=verbose,\n",
    "              alias=alias,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1163c1de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/auto.py#L851){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### AutoStemGNN\n",
       "\n",
       ">      AutoStemGNN (h, n_series, loss=MAE(), valid_loss=None, config=None,\n",
       ">                   search_alg=<ray.tune.search.basic_variant.BasicVariantGenera\n",
       ">                   tor object at 0x7f99430ee1f0>, num_samples=10,\n",
       ">                   refit_with_val=False, cpus=4, gpus=0, verbose=False,\n",
       ">                   alias=None)\n",
       "\n",
       "Class for Automatic Hyperparameter Optimization, it builds on top of `ray` to\n",
       "give access to a wide variety of hyperparameter optimization tools ranging\n",
       "from classic grid search, to Bayesian optimization and HyperBand algorithm.\n",
       "\n",
       "The validation loss to be optimized is defined by the `config['loss']` dictionary\n",
       "value, the config also contains the rest of the hyperparameter search space.\n",
       "\n",
       "It is important to note that the success of this hyperparameter optimization\n",
       "heavily relies on a strong correlation between the validation and test periods.\n",
       "\n",
       "**Parameters:**<br>\n",
       "`cls_model`: PyTorch/PyTorchLightning model, see `neuralforecast.models` [collection here](https://nixtla.github.io/neuralforecast/models.html).<br>\n",
       "`h`: int, forecast horizon.<br>\n",
       "`loss`: PyTorch module, instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
       "`valid_loss`: PyTorch module=`loss`, instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
       "`config`: dict, dictionary with ray.tune defined search space.<br>\n",
       "`search_alg`: ray.tune.search variant, BasicVariantGenerator, HyperOptSearch, DragonflySearch, TuneBOHB for details\n",
       "    see [tune.search](https://docs.ray.io/en/latest/tune/api_docs/suggestion.html#).<br>\n",
       "`num_samples`: int, number of hyperparameter optimization steps/samples.<br>\n",
       "`cpus`: int, number of cpus to use during optimization, default all available.<br>\n",
       "`gpus`: int, number of gpus to use during optimization, default all available.<br>\n",
       "`refit_wo_val`: bool, number of gpus to use during optimization, default all available.<br>\n",
       "`verbose`: bool, wether print partial outputs.<br>\n",
       "`alias`: str, optional,  Custom name of the model.<br>"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/auto.py#L851){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### AutoStemGNN\n",
       "\n",
       ">      AutoStemGNN (h, n_series, loss=MAE(), valid_loss=None, config=None,\n",
       ">                   search_alg=<ray.tune.search.basic_variant.BasicVariantGenera\n",
       ">                   tor object at 0x7f99430ee1f0>, num_samples=10,\n",
       ">                   refit_with_val=False, cpus=4, gpus=0, verbose=False,\n",
       ">                   alias=None)\n",
       "\n",
       "Class for Automatic Hyperparameter Optimization, it builds on top of `ray` to\n",
       "give access to a wide variety of hyperparameter optimization tools ranging\n",
       "from classic grid search, to Bayesian optimization and HyperBand algorithm.\n",
       "\n",
       "The validation loss to be optimized is defined by the `config['loss']` dictionary\n",
       "value, the config also contains the rest of the hyperparameter search space.\n",
       "\n",
       "It is important to note that the success of this hyperparameter optimization\n",
       "heavily relies on a strong correlation between the validation and test periods.\n",
       "\n",
       "**Parameters:**<br>\n",
       "`cls_model`: PyTorch/PyTorchLightning model, see `neuralforecast.models` [collection here](https://nixtla.github.io/neuralforecast/models.html).<br>\n",
       "`h`: int, forecast horizon.<br>\n",
       "`loss`: PyTorch module, instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
       "`valid_loss`: PyTorch module=`loss`, instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
       "`config`: dict, dictionary with ray.tune defined search space.<br>\n",
       "`search_alg`: ray.tune.search variant, BasicVariantGenerator, HyperOptSearch, DragonflySearch, TuneBOHB for details\n",
       "    see [tune.search](https://docs.ray.io/en/latest/tune/api_docs/suggestion.html#).<br>\n",
       "`num_samples`: int, number of hyperparameter optimization steps/samples.<br>\n",
       "`cpus`: int, number of cpus to use during optimization, default all available.<br>\n",
       "`gpus`: int, number of gpus to use during optimization, default all available.<br>\n",
       "`refit_wo_val`: bool, number of gpus to use during optimization, default all available.<br>\n",
       "`verbose`: bool, wether print partial outputs.<br>\n",
       "`alias`: str, optional,  Custom name of the model.<br>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(AutoStemGNN, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ca99c2ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 1\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "# Use your own config or AutoNHITS.default_config\n",
    "config = dict(max_steps=1, val_check_steps=1, input_size=12)\n",
    "model = AutoStemGNN(h=12, n_series=1, config=config, num_samples=1, cpus=1)\n",
    "\n",
    "# Fit and predict\n",
    "model.fit(dataset=dataset)\n",
    "y_hat = model.predict(dataset=dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "62652150",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# Y_plot_df = Y_test_df[['unique_id', 'ds', 'y']].copy()\n",
    "# Y_plot_df['AutoStemGNN'] = y_hat\n",
    "\n",
    "# pd.concat([Y_train_df, Y_plot_df]).drop('unique_id', axis=1).set_index('ds').plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "58844aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class AutoHINT(BaseAuto):\n",
    "\n",
    "    def __init__(self,\n",
    "                 cls_model,\n",
    "                 h,\n",
    "                 loss,\n",
    "                 valid_loss,\n",
    "                 S,\n",
    "                 config,\n",
    "                 search_alg=BasicVariantGenerator(random_state=1),\n",
    "                 num_samples=10,\n",
    "                 cpus=cpu_count(),\n",
    "                 gpus=torch.cuda.device_count(),\n",
    "                 refit_with_val=False,\n",
    "                 verbose=False,\n",
    "                 alias=None):\n",
    "        super(AutoHINT, self).__init__(\n",
    "              cls_model=cls_model, \n",
    "              h=h,\n",
    "              loss=loss,\n",
    "              valid_loss=valid_loss,\n",
    "              config=config,\n",
    "              search_alg=search_alg,\n",
    "              num_samples=num_samples, \n",
    "              refit_with_val=refit_with_val,\n",
    "              cpus=cpus,\n",
    "              gpus=gpus,\n",
    "              verbose=verbose,\n",
    "              alias=alias,\n",
    "        )\n",
    "        # Validate presence of reconciliation strategy\n",
    "        # parameter in configuration space\n",
    "        if not ('reconciliation' in config.keys()):\n",
    "            raise Exception(\"config needs reconciliation, \\\n",
    "                            try tune.choice(['BottomUp', 'MinTraceOLS', 'MinTraceWLS'])\")\n",
    "        self.S = S\n",
    "\n",
    "    def _fit_model(self, cls_model, config,\n",
    "                   dataset, val_size, test_size):\n",
    "        # Overwrite _fit_model for HINT two-stage instantiation\n",
    "        reconciliation = config.pop('reconciliation')\n",
    "        base_model = cls_model(**config)\n",
    "        model = HINT(h=base_model.h, model=base_model, \n",
    "                     S=self.S, reconciliation=reconciliation)\n",
    "        model.fit(\n",
    "            dataset,\n",
    "            val_size=val_size, \n",
    "            test_size=test_size\n",
    "        )\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "622a0888",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/auto.py#L912){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### AutoHINT\n",
       "\n",
       ">      AutoHINT (cls_model, h, loss, valid_loss, S, config,\n",
       ">                search_alg=<ray.tune.search.basic_variant.BasicVariantGenerator\n",
       ">                object at 0x7f99434f8670>, num_samples=10, cpus=4, gpus=0,\n",
       ">                refit_with_val=False, verbose=False, alias=None)\n",
       "\n",
       "Class for Automatic Hyperparameter Optimization, it builds on top of `ray` to\n",
       "give access to a wide variety of hyperparameter optimization tools ranging\n",
       "from classic grid search, to Bayesian optimization and HyperBand algorithm.\n",
       "\n",
       "The validation loss to be optimized is defined by the `config['loss']` dictionary\n",
       "value, the config also contains the rest of the hyperparameter search space.\n",
       "\n",
       "It is important to note that the success of this hyperparameter optimization\n",
       "heavily relies on a strong correlation between the validation and test periods.\n",
       "\n",
       "**Parameters:**<br>\n",
       "`cls_model`: PyTorch/PyTorchLightning model, see `neuralforecast.models` [collection here](https://nixtla.github.io/neuralforecast/models.html).<br>\n",
       "`h`: int, forecast horizon.<br>\n",
       "`loss`: PyTorch module, instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
       "`valid_loss`: PyTorch module=`loss`, instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
       "`config`: dict, dictionary with ray.tune defined search space.<br>\n",
       "`search_alg`: ray.tune.search variant, BasicVariantGenerator, HyperOptSearch, DragonflySearch, TuneBOHB for details\n",
       "    see [tune.search](https://docs.ray.io/en/latest/tune/api_docs/suggestion.html#).<br>\n",
       "`num_samples`: int, number of hyperparameter optimization steps/samples.<br>\n",
       "`cpus`: int, number of cpus to use during optimization, default all available.<br>\n",
       "`gpus`: int, number of gpus to use during optimization, default all available.<br>\n",
       "`refit_wo_val`: bool, number of gpus to use during optimization, default all available.<br>\n",
       "`verbose`: bool, wether print partial outputs.<br>\n",
       "`alias`: str, optional,  Custom name of the model.<br>"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/auto.py#L912){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### AutoHINT\n",
       "\n",
       ">      AutoHINT (cls_model, h, loss, valid_loss, S, config,\n",
       ">                search_alg=<ray.tune.search.basic_variant.BasicVariantGenerator\n",
       ">                object at 0x7f99434f8670>, num_samples=10, cpus=4, gpus=0,\n",
       ">                refit_with_val=False, verbose=False, alias=None)\n",
       "\n",
       "Class for Automatic Hyperparameter Optimization, it builds on top of `ray` to\n",
       "give access to a wide variety of hyperparameter optimization tools ranging\n",
       "from classic grid search, to Bayesian optimization and HyperBand algorithm.\n",
       "\n",
       "The validation loss to be optimized is defined by the `config['loss']` dictionary\n",
       "value, the config also contains the rest of the hyperparameter search space.\n",
       "\n",
       "It is important to note that the success of this hyperparameter optimization\n",
       "heavily relies on a strong correlation between the validation and test periods.\n",
       "\n",
       "**Parameters:**<br>\n",
       "`cls_model`: PyTorch/PyTorchLightning model, see `neuralforecast.models` [collection here](https://nixtla.github.io/neuralforecast/models.html).<br>\n",
       "`h`: int, forecast horizon.<br>\n",
       "`loss`: PyTorch module, instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
       "`valid_loss`: PyTorch module=`loss`, instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
       "`config`: dict, dictionary with ray.tune defined search space.<br>\n",
       "`search_alg`: ray.tune.search variant, BasicVariantGenerator, HyperOptSearch, DragonflySearch, TuneBOHB for details\n",
       "    see [tune.search](https://docs.ray.io/en/latest/tune/api_docs/suggestion.html#).<br>\n",
       "`num_samples`: int, number of hyperparameter optimization steps/samples.<br>\n",
       "`cpus`: int, number of cpus to use during optimization, default all available.<br>\n",
       "`gpus`: int, number of gpus to use during optimization, default all available.<br>\n",
       "`refit_wo_val`: bool, number of gpus to use during optimization, default all available.<br>\n",
       "`verbose`: bool, wether print partial outputs.<br>\n",
       "`alias`: str, optional,  Custom name of the model.<br>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(AutoHINT, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "bf210740",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "def sort_df_hier(Y_df, S_df):\n",
    "    # NeuralForecast core, sorts unique_id lexicographically\n",
    "    # by default, this class matches S_df and Y_hat_df order.    \n",
    "    Y_df.unique_id = Y_df.unique_id.astype('category')\n",
    "    Y_df.unique_id = Y_df.unique_id.cat.set_categories(S_df.index)\n",
    "    Y_df = Y_df.sort_values(by=['unique_id', 'ds'])\n",
    "    return Y_df\n",
    "\n",
    "# -----Create synthetic dataset-----\n",
    "np.random.seed(123)\n",
    "train_steps = 20\n",
    "num_levels = 7\n",
    "level = np.arange(0, 100, 0.1)\n",
    "qs = [[50-lv/2, 50+lv/2] for lv in level]\n",
    "quantiles = np.sort(np.concatenate(qs)/100)\n",
    "\n",
    "levels = ['Top', 'Mid1', 'Mid2', 'Bottom1', 'Bottom2', 'Bottom3', 'Bottom4']\n",
    "unique_ids = np.repeat(levels, train_steps)\n",
    "\n",
    "S = np.array([[1., 1., 1., 1.],\n",
    "              [1., 1., 0., 0.],\n",
    "              [0., 0., 1., 1.],\n",
    "              [1., 0., 0., 0.],\n",
    "              [0., 1., 0., 0.],\n",
    "              [0., 0., 1., 0.],\n",
    "              [0., 0., 0., 1.]])\n",
    "\n",
    "S_dict = {col: S[:, i] for i, col in enumerate(levels[3:])}\n",
    "S_df = pd.DataFrame(S_dict, index=levels)\n",
    "\n",
    "ds = pd.date_range(start='2018-03-31', periods=train_steps, freq='Q').tolist() * num_levels\n",
    "# Create Y_df\n",
    "y_lists = [S @ np.random.uniform(low=100, high=500, size=4) for i in range(train_steps)]\n",
    "y = [elem for tup in zip(*y_lists) for elem in tup]\n",
    "Y_df = pd.DataFrame({'unique_id': unique_ids, 'ds': ds, 'y': y})\n",
    "Y_df = sort_df_hier(Y_df, S_df)\n",
    "\n",
    "hint_dataset, *_ = TimeSeriesDataset.from_df(df=Y_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d41f07c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 1\n",
      "Global seed set to 6\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "# Perform a simple hyperparameter optimization with \n",
    "# NHITS and then reconcile with HINT\n",
    "from neuralforecast.losses.pytorch import GMM, sCRPS\n",
    "\n",
    "base_config = dict(max_steps=1, val_check_steps=1, input_size=8)\n",
    "base_model = AutoNHITS(h=4, loss=GMM(n_components=2, quantiles=quantiles), \n",
    "                       config=base_config, num_samples=1, cpus=1)\n",
    "model = HINT(h=4, S=S_df.values,\n",
    "             model=base_model,  reconciliation='MinTraceOLS')\n",
    "\n",
    "model.fit(dataset=dataset)\n",
    "y_hat = model.predict(dataset=hint_dataset)\n",
    "\n",
    "# Perform a conjunct hyperparameter optimization with \n",
    "# NHITS + HINT reconciliation configurations\n",
    "nhits_config = {\n",
    "       \"learning_rate\": tune.choice([1e-3]),                                     # Initial Learning rate\n",
    "       \"max_steps\": tune.choice([1]),                                            # Number of SGD steps\n",
    "       \"val_check_steps\": tune.choice([1]),                                      # Number of steps between validation\n",
    "       \"input_size\": tune.choice([5 * 12]),                                      # input_size = multiplier * horizon\n",
    "       \"batch_size\": tune.choice([7]),                                           # Number of series in windows\n",
    "       \"windows_batch_size\": tune.choice([256]),                                 # Number of windows in batch\n",
    "       \"n_pool_kernel_size\": tune.choice([[2, 2, 2], [16, 8, 1]]),               # MaxPool's Kernelsize\n",
    "       \"n_freq_downsample\": tune.choice([[168, 24, 1], [24, 12, 1], [1, 1, 1]]), # Interpolation expressivity ratios\n",
    "       \"activation\": tune.choice(['ReLU']),                                      # Type of non-linear activation\n",
    "       \"n_blocks\":  tune.choice([[1, 1, 1]]),                                    # Blocks per each 3 stacks\n",
    "       \"mlp_units\":  tune.choice([[[512, 512], [512, 512], [512, 512]]]),        # 2 512-Layers per block for each stack\n",
    "       \"interpolation_mode\": tune.choice(['linear']),                            # Type of multi-step interpolation\n",
    "       \"random_seed\": tune.randint(1, 10),\n",
    "       \"reconciliation\": tune.choice(['BottomUp', 'MinTraceOLS', 'MinTraceWLS'])\n",
    "    }\n",
    "model = AutoHINT(h=4, S=S_df.values,\n",
    "                 cls_model=NHITS,\n",
    "                 config=nhits_config,\n",
    "                 loss=GMM(n_components=2, level=[80, 90]),\n",
    "                 valid_loss=sCRPS(level=[80, 90]),\n",
    "                 num_samples=1, cpus=1)\n",
    "model.fit(dataset=dataset)\n",
    "y_hat = model.predict(dataset=hint_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d588d9",
   "metadata": {},
   "source": [
    "# TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "fd3a085a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_train_tune pid=18044)\u001b[0m Global seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Epoch 0:   0%|          | 0/1 [00:00<?, ?it/s]                             \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 1\n",
      "\u001b[2m\u001b[36m(_train_tune pid=18044)\u001b[0m /opt/anaconda3/envs/neuralforecast/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n",
      "\u001b[2m\u001b[36m(_train_tune pid=18044)\u001b[0m   warnings.warn('resource_tracker: There appear to be %d '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|| 1/1 [00:00<00:00,  3.94it/s, v_num=0, train_loss_step=39.50]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A[0m \n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0: 100%|| 1/1 [00:00<00:00,  3.78it/s, v_num=0, train_loss_step=39.50, valid_loss=112.0]\n",
      "Epoch 0: 100%|| 1/1 [00:00<00:00,  3.65it/s, v_num=0, train_loss_step=39.50, valid_loss=112.0, train_loss_epoch=39.50]\n",
      "Epoch 0: 100%|| 1/1 [00:00<00:00,  5.31it/s, v_num=370, train_loss_step=39.50, valid_loss=112.0, train_loss_epoch=39.50]\n",
      "Predicting DataLoader 0: 100%|| 1/1 [00:00<00:00, 127.78it/s]\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "## TESTS\n",
    "config = dict(max_steps=1, val_check_steps=1, input_size=12)\n",
    "model = AutoNHITS(h=12, config=config, num_samples=1, cpus=1)\n",
    "\n",
    "# Fit and predict\n",
    "model.fit(dataset=dataset)\n",
    "y_hat = model.predict(dataset=dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "535cf3a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_train_tune pid=18056)\u001b[0m Global seed set to 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/1 [00:00<?, ?it/s]                             \n",
      "Epoch 0: 100%|| 1/1 [00:00<00:00, 10.01it/s, v_num=0, train_loss_step=52.60]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A[0m \n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0: 100%|| 1/1 [00:00<00:00,  9.05it/s, v_num=0, train_loss_step=52.60, valid_loss=3.45e+4]\n",
      "Epoch 0: 100%|| 1/1 [00:00<00:00,  8.38it/s, v_num=0, train_loss_step=52.60, valid_loss=3.45e+4, train_loss_epoch=52.60]\n",
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_train_tune pid=18056)\u001b[0m /opt/anaconda3/envs/neuralforecast/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n",
      "\u001b[2m\u001b[36m(_train_tune pid=18056)\u001b[0m   warnings.warn('resource_tracker: There appear to be %d '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|| 1/1 [00:00<00:00,  6.78it/s, v_num=372, train_loss_step=52.60, valid_loss=3.45e+4, train_loss_epoch=52.60]\n",
      "Predicting DataLoader 0: 100%|| 1/1 [00:00<00:00, 156.60it/s]\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "## TESTS\n",
    "nhits_config = {\n",
    "       \"learning_rate\": tune.choice([1e-3]),                                     # Initial Learning rate\n",
    "       \"max_steps\": tune.choice([1]),                                            # Number of SGD steps\n",
    "       \"val_check_steps\": tune.choice([1]),                                      # Number of steps between validation\n",
    "       \"input_size\": tune.choice([5 * 12]),                                      # input_size = multiplier * horizon\n",
    "       \"batch_size\": tune.choice([7]),                                           # Number of series in windows\n",
    "       \"windows_batch_size\": tune.choice([256]),                                 # Number of windows in batch\n",
    "       \"n_pool_kernel_size\": tune.choice([[2, 2, 2], [16, 8, 1]]),               # MaxPool's Kernelsize\n",
    "       \"n_freq_downsample\": tune.choice([[168, 24, 1], [24, 12, 1], [1, 1, 1]]), # Interpolation expressivity ratios\n",
    "       \"activation\": tune.choice(['ReLU']),                                      # Type of non-linear activation\n",
    "       \"n_blocks\":  tune.choice([[1, 1, 1]]),                                    # Blocks per each 3 stacks\n",
    "       \"mlp_units\":  tune.choice([[[512, 512], [512, 512], [512, 512]]]),        # 2 512-Layers per block for each stack\n",
    "       \"interpolation_mode\": tune.choice(['linear']),                            # Type of multi-step interpolation\n",
    "       \"random_seed\": tune.randint(1, 10),\n",
    "    }\n",
    "\n",
    "model = AutoNHITS(h=12, loss=MAE(), valid_loss=MSE(), config=nhits_config, num_samples=1, cpus=1)\n",
    "\n",
    "# Fit and predict\n",
    "model.fit(dataset=dataset)\n",
    "y_hat = model.predict(dataset=dataset)\n",
    "\n",
    "# Test equality\n",
    "test_eq(str(type(model.valid_loss)), \"<class 'neuralforecast.losses.pytorch.MSE'>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "56493784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_train_tune pid=18074)\u001b[0m Global seed set to 9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 9\n",
      "\u001b[2m\u001b[36m(_train_tune pid=18074)\u001b[0m /opt/anaconda3/envs/neuralforecast/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n",
      "\u001b[2m\u001b[36m(_train_tune pid=18074)\u001b[0m   warnings.warn('resource_tracker: There appear to be %d '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|| 1/1 [00:00<00:00, 56.02it/s, v_num=0, train_loss_step=5.510]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A[0m \n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(_train_tune pid=18074)\u001b[0m \n",
      "Epoch 0: 100%|| 1/1 [00:00<00:00, 23.00it/s, v_num=0, train_loss_step=5.510, valid_loss=0.218]\n",
      "Epoch 0: 100%|| 1/1 [00:00<00:00, 19.01it/s, v_num=0, train_loss_step=5.510, valid_loss=0.218, train_loss_epoch=5.510]\n",
      "Epoch 0: 100%|| 1/1 [00:00<00:00, 10.89it/s, v_num=376, train_loss_step=5.510, valid_loss=0.218, train_loss_epoch=5.510]\n",
      "Predicting DataLoader 0: 100%|| 1/1 [00:00<00:00,  7.55it/s]\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "## TODO: Add unit tests for interactions between loss/valid_loss types\n",
    "## TODO: Unit tests (2 types of networks x 2 types of loss x 2 types of valid loss)\n",
    "from neuralforecast.losses.pytorch import GMM, sCRPS\n",
    "\n",
    "## Checking if base recurrent methods run point valid_loss correctly\n",
    "tcn_config = {\n",
    "       \"learning_rate\": tune.choice([1e-3]),                                     # Initial Learning rate\n",
    "       \"max_steps\": tune.choice([1]),                                            # Number of SGD steps\n",
    "       \"val_check_steps\": tune.choice([1]),                                      # Number of steps between validation\n",
    "       \"input_size\": tune.choice([5 * 12]),                                      # input_size = multiplier * horizon\n",
    "       \"batch_size\": tune.choice([7]),                                           # Number of series in windows\n",
    "       \"random_seed\": tune.randint(1, 10),\n",
    "    }\n",
    "\n",
    "model = AutoTCN(h=12, \n",
    "                loss=MAE(), \n",
    "                valid_loss=MSE(), \n",
    "                config=tcn_config, num_samples=1, cpus=1)\n",
    "\n",
    "# Fit and predict\n",
    "model.fit(dataset=dataset)\n",
    "y_hat = model.predict(dataset=dataset)\n",
    "\n",
    "## Checking if base recurrent methods run quantile valid_loss correctly\n",
    "model = AutoTCN(h=12, \n",
    "                loss=GMM(n_components=2, level=[80, 90]),\n",
    "                valid_loss=sCRPS(level=[80, 90]),\n",
    "                config=tcn_config, num_samples=1, cpus=1)\n",
    "\n",
    "# Fit and predict\n",
    "model.fit(dataset=dataset)\n",
    "y_hat = model.predict(dataset=dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5a18b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
