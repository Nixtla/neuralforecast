{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77bc74d-58a1-4c14-b477-f52d28f2a869",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp models.xlstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15392f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "12fa25a4",
   "metadata": {},
   "source": [
    "# xLSTM\n",
    "The `xLSTM` was built to improve the `LSTM` architecture by introducing exponential gating and a modified memory structure. This implementation uses the `xLSTM` block stack to encode a timeseries, and subsequently an `MLP` decoder to create forecasts.\n",
    "\n",
    "**References**<br>-[Maximilian Beck and Korbinian Pöppel and Markus Spanring and Andreas Auer and Oleksandra Prudnikova and Michael Kopp and Günter Klambauer and Johannes Brandstetter and Sepp Hochreiter. \"xLSTM: Extended Long Short-Term Memory\" (2024).](https://arxiv.org/abs/2405.04517})<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2508f7a9-1433-4ad8-8f2f-0078c6ed6c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/osprangers/Repositories/neuralforecast/.venv/lib/python3.10/site-packages/nbdev/doclinks.py:17: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources,importlib\n",
      "/home/osprangers/Repositories/neuralforecast/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-07-25 09:37:27,834\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2025-07-25 09:37:27,895\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "import logging\n",
    "from fastcore.test import test_eq\n",
    "from nbdev.showdoc import show_doc\n",
    "from neuralforecast.common._model_checks import check_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44065066-e72a-431f-938f-1528adef9fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "import warnings\n",
    "import torch.nn as nn\n",
    "\n",
    "from neuralforecast.losses.pytorch import MAE\n",
    "from neuralforecast.common._base_model import BaseModel\n",
    "from neuralforecast.common._modules import MLP\n",
    "\n",
    "try:\n",
    "    from xlstm.xlstm_block_stack import xLSTMBlockStack, xLSTMBlockStackConfig\n",
    "    from xlstm.blocks.mlstm.block import mLSTMBlockConfig\n",
    "    from xlstm.blocks.slstm.block import sLSTMBlockConfig\n",
    "    IS_XLSTM_INSTALLED = True\n",
    "except ImportError:\n",
    "    IS_XLSTM_INSTALLED = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55627eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.util.find_spec(\"ninja_\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a356de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class xLSTM(BaseModel):\n",
    "    \"\"\" xLSTM\n",
    "\n",
    "    xLSTM encoder, with MLP decoder.\n",
    "\n",
    "    **Parameters:**<br>\n",
    "    `h`: int, forecast horizon.<br>\n",
    "    `input_size`: int, considered autorregresive inputs (lags), y=[1,2,3,4] input_size=2 -> lags=[1,2].<br>\n",
    "    `encoder_n_blocks`: int=2, number of blocks for the xLSTM.<br>\n",
    "    `encoder_hidden_size`: int=128, units for the xLSTM's hidden state size.<br>\n",
    "    `encoder_bias`: bool=True, whether or not to use biases within xLSTM blocks.<br>\n",
    "    `encoder_dropout`: float=0., dropout regularization applied within xLSTM blocks.<br>\n",
    "    `decoder_hidden_size`: int=128, size of hidden layer for the MLP decoder.<br>\n",
    "    `decoder_layers`: int=2, number of layers for the MLP decoder.<br>\n",
    "    `decoder_dropout`: float=0., dropout regularization applied within the MLP decoder.<br>\n",
    "    `decoder_activation`: str='GELU', activation function for the MLP decoder, see [activations collection](https://docs.pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity).<br>\n",
    "    `backbone`: str='sLSTM', backbone for the xLSTM, either 'sLSTM' or 'mLSTM'.<br>\n",
    "    `futr_exog_list`: str list, future exogenous columns.<br>\n",
    "    `hist_exog_list`: str list, historic exogenous columns.<br>\n",
    "    `stat_exog_list`: str list, static exogenous columns.<br>\n",
    "    `exclude_insample_y`: bool=False, whether to exclude the target variable from the input.<br>\n",
    "    `recurrent`: bool=False, whether to produce forecasts recursively (True) or direct (False).<br>\n",
    "    `loss`: PyTorch module, instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
    "    `valid_loss`: PyTorch module=`loss`, instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
    "    `max_steps`: int=1000, maximum number of training steps.<br>\n",
    "    `learning_rate`: float=1e-3, Learning rate between (0, 1).<br>\n",
    "    `num_lr_decays`: int=-1, Number of learning rate decays, evenly distributed across max_steps.<br>\n",
    "    `early_stop_patience_steps`: int=-1, Number of validation iterations before early stopping.<br>\n",
    "    `val_check_steps`: int=100, Number of training steps between every validation loss check.<br>\n",
    "    `batch_size`: int=32, number of differentseries in each batch.<br>\n",
    "    `valid_batch_size`: int=None, number of different series in each validation and test batch.<br>\n",
    "    `windows_batch_size`: int=128, number of windows to sample in each training batch, default uses all.<br>\n",
    "    `inference_windows_batch_size`: int=1024, number of windows to sample in each inference batch, -1 uses all.<br>\n",
    "    `start_padding_enabled`: bool=False, if True, the model will pad the time series with zeros at the beginning, by input size.<br>\n",
    "    `training_data_availability_threshold`: Union[float, List[float]]=0.0, minimum fraction of valid data points required for training windows. Single float applies to both insample and outsample; list of two floats specifies [insample_fraction, outsample_fraction]. Default 0.0 allows windows with only 1 valid data point (current behavior).<br>\n",
    "    `step_size`: int=1, step size between each window of temporal data.<br>    \n",
    "    `scaler_type`: str='robust', type of scaler for temporal inputs normalization see [temporal scalers](https://nixtla.github.io/neuralforecast/common.scalers.html).<br>\n",
    "    `random_seed`: int=1, random_seed for pytorch initializer and numpy generators.<br>\n",
    "    `drop_last_loader`: bool=False, if True `TimeSeriesDataLoader` drops last non-full batch.<br>\n",
    "    `alias`: str, optional,  Custom name of the model.<br>\n",
    "    `optimizer`: Subclass of 'torch.optim.Optimizer', optional, user specified optimizer instead of the default choice (Adam).<br>\n",
    "    `optimizer_kwargs`: dict, optional, list of parameters used by the user specified `optimizer`.<br>\n",
    "    `lr_scheduler`: Subclass of 'torch.optim.lr_scheduler.LRScheduler', optional, user specified lr_scheduler instead of the default choice (StepLR).<br>\n",
    "    `lr_scheduler_kwargs`: dict, optional, list of parameters used by the user specified `lr_scheduler`.<br>    \n",
    "    `dataloader_kwargs`: dict, optional, list of parameters passed into the PyTorch Lightning dataloader by the `TimeSeriesDataLoader`. <br>\n",
    "    `**trainer_kwargs`: int,  keyword trainer arguments inherited from [PyTorch Lighning's trainer](https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.trainer.trainer.Trainer.html?highlight=trainer).<br>\n",
    "\n",
    "    **References:**<br>\n",
    "    -[Maximilian Beck, Korbinian Pöppel, Markus Spanring, Andreas Auer, Oleksandra Prudnikova, Michael Kopp, Günter Klambauer, Johannes Brandstetter, Sepp Hochreiter (2024). \"xLSTM: Extended Long Short-Term Memory\"](https://arxiv.org/abs/2405.04517)\n",
    "\n",
    "    \"\"\"\n",
    "    # Class attributes\n",
    "    EXOGENOUS_FUTR = True\n",
    "    EXOGENOUS_HIST = True\n",
    "    EXOGENOUS_STAT = True\n",
    "    MULTIVARIATE = False    # If the model produces multivariate forecasts (True) or univariate (False)\n",
    "    RECURRENT = False        # If the model produces forecasts recursively (True) or direct (False)\n",
    "\n",
    "    def __init__(self,\n",
    "                 h: int,\n",
    "                 input_size: int = -1,\n",
    "                 inference_input_size: Optional[int] = None,\n",
    "                 h_train: int = 1,\n",
    "                 encoder_n_blocks: int = 2,\n",
    "                 encoder_hidden_size: int = 128,\n",
    "                 encoder_bias: bool = True,\n",
    "                 encoder_dropout: float = 0.1,\n",
    "                 decoder_hidden_size: int = 128,\n",
    "                 decoder_layers: int = 1,\n",
    "                 decoder_dropout: float = 0.,\n",
    "                 decoder_activation: str = 'GELU',\n",
    "                 backbone: str = \"sLSTM\",\n",
    "                 futr_exog_list = None,\n",
    "                 hist_exog_list = None,\n",
    "                 stat_exog_list = None,\n",
    "                 exclude_insample_y = False,\n",
    "                 recurrent = False,\n",
    "                 loss = MAE(),\n",
    "                 valid_loss = None,\n",
    "                 max_steps: int = 1000,\n",
    "                 learning_rate: float = 1e-3,\n",
    "                 num_lr_decays: int = -1,\n",
    "                 early_stop_patience_steps: int =-1,\n",
    "                 val_check_steps: int = 100,\n",
    "                 batch_size = 32,\n",
    "                 valid_batch_size: Optional[int] = None,\n",
    "                 windows_batch_size = 128,\n",
    "                 inference_windows_batch_size = 1024,\n",
    "                 start_padding_enabled = False,\n",
    "                 training_data_availability_threshold = 0.0,\n",
    "                 step_size: int = 1,\n",
    "                 scaler_type: str = 'robust',\n",
    "                 random_seed = 1,\n",
    "                 drop_last_loader = False,\n",
    "                 alias: Optional[str] = None,\n",
    "                 optimizer = None,\n",
    "                 optimizer_kwargs = None,\n",
    "                 lr_scheduler = None,\n",
    "                 lr_scheduler_kwargs = None,\n",
    "                 dataloader_kwargs = None,\n",
    "                 **trainer_kwargs):\n",
    "        \n",
    "        if recurrent:\n",
    "            warnings.warn(\"xLSTM does not support recurrent forecasts. Setting recurrent=False.\")\n",
    "            recurrent = False\n",
    "\n",
    "        super(xLSTM, self).__init__(\n",
    "            h=h,\n",
    "            input_size=input_size,\n",
    "            inference_input_size=inference_input_size,\n",
    "            h_train=h_train,\n",
    "            futr_exog_list=futr_exog_list,\n",
    "            hist_exog_list=hist_exog_list,\n",
    "            stat_exog_list=stat_exog_list,\n",
    "            exclude_insample_y = exclude_insample_y,\n",
    "            loss=loss,\n",
    "            valid_loss=valid_loss,\n",
    "            max_steps=max_steps,\n",
    "            learning_rate=learning_rate,\n",
    "            num_lr_decays=num_lr_decays,\n",
    "            early_stop_patience_steps=early_stop_patience_steps,\n",
    "            val_check_steps=val_check_steps,\n",
    "            batch_size=batch_size,\n",
    "            valid_batch_size=valid_batch_size,\n",
    "            windows_batch_size=windows_batch_size,\n",
    "            inference_windows_batch_size=inference_windows_batch_size,\n",
    "            start_padding_enabled=start_padding_enabled,\n",
    "            training_data_availability_threshold=training_data_availability_threshold,\n",
    "            step_size=step_size,\n",
    "            scaler_type=scaler_type,\n",
    "            random_seed=random_seed,\n",
    "            drop_last_loader=drop_last_loader,\n",
    "            alias=alias,\n",
    "            optimizer=optimizer,\n",
    "            optimizer_kwargs=optimizer_kwargs,\n",
    "            lr_scheduler=lr_scheduler,\n",
    "            lr_scheduler_kwargs=lr_scheduler_kwargs,\n",
    "            dataloader_kwargs=dataloader_kwargs,\n",
    "            **trainer_kwargs\n",
    "        )\n",
    "\n",
    "        if not IS_XLSTM_INSTALLED:\n",
    "            raise ImportError(\n",
    "                \"Please install `xlstm`. You also need to install `mlstm_kernels` for backend='mLSTM' and `ninja` for backend='sLSTM'.\"\n",
    "            )        \n",
    "\n",
    "        # xLSTM input size (1 for target variable y)\n",
    "        input_encoder = 1 + self.hist_exog_size + self.stat_exog_size + self.futr_exog_size\n",
    "\n",
    "        # Architecture\n",
    "        self.feature_projection = nn.Linear(\n",
    "            in_features=input_encoder,\n",
    "            out_features=encoder_hidden_size\n",
    "        )\n",
    "        if backbone == \"sLSTM\":\n",
    "            block_stack_config = xLSTMBlockStackConfig(slstm_block=sLSTMBlockConfig(), context_length=input_size, num_blocks=encoder_n_blocks, embedding_dim=encoder_hidden_size, bias=encoder_bias, dropout=encoder_dropout)\n",
    "        elif backbone == \"mLSTM\":\n",
    "            block_stack_config = xLSTMBlockStackConfig(mlstm_block=mLSTMBlockConfig(), context_length=input_size, num_blocks=encoder_n_blocks, embedding_dim=encoder_hidden_size, bias=encoder_bias, dropout=encoder_dropout)        \n",
    "        self.hist_encoder = xLSTMBlockStack(block_stack_config)        \n",
    "\n",
    "        # Decoder MLP\n",
    "        self.mlp_decoder = MLP(in_features=encoder_hidden_size + self.futr_exog_size,\n",
    "                            out_features=self.loss.outputsize_multiplier,\n",
    "                            hidden_size=decoder_hidden_size,\n",
    "                            num_layers=decoder_layers,\n",
    "                            activation=decoder_activation,\n",
    "                            dropout=decoder_dropout)\n",
    "        self.temporal_projection = nn.Linear(self.input_size, self.h)            \n",
    "\n",
    "    def forward(self, windows_batch):\n",
    "        \n",
    "        # Parse windows_batch\n",
    "        encoder_input = windows_batch['insample_y']                         # [B, seq_len, 1]\n",
    "        futr_exog     = windows_batch['futr_exog']                          # [B, seq_len, F]\n",
    "        hist_exog     = windows_batch['hist_exog']                          # [B, seq_len, X]\n",
    "        stat_exog     = windows_batch['stat_exog']                          # [B, S]\n",
    "\n",
    "        # Concatenate y, historic and static inputs              \n",
    "        batch_size, seq_len = encoder_input.shape[:2]\n",
    "        if self.hist_exog_size > 0:\n",
    "            encoder_input = torch.cat((encoder_input, hist_exog), dim=2)    # [B, seq_len, 1] + [B, seq_len, X] -> [B, seq_len, 1 + X]\n",
    "\n",
    "        if self.stat_exog_size > 0:\n",
    "            # print(encoder_input.shape)\n",
    "            stat_exog = stat_exog.unsqueeze(1).repeat(1, seq_len, 1)        # [B, S] -> [B, seq_len, S]\n",
    "            encoder_input = torch.cat((encoder_input, stat_exog), dim=2)    # [B, seq_len, 1 + X] + [B, seq_len, S] -> [B, seq_len, 1 + X + S]\n",
    "\n",
    "        if self.futr_exog_size > 0:\n",
    "            encoder_input = torch.cat((encoder_input, \n",
    "                                       futr_exog[:, :seq_len]), dim=2)      # [B, seq_len, 1 + X + S] + [B, seq_len, F] -> [B, seq_len, 1 + X + S + F]\n",
    "\n",
    "        encoder_input = self.feature_projection(encoder_input)                    # [B, seq_len, 1 + X + S + F] -> [B, seq_len, rnn_hidden_state]\n",
    "        hidden_state = self.hist_encoder(encoder_input)                 # [B, seq_len, rnn_hidden_state]\n",
    "        hidden_state = hidden_state.permute(0, 2, 1)               # [B, seq_len, rnn_hidden_state] -> [B, rnn_hidden_state, seq_len]\n",
    "        hidden_state = self.temporal_projection(hidden_state)        # [B, rnn_hidden_state, seq_len] -> [B, rnn_hidden_state, h]\n",
    "        hidden_state = hidden_state.permute(0, 2, 1)               # [B, rnn_hidden_state, h] -> [B, h, rnn_hidden_state]\n",
    "        \n",
    "        if self.futr_exog_size > 0:\n",
    "            futr_exog_futr = futr_exog[:, -self.h:]                    # [B, h, F]\n",
    "            hidden_state = torch.cat((hidden_state, \n",
    "                                        futr_exog_futr), dim=-1)         # [B, h, rnn_hidden_state] + [B, h, F] -> [B, h, rnn_hidden_state + F]\n",
    "\n",
    "        output = self.mlp_decoder(hidden_state)                        # [B, h, rnn_hidden_state + F] -> [B, h, n_output]\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc06a06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/models/xlstm.py#L26){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### xLSTM\n",
       "\n",
       ">      xLSTM (h:int, input_size:int=-1, inference_input_size:Optional[int]=None,\n",
       ">             h_train:int=1, encoder_n_blocks:int=2,\n",
       ">             encoder_hidden_size:int=128, encoder_bias:bool=True,\n",
       ">             encoder_dropout:float=0.1, decoder_hidden_size:int=128,\n",
       ">             decoder_layers:int=1, decoder_dropout:float=0.0,\n",
       ">             decoder_activation:str='GELU', backbone:str='sLSTM',\n",
       ">             futr_exog_list=None, hist_exog_list=None, stat_exog_list=None,\n",
       ">             exclude_insample_y=False, recurrent=False, loss=MAE(),\n",
       ">             valid_loss=None, max_steps:int=1000, learning_rate:float=0.001,\n",
       ">             num_lr_decays:int=-1, early_stop_patience_steps:int=-1,\n",
       ">             val_check_steps:int=100, batch_size=32,\n",
       ">             valid_batch_size:Optional[int]=None, windows_batch_size=128,\n",
       ">             inference_windows_batch_size=1024, start_padding_enabled=False,\n",
       ">             training_data_availability_threshold=0.0, step_size:int=1,\n",
       ">             scaler_type:str='robust', random_seed=1, drop_last_loader=False,\n",
       ">             alias:Optional[str]=None, optimizer=None, optimizer_kwargs=None,\n",
       ">             lr_scheduler=None, lr_scheduler_kwargs=None,\n",
       ">             dataloader_kwargs=None, **trainer_kwargs)\n",
       "\n",
       "*xLSTM\n",
       "\n",
       "xLSTM encoder, with MLP decoder.\n",
       "\n",
       "**Parameters:**<br>\n",
       "`h`: int, forecast horizon.<br>\n",
       "`input_size`: int, considered autorregresive inputs (lags), y=[1,2,3,4] input_size=2 -> lags=[1,2].<br>\n",
       "`encoder_n_blocks`: int=2, number of blocks for the xLSTM.<br>\n",
       "`encoder_hidden_size`: int=128, units for the xLSTM's hidden state size.<br>\n",
       "`encoder_bias`: bool=True, whether or not to use biases within xLSTM blocks.<br>\n",
       "`encoder_dropout`: float=0., dropout regularization applied within xLSTM blocks.<br>\n",
       "`decoder_hidden_size`: int=128, size of hidden layer for the MLP decoder.<br>\n",
       "`decoder_layers`: int=2, number of layers for the MLP decoder.<br>\n",
       "`decoder_dropout`: float=0., dropout regularization applied within the MLP decoder.<br>\n",
       "`decoder_activation`: str='GELU', activation function for the MLP decoder, see [activations collection](https://docs.pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity).<br>\n",
       "`futr_exog_list`: str list, future exogenous columns.<br>\n",
       "`hist_exog_list`: str list, historic exogenous columns.<br>\n",
       "`stat_exog_list`: str list, static exogenous columns.<br>\n",
       "`exclude_insample_y`: bool=False, whether to exclude the target variable from the input.<br>\n",
       "`recurrent`: bool=False, whether to produce forecasts recursively (True) or direct (False).<br>\n",
       "`loss`: PyTorch module, instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
       "`valid_loss`: PyTorch module=`loss`, instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
       "`max_steps`: int=1000, maximum number of training steps.<br>\n",
       "`learning_rate`: float=1e-3, Learning rate between (0, 1).<br>\n",
       "`num_lr_decays`: int=-1, Number of learning rate decays, evenly distributed across max_steps.<br>\n",
       "`early_stop_patience_steps`: int=-1, Number of validation iterations before early stopping.<br>\n",
       "`val_check_steps`: int=100, Number of training steps between every validation loss check.<br>\n",
       "`batch_size`: int=32, number of differentseries in each batch.<br>\n",
       "`valid_batch_size`: int=None, number of different series in each validation and test batch.<br>\n",
       "`windows_batch_size`: int=128, number of windows to sample in each training batch, default uses all.<br>\n",
       "`inference_windows_batch_size`: int=1024, number of windows to sample in each inference batch, -1 uses all.<br>\n",
       "`start_padding_enabled`: bool=False, if True, the model will pad the time series with zeros at the beginning, by input size.<br>\n",
       "`training_data_availability_threshold`: Union[float, List[float]]=0.0, minimum fraction of valid data points required for training windows. Single float applies to both insample and outsample; list of two floats specifies [insample_fraction, outsample_fraction]. Default 0.0 allows windows with only 1 valid data point (current behavior).<br>\n",
       "`step_size`: int=1, step size between each window of temporal data.<br>    \n",
       "`scaler_type`: str='robust', type of scaler for temporal inputs normalization see [temporal scalers](https://nixtla.github.io/neuralforecast/common.scalers.html).<br>\n",
       "`random_seed`: int=1, random_seed for pytorch initializer and numpy generators.<br>\n",
       "`drop_last_loader`: bool=False, if True `TimeSeriesDataLoader` drops last non-full batch.<br>\n",
       "`alias`: str, optional,  Custom name of the model.<br>\n",
       "`optimizer`: Subclass of 'torch.optim.Optimizer', optional, user specified optimizer instead of the default choice (Adam).<br>\n",
       "`optimizer_kwargs`: dict, optional, list of parameters used by the user specified `optimizer`.<br>\n",
       "`lr_scheduler`: Subclass of 'torch.optim.lr_scheduler.LRScheduler', optional, user specified lr_scheduler instead of the default choice (StepLR).<br>\n",
       "`lr_scheduler_kwargs`: dict, optional, list of parameters used by the user specified `lr_scheduler`.<br>    \n",
       "`dataloader_kwargs`: dict, optional, list of parameters passed into the PyTorch Lightning dataloader by the `TimeSeriesDataLoader`. <br>\n",
       "`**trainer_kwargs`: int,  keyword trainer arguments inherited from [PyTorch Lighning's trainer](https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.trainer.trainer.Trainer.html?highlight=trainer).<br>\n",
       "\n",
       "**References:**<br>\n",
       "-[Maximilian Beck, Korbinian Pöppel, Markus Spanring, Andreas Auer, Oleksandra Prudnikova, Michael Kopp, Günter Klambauer, Johannes Brandstetter, Sepp Hochreiter (2024). \"xLSTM: Extended Long Short-Term Memory\"](https://arxiv.org/abs/2405.04517)*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/models/xlstm.py#L26){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### xLSTM\n",
       "\n",
       ">      xLSTM (h:int, input_size:int=-1, inference_input_size:Optional[int]=None,\n",
       ">             h_train:int=1, encoder_n_blocks:int=2,\n",
       ">             encoder_hidden_size:int=128, encoder_bias:bool=True,\n",
       ">             encoder_dropout:float=0.1, decoder_hidden_size:int=128,\n",
       ">             decoder_layers:int=1, decoder_dropout:float=0.0,\n",
       ">             decoder_activation:str='GELU', backbone:str='sLSTM',\n",
       ">             futr_exog_list=None, hist_exog_list=None, stat_exog_list=None,\n",
       ">             exclude_insample_y=False, recurrent=False, loss=MAE(),\n",
       ">             valid_loss=None, max_steps:int=1000, learning_rate:float=0.001,\n",
       ">             num_lr_decays:int=-1, early_stop_patience_steps:int=-1,\n",
       ">             val_check_steps:int=100, batch_size=32,\n",
       ">             valid_batch_size:Optional[int]=None, windows_batch_size=128,\n",
       ">             inference_windows_batch_size=1024, start_padding_enabled=False,\n",
       ">             training_data_availability_threshold=0.0, step_size:int=1,\n",
       ">             scaler_type:str='robust', random_seed=1, drop_last_loader=False,\n",
       ">             alias:Optional[str]=None, optimizer=None, optimizer_kwargs=None,\n",
       ">             lr_scheduler=None, lr_scheduler_kwargs=None,\n",
       ">             dataloader_kwargs=None, **trainer_kwargs)\n",
       "\n",
       "*xLSTM\n",
       "\n",
       "xLSTM encoder, with MLP decoder.\n",
       "\n",
       "**Parameters:**<br>\n",
       "`h`: int, forecast horizon.<br>\n",
       "`input_size`: int, considered autorregresive inputs (lags), y=[1,2,3,4] input_size=2 -> lags=[1,2].<br>\n",
       "`encoder_n_blocks`: int=2, number of blocks for the xLSTM.<br>\n",
       "`encoder_hidden_size`: int=128, units for the xLSTM's hidden state size.<br>\n",
       "`encoder_bias`: bool=True, whether or not to use biases within xLSTM blocks.<br>\n",
       "`encoder_dropout`: float=0., dropout regularization applied within xLSTM blocks.<br>\n",
       "`decoder_hidden_size`: int=128, size of hidden layer for the MLP decoder.<br>\n",
       "`decoder_layers`: int=2, number of layers for the MLP decoder.<br>\n",
       "`decoder_dropout`: float=0., dropout regularization applied within the MLP decoder.<br>\n",
       "`decoder_activation`: str='GELU', activation function for the MLP decoder, see [activations collection](https://docs.pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity).<br>\n",
       "`futr_exog_list`: str list, future exogenous columns.<br>\n",
       "`hist_exog_list`: str list, historic exogenous columns.<br>\n",
       "`stat_exog_list`: str list, static exogenous columns.<br>\n",
       "`exclude_insample_y`: bool=False, whether to exclude the target variable from the input.<br>\n",
       "`recurrent`: bool=False, whether to produce forecasts recursively (True) or direct (False).<br>\n",
       "`loss`: PyTorch module, instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
       "`valid_loss`: PyTorch module=`loss`, instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
       "`max_steps`: int=1000, maximum number of training steps.<br>\n",
       "`learning_rate`: float=1e-3, Learning rate between (0, 1).<br>\n",
       "`num_lr_decays`: int=-1, Number of learning rate decays, evenly distributed across max_steps.<br>\n",
       "`early_stop_patience_steps`: int=-1, Number of validation iterations before early stopping.<br>\n",
       "`val_check_steps`: int=100, Number of training steps between every validation loss check.<br>\n",
       "`batch_size`: int=32, number of differentseries in each batch.<br>\n",
       "`valid_batch_size`: int=None, number of different series in each validation and test batch.<br>\n",
       "`windows_batch_size`: int=128, number of windows to sample in each training batch, default uses all.<br>\n",
       "`inference_windows_batch_size`: int=1024, number of windows to sample in each inference batch, -1 uses all.<br>\n",
       "`start_padding_enabled`: bool=False, if True, the model will pad the time series with zeros at the beginning, by input size.<br>\n",
       "`training_data_availability_threshold`: Union[float, List[float]]=0.0, minimum fraction of valid data points required for training windows. Single float applies to both insample and outsample; list of two floats specifies [insample_fraction, outsample_fraction]. Default 0.0 allows windows with only 1 valid data point (current behavior).<br>\n",
       "`step_size`: int=1, step size between each window of temporal data.<br>    \n",
       "`scaler_type`: str='robust', type of scaler for temporal inputs normalization see [temporal scalers](https://nixtla.github.io/neuralforecast/common.scalers.html).<br>\n",
       "`random_seed`: int=1, random_seed for pytorch initializer and numpy generators.<br>\n",
       "`drop_last_loader`: bool=False, if True `TimeSeriesDataLoader` drops last non-full batch.<br>\n",
       "`alias`: str, optional,  Custom name of the model.<br>\n",
       "`optimizer`: Subclass of 'torch.optim.Optimizer', optional, user specified optimizer instead of the default choice (Adam).<br>\n",
       "`optimizer_kwargs`: dict, optional, list of parameters used by the user specified `optimizer`.<br>\n",
       "`lr_scheduler`: Subclass of 'torch.optim.lr_scheduler.LRScheduler', optional, user specified lr_scheduler instead of the default choice (StepLR).<br>\n",
       "`lr_scheduler_kwargs`: dict, optional, list of parameters used by the user specified `lr_scheduler`.<br>    \n",
       "`dataloader_kwargs`: dict, optional, list of parameters passed into the PyTorch Lightning dataloader by the `TimeSeriesDataLoader`. <br>\n",
       "`**trainer_kwargs`: int,  keyword trainer arguments inherited from [PyTorch Lighning's trainer](https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.trainer.trainer.Trainer.html?highlight=trainer).<br>\n",
       "\n",
       "**References:**<br>\n",
       "-[Maximilian Beck, Korbinian Pöppel, Markus Spanring, Andreas Auer, Oleksandra Prudnikova, Michael Kopp, Günter Klambauer, Johannes Brandstetter, Sepp Hochreiter (2024). \"xLSTM: Extended Long Short-Term Memory\"](https://arxiv.org/abs/2405.04517)*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(xLSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a23696b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### xLSTM.fit\n",
       "\n",
       ">      xLSTM.fit (dataset, val_size=0, test_size=0, random_seed=None,\n",
       ">                 distributed_config=None)\n",
       "\n",
       "*Fit.\n",
       "\n",
       "The `fit` method, optimizes the neural network's weights using the\n",
       "initialization parameters (`learning_rate`, `windows_batch_size`, ...)\n",
       "and the `loss` function as defined during the initialization.\n",
       "Within `fit` we use a PyTorch Lightning `Trainer` that\n",
       "inherits the initialization's `self.trainer_kwargs`, to customize\n",
       "its inputs, see [PL's trainer arguments](https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.trainer.trainer.Trainer.html?highlight=trainer).\n",
       "\n",
       "The method is designed to be compatible with SKLearn-like classes\n",
       "and in particular to be compatible with the StatsForecast library.\n",
       "\n",
       "By default the `model` is not saving training checkpoints to protect\n",
       "disk memory, to get them change `enable_checkpointing=True` in `__init__`.\n",
       "\n",
       "**Parameters:**<br>\n",
       "`dataset`: NeuralForecast's `TimeSeriesDataset`, see [documentation](https://nixtla.github.io/neuralforecast/tsdataset.html).<br>\n",
       "`val_size`: int, validation size for temporal cross-validation.<br>\n",
       "`random_seed`: int=None, random_seed for pytorch initializer and numpy generators, overwrites model.__init__'s.<br>\n",
       "`test_size`: int, test size for temporal cross-validation.<br>*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### xLSTM.fit\n",
       "\n",
       ">      xLSTM.fit (dataset, val_size=0, test_size=0, random_seed=None,\n",
       ">                 distributed_config=None)\n",
       "\n",
       "*Fit.\n",
       "\n",
       "The `fit` method, optimizes the neural network's weights using the\n",
       "initialization parameters (`learning_rate`, `windows_batch_size`, ...)\n",
       "and the `loss` function as defined during the initialization.\n",
       "Within `fit` we use a PyTorch Lightning `Trainer` that\n",
       "inherits the initialization's `self.trainer_kwargs`, to customize\n",
       "its inputs, see [PL's trainer arguments](https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.trainer.trainer.Trainer.html?highlight=trainer).\n",
       "\n",
       "The method is designed to be compatible with SKLearn-like classes\n",
       "and in particular to be compatible with the StatsForecast library.\n",
       "\n",
       "By default the `model` is not saving training checkpoints to protect\n",
       "disk memory, to get them change `enable_checkpointing=True` in `__init__`.\n",
       "\n",
       "**Parameters:**<br>\n",
       "`dataset`: NeuralForecast's `TimeSeriesDataset`, see [documentation](https://nixtla.github.io/neuralforecast/tsdataset.html).<br>\n",
       "`val_size`: int, validation size for temporal cross-validation.<br>\n",
       "`random_seed`: int=None, random_seed for pytorch initializer and numpy generators, overwrites model.__init__'s.<br>\n",
       "`test_size`: int, test size for temporal cross-validation.<br>*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(xLSTM.fit, name='xLSTM.fit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8475d33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### xLSTM.predict\n",
       "\n",
       ">      xLSTM.predict (dataset, test_size=None, step_size=1, random_seed=None,\n",
       ">                     quantiles=None, **data_module_kwargs)\n",
       "\n",
       "*Predict.\n",
       "\n",
       "Neural network prediction with PL's `Trainer` execution of `predict_step`.\n",
       "\n",
       "**Parameters:**<br>\n",
       "`dataset`: NeuralForecast's `TimeSeriesDataset`, see [documentation](https://nixtla.github.io/neuralforecast/tsdataset.html).<br>\n",
       "`test_size`: int=None, test size for temporal cross-validation.<br>\n",
       "`step_size`: int=1, Step size between each window.<br>\n",
       "`random_seed`: int=None, random_seed for pytorch initializer and numpy generators, overwrites model.__init__'s.<br>\n",
       "`quantiles`: list of floats, optional (default=None), target quantiles to predict. <br>\n",
       "`**data_module_kwargs`: PL's TimeSeriesDataModule args, see [documentation](https://pytorch-lightning.readthedocs.io/en/1.6.1/extensions/datamodules.html#using-a-datamodule).*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### xLSTM.predict\n",
       "\n",
       ">      xLSTM.predict (dataset, test_size=None, step_size=1, random_seed=None,\n",
       ">                     quantiles=None, **data_module_kwargs)\n",
       "\n",
       "*Predict.\n",
       "\n",
       "Neural network prediction with PL's `Trainer` execution of `predict_step`.\n",
       "\n",
       "**Parameters:**<br>\n",
       "`dataset`: NeuralForecast's `TimeSeriesDataset`, see [documentation](https://nixtla.github.io/neuralforecast/tsdataset.html).<br>\n",
       "`test_size`: int=None, test size for temporal cross-validation.<br>\n",
       "`step_size`: int=1, Step size between each window.<br>\n",
       "`random_seed`: int=None, random_seed for pytorch initializer and numpy generators, overwrites model.__init__'s.<br>\n",
       "`quantiles`: list of floats, optional (default=None), target quantiles to predict. <br>\n",
       "`**data_module_kwargs`: PL's TimeSeriesDataModule args, see [documentation](https://pytorch-lightning.readthedocs.io/en/1.6.1/extensions/datamodules.html#using-a-datamodule).*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(xLSTM.predict, name='xLSTM.predict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09d7a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xLSTM: checking forecast AirPassengers dataset\n",
      "{'verbose': True, 'with_cuda': True, 'extra_ldflags': ['-L/usr/local/cuda/lib', '-lcublas'], 'extra_cflags': ['-DSLSTM_HIDDEN_SIZE=128', '-DSLSTM_BATCH_SIZE=8', '-DSLSTM_NUM_HEADS=4', '-DSLSTM_NUM_STATES=4', '-DSLSTM_DTYPE_B=float', '-DSLSTM_DTYPE_R=__nv_bfloat16', '-DSLSTM_DTYPE_W=__nv_bfloat16', '-DSLSTM_DTYPE_G=__nv_bfloat16', '-DSLSTM_DTYPE_S=__nv_bfloat16', '-DSLSTM_DTYPE_A=float', '-DSLSTM_NUM_GATES=4', '-DSLSTM_SIMPLE_AGG=true', '-DSLSTM_GRADIENT_RECURRENT_CLIPVAL_VALID=false', '-DSLSTM_GRADIENT_RECURRENT_CLIPVAL=0.0', '-DSLSTM_FORWARD_CLIPVAL_VALID=false', '-DSLSTM_FORWARD_CLIPVAL=0.0', '-U__CUDA_NO_HALF_OPERATORS__', '-U__CUDA_NO_HALF_CONVERSIONS__', '-U__CUDA_NO_BFLOAT16_OPERATORS__', '-U__CUDA_NO_BFLOAT16_CONVERSIONS__', '-U__CUDA_NO_BFLOAT162_OPERATORS__', '-U__CUDA_NO_BFLOAT162_CONVERSIONS__'], 'extra_cuda_cflags': ['-Xptxas=\"-v\"', '-gencode', 'arch=compute_80,code=compute_80', '-res-usage', '--use_fast_math', '-O3', '-Xptxas -O3', '--extra-device-vectorization', '-DSLSTM_HIDDEN_SIZE=128', '-DSLSTM_BATCH_SIZE=8', '-DSLSTM_NUM_HEADS=4', '-DSLSTM_NUM_STATES=4', '-DSLSTM_DTYPE_B=float', '-DSLSTM_DTYPE_R=__nv_bfloat16', '-DSLSTM_DTYPE_W=__nv_bfloat16', '-DSLSTM_DTYPE_G=__nv_bfloat16', '-DSLSTM_DTYPE_S=__nv_bfloat16', '-DSLSTM_DTYPE_A=float', '-DSLSTM_NUM_GATES=4', '-DSLSTM_SIMPLE_AGG=true', '-DSLSTM_GRADIENT_RECURRENT_CLIPVAL_VALID=false', '-DSLSTM_GRADIENT_RECURRENT_CLIPVAL=0.0', '-DSLSTM_FORWARD_CLIPVAL_VALID=false', '-DSLSTM_FORWARD_CLIPVAL=0.0', '-U__CUDA_NO_HALF_OPERATORS__', '-U__CUDA_NO_HALF_CONVERSIONS__', '-U__CUDA_NO_BFLOAT16_OPERATORS__', '-U__CUDA_NO_BFLOAT16_CONVERSIONS__', '-U__CUDA_NO_BFLOAT162_OPERATORS__', '-U__CUDA_NO_BFLOAT162_CONVERSIONS__']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:xlstm.blocks.slstm.src.cuda_init:Before compilation and loading of slstm.\n",
      "Using /home/osprangers/.cache/torch_extensions/py310_cu128 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/osprangers/.cache/torch_extensions/py310_cu128/slstm_HS128BS8NH4NS4DBfDRbDWbDGbDSbDAfNG4SA1GRCV0GRC0d0FCV0FC0d0/build.ninja...\n",
      "Building extension module slstm_HS128BS8NH4NS4DBfDRbDWbDGbDSbDAfNG4SA1GRCV0GRC0d0FCV0FC0d0...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "Loading extension module slstm_HS128BS8NH4NS4DBfDRbDWbDGbDSbDAfNG4SA1GRCV0GRC0d0FCV0FC0d0...\n",
      "INFO:xlstm.blocks.slstm.src.cuda_init:After compilation and loading of slstm.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ninja: no work to do.\n",
      "{'verbose': True, 'with_cuda': True, 'extra_ldflags': ['-L/usr/local/cuda/lib', '-lcublas'], 'extra_cflags': ['-DSLSTM_HIDDEN_SIZE=128', '-DSLSTM_BATCH_SIZE=8', '-DSLSTM_NUM_HEADS=4', '-DSLSTM_NUM_STATES=4', '-DSLSTM_DTYPE_B=float', '-DSLSTM_DTYPE_R=__nv_bfloat16', '-DSLSTM_DTYPE_W=__nv_bfloat16', '-DSLSTM_DTYPE_G=__nv_bfloat16', '-DSLSTM_DTYPE_S=__nv_bfloat16', '-DSLSTM_DTYPE_A=float', '-DSLSTM_NUM_GATES=4', '-DSLSTM_SIMPLE_AGG=true', '-DSLSTM_GRADIENT_RECURRENT_CLIPVAL_VALID=false', '-DSLSTM_GRADIENT_RECURRENT_CLIPVAL=0.0', '-DSLSTM_FORWARD_CLIPVAL_VALID=false', '-DSLSTM_FORWARD_CLIPVAL=0.0', '-U__CUDA_NO_HALF_OPERATORS__', '-U__CUDA_NO_HALF_CONVERSIONS__', '-U__CUDA_NO_BFLOAT16_OPERATORS__', '-U__CUDA_NO_BFLOAT16_CONVERSIONS__', '-U__CUDA_NO_BFLOAT162_OPERATORS__', '-U__CUDA_NO_BFLOAT162_CONVERSIONS__'], 'extra_cuda_cflags': ['-Xptxas=\"-v\"', '-gencode', 'arch=compute_80,code=compute_80', '-res-usage', '--use_fast_math', '-O3', '-Xptxas -O3', '--extra-device-vectorization', '-DSLSTM_HIDDEN_SIZE=128', '-DSLSTM_BATCH_SIZE=8', '-DSLSTM_NUM_HEADS=4', '-DSLSTM_NUM_STATES=4', '-DSLSTM_DTYPE_B=float', '-DSLSTM_DTYPE_R=__nv_bfloat16', '-DSLSTM_DTYPE_W=__nv_bfloat16', '-DSLSTM_DTYPE_G=__nv_bfloat16', '-DSLSTM_DTYPE_S=__nv_bfloat16', '-DSLSTM_DTYPE_A=float', '-DSLSTM_NUM_GATES=4', '-DSLSTM_SIMPLE_AGG=true', '-DSLSTM_GRADIENT_RECURRENT_CLIPVAL_VALID=false', '-DSLSTM_GRADIENT_RECURRENT_CLIPVAL=0.0', '-DSLSTM_FORWARD_CLIPVAL_VALID=false', '-DSLSTM_FORWARD_CLIPVAL=0.0', '-U__CUDA_NO_HALF_OPERATORS__', '-U__CUDA_NO_HALF_CONVERSIONS__', '-U__CUDA_NO_BFLOAT16_OPERATORS__', '-U__CUDA_NO_BFLOAT16_CONVERSIONS__', '-U__CUDA_NO_BFLOAT162_OPERATORS__', '-U__CUDA_NO_BFLOAT162_CONVERSIONS__']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:xlstm.blocks.slstm.src.cuda_init:Before compilation and loading of slstm.\n",
      "Using /home/osprangers/.cache/torch_extensions/py310_cu128 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module slstm_HS128BS8NH4NS4DBfDRbDWbDGbDSbDAfNG4SA1GRCV0GRC0d0FCV0FC0d0, skipping build step...\n",
      "Loading extension module slstm_HS128BS8NH4NS4DBfDRbDWbDGbDSbDAfNG4SA1GRCV0GRC0d0FCV0FC0d0...\n",
      "INFO:xlstm.blocks.slstm.src.cuda_init:After compilation and loading of slstm.\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "# Unit tests for models\n",
    "logging.getLogger(\"pytorch_lightning\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"lightning_fabric\").setLevel(logging.ERROR)\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    check_model(xLSTM, [\"airpassengers\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac34472d-5670-45b5-a7c5-1dba54f8e782",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from neuralforecast.utils import AirPassengersDF as Y_df\n",
    "from neuralforecast.tsdataset import TimeSeriesDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35421df",
   "metadata": {},
   "source": [
    "## Usage Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af48f383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'verbose': True, 'with_cuda': True, 'extra_ldflags': ['-L/usr/local/cuda/lib', '-lcublas'], 'extra_cflags': ['-DSLSTM_HIDDEN_SIZE=64', '-DSLSTM_BATCH_SIZE=8', '-DSLSTM_NUM_HEADS=4', '-DSLSTM_NUM_STATES=4', '-DSLSTM_DTYPE_B=float', '-DSLSTM_DTYPE_R=__nv_bfloat16', '-DSLSTM_DTYPE_W=__nv_bfloat16', '-DSLSTM_DTYPE_G=__nv_bfloat16', '-DSLSTM_DTYPE_S=__nv_bfloat16', '-DSLSTM_DTYPE_A=float', '-DSLSTM_NUM_GATES=4', '-DSLSTM_SIMPLE_AGG=true', '-DSLSTM_GRADIENT_RECURRENT_CLIPVAL_VALID=false', '-DSLSTM_GRADIENT_RECURRENT_CLIPVAL=0.0', '-DSLSTM_FORWARD_CLIPVAL_VALID=false', '-DSLSTM_FORWARD_CLIPVAL=0.0', '-U__CUDA_NO_HALF_OPERATORS__', '-U__CUDA_NO_HALF_CONVERSIONS__', '-U__CUDA_NO_BFLOAT16_OPERATORS__', '-U__CUDA_NO_BFLOAT16_CONVERSIONS__', '-U__CUDA_NO_BFLOAT162_OPERATORS__', '-U__CUDA_NO_BFLOAT162_CONVERSIONS__'], 'extra_cuda_cflags': ['-Xptxas=\"-v\"', '-gencode', 'arch=compute_80,code=compute_80', '-res-usage', '--use_fast_math', '-O3', '-Xptxas -O3', '--extra-device-vectorization', '-DSLSTM_HIDDEN_SIZE=64', '-DSLSTM_BATCH_SIZE=8', '-DSLSTM_NUM_HEADS=4', '-DSLSTM_NUM_STATES=4', '-DSLSTM_DTYPE_B=float', '-DSLSTM_DTYPE_R=__nv_bfloat16', '-DSLSTM_DTYPE_W=__nv_bfloat16', '-DSLSTM_DTYPE_G=__nv_bfloat16', '-DSLSTM_DTYPE_S=__nv_bfloat16', '-DSLSTM_DTYPE_A=float', '-DSLSTM_NUM_GATES=4', '-DSLSTM_SIMPLE_AGG=true', '-DSLSTM_GRADIENT_RECURRENT_CLIPVAL_VALID=false', '-DSLSTM_GRADIENT_RECURRENT_CLIPVAL=0.0', '-DSLSTM_FORWARD_CLIPVAL_VALID=false', '-DSLSTM_FORWARD_CLIPVAL=0.0', '-U__CUDA_NO_HALF_OPERATORS__', '-U__CUDA_NO_HALF_CONVERSIONS__', '-U__CUDA_NO_BFLOAT16_OPERATORS__', '-U__CUDA_NO_BFLOAT16_CONVERSIONS__', '-U__CUDA_NO_BFLOAT162_OPERATORS__', '-U__CUDA_NO_BFLOAT162_CONVERSIONS__']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:xlstm.blocks.slstm.src.cuda_init:Before compilation and loading of slstm.\n",
      "Using /home/osprangers/.cache/torch_extensions/py310_cu128 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/osprangers/.cache/torch_extensions/py310_cu128/slstm_HS64BS8NH4NS4DBfDRbDWbDGbDSbDAfNG4SA1GRCV0GRC0d0FCV0FC0d0/build.ninja...\n",
      "/home/osprangers/Repositories/neuralforecast/.venv/lib/python3.10/site-packages/torch/utils/cpp_extension.py:2356: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n",
      "Building extension module slstm_HS64BS8NH4NS4DBfDRbDWbDGbDSbDAfNG4SA1GRCV0GRC0d0FCV0FC0d0...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "Loading extension module slstm_HS64BS8NH4NS4DBfDRbDWbDGbDSbDAfNG4SA1GRCV0GRC0d0FCV0FC0d0...\n",
      "INFO:xlstm.blocks.slstm.src.cuda_init:After compilation and loading of slstm.\n",
      "/home/osprangers/Repositories/neuralforecast/.venv/lib/python3.10/site-packages/xlstm/blocks/slstm/cell.py:546: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  def forward(ctx, training, *inputs):\n",
      "/home/osprangers/Repositories/neuralforecast/.venv/lib/python3.10/site-packages/xlstm/blocks/slstm/cell.py:571: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, grad_s):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ninja: no work to do.\n",
      "Epoch 399: 100%|██████████| 1/1 [00:00<00:00, 30.08it/s, v_num=456, train_loss_step=4.360, train_loss_epoch=4.360]\n",
      "Predicting DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 16.69it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| eval: false\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from neuralforecast import NeuralForecast\n",
    "# from neuralforecast.models import xLSTM\n",
    "from neuralforecast.losses.pytorch import DistributionLoss\n",
    "from neuralforecast.utils import AirPassengersPanel, AirPassengersStatic\n",
    "\n",
    "Y_train_df = AirPassengersPanel[AirPassengersPanel.ds<AirPassengersPanel['ds'].values[-12]] # 132 train\n",
    "Y_test_df = AirPassengersPanel[AirPassengersPanel.ds>=AirPassengersPanel['ds'].values[-12]].reset_index(drop=True) # 12 test\n",
    "\n",
    "nf = NeuralForecast(\n",
    "    models=[xLSTM(h=12, \n",
    "                 input_size=8,\n",
    "                 loss=DistributionLoss(distribution=\"Normal\", level=[80, 90]),\n",
    "                 encoder_n_blocks=1,\n",
    "                 encoder_hidden_size=64,\n",
    "                 decoder_hidden_size=64,\n",
    "                 decoder_layers=1,\n",
    "                 max_steps=400,\n",
    "                 futr_exog_list=['y_[lag12]'],\n",
    "                 stat_exog_list=['airline1'],\n",
    "                 )\n",
    "    ],\n",
    "    freq='ME'\n",
    ")\n",
    "nf.fit(df=Y_train_df, static_df=AirPassengersStatic)\n",
    "Y_hat_df = nf.predict(futr_df=Y_test_df)\n",
    "\n",
    "# Plots\n",
    "Y_hat_df = Y_hat_df.reset_index(drop=False).drop(columns=['unique_id','ds'])\n",
    "plot_df = pd.concat([Y_test_df, Y_hat_df], axis=1)\n",
    "plot_df = pd.concat([Y_train_df, plot_df])\n",
    "\n",
    "plot_df = plot_df[plot_df.unique_id=='Airline1'].drop('unique_id', axis=1)\n",
    "plt.plot(plot_df['ds'], plot_df['y'], c='black', label='True')\n",
    "plt.plot(plot_df['ds'], plot_df['xLSTM-median'], c='blue', label='median')\n",
    "plt.fill_between(x=plot_df['ds'][-12:], \n",
    "                 y1=plot_df['xLSTM-lo-90'][-12:].values,\n",
    "                 y2=plot_df['xLSTM-hi-90'][-12:].values,\n",
    "                 alpha=0.4, label='level 90')\n",
    "plt.grid()\n",
    "plt.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
