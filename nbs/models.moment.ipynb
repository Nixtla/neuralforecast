{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp models.moment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MOMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MOMENT is a family of open-source foundation models for time series forecasting. MOMENT uses a Transformer architecture, and works by learning embeddings of patches of time series and reconstructing the learned embeddings onto an output dimension to fulfil a set of time series tasks, such as forecasting, classification, anomaly detection and imputation. In this implementation, only the forecasting task is supported. This is a multivariate forecasting architecture. MOMENT does not support exogenous variables.\n",
    "\n",
    "**References**<br>\n",
    "-[Goswami, Mononito, Konrad Szafer, Arjun Choudhry, Yifu Cai, Shuo Li, and Artur Dubrawski. (2024). MOMENT: A Family of Open Time-Series Foundation Models. International Conference on Machine Learning, Vienna, Austria. PMLR 235](https://arxiv.org/pdf/2402.03885)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Figure 1. Overview of MOMENT.](imgs_models/moment.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from fastcore.test import test_eq\n",
    "from nbdev.showdoc import show_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from typing import Optional\n",
    "\n",
    "import warnings\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "from neuralforecast.losses.pytorch import MAE\n",
    "from neuralforecast.common._base_multivariate import BaseMultivariate\n",
    "try:\n",
    "    from transformers import T5Config, T5EncoderModel, T5Model\n",
    "    IS_TRANSFORMERS_INSTALLED = True\n",
    "except:\n",
    "    IS_TRANSFORMERS_INSTALLED = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Auxiliary Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RevIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def nanvar(tensor, dim=None, keepdim=False):\n",
    "    tensor_mean = tensor.nanmean(dim=dim, keepdim=True)\n",
    "    output = (tensor - tensor_mean).square().nanmean(dim=dim, keepdim=keepdim)\n",
    "    return output\n",
    "\n",
    "\n",
    "def nanstd(tensor, dim=None, keepdim=False):\n",
    "    output = nanvar(tensor, dim=dim, keepdim=keepdim)\n",
    "    output = output.sqrt()\n",
    "    return output\n",
    "\n",
    "class RevIN(nn.Module):\n",
    "    def __init__(self, num_features: int, eps: float = 1e-5, affine: bool = False):\n",
    "        \"\"\"\n",
    "        :param num_features: the number of features or channels\n",
    "        :param eps: a value added for numerical stability\n",
    "        :param affine: if True, RevIN has learnable affine parameters\n",
    "        \"\"\"\n",
    "        super(RevIN, self).__init__()\n",
    "        self.num_features = num_features\n",
    "        self.eps = eps\n",
    "        self.affine = affine\n",
    "\n",
    "        if self.affine:\n",
    "            self._init_params()\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mode: str = \"norm\", mask: torch.Tensor = None):\n",
    "        \"\"\"\n",
    "        :param x: input tensor of shape (batch_size, n_channels, seq_len)\n",
    "        :param mode: 'norm' or 'denorm'\n",
    "        :param mask: input mask of shape (batch_size, seq_len)\n",
    "        :return: RevIN transformed tensor\n",
    "        \"\"\"\n",
    "        if mode == \"norm\":\n",
    "            self._get_statistics(x, mask=mask)\n",
    "            x = self._normalize(x)\n",
    "        elif mode == \"denorm\":\n",
    "            x = self._denormalize(x)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        return x\n",
    "\n",
    "    def _init_params(self):\n",
    "        # initialize RevIN params: (C,)\n",
    "        self.affine_weight = nn.Parameter(torch.ones(1, self.num_features, 1))\n",
    "        self.affine_bias = nn.Parameter(torch.zeros(1, self.num_features, 1))\n",
    "\n",
    "    def _get_statistics(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        x    : batch_size x n_channels x seq_len\n",
    "        mask : batch_size x seq_len\n",
    "        \"\"\"\n",
    "        if mask is None:\n",
    "            mask = torch.ones((x.shape[0], x.shape[-1]))\n",
    "        n_channels = x.shape[1]\n",
    "        mask = mask.unsqueeze(1).repeat(1, n_channels, 1).bool()\n",
    "        # Set masked positions to NaN, and unmasked positions are taken from x\n",
    "        masked_x = torch.where(mask, x, torch.nan)\n",
    "        self.mean = torch.nanmean(masked_x, dim=-1, keepdim=True).detach()\n",
    "        self.stdev = nanstd(masked_x, dim=-1, keepdim=True).detach() + self.eps\n",
    "        # self.stdev = torch.sqrt(\n",
    "        #     torch.var(masked_x, dim=-1, keepdim=True) + self.eps).get_data().detach()\n",
    "        # NOTE: By default not bessel correction\n",
    "\n",
    "    def _normalize(self, x):\n",
    "        x = x - self.mean\n",
    "        x = x / self.stdev\n",
    "\n",
    "        if self.affine:\n",
    "            x = x * self.affine_weight\n",
    "            x = x + self.affine_bias\n",
    "        return x\n",
    "\n",
    "    def _denormalize(self, x):\n",
    "        if self.affine:\n",
    "            x = x - self.affine_bias\n",
    "            x = x / (self.affine_weight + self.eps * self.eps)\n",
    "        x = x * self.stdev\n",
    "        x = x + self.mean\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000, model_name=\"MOMENT\"):\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "        self.model_name = model_name\n",
    "\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model).float()\n",
    "        pe.require_grad = False\n",
    "\n",
    "        position = torch.arange(0, max_len).float().unsqueeze(1)\n",
    "        div_term = (\n",
    "            torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model)\n",
    "        ).exp()\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if (\n",
    "            self.model_name == \"MOMENT\"\n",
    "            or self.model_name == \"TimesNet\"\n",
    "            or self.model_name == \"GPT4TS\"\n",
    "        ):\n",
    "            return self.pe[:, : x.size(2)]\n",
    "        else:\n",
    "            return self.pe[:, : x.size(1)]\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int = 768,\n",
    "        seq_len: int = 512,\n",
    "        patch_len: int = 8,\n",
    "        stride: int = 8,\n",
    "        dropout: float = 0.1,\n",
    "        add_positional_embedding: bool = False,\n",
    "        value_embedding_bias: bool = False,\n",
    "        orth_gain: float = 1.41,\n",
    "    ):\n",
    "        super(PatchEmbedding, self).__init__()\n",
    "        self.patch_len = patch_len\n",
    "        self.seq_len = seq_len\n",
    "        self.stride = stride\n",
    "        self.d_model = d_model\n",
    "        self.add_positional_embedding = add_positional_embedding\n",
    "\n",
    "        self.value_embedding = nn.Linear(patch_len, d_model, bias=value_embedding_bias)\n",
    "        self.mask_embedding = nn.Parameter(torch.zeros(d_model))\n",
    "\n",
    "        if orth_gain is not None:\n",
    "            torch.nn.init.orthogonal_(self.value_embedding.weight, gain=orth_gain)\n",
    "            if value_embedding_bias:\n",
    "                self.value_embedding.bias.data.zero_()\n",
    "            # torch.nn.init.orthogonal_(self.mask_embedding, gain=orth_gain) # Fails\n",
    "\n",
    "        # Positional embedding\n",
    "        if self.add_positional_embedding:\n",
    "            self.position_embedding = PositionalEmbedding(d_model)\n",
    "\n",
    "        # Residual dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask: torch.Tensor = None) -> torch.Tensor:\n",
    "        mask = Masking.convert_seq_to_patch_view(\n",
    "            mask, patch_len=self.patch_len\n",
    "        ).unsqueeze(-1)\n",
    "        # mask : [batch_size x n_patches x 1]\n",
    "        n_channels = x.shape[1]\n",
    "        mask = (\n",
    "            mask.repeat_interleave(self.d_model, dim=-1)\n",
    "            .unsqueeze(1)\n",
    "            .repeat(1, n_channels, 1, 1)\n",
    "        )\n",
    "        # mask : [batch_size x n_channels x n_patches x d_model]\n",
    "\n",
    "        # Input encoding\n",
    "        x = mask * self.value_embedding(x) + (1 - mask) * self.mask_embedding\n",
    "        if self.add_positional_embedding:\n",
    "            x = x + self.position_embedding(x)\n",
    "\n",
    "        return self.dropout(x)\n",
    "    \n",
    "class Patching(nn.Module):\n",
    "    def __init__(self, patch_len: int, stride: int):\n",
    "        super().__init__()\n",
    "        self.patch_len = patch_len\n",
    "        self.stride = stride\n",
    "        if self.stride != self.patch_len:\n",
    "            warnings.warn(\n",
    "                \"Stride and patch length are not equal. \"\n",
    "                \"This may lead to unexpected behavior.\"\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unfold(dimension=-1, size=self.patch_len, step=self.stride)\n",
    "        # x : [batch_size x n_channels x num_patch x patch_len]\n",
    "        return x        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "class Masking:\n",
    "    def __init__(\n",
    "        self, mask_ratio: float = 0.3, patch_len: int = 8, stride: Optional[int] = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Indices with 0 mask are hidden, and with 1 are observed.\n",
    "        \"\"\"\n",
    "        self.mask_ratio = mask_ratio\n",
    "        self.patch_len = patch_len\n",
    "        self.stride = patch_len if stride is None else stride\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_seq_to_patch_view(\n",
    "        mask: torch.Tensor, patch_len: int = 8, stride: Optional[int] = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            mask : torch.Tensor of shape [batch_size x seq_len]\n",
    "        Output\n",
    "            mask : torch.Tensor of shape [batch_size x n_patches]\n",
    "        \"\"\"\n",
    "        stride = patch_len if stride is None else stride\n",
    "        mask = mask.unfold(dimension=-1, size=patch_len, step=stride)\n",
    "        # mask : [batch_size x n_patches x patch_len]\n",
    "        return (mask.sum(dim=-1) == patch_len).long()\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_patch_to_seq_view(\n",
    "        mask: torch.Tensor,\n",
    "        patch_len: int = 8,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            mask : torch.Tensor of shape [batch_size x n_patches]\n",
    "        Output:\n",
    "            mask : torch.Tensor of shape [batch_size x seq_len]\n",
    "        \"\"\"\n",
    "        return mask.repeat_interleave(patch_len, dim=-1)\n",
    "\n",
    "    def generate_mask(self, x: torch.Tensor, input_mask: Optional[torch.Tensor] = None):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            x : torch.Tensor of shape\n",
    "            [batch_size x n_channels x n_patches x patch_len] or\n",
    "            [batch_size x n_channels x seq_len]\n",
    "            input_mask: torch.Tensor of shape [batch_size x seq_len] or\n",
    "            [batch_size x n_patches]\n",
    "        Output:\n",
    "            mask : torch.Tensor of shape [batch_size x seq_len]\n",
    "        \"\"\"\n",
    "        if x.ndim == 4:\n",
    "            return self._mask_patch_view(x, input_mask=input_mask)\n",
    "        elif x.ndim == 3:\n",
    "            return self._mask_seq_view(x, input_mask=input_mask)\n",
    "\n",
    "    def _mask_patch_view(self, x, input_mask=None):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            x : torch.Tensor of shape\n",
    "            [batch_size x n_channels x n_patches x patch_len]\n",
    "            input_mask: torch.Tensor of shape [batch_size x seq_len]\n",
    "        Output:\n",
    "            mask : torch.Tensor of shape [batch_size x n_patches]\n",
    "        \"\"\"\n",
    "        input_mask = self.convert_seq_to_patch_view(\n",
    "            input_mask, self.patch_len, self.stride\n",
    "        )\n",
    "        n_observed_patches = input_mask.sum(dim=-1, keepdim=True)  # batch_size x 1\n",
    "\n",
    "        batch_size, _, n_patches, _ = x.shape\n",
    "        len_keep = torch.ceil(n_observed_patches * (1 - self.mask_ratio)).long()\n",
    "        noise = torch.rand(\n",
    "            batch_size, n_patches, device=x.device\n",
    "        )  # noise in [0, 1], batch_size x n_channels x n_patches\n",
    "        noise = torch.where(\n",
    "            input_mask == 1, noise, torch.ones_like(noise)\n",
    "        )  # only keep the noise of observed patches\n",
    "\n",
    "        # Sort noise for each sample\n",
    "        ids_shuffle = torch.argsort(\n",
    "            noise, dim=1\n",
    "        )  # Ascend: small is keep, large is remove\n",
    "        ids_restore = torch.argsort(\n",
    "            ids_shuffle, dim=1\n",
    "        )  # ids_restore: [batch_size x n_patches]\n",
    "\n",
    "        # Generate the binary mask: 0 is keep, 1 is remove\n",
    "        mask = torch.zeros(\n",
    "            [batch_size, n_patches], device=x.device\n",
    "        )  # mask: [batch_size x n_patches]\n",
    "        for i in range(batch_size):\n",
    "            mask[i, : len_keep[i]] = 1\n",
    "\n",
    "        # Unshuffle to get the binary mask\n",
    "        mask = torch.gather(mask, dim=1, index=ids_restore)\n",
    "\n",
    "        return mask.long()\n",
    "\n",
    "    def _mask_seq_view(self, x, input_mask=None):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            x : torch.Tensor of shape\n",
    "            [batch_size x n_channels x seq_len]\n",
    "            input_mask: torch.Tensor of shape [batch_size x seq_len]\n",
    "        Output:\n",
    "            mask : torch.Tensor of shape [batch_size x seq_len]\n",
    "        \"\"\"\n",
    "        x = x.unfold(dimension=-1, size=self.patch_len, step=self.stride)\n",
    "        mask = self._mask_patch_view(x, input_mask=input_mask)\n",
    "        return self.convert_patch_to_seq_view(mask, self.patch_len).long()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "class ForecastingHead(nn.Module):\n",
    "    def __init__(\n",
    "        self, head_nf: int = 768 * 64, forecast_horizon: int = 96, head_dropout: float = 0.0\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten(start_dim=-2)\n",
    "        self.dropout = nn.Dropout(head_dropout)\n",
    "        self.linear = nn.Linear(head_nf, forecast_horizon)\n",
    "\n",
    "    def forward(self, x, input_mask: torch.Tensor = None):\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. MOMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class MOMENT(BaseMultivariate):\n",
    "    \"\"\" MOMENT\n",
    "    \n",
    "    MOMENT is a family of open-source foundation models for time series forecasting. MOMENT uses a Transformer architecture, and works by learning embeddings of patches of time series and reconstructing the learned embeddings onto an output dimension to fulfil a set of time series tasks, such as forecasting, classification, anomaly detection and imputation. In this implementation, only the forecasting task is supported. This is a multivariate forecasting architecture.\n",
    "\n",
    "    **Parameters:**<br>\n",
    "    `h`: int, forecast horizon.<br>\n",
    "    `input_size`: int, considered autorregresive inputs (lags), y=[1,2,3,4] input_size=2 -> lags=[1,2].<br>\n",
    "    `n_series`: int, number of time-series.<br>\n",
    "    `d_model`: int=16, dimension for the Transformer backbone if not using a pretrained model.<br>\n",
    "    `revin_affine`: bool=False, if True adds learnable parameters to RevIN normalization. <br>\n",
    "    `patch_len`: int=8, length of patches.<br>\n",
    "    `patch_stride_len`: int=8, stride between each patch.<br>\n",
    "    `dropout`: float=0.1, dropout rate used for the dropout layers in the encoder.<br>\n",
    "    `head_dropout`: float=0.1, dropout rate used for the dropout layers in the decoder.<br>\n",
    "    `add_positional_embedding`: bool=False, adds Positional Embeddings if True.<br>\n",
    "    `value_embedding_bias`: bool=False, adds bias term to Value Embedding if True.<br>\n",
    "    `orth_gain`: float=1.41, gain used for orthogonal initialization of Patch Embeddings.<br>\n",
    "    `mask_ratio`: float=0.0, fraction of patches masked.<br>\n",
    "    `randomly_initialize_backbone`: bool=False, if True this will load a randomly initialized Transformer. Otherwise, a pre-trained model is chosen.<br>\n",
    "    `transformer_backbone`: str='google/flan-t5-base', backbone model used. Choose from \"google/flan-t5-small\", \"google/flan-t5-base\", \"google/flan-t5-large\", \"google/flan-t5-xl\", \"google/flan-t5-xxl\".<br>\n",
    "    `futr_exog_list`: str list, future exogenous columns.<br>\n",
    "    `hist_exog_list`: str list, historic exogenous columns.<br>\n",
    "    `stat_exog_list`: str list, static exogenous columns.<br>\n",
    "    `exclude_insample_y`: bool=False, the model skips the autoregressive features y[t-input_size:t] if True.<br>\n",
    "    `loss`: PyTorch module, instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
    "    `valid_loss`: PyTorch module=`loss`, instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
    "    `max_steps`: int=1000, maximum number of training steps.<br>\n",
    "    `learning_rate`: float=1e-3, Learning rate between (0, 1).<br>\n",
    "    `num_lr_decays`: int=-1, Number of learning rate decays, evenly distributed across max_steps.<br>\n",
    "    `early_stop_patience_steps`: int=-1, Number of validation iterations before early stopping.<br>\n",
    "    `val_check_steps`: int=100, Number of training steps between every validation loss check.<br>\n",
    "    `batch_size`: int=32, number of different series in each batch.<br>\n",
    "    `valid_batch_size`: int=None, number of different series in each validation and test batch, if None uses batch_size.<br>\n",
    "    `windows_batch_size`: int=1024, number of windows to sample in each training batch, default uses all.<br>\n",
    "    `inference_windows_batch_size`: int=-1, number of windows to sample in each inference batch, -1 uses all.<br>\n",
    "    `start_padding_enabled`: bool=False, if True, the model will pad the time series with zeros at the beginning, by input size.<br>\n",
    "    `step_size`: int=1, step size between each window of temporal data.<br>\n",
    "    `scaler_type`: str='identity', type of scaler for temporal inputs normalization see [temporal scalers](https://nixtla.github.io/neuralforecast/common.scalers.html).<br>\n",
    "    `random_seed`: int=1, random_seed for pytorch initializer and numpy generators.<br>\n",
    "    `num_workers_loader`: int=os.cpu_count(), workers to be used by `TimeSeriesDataLoader`.<br>\n",
    "    `drop_last_loader`: bool=False, if True `TimeSeriesDataLoader` drops last non-full batch.<br>\n",
    "    `alias`: str, optional,  Custom name of the model.<br>\n",
    "    `optimizer`: Subclass of 'torch.optim.Optimizer', optional, user specified optimizer instead of the default choice (Adam).<br>\n",
    "    `optimizer_kwargs`: dict, optional, list of parameters used by the user specified `optimizer`.<br>\n",
    "    `**trainer_kwargs`: int,  keyword trainer arguments inherited from [PyTorch Lighning's trainer](https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.trainer.trainer.Trainer.html?highlight=trainer).<br>    \n",
    "\n",
    "    **References**<br>\n",
    "    - [Goswami, Mononito, Konrad Szafer, Arjun Choudhry, Yifu Cai, Shuo Li, and Artur Dubrawski. (2024). MOMENT: A Family of Open Time-Series Foundation Models. International Conference on Machine Learning, Vienna, Austria. PMLR 235](https://arxiv.org/pdf/2402.03885)\n",
    "    \"\"\"\n",
    "    # Class attributes\n",
    "    SAMPLING_TYPE = 'multivariate'\n",
    "    \n",
    "    def __init__(self,\n",
    "                 h: int,\n",
    "                 input_size: int,\n",
    "                 n_series: int,\n",
    "                 d_model: Optional[int] = None,\n",
    "                 revin_affine: bool=False,\n",
    "                 patch_len: int=8,\n",
    "                 patch_stride_len: int=8, \n",
    "                 dropout: float=0.1,\n",
    "                 head_dropout: float=0.1, \n",
    "                 add_positional_embedding: bool=False, \n",
    "                 value_embedding_bias: bool=False, \n",
    "                 orth_gain: float=1.41, \n",
    "                 mask_ratio: float=0.0, \n",
    "                 randomly_initialize_backbone: bool=False,\n",
    "                 transformer_backbone: str='google/flan-t5-base',\n",
    "                 futr_exog_list = None,\n",
    "                 hist_exog_list = None,\n",
    "                 stat_exog_list = None,\n",
    "                 loss = MAE(),\n",
    "                 valid_loss = None,\n",
    "                 max_steps: int = 5,\n",
    "                 learning_rate: float = 1e-3,\n",
    "                 num_lr_decays: int = -1,\n",
    "                 early_stop_patience_steps: int =-1,\n",
    "                 val_check_steps: int = 100,\n",
    "                 batch_size: int = 32,\n",
    "                 step_size: int = 1,\n",
    "                 scaler_type: str = 'identity',\n",
    "                 random_seed: int = 1,\n",
    "                 num_workers_loader: int = 0,\n",
    "                 drop_last_loader: bool = False,\n",
    "                 optimizer = None,\n",
    "                 optimizer_kwargs = None,\n",
    "                 **trainer_kwargs):\n",
    "\n",
    "        super(MOMENT, self).__init__(\n",
    "            h=h,\n",
    "            input_size=input_size,\n",
    "            n_series=n_series,\n",
    "            futr_exog_list=futr_exog_list,\n",
    "            hist_exog_list=hist_exog_list,\n",
    "            stat_exog_list=stat_exog_list,\n",
    "            loss=loss,\n",
    "            valid_loss=valid_loss,\n",
    "            max_steps=max_steps,\n",
    "            learning_rate=learning_rate,\n",
    "            num_lr_decays=num_lr_decays,\n",
    "            early_stop_patience_steps=early_stop_patience_steps,\n",
    "            val_check_steps=val_check_steps,\n",
    "            batch_size=batch_size,\n",
    "            step_size=step_size,\n",
    "            scaler_type=scaler_type,\n",
    "            random_seed=random_seed,\n",
    "            num_workers_loader=num_workers_loader,\n",
    "            drop_last_loader=drop_last_loader,\n",
    "            optimizer=optimizer,\n",
    "            optimizer_kwargs=optimizer_kwargs,\n",
    "            **trainer_kwargs\n",
    "        )\n",
    "               \n",
    "        SUPPORTED_HUGGINGFACE_MODELS = [\n",
    "            \"google/flan-t5-small\",\n",
    "            \"google/flan-t5-base\",\n",
    "            \"google/flan-t5-large\",\n",
    "            \"google/flan-t5-xl\",\n",
    "            \"google/flan-t5-xxl\",\n",
    "        ]\n",
    "       \n",
    "        if d_model is None and transformer_backbone in SUPPORTED_HUGGINGFACE_MODELS:\n",
    "            if not IS_TRANSFORMERS_INSTALLED:\n",
    "                raise ImportError(\"Please install `transformers` to use MOMENT with a \"\n",
    "                                  \"HuggingFace model\")            \n",
    "            d_model = self.get_huggingface_model_dimensions(transformer_backbone)\n",
    "        elif d_model is None:\n",
    "            raise Exception(\"d_model must be specified if transformer backbone unless \"\n",
    "                            \"transformer backbone is a Huggingface model.\")\n",
    "\n",
    "        #----------------------------------- Parse dimensions -----------------------------------#\n",
    "        self.futr_exog_size = len(self.futr_exog_list)\n",
    "        self.hist_exog_size = len(self.hist_exog_list)\n",
    "        self.stat_exog_size = len(self.stat_exog_list)        \n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.patch_len = patch_len\n",
    "\n",
    "        if stat_exog_list is not None:\n",
    "            raise Exception(\"MOMENT does not support static exogenous variables\")\n",
    "        if futr_exog_list is not None:\n",
    "            raise Exception(\"MOMENT does not support future exogenous variables\")\n",
    "        if hist_exog_list is not None:\n",
    "            raise Exception(\"MOMENT does not support historical exogenous variables\")\n",
    "\n",
    "        #---------------------------------- Instantiate Model -----------------------------------#\n",
    "\n",
    "        self.normalizer = RevIN(\n",
    "            num_features=1, affine=revin_affine\n",
    "        )\n",
    "\n",
    "        self.tokenizer = Patching(\n",
    "            patch_len=patch_len, stride=patch_stride_len\n",
    "        )\n",
    "        self.patch_embedding = PatchEmbedding(\n",
    "            d_model=d_model,\n",
    "            seq_len=input_size,\n",
    "            patch_len=patch_len,\n",
    "            stride=patch_stride_len,\n",
    "            dropout=dropout,\n",
    "            add_positional_embedding=add_positional_embedding,\n",
    "            value_embedding_bias=value_embedding_bias,\n",
    "            orth_gain=orth_gain,\n",
    "        )\n",
    "        self.mask_generator = Masking(mask_ratio=mask_ratio)\n",
    "        self.encoder = self._get_transformer_backbone(transformer_backbone, randomly_initialize_backbone)\n",
    "\n",
    "        num_patches = (max(input_size, patch_len) - patch_len) // patch_stride_len + 1\n",
    "        self.head_nf = d_model * num_patches\n",
    "\n",
    "        self.head = ForecastingHead(\n",
    "                head_nf = self.head_nf,\n",
    "                forecast_horizon = h,\n",
    "                head_dropout = head_dropout,\n",
    "            )\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_transformer_backbone(transformer_backbone, randomly_initialize_backbone=False) -> nn.Module:\n",
    "        if randomly_initialize_backbone:\n",
    "            model_config = T5Config.from_pretrained(transformer_backbone)\n",
    "            transformer_backbone = T5Model(model_config)\n",
    "        else:\n",
    "            transformer_backbone = T5EncoderModel.from_pretrained(\n",
    "                transformer_backbone\n",
    "            )\n",
    "        transformer_backbone = transformer_backbone.get_encoder()\n",
    "\n",
    "        return transformer_backbone\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_huggingface_model_dimensions(model_name: str = \"google/flan-t5-base\"):\n",
    "        config = T5Config.from_pretrained(model_name)\n",
    "        return config.d_model\n",
    "\n",
    "    def forward(self, windows_batch):\n",
    "        # Parse windows_batch\n",
    "        x             = windows_batch['insample_y']                     #   [B, L, n_series (N)]\n",
    "\n",
    "        batch_size, seq_len = x.shape[:2]                               #   B = batch_size, L = seq_len\n",
    "\n",
    "        x_enc = x.permute(0, 2, 1)                                      #   [B, L, N] -> [B, N, L]\n",
    "        input_mask = torch.ones((batch_size, seq_len),\n",
    "                                device=x_enc.device,\n",
    "                                dtype=x_enc.dtype)\n",
    "\n",
    "        n_channels = x_enc.shape[1]\n",
    "\n",
    "        x_enc = self.normalizer(x=x_enc, mask=input_mask, mode=\"norm\")\n",
    "        x_enc = torch.nan_to_num(x_enc, nan=0, posinf=0, neginf=0)\n",
    "\n",
    "        x_enc = self.tokenizer(x = x_enc)\n",
    "        enc_in = self.patch_embedding(x_enc, mask=torch.ones_like(input_mask))\n",
    "\n",
    "        n_patches = enc_in.shape[2]\n",
    "        enc_in = enc_in.reshape(\n",
    "            (batch_size * n_channels, n_patches, self.d_model)\n",
    "        )\n",
    "\n",
    "        patch_view_mask = Masking.convert_seq_to_patch_view(input_mask, self.patch_len)\n",
    "        attention_mask = patch_view_mask.repeat_interleave(n_channels, dim=0)\n",
    "        outputs = self.encoder(inputs_embeds=enc_in, attention_mask=attention_mask)\n",
    "        enc_out = outputs.last_hidden_state\n",
    "        enc_out = enc_out.reshape((-1, \n",
    "                                   n_channels, \n",
    "                                   n_patches, \n",
    "                                   self.d_model))                       # [B, N, n_patches, d_model]\n",
    "\n",
    "        dec_out = self.head(enc_out)                                    # [B, N, h]\n",
    "        dec_out = self.normalizer(x=dec_out, mode=\"denorm\")\n",
    "\n",
    "        dec_out = dec_out.permute(0, 2, 1)                              # [B, N, h] -> [B, h, N]\n",
    "\n",
    "        # Map to output domain\n",
    "        forecast = self.loss.domain_map(dec_out)\n",
    "        \n",
    "        # domain_map might have squeezed the last dimension in case n_series == 1\n",
    "        # Note that this fails in case of a tuple loss, but Multivariate does not support tuple losses yet.\n",
    "        if forecast.ndim == 2:\n",
    "            return forecast.unsqueeze(-1)\n",
    "        else:\n",
    "            return forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(MOMENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(MOMENT.fit, name='MOMENT.fit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(MOMENT.predict, name='MOMENT.predict')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from neuralforecast import NeuralForecast\n",
    "from neuralforecast.utils import AirPassengersPanel, AirPassengersStatic\n",
    "from neuralforecast.losses.pytorch import MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_df = AirPassengersPanel[AirPassengersPanel.ds<AirPassengersPanel['ds'].values[-12]].reset_index(drop=True) # 132 train\n",
    "Y_test_df = AirPassengersPanel[AirPassengersPanel.ds>=AirPassengersPanel['ds'].values[-12]].reset_index(drop=True) # 12 test\n",
    "\n",
    "model = MOMENT(h=12,\n",
    "                input_size=24,\n",
    "                n_series=2,\n",
    "                scaler_type='identity',\n",
    "                max_steps=5,\n",
    "                )\n",
    "\n",
    "fcst = NeuralForecast(models=[model], freq='M')\n",
    "fcst.fit(df=Y_train_df, static_df=AirPassengersStatic, val_size=12)\n",
    "forecasts = fcst.predict(futr_df=Y_test_df)\n",
    "\n",
    "# Plot predictions\n",
    "fig, ax = plt.subplots(1, 1, figsize = (20, 7))\n",
    "Y_hat_df = forecasts.reset_index(drop=False).drop(columns=['unique_id','ds'])\n",
    "plot_df = pd.concat([Y_test_df, Y_hat_df], axis=1)\n",
    "plot_df = pd.concat([Y_train_df, plot_df])\n",
    "\n",
    "plot_df = plot_df[plot_df.unique_id=='Airline1'].drop('unique_id', axis=1)\n",
    "plt.plot(plot_df['ds'], plot_df['y'], c='black', label='True')\n",
    "plt.plot(plot_df['ds'], plot_df['MOMENT'], c='blue', label='Forecast')\n",
    "ax.set_title('AirPassengers Forecast', fontsize=22)\n",
    "ax.set_ylabel('Monthly Passengers', fontsize=20)\n",
    "ax.set_xlabel('Year', fontsize=20)\n",
    "ax.legend(prop={'size': 15})\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcst = NeuralForecast(models=[model], freq='M')\n",
    "forecasts = fcst.cross_validation(df=AirPassengersPanel, static_df=AirPassengersStatic, n_windows=2, step_size=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions\n",
    "fig, ax = plt.subplots(1, 1, figsize = (20, 7))\n",
    "Y_hat_df = forecasts.loc['Airline1']\n",
    "Y_df = AirPassengersPanel[AirPassengersPanel['unique_id']=='Airline1']\n",
    "\n",
    "plt.plot(Y_df['ds'], Y_df['y'], c='black', label='True')\n",
    "plt.plot(Y_hat_df['ds'], Y_hat_df['MOMENT'], c='blue', label='Forecast')\n",
    "ax.set_title('AirPassengers Forecast', fontsize=22)\n",
    "ax.set_ylabel('Monthly Passengers', fontsize=20)\n",
    "ax.set_xlabel('Year', fontsize=20)\n",
    "ax.legend(prop={'size': 15})\n",
    "ax.grid()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
