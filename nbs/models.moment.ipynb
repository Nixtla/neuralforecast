{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp models.moment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MOMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MOMENT is a family of open-source foundation models for time series forecasting. MOMENT uses a Transformer architecture, and works by learning embeddings of patches of time series and reconstructing the learned embeddings onto an output dimension to fulfil a set of time series tasks, such as forecasting, classification, anomaly detection and imputation. In this implementation, only the forecasting task is supported. This is a univariate forecasting architecture. MOMENT does not support exogenous variables.\n",
    "\n",
    "**References**<br>\n",
    "-[Goswami, Mononito, Konrad Szafer, Arjun Choudhry, Yifu Cai, Shuo Li, and Artur Dubrawski. (2024). MOMENT: A Family of Open Time-Series Foundation Models. International Conference on Machine Learning, Vienna, Austria. PMLR 235](https://arxiv.org/pdf/2402.03885)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Figure 1. Overview of MOMENT.](imgs_models/moment.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from fastcore.test import test_eq\n",
    "from nbdev.showdoc import show_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from typing import Optional, Union\n",
    "\n",
    "import warnings\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import numpy.typing as npt\n",
    "\n",
    "from copy import deepcopy\n",
    "from dataclasses import dataclass\n",
    "from argparse import Namespace\n",
    "from neuralforecast.losses.pytorch import MAE\n",
    "from neuralforecast.common._base_windows import BaseWindows\n",
    "try:\n",
    "    from transformers import T5Config, T5EncoderModel, T5Model\n",
    "    from huggingface_hub import PyTorchModelHubMixin\n",
    "    IS_TRANSFORMERS_INSTALLED = True\n",
    "except:\n",
    "    IS_TRANSFORMERS_INSTALLED = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Auxiliary Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RevIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def nanvar(tensor, dim=None, keepdim=False):\n",
    "    tensor_mean = tensor.nanmean(dim=dim, keepdim=True)\n",
    "    output = (tensor - tensor_mean).square().nanmean(dim=dim, keepdim=keepdim)\n",
    "    return output\n",
    "\n",
    "\n",
    "def nanstd(tensor, dim=None, keepdim=False):\n",
    "    output = nanvar(tensor, dim=dim, keepdim=keepdim)\n",
    "    output = output.sqrt()\n",
    "    return output\n",
    "\n",
    "class RevIN(nn.Module):\n",
    "    def __init__(self, num_features: int, eps: float = 1e-5, affine: bool = False):\n",
    "        \"\"\"\n",
    "        :param num_features: the number of features or channels\n",
    "        :param eps: a value added for numerical stability\n",
    "        :param affine: if True, RevIN has learnable affine parameters\n",
    "        \"\"\"\n",
    "        super(RevIN, self).__init__()\n",
    "        self.num_features = num_features\n",
    "        self.eps = eps\n",
    "        self.affine = affine\n",
    "\n",
    "        if self.affine:\n",
    "            self._init_params()\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mode: str = \"norm\", mask: torch.Tensor = None):\n",
    "        \"\"\"\n",
    "        :param x: input tensor of shape (batch_size, n_channels, seq_len)\n",
    "        :param mode: 'norm' or 'denorm'\n",
    "        :param mask: input mask of shape (batch_size, seq_len)\n",
    "        :return: RevIN transformed tensor\n",
    "        \"\"\"\n",
    "        if mode == \"norm\":\n",
    "            self._get_statistics(x, mask=mask)\n",
    "            x = self._normalize(x)\n",
    "        elif mode == \"denorm\":\n",
    "            x = self._denormalize(x)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        return x\n",
    "\n",
    "    def _init_params(self):\n",
    "        # initialize RevIN params: (C,)\n",
    "        self.affine_weight = nn.Parameter(torch.ones(1, self.num_features, 1))\n",
    "        self.affine_bias = nn.Parameter(torch.zeros(1, self.num_features, 1))\n",
    "\n",
    "    def _get_statistics(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        x    : batch_size x n_channels x seq_len\n",
    "        mask : batch_size x seq_len\n",
    "        \"\"\"\n",
    "        if mask is None:\n",
    "            mask = torch.ones((x.shape[0], x.shape[-1]))\n",
    "        n_channels = x.shape[1]\n",
    "        mask = mask.unsqueeze(1).repeat(1, n_channels, 1).bool()\n",
    "        # Set masked positions to NaN, and unmasked positions are taken from x\n",
    "        masked_x = torch.where(mask, x, torch.nan)\n",
    "        self.mean = torch.nanmean(masked_x, dim=-1, keepdim=True).detach()\n",
    "        self.stdev = nanstd(masked_x, dim=-1, keepdim=True).detach() + self.eps\n",
    "        # self.stdev = torch.sqrt(\n",
    "        #     torch.var(masked_x, dim=-1, keepdim=True) + self.eps).get_data().detach()\n",
    "        # NOTE: By default not bessel correction\n",
    "\n",
    "    def _normalize(self, x):\n",
    "        x = x - self.mean\n",
    "        x = x / self.stdev\n",
    "\n",
    "        if self.affine:\n",
    "            x = x * self.affine_weight\n",
    "            x = x + self.affine_bias\n",
    "        return x\n",
    "\n",
    "    def _denormalize(self, x):\n",
    "        if self.affine:\n",
    "            x = x - self.affine_bias\n",
    "            x = x / (self.affine_weight + self.eps * self.eps)\n",
    "        x = x * self.stdev\n",
    "        x = x + self.mean\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "class Masking:\n",
    "    def __init__(\n",
    "        self, mask_ratio: float = 0.3, patch_len: int = 8, stride: Optional[int] = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Indices with 0 mask are hidden, and with 1 are observed.\n",
    "        \"\"\"\n",
    "        self.mask_ratio = mask_ratio\n",
    "        self.patch_len = patch_len\n",
    "        self.stride = patch_len if stride is None else stride\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_seq_to_patch_view(\n",
    "        mask: torch.Tensor, patch_len: int = 8, stride: Optional[int] = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            mask : torch.Tensor of shape [batch_size x seq_len]\n",
    "        Output\n",
    "            mask : torch.Tensor of shape [batch_size x n_patches]\n",
    "        \"\"\"\n",
    "        stride = patch_len if stride is None else stride\n",
    "        mask = mask.unfold(dimension=-1, size=patch_len, step=stride)\n",
    "        # mask : [batch_size x n_patches x patch_len]\n",
    "        return (mask.sum(dim=-1) == patch_len).long()\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_patch_to_seq_view(\n",
    "        mask: torch.Tensor,\n",
    "        patch_len: int = 8,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            mask : torch.Tensor of shape [batch_size x n_patches]\n",
    "        Output:\n",
    "            mask : torch.Tensor of shape [batch_size x seq_len]\n",
    "        \"\"\"\n",
    "        return mask.repeat_interleave(patch_len, dim=-1)\n",
    "\n",
    "    def generate_mask(self, x: torch.Tensor, input_mask: Optional[torch.Tensor] = None):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            x : torch.Tensor of shape\n",
    "            [batch_size x n_channels x n_patches x patch_len] or\n",
    "            [batch_size x n_channels x seq_len]\n",
    "            input_mask: torch.Tensor of shape [batch_size x seq_len] or\n",
    "            [batch_size x n_patches]\n",
    "        Output:\n",
    "            mask : torch.Tensor of shape [batch_size x seq_len]\n",
    "        \"\"\"\n",
    "        if x.ndim == 4:\n",
    "            return self._mask_patch_view(x, input_mask=input_mask)\n",
    "        elif x.ndim == 3:\n",
    "            return self._mask_seq_view(x, input_mask=input_mask)\n",
    "\n",
    "    def _mask_patch_view(self, x, input_mask=None):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            x : torch.Tensor of shape\n",
    "            [batch_size x n_channels x n_patches x patch_len]\n",
    "            input_mask: torch.Tensor of shape [batch_size x seq_len]\n",
    "        Output:\n",
    "            mask : torch.Tensor of shape [batch_size x n_patches]\n",
    "        \"\"\"\n",
    "        input_mask = self.convert_seq_to_patch_view(\n",
    "            input_mask, self.patch_len, self.stride\n",
    "        )\n",
    "        n_observed_patches = input_mask.sum(dim=-1, keepdim=True)  # batch_size x 1\n",
    "\n",
    "        batch_size, _, n_patches, _ = x.shape\n",
    "        len_keep = torch.ceil(n_observed_patches * (1 - self.mask_ratio)).long()\n",
    "        noise = torch.rand(\n",
    "            batch_size, n_patches, device=x.device\n",
    "        )  # noise in [0, 1], batch_size x n_channels x n_patches\n",
    "        noise = torch.where(\n",
    "            input_mask == 1, noise, torch.ones_like(noise)\n",
    "        )  # only keep the noise of observed patches\n",
    "\n",
    "        # Sort noise for each sample\n",
    "        ids_shuffle = torch.argsort(\n",
    "            noise, dim=1\n",
    "        )  # Ascend: small is keep, large is remove\n",
    "        ids_restore = torch.argsort(\n",
    "            ids_shuffle, dim=1\n",
    "        )  # ids_restore: [batch_size x n_patches]\n",
    "\n",
    "        # Generate the binary mask: 0 is keep, 1 is remove\n",
    "        mask = torch.zeros(\n",
    "            [batch_size, n_patches], device=x.device\n",
    "        )  # mask: [batch_size x n_patches]\n",
    "        for i in range(batch_size):\n",
    "            mask[i, : len_keep[i]] = 1\n",
    "\n",
    "        # Unshuffle to get the binary mask\n",
    "        mask = torch.gather(mask, dim=1, index=ids_restore)\n",
    "\n",
    "        return mask.long()\n",
    "\n",
    "    def _mask_seq_view(self, x, input_mask=None):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            x : torch.Tensor of shape\n",
    "            [batch_size x n_channels x seq_len]\n",
    "            input_mask: torch.Tensor of shape [batch_size x seq_len]\n",
    "        Output:\n",
    "            mask : torch.Tensor of shape [batch_size x seq_len]\n",
    "        \"\"\"\n",
    "        x = x.unfold(dimension=-1, size=self.patch_len, step=self.stride)\n",
    "        mask = self._mask_patch_view(x, input_mask=input_mask)\n",
    "        return self.convert_patch_to_seq_view(mask, self.patch_len).long()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000, model_name=\"MOMENT\"):\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "        self.model_name = model_name\n",
    "\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model).float()\n",
    "        pe.require_grad = False\n",
    "\n",
    "        position = torch.arange(0, max_len).float().unsqueeze(1)\n",
    "        div_term = (\n",
    "            torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model)\n",
    "        ).exp()\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if (\n",
    "            self.model_name == \"MOMENT\"\n",
    "            or self.model_name == \"TimesNet\"\n",
    "            or self.model_name == \"GPT4TS\"\n",
    "        ):\n",
    "            return self.pe[:, : x.size(2)]\n",
    "        else:\n",
    "            return self.pe[:, : x.size(1)]\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int = 1024,\n",
    "        seq_len: int = 512,\n",
    "        patch_len: int = 8,\n",
    "        stride: int = 8,\n",
    "        dropout: float = 0.1,\n",
    "        add_positional_embedding: bool = False,\n",
    "        value_embedding_bias: bool = False,\n",
    "        orth_gain: float = 1.41,\n",
    "    ):\n",
    "        super(PatchEmbedding, self).__init__()\n",
    "        self.patch_len = patch_len\n",
    "        self.seq_len = seq_len\n",
    "        self.stride = stride\n",
    "        self.d_model = d_model\n",
    "        self.add_positional_embedding = add_positional_embedding\n",
    "\n",
    "        self.value_embedding = nn.Linear(patch_len, d_model, bias=value_embedding_bias)\n",
    "        self.mask_embedding = nn.Parameter(torch.zeros(d_model))\n",
    "\n",
    "        if orth_gain is not None:\n",
    "            torch.nn.init.orthogonal_(self.value_embedding.weight, gain=orth_gain)\n",
    "            if value_embedding_bias:\n",
    "                self.value_embedding.bias.data.zero_()\n",
    "            # torch.nn.init.orthogonal_(self.mask_embedding, gain=orth_gain) # Fails\n",
    "\n",
    "        # Positional embedding\n",
    "        if self.add_positional_embedding:\n",
    "            self.position_embedding = PositionalEmbedding(d_model)\n",
    "\n",
    "        # Residual dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask: torch.Tensor = None) -> torch.Tensor:\n",
    "        mask = Masking.convert_seq_to_patch_view(\n",
    "            mask, patch_len=self.patch_len\n",
    "        ).unsqueeze(-1)\n",
    "        # mask : [batch_size x n_patches x 1]\n",
    "        n_channels = x.shape[1]\n",
    "        mask = (\n",
    "            mask.repeat_interleave(self.d_model, dim=-1)\n",
    "            .unsqueeze(1)\n",
    "            .repeat(1, n_channels, 1, 1)\n",
    "        )\n",
    "        # mask : [batch_size x n_channels x n_patches x d_model]\n",
    "\n",
    "        # Input encoding\n",
    "        x = mask * self.value_embedding(x) + (1 - mask) * self.mask_embedding\n",
    "        if self.add_positional_embedding:\n",
    "            x = x + self.position_embedding(x)\n",
    "\n",
    "        return self.dropout(x)\n",
    "    \n",
    "class Patching(nn.Module):\n",
    "    def __init__(self, patch_len: int, stride: int):\n",
    "        super().__init__()\n",
    "        self.patch_len = patch_len\n",
    "        self.stride = stride\n",
    "        if self.stride != self.patch_len:\n",
    "            warnings.warn(\n",
    "                \"Stride and patch length are not equal. \"\n",
    "                \"This may lead to unexpected behavior.\"\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unfold(dimension=-1, size=self.patch_len, step=self.stride)\n",
    "        # x : [batch_size x n_channels x num_patch x patch_len]\n",
    "        return x        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "class NamespaceWithDefaults(Namespace):\n",
    "    @classmethod\n",
    "    def from_namespace(cls, namespace):\n",
    "        new_instance = cls()\n",
    "        for attr in dir(namespace):\n",
    "            if not attr.startswith(\"__\"):\n",
    "                setattr(new_instance, attr, getattr(namespace, attr))\n",
    "        return new_instance\n",
    "\n",
    "    def getattr(self, key, default=None):\n",
    "        return getattr(self, key, default)\n",
    "\n",
    "@dataclass\n",
    "class TimeseriesOutputs:\n",
    "    forecast: npt.NDArray = None\n",
    "    anomaly_scores: npt.NDArray = None\n",
    "    logits: npt.NDArray = None\n",
    "    labels: Optional[str] = None\n",
    "    input_mask: npt.NDArray = None\n",
    "    pretrain_mask: npt.NDArray = None\n",
    "    reconstruction: npt.NDArray = None\n",
    "    embeddings: npt.NDArray = None\n",
    "    metadata: Optional[dict] = None\n",
    "    illegal_output: bool = False\n",
    "\n",
    "@dataclass\n",
    "class TASKS:\n",
    "    RECONSTRUCTION: str = \"reconstruction\"\n",
    "    FORECASTING: str = \"forecasting\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "SUPPORTED_HUGGINGFACE_MODELS = [\n",
    "    \"google/flan-t5-small\",\n",
    "    \"google/flan-t5-base\",\n",
    "    \"google/flan-t5-large\",\n",
    "    \"google/flan-t5-xl\",\n",
    "    \"google/flan-t5-xxl\",\n",
    "]\n",
    "\n",
    "class PretrainHead(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int = 1024,\n",
    "        patch_len: int = 8,\n",
    "        head_dropout: float = 0.1,\n",
    "        orth_gain: float = 1.41,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(head_dropout)\n",
    "        self.linear = nn.Linear(d_model, patch_len)\n",
    "\n",
    "        if orth_gain is not None:\n",
    "            torch.nn.init.orthogonal_(self.linear.weight, gain=orth_gain)\n",
    "            self.linear.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(self.dropout(x))\n",
    "        x = x.flatten(start_dim=2, end_dim=3)\n",
    "        return x\n",
    "\n",
    "class ForecastingHead(nn.Module):\n",
    "    def __init__(\n",
    "        self, head_nf: int = 1024 * 64, forecast_horizon: int = 96, head_dropout: int = 0\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten(start_dim=-2)\n",
    "        self.dropout = nn.Dropout(head_dropout)\n",
    "        self.linear = nn.Linear(head_nf, forecast_horizon)\n",
    "\n",
    "    def forward(self, x, input_mask: torch.Tensor = None):\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class MOMENT_module(nn.Module):\n",
    "    def __init__(self, config: Union[Namespace, dict], **kwargs: dict):\n",
    "        super().__init__()\n",
    "        config = self._update_inputs(config, **kwargs)\n",
    "        config = self._validate_inputs(config)\n",
    "        self.config = config\n",
    "        self.task_name = config.task_name\n",
    "        self.seq_len = config.seq_len\n",
    "        self.patch_len = config.patch_len\n",
    "\n",
    "        self.normalizer = RevIN(\n",
    "            num_features=1, affine=config.getattr(\"revin_affine\", False)\n",
    "        )\n",
    "        self.tokenizer = Patching(\n",
    "            patch_len=config.patch_len, stride=config.patch_stride_len\n",
    "        )\n",
    "        self.patch_embedding = PatchEmbedding(\n",
    "            d_model=config.d_model,\n",
    "            seq_len=config.seq_len,\n",
    "            patch_len=config.patch_len,\n",
    "            stride=config.patch_stride_len,\n",
    "            dropout=config.getattr(\"dropout\", 0.1),\n",
    "            add_positional_embedding=config.getattr(\"add_positional_embedding\", True),\n",
    "            value_embedding_bias=config.getattr(\"value_embedding_bias\", False),\n",
    "            orth_gain=config.getattr(\"orth_gain\", 1.41),\n",
    "        )\n",
    "        self.mask_generator = Masking(mask_ratio=config.getattr(\"mask_ratio\", 0.0))\n",
    "        self.encoder = self._get_transformer_backbone(config)\n",
    "        self.head = self._get_head(self.task_name)\n",
    "\n",
    "        # Frozen parameters\n",
    "        self.freeze_embedder = config.getattr(\"freeze_embedder\", True)\n",
    "        self.freeze_encoder = config.getattr(\"freeze_encoder\", True)\n",
    "        self.freeze_head = config.getattr(\"freeze_head\", False)\n",
    "\n",
    "        if self.freeze_embedder:\n",
    "            self.patch_embedding = freeze_parameters(self.patch_embedding)\n",
    "        if self.freeze_encoder:\n",
    "            self.encoder = freeze_parameters(self.encoder)\n",
    "        if self.freeze_head:\n",
    "            self.head = freeze_parameters(self.head)\n",
    "\n",
    "    def _update_inputs(\n",
    "        self, config: Union[Namespace, dict], **kwargs: dict\n",
    "    ) -> NamespaceWithDefaults:\n",
    "        if isinstance(config, dict) and \"model_kwargs\" in kwargs:\n",
    "            return NamespaceWithDefaults(**{**config, **kwargs[\"model_kwargs\"]})\n",
    "        else:\n",
    "            return NamespaceWithDefaults.from_namespace(config)\n",
    "\n",
    "    def _validate_inputs(self, config: NamespaceWithDefaults) -> NamespaceWithDefaults:\n",
    "        if (\n",
    "            config.d_model is None\n",
    "            and config.transformer_backbone in SUPPORTED_HUGGINGFACE_MODELS\n",
    "        ):\n",
    "            config.d_model = self.get_huggingface_model_dimensions(\n",
    "                config.transformer_backbone\n",
    "            )\n",
    "        elif config.d_model is None:\n",
    "            raise ValueError(\n",
    "                \"d_model must be specified if transformer backbone \"\n",
    "                \"unless transformer backbone is a Huggingface model.\"\n",
    "            )\n",
    "\n",
    "        if config.transformer_type not in [\n",
    "            \"encoder_only\",\n",
    "            \"decoder_only\",\n",
    "            \"encoder_decoder\",\n",
    "        ]:\n",
    "            raise ValueError(\n",
    "                \"transformer_type must be one of \"\n",
    "                \"['encoder_only', 'decoder_only', 'encoder_decoder']\"\n",
    "            )\n",
    "\n",
    "        if config.patch_stride_len != config.patch_len:\n",
    "            warnings.warn(\"Patch stride length is not equal to patch length.\")\n",
    "        return config\n",
    "\n",
    "    def _get_head(self, task_name: str) -> nn.Module:\n",
    "        if task_name == TASKS.RECONSTRUCTION:\n",
    "            return PretrainHead(\n",
    "                self.config.d_model,\n",
    "                self.config.patch_len,\n",
    "                self.config.getattr(\"dropout\", 0.1),\n",
    "                self.config.getattr(\"orth_gain\", 1.41),\n",
    "            )\n",
    "        elif task_name == TASKS.FORECASTING:\n",
    "            num_patches = (\n",
    "                max(self.config.seq_len, self.config.patch_len) - self.config.patch_len\n",
    "            ) // self.config.patch_stride_len + 1\n",
    "            self.head_nf = self.config.d_model * num_patches\n",
    "            return ForecastingHead(\n",
    "                self.head_nf,\n",
    "                self.config.forecast_horizon,\n",
    "                self.config.getattr(\"head_dropout\", 0.1),\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Task {task_name} not implemented.\")\n",
    "\n",
    "    def _get_transformer_backbone(self, config) -> nn.Module:\n",
    "        if config.getattr(\"randomly_initialize_backbone\", False):\n",
    "            model_config = T5Config.from_pretrained(config.transformer_backbone)\n",
    "            transformer_backbone = T5Model(model_config)\n",
    "        else:\n",
    "            transformer_backbone = T5EncoderModel.from_pretrained(\n",
    "                config.transformer_backbone\n",
    "            )\n",
    "\n",
    "        transformer_backbone = transformer_backbone.get_encoder()\n",
    "        return transformer_backbone\n",
    "\n",
    "    def __call__(self, *args, **kwargs) -> TimeseriesOutputs:\n",
    "        return self.forward(*args, **kwargs)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_huggingface_model_dimensions(model_name):\n",
    "        config = T5Config.from_pretrained(model_name)\n",
    "        return config.d_model\n",
    "\n",
    "    def reconstruction(\n",
    "        self,\n",
    "        x_enc: torch.Tensor,\n",
    "        input_mask: torch.Tensor = None,\n",
    "        mask: torch.Tensor = None,\n",
    "        **kwargs,\n",
    "    ) -> TimeseriesOutputs:\n",
    "        batch_size, n_channels, _ = x_enc.shape\n",
    "\n",
    "        if mask is None:\n",
    "            mask = self.mask_generator.generate_mask(x=x_enc, input_mask=input_mask)\n",
    "            mask = mask.to(x_enc.device)  # mask: [batch_size x seq_len]\n",
    "\n",
    "        x_enc = self.normalizer(x=x_enc, mask=mask * input_mask, mode=\"norm\")\n",
    "        # Prevent too short time-series from causing NaNs\n",
    "        x_enc = torch.nan_to_num(x_enc, nan=0, posinf=0, neginf=0)\n",
    "\n",
    "        x_enc = self.tokenizer(x=x_enc)\n",
    "        enc_in = self.patch_embedding(x_enc, mask=mask)\n",
    "\n",
    "        n_patches = enc_in.shape[2]\n",
    "        enc_in = enc_in.reshape(\n",
    "            (batch_size * n_channels, n_patches, self.config.d_model)\n",
    "        )\n",
    "\n",
    "        patch_view_mask = Masking.convert_seq_to_patch_view(input_mask, self.patch_len)\n",
    "        attention_mask = patch_view_mask.repeat_interleave(n_channels, dim=0)\n",
    "        if self.config.transformer_type == \"encoder_decoder\":\n",
    "            outputs = self.encoder(\n",
    "                inputs_embeds=enc_in,\n",
    "                decoder_inputs_embeds=enc_in,\n",
    "                attention_mask=attention_mask,\n",
    "            )\n",
    "        else:\n",
    "            outputs = self.encoder(inputs_embeds=enc_in, attention_mask=attention_mask)\n",
    "        enc_out = outputs.last_hidden_state\n",
    "\n",
    "        enc_out = enc_out.reshape((-1, n_channels, n_patches, self.config.d_model))\n",
    "\n",
    "        dec_out = self.head(enc_out)  # [batch_size x n_channels x seq_len]\n",
    "        dec_out = self.normalizer(x=dec_out, mode=\"denorm\")\n",
    "\n",
    "        if self.config.getattr(\"debug\", False):\n",
    "            illegal_output = self._check_model_weights_for_illegal_values()\n",
    "        else:\n",
    "            illegal_output = None\n",
    "\n",
    "        return TimeseriesOutputs(\n",
    "            input_mask=input_mask,\n",
    "            reconstruction=dec_out,\n",
    "            pretrain_mask=mask,\n",
    "            illegal_output=illegal_output,\n",
    "        )\n",
    "\n",
    "    def forecast(\n",
    "        self, x_enc: torch.Tensor, input_mask: torch.Tensor = None, **kwargs\n",
    "    ) -> TimeseriesOutputs:\n",
    "        batch_size, n_channels, seq_len = x_enc.shape\n",
    "\n",
    "        x_enc = self.normalizer(x=x_enc, mask=input_mask, mode=\"norm\")\n",
    "        x_enc = torch.nan_to_num(x_enc, nan=0, posinf=0, neginf=0)\n",
    "\n",
    "        x_enc = self.tokenizer(x=x_enc)\n",
    "        enc_in = self.patch_embedding(x_enc, mask=torch.ones_like(input_mask))\n",
    "\n",
    "        n_patches = enc_in.shape[2]\n",
    "        enc_in = enc_in.reshape(\n",
    "            (batch_size * n_channels, n_patches, self.config.d_model)\n",
    "        )\n",
    "\n",
    "        patch_view_mask = Masking.convert_seq_to_patch_view(input_mask, self.patch_len)\n",
    "        attention_mask = patch_view_mask.repeat_interleave(n_channels, dim=0)\n",
    "        outputs = self.encoder(inputs_embeds=enc_in, attention_mask=attention_mask)\n",
    "        enc_out = outputs.last_hidden_state\n",
    "        enc_out = enc_out.reshape((-1, n_channels, n_patches, self.config.d_model))\n",
    "        # [batch_size x n_channels x n_patches x d_model]\n",
    "\n",
    "        dec_out = self.head(enc_out)  # [batch_size x n_channels x forecast_horizon]\n",
    "        dec_out = self.normalizer(x=dec_out, mode=\"denorm\")\n",
    "\n",
    "        return TimeseriesOutputs(input_mask=input_mask, forecast=dec_out)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x_enc: torch.Tensor,\n",
    "        mask: torch.Tensor = None,\n",
    "        input_mask: torch.Tensor = None,\n",
    "        **kwargs,\n",
    "    ) -> TimeseriesOutputs:\n",
    "        if input_mask is None:\n",
    "            input_mask = torch.ones_like(x_enc[:, 0, :])\n",
    "\n",
    "        if self.task_name == TASKS.RECONSTRUCTION:\n",
    "            return self.reconstruction(\n",
    "                x_enc=x_enc, mask=mask, input_mask=input_mask, **kwargs\n",
    "            )\n",
    "        elif self.task_name == TASKS.FORECASTING:\n",
    "            return self.forecast(x_enc=x_enc, input_mask=input_mask, **kwargs)\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Task {self.task_name} not implemented.\")\n",
    "\n",
    "if IS_TRANSFORMERS_INSTALLED:\n",
    "    class MOMENTPipeline(MOMENT_module, PyTorchModelHubMixin):\n",
    "        def __init__(self, config: Union[Namespace, dict], **kwargs: dict):\n",
    "            self._validate_model_kwargs(**kwargs)\n",
    "            self.new_task_name = kwargs.get(\"model_kwargs\", {}).pop(\n",
    "                \"task_name\", TASKS.RECONSTRUCTION\n",
    "            )\n",
    "            super().__init__(config, **kwargs)\n",
    "\n",
    "        def _validate_model_kwargs(self, **kwargs: dict) -> None:\n",
    "            kwargs = deepcopy(kwargs)\n",
    "            kwargs.setdefault(\"model_kwargs\", {\"task_name\": TASKS.RECONSTRUCTION})\n",
    "            kwargs[\"model_kwargs\"].setdefault(\"task_name\", TASKS.RECONSTRUCTION)\n",
    "            config = Namespace(**kwargs[\"model_kwargs\"])\n",
    "\n",
    "            if config.task_name == TASKS.FORECASTING:\n",
    "                if not hasattr(config, \"forecast_horizon\"):\n",
    "                    raise ValueError(\n",
    "                        \"forecast_horizon must be specified for long-horizon forecasting.\"\n",
    "                    )\n",
    "\n",
    "        def init(self) -> None:\n",
    "            if self.new_task_name != TASKS.RECONSTRUCTION:\n",
    "                self.task_name = self.new_task_name\n",
    "                self.head = self._get_head(self.new_task_name)\n",
    "\n",
    "def freeze_parameters(model):\n",
    "    \"\"\"\n",
    "    Freeze parameters of the model\n",
    "    \"\"\"\n",
    "    # Freeze the parameters\n",
    "    for name, param in model.named_parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. MOMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class MOMENT(BaseWindows):\n",
    "    \"\"\" MOMENT\n",
    "    \n",
    "    MOMENT is a family of open-source foundation models for time series forecasting. MOMENT uses a Transformer architecture, and works by learning embeddings of patches of time series and reconstructing the learned embeddings onto an output dimension to fulfil a set of time series tasks, such as forecasting, classification, anomaly detection and imputation. In this implementation, only the forecasting task is supported. This is a univariate forecasting architecture.\n",
    "\n",
    "    **Parameters:**<br>\n",
    "    `h`: int, forecast horizon.<br>\n",
    "    `input_size`: int, considered autorregresive inputs (lags), y=[1,2,3,4] input_size=2 -> lags=[1,2].<br>\n",
    "    `model_name`: str=\"AutonLab/MOMENT-1-large\", which model to use. Supported models: [\"AutonLab/MOMENT-1-large\"] <br> \n",
    "    `futr_exog_list`: str list, future exogenous columns.<br>\n",
    "    `hist_exog_list`: str list, historic exogenous columns.<br>\n",
    "    `stat_exog_list`: str list, static exogenous columns.<br>\n",
    "    `exclude_insample_y`: bool=False, the model skips the autoregressive features y[t-input_size:t] if True.<br>\n",
    "    `loss`: PyTorch module, instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
    "    `valid_loss`: PyTorch module=`loss`, instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
    "    `max_steps`: int=1000, maximum number of training steps.<br>\n",
    "    `learning_rate`: float=1e-3, Learning rate between (0, 1).<br>\n",
    "    `num_lr_decays`: int=-1, Number of learning rate decays, evenly distributed across max_steps.<br>\n",
    "    `early_stop_patience_steps`: int=-1, Number of validation iterations before early stopping.<br>\n",
    "    `val_check_steps`: int=100, Number of training steps between every validation loss check.<br>\n",
    "    `batch_size`: int=32, number of different series in each batch.<br>\n",
    "    `valid_batch_size`: int=None, number of different series in each validation and test batch, if None uses batch_size.<br>\n",
    "    `windows_batch_size`: int=1024, number of windows to sample in each training batch, default uses all.<br>\n",
    "    `inference_windows_batch_size`: int=-1, number of windows to sample in each inference batch, -1 uses all.<br>\n",
    "    `start_padding_enabled`: bool=False, if True, the model will pad the time series with zeros at the beginning, by input size.<br>\n",
    "    `step_size`: int=1, step size between each window of temporal data.<br>\n",
    "    `scaler_type`: str='identity', type of scaler for temporal inputs normalization see [temporal scalers](https://nixtla.github.io/neuralforecast/common.scalers.html).<br>\n",
    "    `random_seed`: int=1, random_seed for pytorch initializer and numpy generators.<br>\n",
    "    `num_workers_loader`: int=os.cpu_count(), workers to be used by `TimeSeriesDataLoader`.<br>\n",
    "    `drop_last_loader`: bool=False, if True `TimeSeriesDataLoader` drops last non-full batch.<br>\n",
    "    `alias`: str, optional,  Custom name of the model.<br>\n",
    "    `optimizer`: Subclass of 'torch.optim.Optimizer', optional, user specified optimizer instead of the default choice (Adam).<br>\n",
    "    `optimizer_kwargs`: dict, optional, list of parameters used by the user specified `optimizer`.<br>\n",
    "    `**trainer_kwargs`: int,  keyword trainer arguments inherited from [PyTorch Lighning's trainer](https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.trainer.trainer.Trainer.html?highlight=trainer).<br>    \n",
    "\n",
    "    **References**<br>\n",
    "    - [Goswami, Mononito, Konrad Szafer, Arjun Choudhry, Yifu Cai, Shuo Li, and Artur Dubrawski. (2024). MOMENT: A Family of Open Time-Series Foundation Models. International Conference on Machine Learning, Vienna, Austria. PMLR 235](https://arxiv.org/pdf/2402.03885)\n",
    "    \"\"\"\n",
    "    # Class attributes\n",
    "    SAMPLING_TYPE = 'windows'\n",
    "    \n",
    "    def __init__(self,\n",
    "                 h: int,\n",
    "                 input_size: int,\n",
    "                 model_name = \"AutonLab/MOMENT-1-large\",\n",
    "                 futr_exog_list = None,\n",
    "                 hist_exog_list = None,\n",
    "                 stat_exog_list = None,\n",
    "                 exclude_insample_y = False,\n",
    "                 loss = MAE(),\n",
    "                 valid_loss = None,\n",
    "                 max_steps: int = 5,\n",
    "                 learning_rate: float = 1e-3,\n",
    "                 num_lr_decays: int = -1,\n",
    "                 early_stop_patience_steps: int =-1,\n",
    "                 val_check_steps: int = 100,\n",
    "                 batch_size: int = 32,\n",
    "                 valid_batch_size: Optional[int] = None,\n",
    "                 windows_batch_size = 1024,\n",
    "                 inference_windows_batch_size = 1024,\n",
    "                 start_padding_enabled = False,\n",
    "                 step_size: int = 1,\n",
    "                 scaler_type: str = 'identity',\n",
    "                 random_seed: int = 1,\n",
    "                 num_workers_loader: int = 0,\n",
    "                 drop_last_loader: bool = False,\n",
    "                 optimizer = None,\n",
    "                 optimizer_kwargs = None,\n",
    "                 **trainer_kwargs):\n",
    "\n",
    "        super(MOMENT, self).__init__(\n",
    "            h=h,\n",
    "            input_size=input_size,\n",
    "            futr_exog_list=futr_exog_list,\n",
    "            hist_exog_list=hist_exog_list,\n",
    "            stat_exog_list=stat_exog_list,\n",
    "            exclude_insample_y = exclude_insample_y,\n",
    "            loss=loss,\n",
    "            valid_loss=valid_loss,\n",
    "            max_steps=max_steps,\n",
    "            learning_rate=learning_rate,\n",
    "            num_lr_decays=num_lr_decays,\n",
    "            early_stop_patience_steps=early_stop_patience_steps,\n",
    "            val_check_steps=val_check_steps,\n",
    "            batch_size=batch_size,\n",
    "            valid_batch_size=valid_batch_size,\n",
    "            windows_batch_size=windows_batch_size,\n",
    "            inference_windows_batch_size=inference_windows_batch_size,\n",
    "            start_padding_enabled=start_padding_enabled,\n",
    "            step_size=step_size,\n",
    "            scaler_type=scaler_type,\n",
    "            random_seed=random_seed,\n",
    "            num_workers_loader=num_workers_loader,\n",
    "            drop_last_loader=drop_last_loader,\n",
    "            optimizer=optimizer,\n",
    "            optimizer_kwargs=optimizer_kwargs,\n",
    "            **trainer_kwargs\n",
    "        )\n",
    "               \n",
    "        #----------------------------------- Parse dimensions -----------------------------------#\n",
    "        self.futr_exog_size = len(self.futr_exog_list)\n",
    "        self.hist_exog_size = len(self.hist_exog_list)\n",
    "        self.stat_exog_size = len(self.stat_exog_list)        \n",
    "\n",
    "        if stat_exog_list is not None:\n",
    "            raise Exception(\"MOMENT does not support static exogenous variables\")\n",
    "        if futr_exog_list is not None:\n",
    "            raise Exception(\"MOMENT does not support future exogenous variables\")\n",
    "        if hist_exog_list is not None:\n",
    "            raise Exception(\"MOMENT does not support historical exogenous variables\")\n",
    "        if self.loss.outputsize_multiplier != 1:\n",
    "            raise Exception(f\"MOMENT does not support {loss}. Please use a point loss (MAE, MSE, ...).\")\n",
    "        if self.valid_loss.outputsize_multiplier != 1:\n",
    "            raise Exception(f\"MOMENT does not support {valid_loss}. Please use a point valid_loss (MAE, MSE, ...).\")\n",
    "        \n",
    "        SUPPORTED_MODELS = [\"AutonLab/MOMENT-1-large\"]\n",
    "        if model_name not in SUPPORTED_MODELS:\n",
    "            raise Exception(f\"model_name='{model_name}' not supported. Please use a \"\n",
    "                            f\"model_name from this list: {SUPPORTED_MODELS}\")\n",
    "            \n",
    "        #---------------------------------- Instantiate Model -----------------------------------#\n",
    "        if not IS_TRANSFORMERS_INSTALLED:\n",
    "            raise ImportError(\"Please install `transformers` to use MOMENT\")\n",
    "        \n",
    "        moment = MOMENTPipeline.from_pretrained(\n",
    "            model_name, \n",
    "            model_kwargs={\n",
    "                'task_name': 'forecasting',\n",
    "                'forecast_horizon': h,\n",
    "                'seq_len': input_size,\n",
    "                'head_dropout': 0.1,\n",
    "                'weight_decay': 0,\n",
    "                'freeze_embedder': True,\n",
    "                'freeze_encoder': True,\n",
    "                'freeze_head': False,\n",
    "            },\n",
    "        )\n",
    "        moment.init()\n",
    "        self.moment = moment\n",
    "\n",
    "    def forward(self, windows_batch):\n",
    "        # Parse windows_batch\n",
    "        x_enc      = windows_batch['insample_y'].unsqueeze(-2)    #   [B, L] -> [B, 1, L]\n",
    "        input_mask = windows_batch['insample_mask']               #   [B, L]\n",
    "\n",
    "        #  Run MOMENT\n",
    "        output = self.moment(x_enc=x_enc, input_mask=input_mask)\n",
    "        \n",
    "        # Map to output domain\n",
    "        output = output.forecast.squeeze()                        # [B, 1, h] -> [B, h]\n",
    "        forecast = self.loss.domain_map(output)\n",
    "        \n",
    "        return forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(MOMENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(MOMENT.fit, name='MOMENT.fit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(MOMENT.predict, name='MOMENT.predict')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from neuralforecast import NeuralForecast\n",
    "from neuralforecast.utils import AirPassengersPanel, AirPassengersStatic\n",
    "from neuralforecast.losses.pytorch import MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "Y_train_df = AirPassengersPanel[AirPassengersPanel.ds<AirPassengersPanel['ds'].values[-12]].reset_index(drop=True) # 132 train\n",
    "Y_test_df = AirPassengersPanel[AirPassengersPanel.ds>=AirPassengersPanel['ds'].values[-12]].reset_index(drop=True) # 12 test\n",
    "\n",
    "model = MOMENT(h=12,\n",
    "                input_size=24,\n",
    "                scaler_type='identity',\n",
    "                max_steps=5,\n",
    "                )\n",
    "\n",
    "fcst = NeuralForecast(models=[model], freq='M')\n",
    "fcst.fit(df=Y_train_df, static_df=AirPassengersStatic, val_size=12)\n",
    "forecasts = fcst.predict(futr_df=Y_test_df)\n",
    "\n",
    "# Plot predictions\n",
    "fig, ax = plt.subplots(1, 1, figsize = (20, 7))\n",
    "Y_hat_df = forecasts.reset_index(drop=False).drop(columns=['unique_id','ds'])\n",
    "plot_df = pd.concat([Y_test_df, Y_hat_df], axis=1)\n",
    "plot_df = pd.concat([Y_train_df, plot_df])\n",
    "\n",
    "plot_df = plot_df[plot_df.unique_id=='Airline1'].drop('unique_id', axis=1)\n",
    "plt.plot(plot_df['ds'], plot_df['y'], c='black', label='True')\n",
    "plt.plot(plot_df['ds'], plot_df['MOMENT'], c='blue', label='Forecast')\n",
    "ax.set_title('AirPassengers Forecast', fontsize=22)\n",
    "ax.set_ylabel('Monthly Passengers', fontsize=20)\n",
    "ax.set_xlabel('Year', fontsize=20)\n",
    "ax.legend(prop={'size': 15})\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "fcst = NeuralForecast(models=[model], freq='M')\n",
    "forecasts = fcst.cross_validation(df=AirPassengersPanel, static_df=AirPassengersStatic, n_windows=2, step_size=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "# Plot predictions\n",
    "fig, ax = plt.subplots(1, 1, figsize = (20, 7))\n",
    "Y_hat_df = forecasts.loc['Airline1']\n",
    "Y_df = AirPassengersPanel[AirPassengersPanel['unique_id']=='Airline1']\n",
    "\n",
    "plt.plot(Y_df['ds'], Y_df['y'], c='black', label='True')\n",
    "plt.plot(Y_hat_df['ds'], Y_hat_df['MOMENT'], c='blue', label='Forecast')\n",
    "ax.set_title('AirPassengers Forecast', fontsize=22)\n",
    "ax.set_ylabel('Monthly Passengers', fontsize=20)\n",
    "ax.set_xlabel('Year', fontsize=20)\n",
    "ax.legend(prop={'size': 15})\n",
    "ax.grid()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
