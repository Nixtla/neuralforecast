{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524620c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15392f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fa25a4",
   "metadata": {},
   "source": [
    "# <span style=\"color:DarkOrange\"> Core </span>\n",
    "> NeuralForecast contains two main components, PyTorch implementations deep learning predictive models, as well as parallelization and distributed computation utilities. The first component comprises low-level PyTorch model estimator classes like `models.NBEATS` and `models.RNN`. The second component is a high-level `core.NeuralForecast` wrapper class that operates with sets of time series data stored in pandas DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2508f7a9-1433-4ad8-8f2f-0078c6ed6c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import shutil\n",
    "from fastcore.test import test_eq\n",
    "from nbdev.showdoc import show_doc\n",
    "from neuralforecast.utils import generate_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44065066-e72a-431f-938f-1528adef9fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import os\n",
    "import pickle\n",
    "import warnings\n",
    "\n",
    "from os.path import isfile, join\n",
    "from typing import Any, List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from neuralforecast.tsdataset import TimeSeriesDataset\n",
    "from neuralforecast.models import (\n",
    "    GRU, LSTM, RNN, TCN, DilatedRNN,\n",
    "    MLP, NHITS, NBEATS, NBEATSx,\n",
    "    TFT, VanillaTransformer,\n",
    "    Informer, Autoformer,\n",
    "    StemGNN, PatchTST\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45471e9f-4050-4a6c-bb05-4de144754da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def _cv_dates(last_dates, freq, h, test_size, step_size=1):\n",
    "    #assuming step_size = 1\n",
    "    if (test_size - h) % step_size:\n",
    "        raise Exception('`test_size - h` should be module `step_size`')\n",
    "    n_windows = int((test_size - h) / step_size) + 1\n",
    "    if len(np.unique(last_dates)) == 1:\n",
    "        if issubclass(last_dates.dtype.type, np.integer):\n",
    "            total_dates = np.arange(last_dates[0] - test_size + 1, last_dates[0] + 1)\n",
    "            out = np.empty((h * n_windows, 2), dtype=last_dates.dtype)\n",
    "            freq = 1\n",
    "        else:\n",
    "            total_dates = pd.date_range(end=last_dates[0], periods=test_size, freq=freq)\n",
    "            out = np.empty((h * n_windows, 2), dtype='datetime64[s]')\n",
    "        for i_window, cutoff in enumerate(range(-test_size, -h + 1, step_size), start=0):\n",
    "            end_cutoff = cutoff + h\n",
    "            out[h * i_window : h * (i_window + 1), 0] = total_dates[cutoff:] if end_cutoff == 0 else total_dates[cutoff:end_cutoff]\n",
    "            out[h * i_window : h * (i_window + 1), 1] = np.tile(total_dates[cutoff] - freq, h)\n",
    "        dates = pd.DataFrame(np.tile(out, (len(last_dates), 1)), columns=['ds', 'cutoff'])\n",
    "    else:\n",
    "        dates = pd.concat([_cv_dates(np.array([ld]), freq, h, test_size, step_size) for ld in last_dates])\n",
    "        dates = dates.reset_index(drop=True)\n",
    "    return dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b2e948",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def _insample_dates(uids, last_dates, freq, h, len_series, step_size=1):\n",
    "    if (len(np.unique(last_dates)) == 1) and (len(np.unique(len_series)) == 1):\n",
    "        # Dates can be generated simulatenously if ld and ls are the same for all series\n",
    "        dates = _cv_dates(last_dates, freq, h, len_series[0], step_size)\n",
    "        dates['unique_id'] = np.repeat(uids, len(dates)//len(uids))\n",
    "    else:\n",
    "        dates = []\n",
    "        for ui, ld, ls in zip(uids, last_dates, len_series):\n",
    "        # Dates have to be generated for each series separately, considering its own ld and ls\n",
    "            dates_series = _cv_dates(np.array([ld]), freq, h, ls, step_size)\n",
    "            dates_series['unique_id'] = ui\n",
    "            dates.append(dates_series)\n",
    "        dates = pd.concat(dates)\n",
    "    dates = dates.reset_index(drop=True)\n",
    "    dates = dates[['unique_id', 'ds', 'cutoff']]\n",
    "    return dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ff82c9-3cb5-407f-be34-141d265e8930",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "ds_int_cv_test = pd.DataFrame({\n",
    "    'ds': np.hstack([\n",
    "        [46, 47, 48],\n",
    "        [47, 48, 49],\n",
    "        [48, 49, 50]\n",
    "    ]),\n",
    "    'cutoff': [45] * 3 + [46] * 3 + [47] * 3\n",
    "}, dtype=np.int64)\n",
    "test_eq(ds_int_cv_test, _cv_dates(np.array([50], dtype=np.int64), 'D', 3, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac141f22-ee59-408a-a5e8-4a5f72cb77e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "ds_int_cv_test = pd.DataFrame({\n",
    "    'ds': np.hstack([\n",
    "        [46, 47, 48],\n",
    "        [48, 49, 50]\n",
    "    ]),\n",
    "    'cutoff': [45] * 3 + [47] * 3\n",
    "}, dtype=np.int64)\n",
    "test_eq(ds_int_cv_test, _cv_dates(np.array([50], dtype=np.int64), 'D', 3, 5, step_size=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2874985-3d14-40d2-a607-a774ed4abd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "for e_e in [True, False]:\n",
    "    n_series = 2\n",
    "    ga, indices, dates, ds = TimeSeriesDataset.from_df(generate_series(n_series, equal_ends=e_e), sort_df=True)\n",
    "    freq = pd.tseries.frequencies.to_offset('D')\n",
    "    horizon = 3\n",
    "    test_size = 5\n",
    "    df_dates = _cv_dates(last_dates=dates, freq=freq, h=horizon, test_size=test_size)\n",
    "    test_eq(len(df_dates), n_series * horizon * (test_size - horizon + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c58a8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "MODEL_FILENAME_DICT = {'gru': GRU, 'lstm': LSTM, 'rnn': RNN, \n",
    "                       'tcn': TCN, 'dilatedrnn': DilatedRNN,\n",
    "                       'mlp': MLP, 'nbeats': NBEATS, 'nbeatsx': NBEATSx, 'nhits': NHITS,  \n",
    "                       'tft': TFT, 'stemgnn': StemGNN, 'informer': Informer,\n",
    "                       'autogru': GRU, 'autolstm': LSTM, 'autornn': RNN,\n",
    "                       'autotcn': TCN, 'autodilatedrnn': DilatedRNN,\n",
    "                       'automlp': MLP, 'autonbeats': NBEATS, 'autonhits': NHITS,\n",
    "                       'autotft': TFT, 'autovanillatransformer': VanillaTransformer,\n",
    "                       'autoinformer': Informer, 'autoautoformer': Autoformer,\n",
    "                       'autopatchtst': PatchTST,\n",
    "                       'autostemgnn': StemGNN, \n",
    "                       }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4dae43c-4d11-4bbc-a431-ac33b004859a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class NeuralForecast:\n",
    "    \n",
    "    def __init__(self, \n",
    "                 models: List[Any],\n",
    "                 freq: str):\n",
    "        \"\"\"\n",
    "        The `core.StatsForecast` class allows you to efficiently fit multiple `NeuralForecast` models \n",
    "        for large sets of time series. It operates with pandas DataFrame `df` that identifies series \n",
    "        and datestamps with the `unique_id` and `ds` columns. The `y` column denotes the target \n",
    "        time series variable.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        models : List[typing.Any]\n",
    "            Instantiated `neuralforecast.models` \n",
    "            see [collection here](https://nixtla.github.io/neuralforecast/models.html).\n",
    "        freq : str\n",
    "            Frequency of the data, \n",
    "            see [panda's available frequencies](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases).\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        self : NeuralForecast\n",
    "            Returns instantiated `NeuralForecast` class.\n",
    "        \"\"\"\n",
    "        assert all(model.h == models[0].h for model in models), 'All models should have the same horizon'\n",
    "\n",
    "        self.h = models[0].h\n",
    "        self.models = models\n",
    "        self.freq = pd.tseries.frequencies.to_offset(freq)\n",
    "\n",
    "        # Flags and attributes\n",
    "        self._fitted = False\n",
    "\n",
    "    def _prepare_fit(self, df, static_df, sort_df):\n",
    "        #TODO: uids, last_dates and ds should be properties of the dataset class. See github issue.\n",
    "        dataset, uids, last_dates, ds = TimeSeriesDataset.from_df(df=df,\n",
    "                                                                  static_df=static_df,\n",
    "                                                                  sort_df=sort_df)\n",
    "        return dataset, uids, last_dates, ds\n",
    "\n",
    "    def fit(self,\n",
    "            df: Optional[pd.DataFrame] = None,\n",
    "            static_df: Optional[pd.DataFrame] = None,\n",
    "            val_size: Optional[int] = 0,\n",
    "            sort_df: bool = True,\n",
    "            verbose: bool = False):\n",
    "        \"\"\"Fit the core.NeuralForecast.\n",
    "\n",
    "        Fit `models` to a large set of time series from DataFrame `df`.\n",
    "        and store fitted models for later inspection.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pandas.DataFrame, optional (default=None)\n",
    "            DataFrame with columns [`unique_id`, `ds`, `y`] and exogenous variables.\n",
    "            If None, a previously stored dataset is required.\n",
    "        static_df : pandas.DataFrame, optional (default=None)\n",
    "            DataFrame with columns [`unique_id`, `ds`] and static exogenous.\n",
    "        val_size : int, optional (default=0)\n",
    "            Size of validation set.\n",
    "        sort_df : bool, optional (default=False)\n",
    "            Sort `df` before fitting.\n",
    "        verbose : bool (default=False)\n",
    "            Print processing steps.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : NeuralForecast\n",
    "            Returns `NeuralForecast` class with fitted `models`.\n",
    "        \"\"\"\n",
    "        if (df is None) and not (hasattr(self, 'dataset')):\n",
    "            raise Exception('You must pass a DataFrame or have one stored.')\n",
    "\n",
    "        # Process and save new dataset (in self)\n",
    "        if df is not None:\n",
    "            self.dataset, self.uids, self.last_dates, self.ds = self._prepare_fit(df=df, static_df=static_df, sort_df=sort_df)\n",
    "            self.sort_df = sort_df\n",
    "        else:\n",
    "            if verbose: print('Using stored dataset.')\n",
    "\n",
    "        if val_size is not None:\n",
    "            if self.dataset.min_size < val_size:\n",
    "                warnings.warn('Validation set size is larger than the shorter time-series.')\n",
    "\n",
    "        #train + validation\n",
    "        for model in self.models:\n",
    "            model.fit(self.dataset, val_size=val_size)\n",
    "        #train with the full dataset\n",
    "\n",
    "        self._fitted = True\n",
    "\n",
    "    def _make_future_df(self, h: int):\n",
    "        if issubclass(self.last_dates.dtype.type, np.integer):\n",
    "            last_date_f = lambda x: np.arange(x + 1, x + 1 + h, dtype=self.last_dates.dtype)\n",
    "        else:\n",
    "            last_date_f = lambda x: pd.date_range(x + self.freq, periods=h, freq=self.freq)\n",
    "        if len(np.unique(self.last_dates)) == 1:\n",
    "            dates = np.tile(last_date_f(self.last_dates[0]), len(self.dataset))\n",
    "        else:\n",
    "            dates = np.hstack([last_date_f(last_date)\n",
    "                               for last_date in self.last_dates])\n",
    "        idx = pd.Index(np.repeat(self.uids, h), name='unique_id')\n",
    "        df = pd.DataFrame({'ds': dates}, index=idx)\n",
    "        return df\n",
    "\n",
    "    def predict(self,\n",
    "                df: Optional[pd.DataFrame] = None,\n",
    "                static_df: Optional[pd.DataFrame] = None,\n",
    "                futr_df: Optional[pd.DataFrame] = None,\n",
    "                sort_df: bool = True,\n",
    "                verbose: bool = False,\n",
    "                **data_kwargs):\n",
    "        \"\"\"Predict with core.NeuralForecast.\n",
    "\n",
    "        Use stored fitted `models` to predict large set of time series from DataFrame `df`.        \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pandas.DataFrame, optional (default=None)\n",
    "            DataFrame with columns [`unique_id`, `ds`, `y`] and exogenous variables.\n",
    "            If a DataFrame is passed, it is used to generate forecasts.\n",
    "        static_df : pandas.DataFrame, optional (default=None)\n",
    "            DataFrame with columns [`unique_id`, `ds`] and static exogenous.\n",
    "        futr_df : pandas.DataFrame, optional (default=None)\n",
    "            DataFrame with [`unique_id`, `ds`] columns and `df`'s future exogenous.\n",
    "        sort_df : bool (default=True)\n",
    "            Sort `df` before fitting.\n",
    "        verbose : bool (default=False)\n",
    "            Print processing steps.\n",
    "        data_kwargs : kwargs\n",
    "            Extra arguments to be passed to the dataset within each model.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        fcsts_df : pandas.DataFrame\n",
    "            DataFrame with insample `models` columns for point predictions and probabilistic\n",
    "            predictions for all fitted `models`.    \n",
    "        \"\"\"\n",
    "        if (df is None) and not (hasattr(self, 'dataset')):\n",
    "            raise Exception('You must pass a DataFrame or have one stored.')\n",
    "\n",
    "        # Process new dataset but does not store it.\n",
    "        if df is not None:\n",
    "            dataset, *_ = self._prepare_fit(df=df, static_df=static_df, sort_df=sort_df)\n",
    "        else:\n",
    "            dataset = self.dataset\n",
    "            if verbose: print('Using stored dataset.')\n",
    "\n",
    "        cols = []\n",
    "        count_names = {'model': 0}\n",
    "        for model in self.models:\n",
    "            model_name = repr(model)\n",
    "            count_names[model_name] = count_names.get(model_name, -1) + 1\n",
    "            if count_names[model_name] > 0:\n",
    "                model_name += str(count_names[model_name])\n",
    "            cols += [model_name + n for n in model.loss.output_names]\n",
    "\n",
    "        # Placeholder dataframe for predictions with unique_id and ds\n",
    "        fcsts_df = self._make_future_df(h=self.h)\n",
    "\n",
    "        # Update and define new forecasting dataset\n",
    "        if futr_df is not None:\n",
    "            dataset = TimeSeriesDataset.update_dataset(dataset=dataset, future_df=futr_df)\n",
    "        else:\n",
    "            dataset = TimeSeriesDataset.update_dataset(dataset=dataset, future_df=fcsts_df.reset_index())\n",
    "\n",
    "        col_idx = 0\n",
    "        fcsts = np.full((self.h * len(self.uids), len(cols)), fill_value=np.nan)\n",
    "        for model in self.models:\n",
    "            old_test_size = model.get_test_size()\n",
    "            model.set_test_size(self.h) # To predict h steps ahead\n",
    "            model_fcsts = model.predict(dataset=dataset, **data_kwargs)\n",
    "            # Append predictions in memory placeholder\n",
    "            output_length = len(model.loss.output_names)\n",
    "            fcsts[:,col_idx:col_idx+output_length] = model_fcsts\n",
    "            col_idx += output_length\n",
    "            model.set_test_size(old_test_size) # Set back to original value\n",
    "\n",
    "        # Declare predictions pd.DataFrame\n",
    "        fcsts = pd.DataFrame.from_records(fcsts, columns=cols, \n",
    "                                          index=fcsts_df.index)\n",
    "        fcsts_df = pd.concat([fcsts_df, fcsts], axis=1)\n",
    "\n",
    "        return fcsts_df\n",
    "    \n",
    "    def cross_validation(self,\n",
    "                         df: Optional[pd.DataFrame] = None,\n",
    "                         static_df: Optional[pd.DataFrame] = None,\n",
    "                         n_windows: int = 1,\n",
    "                         step_size: int = 1,\n",
    "                         val_size: Optional[int] = 0, \n",
    "                         test_size: Optional[int] = None,\n",
    "                         sort_df: bool = True,\n",
    "                         fit_models: bool= True,\n",
    "                         verbose: bool = False,\n",
    "                         **data_kwargs):\n",
    "        \"\"\"Temporal Cross-Validation with core.NeuralForecast.\n",
    "\n",
    "        `core.NeuralForecast`'s cross-validation efficiently fits a list of NeuralForecast \n",
    "        models through multiple windows, in either chained or rolled manner.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pandas.DataFrame, optional (default=None)\n",
    "            DataFrame with columns [`unique_id`, `ds`, `y`] and exogenous variables.\n",
    "            If None, a previously stored dataset is required.\n",
    "        static_df : pandas.DataFrame, optional (default=None)\n",
    "            DataFrame with columns [`unique_id`, `ds`] and static exogenous.\n",
    "        n_windows : int (default=1)\n",
    "            Number of windows used for cross validation.\n",
    "        step_size : int (default=1)\n",
    "            Step size between each window.\n",
    "        val_size : int, optional (default=None)\n",
    "            Length of validation size. If passed, set `n_windows=None`.\n",
    "        test_size : int, optional (default=None)\n",
    "            Length of test size. If passed, set `n_windows=None`.\n",
    "        sort_df : bool (default=True)\n",
    "            Sort `df` before fitting.\n",
    "        fit_models: bool (default=True)\n",
    "            Fit models before cross-validation.\n",
    "        verbose : bool (default=False)\n",
    "            Print processing steps.\n",
    "        data_kwargs : kwargs\n",
    "            Extra arguments to be passed to the dataset within each model.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        fcsts_df : pandas.DataFrame\n",
    "            DataFrame with insample `models` columns for point predictions and probabilistic\n",
    "            predictions for all fitted `models`.    \n",
    "        \"\"\"\n",
    "        if (df is None) and not (hasattr(self, 'dataset')):\n",
    "            raise Exception('You must pass a DataFrame or have one stored.')\n",
    "\n",
    "        # Process and save new dataset (in self)\n",
    "        if df is not None:\n",
    "            self.dataset, self.uids, self.last_dates, self.ds = self._prepare_fit(df=df, static_df=static_df, sort_df=sort_df)\n",
    "            self.sort_df = sort_df\n",
    "        else:\n",
    "            if verbose: print('Using stored dataset.')\n",
    "\n",
    "        cols = []\n",
    "        count_names = {'model': 0}\n",
    "        for model in self.models:\n",
    "            model_name = repr(model)\n",
    "            count_names[model_name] = count_names.get(model_name, -1) + 1\n",
    "            if count_names[model_name] > 0:\n",
    "                model_name += str(count_names[model_name])\n",
    "            cols += [model_name + n for n in model.loss.output_names]            \n",
    "\n",
    "        h = self.models[0].h\n",
    "        if test_size is None:\n",
    "            test_size = h + step_size * (n_windows - 1)\n",
    "        elif n_windows is None:\n",
    "            if (test_size - h) % step_size:\n",
    "                raise Exception('`test_size - h` should be module `step_size`')\n",
    "            n_windows = int((test_size - h) / step_size) + 1\n",
    "        elif (n_windows is None) and (test_size is None):\n",
    "            raise Exception('you must define `n_windows` or `test_size`')\n",
    "        else:\n",
    "            raise Exception('you must define `n_windows` or `test_size` but not both')\n",
    "\n",
    "        if val_size is not None:\n",
    "            if self.dataset.min_size < (val_size+test_size):\n",
    "                warnings.warn('Validation and test sets are larger than the shorter time-series.')\n",
    "\n",
    "        fcsts_df = _cv_dates(last_dates=self.last_dates, freq=self.freq, \n",
    "                             h=h, test_size=test_size, step_size=step_size)\n",
    "        idx = pd.Index(np.repeat(self.uids, h * n_windows), name='unique_id')\n",
    "        fcsts_df.index = idx\n",
    "\n",
    "        col_idx = 0\n",
    "        fcsts = np.full((self.dataset.n_groups * h * n_windows, len(cols)),\n",
    "                         np.nan, dtype=np.float32)\n",
    "        for model in self.models:\n",
    "            # Fit\n",
    "            if fit_models:\n",
    "                model.fit(dataset=self.dataset,\n",
    "                          val_size=val_size, \n",
    "                          test_size=test_size)\n",
    "            else:\n",
    "                model.set_test_size(test_size=test_size)\n",
    "            \n",
    "            # Predict\n",
    "            model_fcsts = model.predict(self.dataset, step_size=step_size, **data_kwargs)\n",
    "\n",
    "            # Append predictions in memory placeholder\n",
    "            output_length = len(model.loss.output_names)\n",
    "            fcsts[:,col_idx:(col_idx + output_length)] = model_fcsts\n",
    "            col_idx += output_length\n",
    "        if fit_models:\n",
    "            self._fitted = True                \n",
    "\n",
    "        # Add predictions to forecasts DataFrame\n",
    "        fcsts = pd.DataFrame.from_records(fcsts, columns=cols, \n",
    "                                          index=fcsts_df.index)\n",
    "        fcsts_df = pd.concat([fcsts_df, fcsts], axis=1)\n",
    "\n",
    "        # Add original input df's y to forecasts DataFrame\n",
    "        fcsts_df = fcsts_df.merge(df, how='left', on=['unique_id', 'ds'])\n",
    "        return fcsts_df\n",
    "\n",
    "    def predict_insample(self, step_size: int = 1):\n",
    "        \"\"\"Predict insample with core.NeuralForecast.\n",
    "\n",
    "        `core.NeuralForecast`'s `predict_insample` uses stored fitted `models`\n",
    "        to predict historic values of a time series from the stored dataframe.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        step_size : int (default=1)\n",
    "            Step size between each window.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        fcsts_df : pandas.DataFrame\n",
    "            DataFrame with insample predictions for all fitted `models`.    \n",
    "        \"\"\"\n",
    "        if not self._fitted:\n",
    "            raise Exception('The models must be fitted first with `fit` or `cross_validation`.')\n",
    "\n",
    "        for model in self.models:\n",
    "            if model.SAMPLING_TYPE == 'recurrent':\n",
    "                warnings.warn(f'Predict insample might not provide accurate predictions for \\\n",
    "                       recurrent model {repr(model)} yet due to scaling.')\n",
    "                print(f'WARNING: Predict insample might not provide accurate predictions for \\\n",
    "                      recurrent model {repr(model)} yet due to scaling.')\n",
    "        \n",
    "        cols = []\n",
    "        count_names = {'model': 0}\n",
    "        for model in self.models:\n",
    "            model_name = repr(model)\n",
    "            count_names[model_name] = count_names.get(model_name, -1) + 1\n",
    "            if count_names[model_name] > 0:\n",
    "                model_name += str(count_names[model_name])\n",
    "            cols += [model_name + n for n in model.loss.output_names]\n",
    "\n",
    "        # Remove test set from dataset and last dates\n",
    "        test_size = self.models[0].get_test_size()\n",
    "        if test_size>0:\n",
    "            dataset = TimeSeriesDataset.trim_dataset(dataset=self.dataset,\n",
    "                                                     right_trim=test_size,\n",
    "                                                     left_trim=0)\n",
    "            last_dates_train = self.last_dates.shift(-test_size, freq=self.freq)\n",
    "        else:\n",
    "            dataset = self.dataset\n",
    "            last_dates_train = self.last_dates\n",
    "\n",
    "        # Generate dates\n",
    "        len_series = np.diff(dataset.indptr) # Computes the length of each time series based on indptr\n",
    "        fcsts_df = _insample_dates(uids=self.uids,\n",
    "                                   last_dates=last_dates_train,\n",
    "                                   freq=self.freq,\n",
    "                                   h=self.h,\n",
    "                                   len_series=len_series,\n",
    "                                   step_size=step_size)\n",
    "        fcsts_df = fcsts_df.set_index('unique_id')\n",
    "\n",
    "        col_idx = 0\n",
    "        fcsts = np.full((len(fcsts_df), len(cols)),\n",
    "                         np.nan, dtype=np.float32)\n",
    "\n",
    "        for model in self.models:\n",
    "            # Test size is the number of periods to forecast (full size of trimmed dataset)\n",
    "            model.set_test_size(test_size=dataset.max_size)\n",
    "\n",
    "            # Predict\n",
    "            model_fcsts = model.predict(dataset, step_size=step_size)\n",
    "            # Append predictions in memory placeholder\n",
    "            output_length = len(model.loss.output_names)\n",
    "            fcsts[:,col_idx:(col_idx + output_length)] = model_fcsts\n",
    "            col_idx += output_length          \n",
    "            model.set_test_size(test_size=test_size) # Set original test_size      \n",
    "\n",
    "        # Add predictions to forecasts DataFrame\n",
    "        fcsts = pd.DataFrame.from_records(fcsts, columns=cols, \n",
    "                                          index=fcsts_df.index)\n",
    "        fcsts_df = pd.concat([fcsts_df, fcsts], axis=1)\n",
    "\n",
    "        # Add original input df's y to forecasts DataFrame\n",
    "        Y_df = pd.DataFrame.from_records(self.dataset.temporal[:,[0]].numpy(),\n",
    "                                         columns=['y'], index=self.ds)\n",
    "        Y_df = Y_df.reset_index(drop=False)\n",
    "        fcsts_df = fcsts_df.merge(Y_df, how='left', on=['unique_id', 'ds'])\n",
    "        return fcsts_df\n",
    "\n",
    "    def predict_rolled(self,\n",
    "                         df: Optional[pd.DataFrame] = None,\n",
    "                         static_df: Optional[pd.DataFrame] = None,\n",
    "                         n_windows: int = 1,\n",
    "                         step_size: int = 1,\n",
    "                         insample_size: Optional[int] = None,\n",
    "                         sort_df: bool = True,\n",
    "                         verbose: bool = False,\n",
    "                         **data_kwargs):\n",
    "        \"\"\"Predict insample with core.NeuralForecast.\n",
    "\n",
    "        Use stored fitted `models` to predict historic values of a time series from DataFrame `df`.    \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pandas.DataFrame, optional (default=None)\n",
    "            DataFrame with columns [`unique_id`, `ds`, `y`] and exogenous variables.\n",
    "            If None, a previously stored dataset is required.\n",
    "        static_df : pandas.DataFrame, optional (default=None)\n",
    "            DataFrame with columns [`unique_id`, `ds`] and static exogenous.\n",
    "        n_windows : int (default=1)\n",
    "            Number of windows used for cross validation.\n",
    "        step_size : int (default=1)\n",
    "            Step size between each window.\n",
    "        insample_size : int, optional (default=None)\n",
    "            Length of insample size to produce forecasts. If passed, set `n_windows=None`.\n",
    "        sort_df : bool (default=True)\n",
    "            Sort `df` before fitting.\n",
    "        verbose : bool (default=False)\n",
    "            Print processing steps.\n",
    "        data_kwargs : kwargs\n",
    "            Extra arguments to be passed to the dataset within each model.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        fcsts_df : pandas.DataFrame\n",
    "            DataFrame with insample `models` columns for point predictions and probabilistic\n",
    "            predictions for all fitted `models`.    \n",
    "        \"\"\"\n",
    "        print('WARNING: this method will be deprecated. Use `cross_validation` or `predict_insample` instead.')\n",
    "        fcsts_df = self.cross_validation(df=df, static_df=static_df, n_windows=n_windows, \n",
    "                                         step_size=step_size, val_size=0, test_size=insample_size, \n",
    "                                         sort_df=sort_df, fit_models=False, verbose=verbose)\n",
    "        return fcsts_df\n",
    "        \n",
    "    # Save list of models with pytorch lightning save_checkpoint function\n",
    "    def save(self, path: str, model_index: Optional[List]=None, save_dataset: bool=True, overwrite: bool=False):\n",
    "        \"\"\"Save NeuralForecast core class.\n",
    "\n",
    "        `core.NeuralForecast`'s method to save current status of models, dataset, and configuration.\n",
    "        Note that by default the `models` are not saving training checkpoints to save disk memory,\n",
    "        to get them change the individual model `**trainer_kwargs` to include `enable_checkpointing=True`.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        path : str\n",
    "            Directory to save current status.\n",
    "        model_index : list, optional (default=None)\n",
    "            List to specify which models from list of self.models to save.\n",
    "        save_dataset : bool (default=True)\n",
    "            Whether to save dataset or not.\n",
    "        overwrite : bool (default=False)\n",
    "            Whether to overwrite files or not.\n",
    "        \"\"\"\n",
    "        # Standarize path without '/'\n",
    "        if path[-1] == '/':\n",
    "            path = path[:-1]\n",
    "\n",
    "        # Model index list\n",
    "        if model_index is None:\n",
    "            model_index = list(range(len(self.models)))\n",
    "\n",
    "        # Create directory if not exists\n",
    "        os.makedirs(path, exist_ok = True)\n",
    "\n",
    "        # Check if directory is empty to protect overwriting files\n",
    "        dir = os.listdir(path)\n",
    "        \n",
    "        # Checking if the list is empty or not\n",
    "        if (len(dir) > 0) and (not overwrite):\n",
    "            raise Exception('Directory is not empty. Set `overwrite=True` to overwrite files.')\n",
    "\n",
    "        # Save models\n",
    "        count_names = {'model': 0}\n",
    "        for i, model in enumerate(self.models):\n",
    "            # Skip model if not in list\n",
    "            if i not in model_index:\n",
    "                continue\n",
    "\n",
    "            model_name = repr(model).lower().replace('_', '')\n",
    "            count_names[model_name] = count_names.get(model_name, -1) + 1\n",
    "            model.save(f\"{path}/{model_name}_{count_names[model_name]}.ckpt\")\n",
    "\n",
    "        # Save dataset\n",
    "        if (save_dataset) and (hasattr(self, 'dataset')):\n",
    "            with open(f\"{path}/dataset.pkl\", \"wb\") as f:\n",
    "                pickle.dump(self.dataset, f)\n",
    "        elif save_dataset:\n",
    "            raise Exception('You need to have a stored dataset to save it, \\\n",
    "                             set `save_dataset=False` to skip saving dataset.')\n",
    "\n",
    "        # Save configuration and parameters\n",
    "        config_dict = {'h': self.h,\n",
    "                       'freq': self.freq,\n",
    "                       'uids': self.uids,\n",
    "                       'last_dates': self.last_dates,\n",
    "                       'ds': self.ds,\n",
    "                       'sort_df': self.sort_df,\n",
    "                       '_fitted': self._fitted}\n",
    "\n",
    "        with open(f\"{path}/configuration.pkl\", \"wb\") as f:\n",
    "                pickle.dump(config_dict, f)\n",
    "\n",
    "    @staticmethod\n",
    "    def load(path, verbose=False):\n",
    "        \"\"\"Load NeuralForecast\n",
    "\n",
    "        `core.NeuralForecast`'s method to load checkpoint from path.\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        path : str\n",
    "            Directory to save current status.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        result : NeuralForecast\n",
    "            Instantiated `NeuralForecast` class.\n",
    "        \"\"\"\n",
    "        files = [f for f in os.listdir(path) if isfile(join(path, f))]\n",
    "\n",
    "        # Load models\n",
    "        models_ckpt = [f for f in files if f.endswith('.ckpt')]\n",
    "        if len(models_ckpt) == 0:\n",
    "            raise Exception('No model found in directory.') \n",
    "        \n",
    "        if verbose: print(10 * '-' + ' Loading models ' + 10 * '-')\n",
    "        models = []\n",
    "        for model in models_ckpt:\n",
    "            model_name = model.split('_')[0]\n",
    "            models.append(MODEL_FILENAME_DICT[model_name].load_from_checkpoint(f\"{path}/{model}\"))\n",
    "            if verbose: print(f\"Model {model_name} loaded.\")\n",
    "\n",
    "        if verbose: print(10*'-' + ' Loading dataset ' + 10*'-')\n",
    "        # Load dataset\n",
    "        if 'dataset.pkl' in files:\n",
    "            with open(f\"{path}/dataset.pkl\", \"rb\") as f:\n",
    "                dataset = pickle.load(f)\n",
    "            if verbose: print('Dataset loaded.')\n",
    "        else:\n",
    "            dataset = None\n",
    "            if verbose: print('No dataset found in directory.')\n",
    "        \n",
    "        if verbose: print(10*'-' + ' Loading configuration ' + 10*'-')\n",
    "        # Load configuration\n",
    "        if 'configuration.pkl' in files:\n",
    "            with open(f\"{path}/configuration.pkl\", \"rb\") as f:\n",
    "                config_dict = pickle.load(f)\n",
    "            if verbose: print('Configuration loaded.')\n",
    "        else:\n",
    "            raise Exception('No configuration found in directory.')\n",
    "\n",
    "        # Create NeuralForecast object\n",
    "        neuralforecast = NeuralForecast(models=models, freq=config_dict['freq'])\n",
    "\n",
    "        # Dataset\n",
    "        if dataset is not None:\n",
    "            neuralforecast.dataset = dataset\n",
    "            neuralforecast.uids = config_dict['uids']\n",
    "            neuralforecast.last_dates = config_dict['last_dates']\n",
    "            neuralforecast.ds = config_dict['ds']\n",
    "            neuralforecast.sort_df = config_dict['sort_df']\n",
    "\n",
    "        # Fitted flag\n",
    "        neuralforecast._fitted = config_dict['_fitted']\n",
    "\n",
    "        return neuralforecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8898e349-8000-4668-a1c5-52c03c69e85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import logging\n",
    "import warnings\n",
    "logging.getLogger(\"pytorch_lightning\").setLevel(logging.ERROR)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bede563-78c0-40ee-ba76-f06f329cd772",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(NeuralForecast.fit, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90209f6-16da-40a6-8302-1c5c2f66c619",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(NeuralForecast.predict, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a8923a-f4f3-4e60-b9b9-a7088fc9bff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(NeuralForecast.cross_validation, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355df52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(NeuralForecast.predict_rolled, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b534d29d-eecc-43ba-8468-c23305fa24a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import matplotlib.pyplot as plt\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from ray import tune\n",
    "\n",
    "from neuralforecast.auto import (\n",
    "    AutoMLP, AutoNBEATS, \n",
    "    AutoRNN, AutoTCN, AutoDilatedRNN,\n",
    ")\n",
    "\n",
    "from neuralforecast.models.rnn import RNN\n",
    "from neuralforecast.models.tcn import TCN\n",
    "from neuralforecast.models.dilated_rnn import DilatedRNN\n",
    "\n",
    "from neuralforecast.models.mlp import MLP\n",
    "from neuralforecast.models.nhits import NHITS\n",
    "from neuralforecast.models.nbeats import NBEATS\n",
    "from neuralforecast.models.nbeatsx import NBEATSx\n",
    "\n",
    "from neuralforecast.models.tft import TFT\n",
    "from neuralforecast.models.vanillatransformer import VanillaTransformer\n",
    "from neuralforecast.models.informer import Informer\n",
    "from neuralforecast.models.autoformer import Autoformer\n",
    "\n",
    "from neuralforecast.models.stemgnn import StemGNN\n",
    "\n",
    "from neuralforecast.losses.pytorch import MQLoss, MAE, MSE\n",
    "from neuralforecast.utils import AirPassengersDF, AirPassengersPanel, AirPassengersStatic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd1507c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "AirPassengersPanel_train = AirPassengersPanel[AirPassengersPanel['ds'] < AirPassengersPanel['ds'].values[-12]].reset_index(drop=True)\n",
    "AirPassengersPanel_test = AirPassengersPanel[AirPassengersPanel['ds'] >= AirPassengersPanel['ds'].values[-12]].reset_index(drop=True)\n",
    "AirPassengersPanel_test['y'] = np.nan\n",
    "AirPassengersPanel_test['y_[lag12]'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340dd8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# Test predict_insample\n",
    "test_size = 12\n",
    "n_series = 2\n",
    "h = 12\n",
    "\n",
    "config = {'input_size': tune.choice([12, 24]), \n",
    "          'hidden_size': 256,\n",
    "          'max_steps': 1,\n",
    "          'val_check_steps': 1,\n",
    "          'step_size': 12}\n",
    "\n",
    "models = [\n",
    "    NHITS(h=h, input_size=24, loss=MQLoss(level=[80]), max_steps=1, alias='NHITS',scaler_type=None),\n",
    "    AutoMLP(h=12, config=config, num_samples=1),\n",
    "    RNN(h=h, input_size=-1, loss=MAE(), max_steps=1, alias='RNN', scaler_type=None),\n",
    "    ]\n",
    "\n",
    "nf = NeuralForecast(models=models, freq='M')\n",
    "cv = nf.cross_validation(df=AirPassengersPanel_train, static_df=AirPassengersStatic, val_size=0, test_size=test_size, n_windows=None)\n",
    "\n",
    "forecasts = nf.predict_insample(step_size=1)\n",
    "\n",
    "expected_size = n_series*((len(AirPassengersPanel_train)//n_series-test_size)-h+1)*h\n",
    "assert len(forecasts) == expected_size, 'Shape mistmach in predict_insample'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6bbc2c-d38f-4cec-a3ef-15164852479f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# tests aliases\n",
    "config_drnn = {'input_size': tune.choice([-1]), \n",
    "               'encoder_hidden_size': tune.choice([5, 10]),\n",
    "               'max_steps': 1,\n",
    "               'val_check_steps': 1,\n",
    "               'step_size': 1}\n",
    "models = [\n",
    "    # test Auto\n",
    "    AutoDilatedRNN(h=12, config=config_drnn, cpus=1, num_samples=2, alias='AutoDIL'),\n",
    "    # test BaseWindows\n",
    "    NHITS(h=12, input_size=24, loss=MQLoss(level=[80]), max_steps=1, alias='NHITSMQ'),\n",
    "    # test BaseRecurrent\n",
    "    RNN(h=12, input_size=-1, encoder_hidden_size=10, max_steps=1,\n",
    "            stat_exog_list=['airline1'],\n",
    "            futr_exog_list=['trend'], hist_exog_list=['y_[lag12]'], alias='MyRNN'),\n",
    "    # test BaseMultivariate\n",
    "    StemGNN(h=12, input_size=24, n_series=2, max_steps=1, scaler_type='robust', alias='StemMulti'),\n",
    "    # test model without alias\n",
    "    NHITS(h=12, input_size=24, max_steps=1),\n",
    "]\n",
    "nf = NeuralForecast(models=models, freq='M')\n",
    "nf.fit(df=AirPassengersPanel_train, static_df=AirPassengersStatic)\n",
    "forecasts = nf.predict(futr_df=AirPassengersPanel_test)\n",
    "test_eq(\n",
    "    forecasts.columns.to_list(),\n",
    "    ['ds', 'AutoDIL', 'NHITSMQ-median', 'NHITSMQ-lo-80', 'NHITSMQ-hi-80', 'MyRNN', 'StemMulti', 'NHITS']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3779a6-2d03-4ac3-9f01-8bd5cb306845",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "config = {'input_size': tune.choice([12, 24]), \n",
    "          'hidden_size': 256,\n",
    "          'max_steps': 1,\n",
    "          'val_check_steps': 1,\n",
    "          'step_size': 12}\n",
    "\n",
    "config_drnn = {'input_size': tune.choice([-1]), \n",
    "               'encoder_hidden_size': tune.choice([5, 10]),\n",
    "               'max_steps': 1,\n",
    "               'val_check_steps': 1,\n",
    "               'step_size': 1}\n",
    "\n",
    "fcst = NeuralForecast(\n",
    "    models=[\n",
    "        AutoDilatedRNN(h=12, config=config_drnn, cpus=1, num_samples=2),\n",
    "        DilatedRNN(h=12, input_size=-1, encoder_hidden_size=10, max_steps=1,\n",
    "                   stat_exog_list=['airline1'],\n",
    "                   futr_exog_list=['trend'], hist_exog_list=['y_[lag12]']),\n",
    "        RNN(h=12, input_size=-1, encoder_hidden_size=10, max_steps=1,\n",
    "            stat_exog_list=['airline1'],\n",
    "            futr_exog_list=['trend'], hist_exog_list=['y_[lag12]']),\n",
    "        TCN(h=12, input_size=-1, encoder_hidden_size=10, max_steps=1,\n",
    "            stat_exog_list=['airline1'],\n",
    "            futr_exog_list=['trend'], hist_exog_list=['y_[lag12]']),\n",
    "        AutoMLP(h=12, config=config, cpus=1, num_samples=2),\n",
    "        NBEATSx(h=12, input_size=12, max_steps=1,\n",
    "                stat_exog_list=['airline1'],\n",
    "                futr_exog_list=['trend'], hist_exog_list=['y_[lag12]']),\n",
    "        NHITS(h=12, input_size=24, loss=MQLoss(level=[80]), max_steps=1),\n",
    "        NHITS(h=12, input_size=12, max_steps=1,\n",
    "              stat_exog_list=['airline1'],\n",
    "              futr_exog_list=['trend'], hist_exog_list=['y_[lag12]']),\n",
    "        MLP(h=12, input_size=12, max_steps=1,\n",
    "            stat_exog_list=['airline1'],\n",
    "            futr_exog_list=['trend'], hist_exog_list=['y_[lag12]']),\n",
    "        TFT(h=12, input_size=24, max_steps=1),\n",
    "        VanillaTransformer(h=12, input_size=24, max_steps=1),\n",
    "        Informer(h=12, input_size=24, max_steps=1),\n",
    "        Autoformer(h=12, input_size=24, max_steps=1),\n",
    "        PatchTST(h=12, input_size=24, max_steps=1),\n",
    "        StemGNN(h=12, input_size=24, n_series=2, max_steps=1, scaler_type='robust'),\n",
    "    ],\n",
    "    freq='M'\n",
    ")\n",
    "fcst.fit(df=AirPassengersPanel_train, static_df=AirPassengersStatic)\n",
    "forecasts = fcst.predict(futr_df=AirPassengersPanel_test)\n",
    "forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40038532-fd68-4375-b7da-ba5bc2491c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "fig, ax = plt.subplots(1, 1, figsize = (20, 7))\n",
    "plot_df = pd.concat([AirPassengersPanel_train, forecasts.reset_index()]).set_index('ds')\n",
    "\n",
    "plot_df[plot_df['unique_id']=='Airline1'].drop(['unique_id','trend','y_[lag12]'], axis=1).plot(ax=ax, linewidth=2)\n",
    "\n",
    "ax.set_title('AirPassengers Forecast', fontsize=22)\n",
    "ax.set_ylabel('Monthly Passengers', fontsize=20)\n",
    "ax.set_xlabel('Timestamp [t]', fontsize=20)\n",
    "ax.legend(prop={'size': 15})\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d61909b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "fig, ax = plt.subplots(1, 1, figsize = (20, 7))\n",
    "plot_df = pd.concat([AirPassengersPanel_train, forecasts.reset_index()]).set_index('ds')\n",
    "\n",
    "plot_df[plot_df['unique_id']=='Airline2'].drop(['unique_id','trend','y_[lag12]'], axis=1).plot(ax=ax, linewidth=2)\n",
    "\n",
    "ax.set_title('AirPassengers Forecast', fontsize=22)\n",
    "ax.set_ylabel('Monthly Passengers', fontsize=20)\n",
    "ax.set_xlabel('Timestamp [t]', fontsize=20)\n",
    "ax.legend(prop={'size': 15})\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a8162a-3d9d-48df-a314-3a2ce0377e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "config = {'input_size': tune.choice([12, 24]), \n",
    "          'hidden_size': 256,\n",
    "          'max_steps': 1,\n",
    "          'val_check_steps': 1,\n",
    "          'step_size': 12}\n",
    "\n",
    "config_drnn = {'input_size': tune.choice([-1]), \n",
    "               'encoder_hidden_size': tune.choice([5, 10]),\n",
    "               'max_steps': 1,\n",
    "               'val_check_steps': 1,\n",
    "               'step_size': 1}\n",
    "\n",
    "fcst = NeuralForecast(\n",
    "    models=[\n",
    "        DilatedRNN(h=12, input_size=-1, encoder_hidden_size=10, max_steps=1),\n",
    "        AutoMLP(h=12, config=config, cpus=1, num_samples=1),\n",
    "        NHITS(h=12, input_size=12, max_steps=1)\n",
    "    ],\n",
    "    freq='M'\n",
    ")\n",
    "cv_df = fcst.cross_validation(df=AirPassengersPanel, static_df=AirPassengersStatic, n_windows=3, step_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c5ea12-ed87-4e46-ad04-3088e7167dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#test cross validation no leakage\n",
    "def test_cross_validation(df, static_df, h, test_size):\n",
    "    if (test_size - h) % 1:\n",
    "        raise Exception(\"`test_size - h` should be module `step_size`\")\n",
    "    \n",
    "    n_windows = int((test_size - h) / 1) + 1\n",
    "    Y_test_df = df.groupby('unique_id').tail(test_size)\n",
    "    Y_train_df = df.drop(Y_test_df.index)\n",
    "    config = {'input_size': tune.choice([12, 24]),\n",
    "              'step_size': 12, 'hidden_size': 256, 'max_steps': 1, 'val_check_steps': 1}\n",
    "    config_drnn = {'input_size': tune.choice([-1]), 'encoder_hidden_size': tune.choice([5, 10]),\n",
    "                   'max_steps': 1, 'val_check_steps': 1}\n",
    "    fcst = NeuralForecast(\n",
    "        models=[\n",
    "            AutoDilatedRNN(h=12, config=config_drnn, cpus=1, num_samples=1),\n",
    "            DilatedRNN(h=12, input_size=-1, encoder_hidden_size=5, max_steps=1),\n",
    "            RNN(h=12, input_size=-1, encoder_hidden_size=5, max_steps=1,\n",
    "                stat_exog_list=['airline1'], futr_exog_list=['trend'], hist_exog_list=['y_[lag12]']),\n",
    "            TCN(h=12, input_size=-1, encoder_hidden_size=5, max_steps=1,\n",
    "                stat_exog_list=['airline1'], futr_exog_list=['trend'], hist_exog_list=['y_[lag12]']),\n",
    "            AutoMLP(h=12, config=config, cpus=1, num_samples=1),\n",
    "            MLP(h=12, input_size=12, max_steps=1, scaler_type='robust'),\n",
    "            NBEATSx(h=12, input_size=12, max_steps=1,\n",
    "                    stat_exog_list=['airline1'], futr_exog_list=['trend'], hist_exog_list=['y_[lag12]']),\n",
    "            NHITS(h=12, input_size=12, max_steps=1, scaler_type='robust'),\n",
    "            NHITS(h=12, input_size=12, loss=MQLoss(level=[80]), max_steps=1),\n",
    "            TFT(h=12, input_size=24, max_steps=1, scaler_type='robust'),\n",
    "            VanillaTransformer(h=12, input_size=12, max_steps=1, scaler_type=None),\n",
    "            Informer(h=12, input_size=12, max_steps=1, scaler_type=None),\n",
    "            Autoformer(h=12, input_size=12, max_steps=1, scaler_type=None),\n",
    "            PatchTST(h=12, input_size=24, max_steps=1, scaler_type=None),\n",
    "            StemGNN(h=12, input_size=12, n_series=2, max_steps=1, scaler_type='robust')\n",
    "        ],\n",
    "        freq='M'\n",
    "    )\n",
    "    fcst.fit(df=Y_train_df, static_df=static_df)\n",
    "    Y_hat_df = fcst.predict(futr_df=Y_test_df)\n",
    "    Y_hat_df = Y_hat_df.merge(Y_test_df, how='left', on=['unique_id', 'ds'])\n",
    "    last_dates = Y_train_df.groupby('unique_id').tail(1)\n",
    "    last_dates = last_dates[['unique_id', 'ds']].rename(columns={'ds': 'cutoff'})\n",
    "    Y_hat_df = Y_hat_df.merge(last_dates, how='left', on='unique_id')\n",
    "    \n",
    "    #cross validation\n",
    "    fcst = NeuralForecast(\n",
    "        models=[\n",
    "            AutoDilatedRNN(h=12, config=config_drnn, cpus=1, num_samples=1),\n",
    "            DilatedRNN(h=12, input_size=-1, encoder_hidden_size=5, max_steps=1),\n",
    "            RNN(h=12, input_size=-1, encoder_hidden_size=5, max_steps=1,\n",
    "                stat_exog_list=['airline1'], futr_exog_list=['trend'], hist_exog_list=['y_[lag12]']),\n",
    "            TCN(h=12, input_size=-1, encoder_hidden_size=5, max_steps=1,\n",
    "                stat_exog_list=['airline1'], futr_exog_list=['trend'], hist_exog_list=['y_[lag12]']),\n",
    "            AutoMLP(h=12, config=config, cpus=1, num_samples=1),\n",
    "            MLP(h=12, input_size=12, max_steps=1, scaler_type='robust'),\n",
    "            NBEATSx(h=12, input_size=12, max_steps=1,\n",
    "                    stat_exog_list=['airline1'], futr_exog_list=['trend'], hist_exog_list=['y_[lag12]']),\n",
    "            NHITS(h=12, input_size=12, max_steps=1, scaler_type='robust'),\n",
    "            NHITS(h=12, input_size=12, loss=MQLoss(level=[80]), max_steps=1),\n",
    "            TFT(h=12, input_size=24, max_steps=1, scaler_type='robust'),\n",
    "            VanillaTransformer(h=12, input_size=12, max_steps=1, scaler_type=None),\n",
    "            Informer(h=12, input_size=12, max_steps=1, scaler_type=None),\n",
    "            Autoformer(h=12, input_size=12, max_steps=1, scaler_type=None),\n",
    "            PatchTST(h=12, input_size=24, max_steps=1, scaler_type=None),\n",
    "            StemGNN(h=12, input_size=12, n_series=2, max_steps=1, scaler_type='robust')\n",
    "        ],\n",
    "        freq='M'\n",
    "    )\n",
    "    Y_hat_df_cv = fcst.cross_validation(df, static_df=static_df, test_size=test_size, \n",
    "                                        n_windows=None)\n",
    "    for col in ['ds', 'cutoff']:\n",
    "        Y_hat_df_cv[col] = pd.to_datetime(Y_hat_df_cv[col].astype(str))\n",
    "        Y_hat_df[col] = pd.to_datetime(Y_hat_df[col].astype(str))\n",
    "    pd.testing.assert_frame_equal(\n",
    "        Y_hat_df[Y_hat_df_cv.columns],\n",
    "        Y_hat_df_cv,\n",
    "        check_dtype=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0467904-748e-42ec-99cc-bac514626304",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "test_cross_validation(AirPassengersPanel, AirPassengersStatic, h=12, test_size=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf52c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test save and load\n",
    "config = {'input_size': tune.choice([12, 24]), \n",
    "          'hidden_size': 256,\n",
    "          'max_steps': 1,\n",
    "          'val_check_steps': 1,\n",
    "          'step_size': 12}\n",
    "\n",
    "config_drnn = {'input_size': tune.choice([-1]),\n",
    "               'encoder_hidden_size': tune.choice([5, 10]),\n",
    "               'max_steps': 1,\n",
    "               'val_check_steps': 1}\n",
    "\n",
    "fcst = NeuralForecast(\n",
    "    models=[\n",
    "        AutoRNN(h=12, config=config_drnn, cpus=1, num_samples=2, refit_with_val=True),\n",
    "        DilatedRNN(h=12, input_size=-1, encoder_hidden_size=5, max_steps=1),\n",
    "        AutoMLP(h=12, config=config, cpus=1, num_samples=2),\n",
    "        NHITS(h=12, input_size=12, max_steps=1,\n",
    "              futr_exog_list=['trend'], hist_exog_list=['y_[lag12]']),\n",
    "        StemGNN(h=12, input_size=12, n_series=2, max_steps=1, scaler_type='robust')\n",
    "    ],\n",
    "    freq='M'\n",
    ")\n",
    "fcst.fit(AirPassengersPanel_train)\n",
    "forecasts1 = fcst.predict(futr_df=AirPassengersPanel_test)\n",
    "fcst.save(path='./examples/debug_run/', model_index=None, overwrite=True, save_dataset=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22ad495",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test `enable_checkpointing=True` should generate chkpt\n",
    "shutil.rmtree('lightning_logs')\n",
    "fcst = NeuralForecast(\n",
    "    models=[\n",
    "        MLP(h=12, input_size=12, max_steps=10, val_check_steps=5, enable_checkpointing=True),\n",
    "        RNN(h=12, input_size=-1, max_steps=10, val_check_steps=5, enable_checkpointing=True)\n",
    "    ],\n",
    "    freq='M'\n",
    ")\n",
    "fcst.fit(AirPassengersPanel_train)\n",
    "last_log = f\"lightning_logs/{os.listdir('lightning_logs')[-1]}\"\n",
    "no_chkpt_found = ~np.any([file.endswith('checkpoints') for file in os.listdir(last_log)])\n",
    "test_eq(no_chkpt_found, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac7a0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test `enable_checkpointing=False` should not generate chkpt\n",
    "shutil.rmtree('lightning_logs')\n",
    "fcst = NeuralForecast(\n",
    "    models=[\n",
    "        MLP(h=12, input_size=12, max_steps=10, val_check_steps=5),\n",
    "        RNN(h=12, input_size=-1, max_steps=10, val_check_steps=5)\n",
    "    ],\n",
    "    freq='M'\n",
    ")\n",
    "fcst.fit(AirPassengersPanel_train)\n",
    "last_log = f\"lightning_logs/{os.listdir('lightning_logs')[-1]}\"\n",
    "no_chkpt_found = ~np.any([file.endswith('checkpoints') for file in os.listdir(last_log)])\n",
    "test_eq(no_chkpt_found, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a20e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "fcst2 = NeuralForecast.load(path='./examples/debug_run/')\n",
    "forecasts2 = fcst2.predict(futr_df=AirPassengersPanel_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b380063",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "pairwise_tuples = [('AutoRNN', 'RNN'), ('DilatedRNN','DilatedRNN'), ('AutoMLP','MLP'), ('NHITS','NHITS'), ('StemGNN','StemGNN')]\n",
    "for model1, model2 in pairwise_tuples:\n",
    "    np.allclose(forecasts1[model1], forecasts2[model2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8602941e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test short time series\n",
    "config = {'input_size': tune.choice([12, 24]), \n",
    "          'max_steps': 1,\n",
    "          'val_check_steps': 1}\n",
    "\n",
    "fcst = NeuralForecast(\n",
    "    models=[\n",
    "        AutoNBEATS(h=12, config=config, cpus=1, num_samples=2)],\n",
    "    freq='M'\n",
    ")\n",
    "\n",
    "AirPassengersShort = AirPassengersPanel.tail(36+144).reset_index(drop=True)\n",
    "forecasts = fcst.cross_validation(AirPassengersShort, val_size=48, n_windows=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050e4436",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# Test predict_rolled equal cross_validation\n",
    "config = {'input_size': tune.choice([12, 24]), \n",
    "          'hidden_size': 256,\n",
    "          'max_steps': 1,\n",
    "          'val_check_steps': 1,\n",
    "          'step_size': 12}\n",
    "\n",
    "fcst = NeuralForecast(\n",
    "    models=[\n",
    "        RNN(h=12, input_size=-1, encoder_hidden_size=10, max_steps=1,\n",
    "            stat_exog_list=['airline1'],\n",
    "            futr_exog_list=['trend'], hist_exog_list=['y_[lag12]']),\n",
    "        AutoMLP(h=12, config=config, cpus=1, num_samples=2),\n",
    "        NHITS(h=12, input_size=12, max_steps=1,\n",
    "              stat_exog_list=['airline1'],\n",
    "              futr_exog_list=['trend'], hist_exog_list=['y_[lag12]'])\n",
    "    ],\n",
    "    freq='M'\n",
    ")\n",
    "fcst.fit(df=AirPassengersPanel_train, static_df=AirPassengersStatic)\n",
    "cv_df = fcst.cross_validation(df=AirPassengersPanel, static_df=AirPassengersStatic, n_windows=3, step_size=1)\n",
    "insample_df = fcst.predict_rolled(df=AirPassengersPanel, static_df=AirPassengersStatic, n_windows=3, step_size=1)\n",
    "pd.testing.assert_frame_equal(\n",
    "    cv_df,\n",
    "    insample_df,\n",
    "    check_dtype=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a0a1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# Test last window forecasts do not change with n_windows (testing independence of forecasts)\n",
    "config = {'input_size': tune.choice([12, 24]), \n",
    "          'hidden_size': 256,\n",
    "          'max_steps': 1,\n",
    "          'val_check_steps': 1,\n",
    "          'step_size': 12}\n",
    "\n",
    "fcst = NeuralForecast(\n",
    "    models=[\n",
    "        # RNN(h=12, input_size=-1, encoder_hidden_size=10, max_steps=1,\n",
    "        #     stat_exog_list=['airline1'],\n",
    "        #     futr_exog_list=['trend'], hist_exog_list=['y_[lag12]']),\n",
    "        AutoMLP(h=12, config=config, cpus=1, num_samples=2),\n",
    "        NHITS(h=12, input_size=12, max_steps=1,\n",
    "              stat_exog_list=['airline1'],\n",
    "              futr_exog_list=['trend'], hist_exog_list=['y_[lag12]'])\n",
    "    ],\n",
    "    freq='M'\n",
    ")\n",
    "fcst.fit(df=AirPassengersPanel_train, static_df=AirPassengersStatic)\n",
    "\n",
    "insample_df2 = fcst.predict_rolled(df=AirPassengersPanel, static_df=AirPassengersStatic, n_windows=2, step_size=1)\n",
    "insample_df3 = fcst.predict_rolled(df=AirPassengersPanel, static_df=AirPassengersStatic, n_windows=4, step_size=1)\n",
    "\n",
    "insample_df2 = insample_df2[insample_df2['cutoff']=='1959-12-31'].reset_index(drop=True)\n",
    "insample_df3 = insample_df3[insample_df3['cutoff']=='1959-12-31'].reset_index(drop=True)\n",
    "\n",
    "pd.testing.assert_frame_equal(\n",
    "    insample_df2,\n",
    "    insample_df3,\n",
    "    check_dtype=False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
