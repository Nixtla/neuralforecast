{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524620c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp models.fcgaga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15392f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fa25a4",
   "metadata": {},
   "source": [
    "# FC-GAGA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376a8a3a",
   "metadata": {},
   "source": [
    "The FC-GAGA architecture is a multivariate time series forecasting model built on the combination of a fully-connected univariate forecasting NBEATS-like model and a hard graph mechanism. The FC-GAGA method proved state-of-the-art performance on two traffic forecasting datasets.\n",
    "\n",
    "**References**<br>\n",
    "-[FC-GAGA Original Tensorflow implementation.](https://github.com/boreshkinai/fc-gaga/blob/master/model.py)<br>\n",
    "-[Boris N. Oreshkin, Arezou Amini, Lucy Coyle, Mark J. Coates (2021). \"FC-GAGA: Fully Connected Gated Graph Architecture for Spatio-Temporal Traffic Forecasting\". The Association for the Advancement of Artificial Intelligence Conference 2021 (AAAI 2021).](https://arxiv.org/pdf/2007.15531)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606365b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import logging\n",
    "import warnings\n",
    "from fastcore.test import test_eq\n",
    "from nbdev.showdoc import show_doc\n",
    "from neuralforecast.common._model_checks import check_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44065066-e72a-431f-938f-1528adef9fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from neuralforecast.losses.pytorch import MAE\n",
    "from neuralforecast.common._base_model import BaseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a77bb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from fastcore.test import test_eq\n",
    "from nbdev.showdoc import show_doc\n",
    "from neuralforecast.utils import generate_series\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ce8c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def _divide_no_nan(a: torch.Tensor, b: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Auxiliary funtion to handle divide by 0\n",
    "    \"\"\"\n",
    "    div = a / b\n",
    "    div[div != div] = 0.0\n",
    "    div[div == float('inf')] = 0.0\n",
    "    return div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7a9fae-2c29-47e2-874e-ca1f20bf7040",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "class FcBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 block_layers: int,\n",
    "                 hidden_units: int,\n",
    "                 input_size: int,\n",
    "                 output_size: int,\n",
    "                 device=None):\n",
    "        super(FcBlock, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.block_layers = block_layers\n",
    "        \n",
    "        # Define fully connected layers\n",
    "        self.fc_layers = nn.ModuleList()\n",
    "        for i in range(block_layers):\n",
    "            self.fc_layers.append(\n",
    "                nn.Linear(in_features=(hidden_units if i > 0 else input_size), \n",
    "                          out_features=hidden_units,\n",
    "                          device=device)\n",
    "            )\n",
    "        \n",
    "        # Define forecast and backcast layers\n",
    "        self.forecast = nn.Linear(hidden_units, output_size, device=device)\n",
    "        self.backcast = nn.Linear(hidden_units, input_size, device=device)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Forward pass through fully connected layers with ReLU activation\n",
    "        h = F.gelu(self.fc_layers[0](inputs))\n",
    "        for i in range(1, self.block_layers):\n",
    "            h = F.gelu(self.fc_layers[i](h))\n",
    "\n",
    "        # Backcast and forecast\n",
    "        backcast = F.gelu(inputs - self.backcast(h))\n",
    "        forecast = self.forecast(h)\n",
    "        return backcast, forecast\n",
    "\n",
    "\n",
    "class FcGagaLayer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_size: int,\n",
    "                 h,\n",
    "                 outputsize_multiplier,\n",
    "                 hist_input_size,\n",
    "                 stat_input_size,\n",
    "                 n_series: int,\n",
    "                 n_blocks: int,\n",
    "                 block_layers: int,\n",
    "                 hidden_units: int,\n",
    "                 node_id_dim: int,\n",
    "                 epsilon: float=10):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_series = n_series\n",
    "        self.input_size = input_size\n",
    "        self.h = h\n",
    "        self.outputsize_multiplier = outputsize_multiplier\n",
    "        self.hist_input_size = hist_input_size\n",
    "        self.stat_input_size = stat_input_size\n",
    "        self.fcgaga_input_size = (self.n_series+1) * self.input_size + node_id_dim # [B,N,(N+1)*T+S]\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        # TODO: Avoid one_hot_encoding and try embeddings instead\n",
    "        # self.node_id_em = nn.Embedding(num_embeddings=num_nodes, embedding_dim=node_id_dim)\n",
    "        self.node_id_em =  nn.Linear(in_features=stat_input_size, out_features=node_id_dim)\n",
    "\n",
    "        self.time_gate1 = nn.Linear(in_features=(node_id_dim + self.input_size*hist_input_size),\n",
    "                                    out_features=hidden_units)\n",
    "        self.time_gate2 = nn.Linear(in_features=hidden_units,\n",
    "                                    out_features=h)\n",
    "        self.time_gate3 = nn.Linear(in_features=hidden_units,\n",
    "                                    out_features=input_size)\n",
    "\n",
    "        self.blocks = torch.nn.ModuleList()\n",
    "        self.n_blocks = n_blocks\n",
    "        for i in range(n_blocks):\n",
    "            self.blocks.append(FcBlock(block_layers=block_layers, \n",
    "                                       hidden_units=hidden_units,\n",
    "                                       input_size=self.fcgaga_input_size,\n",
    "                                       output_size=h * outputsize_multiplier))\n",
    "\n",
    "    def forward(self,\n",
    "                insample_y: torch.Tensor,\n",
    "                hist_exog: torch.Tensor,\n",
    "                stat_exog: torch.Tensor,\n",
    "                ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "\n",
    "        node_id = self.node_id_em(stat_exog) # [B,N,S]->[B,N,S]\n",
    "\n",
    "        # ------------------------------------ Time Gate  -----------------------------------#\n",
    "        time_gate = self.time_gate1(torch.concat([node_id, hist_exog], axis=2)) # [B,N,S],[B,N,X*T]->[B,N,S+X*T]->[B,N,hidden]\n",
    "        time_gate_forward = self.time_gate2(time_gate)   # [B,N,hidden]->[B,N,H]\n",
    "        time_gate_backward = self.time_gate3(time_gate)  # [B,N,hidden]->[B,N,T]\n",
    "\n",
    "        insample_y = insample_y / (1.0 + time_gate_backward) # [B,N,T]\n",
    "\n",
    "        # ----------------------------------- Graph Gate  -----------------------------------#\n",
    "        node_embeddings = node_id[0,:,:] # [B,N,S]->[N,S]\n",
    "        node_embeddings_dp = torch.einsum(\"ns,ms->nm\", node_embeddings, node_embeddings) # [N,S]x[N,S]->[N,N]\n",
    "        node_embeddings_dp = torch.exp(self.epsilon * node_embeddings_dp) # [N,N]\n",
    "        \n",
    "        level, _ = torch.max(insample_y, dim=-1, keepdim=True) # [B,N,T]->[B,N,1]\n",
    "        \n",
    "        all_node_history = torch.einsum(\"bnt,nm->bnmt\", insample_y, node_embeddings_dp)\n",
    "        all_node_history = torch.reshape(all_node_history, (-1, self.n_series, self.n_series * self.input_size)) # [B,N,N,T]->[B,N,N*T]\n",
    "        all_node_history = _divide_no_nan(all_node_history - level, level)\n",
    "        all_node_history = F.gelu(all_node_history)\n",
    "\n",
    "        history = _divide_no_nan(insample_y, level)\n",
    "        history = torch.concat([history, all_node_history], axis=2) # [B,N,(N+1)*T]\n",
    "        history = torch.concat([history, node_id], axis=2) # [B,N,(N+1)*T+S]\n",
    "\n",
    "        backcast, forecast_out = self.blocks[0](history) # [B,N,(N+1)*T+S]->[B,N,(N+1)*T+S],[B,N,H*D]\n",
    "        for i in range(1, self.n_blocks):\n",
    "            backcast, forecast_block = self.blocks[i](backcast)\n",
    "            forecast_out = forecast_out + forecast_block\n",
    "\n",
    "        # [B,N,H*D]->[B,N,H,D]\n",
    "        forecast_out = torch.reshape(forecast_out, (-1, self.n_series, self.h, self.outputsize_multiplier))\n",
    "        forecast = forecast_out * level[:,:,:,None] # [B,N,H,D] * [B,N,1,1]\n",
    "\n",
    "        forecast = forecast * (1.0 + time_gate_forward[:,:,:,None]) # [B,N,H,D] * [B,N,H,1]\n",
    "        return backcast, forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4982fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# |hide\n",
    "# Test FCGaga forward pass\n",
    "h = 7\n",
    "input_size = 10\n",
    "outputsize_multiplier = 2\n",
    "\n",
    "hist_input_size = 5\n",
    "stat_input_size = 3\n",
    "n_series = 2\n",
    "\n",
    "n_blocks = 2\n",
    "block_layers = 2\n",
    "hidden_units = 16\n",
    "node_id_dim = 8\n",
    "epsilon = 10.0\n",
    "\n",
    "# Create the model\n",
    "fcgaga_layer = FcGagaLayer(input_size=input_size,\n",
    "                           h=h,\n",
    "                           outputsize_multiplier=outputsize_multiplier,\n",
    "                           hist_input_size=hist_input_size,\n",
    "                           stat_input_size=stat_input_size,\n",
    "                           n_series=n_series,\n",
    "                           n_blocks=n_blocks,\n",
    "                           block_layers=block_layers,\n",
    "                           hidden_units=hidden_units,\n",
    "                           node_id_dim=node_id_dim,\n",
    "                           epsilon=epsilon)\n",
    "\n",
    "batch_size = 4\n",
    "insample_y = torch.randn(batch_size, n_series, input_size)\n",
    "hist_exog = torch.randn(batch_size, n_series, hist_input_size * input_size)\n",
    "stat_exog = torch.randn(batch_size, n_series, stat_input_size)\n",
    "\n",
    "# Run forward pass\n",
    "backcast, forecast = fcgaga_layer(insample_y=insample_y,\n",
    "                                  hist_exog=hist_exog,\n",
    "                                  stat_exog=stat_exog)\n",
    "\n",
    "# Check that the forward shapes are correct\n",
    "assert backcast.shape == torch.Size((batch_size, n_series, (n_series+1) * input_size + node_id_dim))\n",
    "assert forecast.shape == torch.Size((batch_size, n_series, h, outputsize_multiplier))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be997aeb-778f-442d-a97a-ff47de2deab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class FCGAGA(BaseModel):\n",
    "    \"\"\" FCGAGA\n",
    "\n",
    "    The FC-GAGA architecture is a multivariate time series forecasting model built on the\n",
    "    combination of a fully-connected univariate forecasting NBEATS-like model and a hard\n",
    "    graph mechanism. The FC-GAGA method proved state-of-the-art performance on two traffic\n",
    "    forecasting datasets.\n",
    "\n",
    "    **Parameters:**<br>\n",
    "    `h`: int, forecast horizon.<br>\n",
    "    `input_size`: int, considered autorregresive inputs (lags), y=[1,2,3,4] input_size=2 -> lags=[1,2].<br>\n",
    "    `n_s_hidden`: int=5, hidden size of static encoder.<br>\n",
    "    `hidden_units`: int, Number of units of each hidden layer.<br>\n",
    "    `n_blocks`: in, Number of blocks within each stack.<br>\n",
    "    `n_stacks`: int, Number of stacks of the network.<br>\n",
    "    `loss`: PyTorch module, instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
    "    `valid_loss`: PyTorch module=`loss`, instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
    "    `max_steps`: int=1000, maximum number of training steps.<br>\n",
    "    `learning_rate`: float=1e-3, Learning rate between (0, 1).<br>\n",
    "    `num_lr_decays`: int=3, Number of learning rate decays, evenly distributed across max_steps.<br>\n",
    "    `early_stop_patience_steps`: int=-1, Number of validation iterations before early stopping.<br>\n",
    "    `val_check_steps`: int=100, Number of training steps between every validation loss check.<br>\n",
    "    `batch_size`: int=32, number of different series in each batch.<br>\n",
    "    `valid_batch_size`: int=None, number of different series in each validation and test batch, if None uses batch_size.<br>\n",
    "    `windows_batch_size`: int=1024, number of windows to sample in each training batch, default uses all.<br>\n",
    "    `inference_windows_batch_size`: int=-1, number of windows to sample in each inference batch, -1 uses all.<br>\n",
    "    `start_padding_enabled`: bool=False, if True, the model will pad the time series with zeros at the beginning, by input size.<br>\n",
    "    `step_size`: int=1, step size between each window of temporal data.<br>\n",
    "    `scaler_type`: str='identity', type of scaler for temporal inputs normalization see [temporal scalers](https://nixtla.github.io/neuralforecast/common.scalers.html).<br>\n",
    "    `random_seed`: int, random_seed for pytorch initializer and numpy generators.<br>\n",
    "    `num_workers_loader`: int=os.cpu_count(), workers to be used by `TimeSeriesDataLoader`.<br>\n",
    "    `drop_last_loader`: bool=False, if True `TimeSeriesDataLoader` drops last non-full batch.<br>\n",
    "    `alias`: str, optional,  Custom name of the model.<br>\n",
    "    `**trainer_kwargs`: int,  keyword trainer arguments inherited from [PyTorch Lighning's trainer](https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.trainer.trainer.Trainer.html?highlight=trainer).<br>\n",
    "\n",
    "    **References:**<br>\n",
    "    -[Boris N. Oreshkin, Arezou Amini, Lucy Coyle, Mark J. Coates (2021).\n",
    "    \"FC-GAGA: Fully Connected Gated Graph Architecture for Spatio-Temporal Traffic Forecasting\". The Association for the Advancement of Artificial Intelligence Conference 2021 (AAAI 2021).](https://arxiv.org/pdf/2007.15531)\n",
    "    \"\"\"\n",
    "    # Class attributes\n",
    "    EXOGENOUS_FUTR = True\n",
    "    EXOGENOUS_HIST = True\n",
    "    EXOGENOUS_STAT = True    \n",
    "    MULTIVARIATE = True     # If the model produces multivariate forecasts (True) or univariate (False)\n",
    "    RECURRENT = False       # If the model produces forecasts recursively (True) or direct (False)\n",
    "    \n",
    "    def __init__(self,\n",
    "                 h,\n",
    "                 input_size,\n",
    "                 futr_exog_list=None,\n",
    "                 hist_exog_list=None,\n",
    "                 stat_exog_list=None,\n",
    "                 hidden_units: int=128,\n",
    "                 n_s_hidden: int=5,\n",
    "                 n_blocks: int=2,\n",
    "                 block_layers: int=3,\n",
    "                 n_stacks: int=3,\n",
    "                 epsilon: float=10,\n",
    "                 loss = MAE(),\n",
    "                 valid_loss = None,\n",
    "                 max_steps: int = 1000,\n",
    "                 learning_rate: float = 1e-3,\n",
    "                 num_lr_decays: int = 3,\n",
    "                 early_stop_patience_steps: int =-1,\n",
    "                 val_check_steps: int = 100,\n",
    "                 batch_size: int = 32,\n",
    "                 valid_batch_size: Optional[int] = None,\n",
    "                 windows_batch_size: int = 1024,\n",
    "                 inference_windows_batch_size: int = -1,\n",
    "                 start_padding_enabled = False,\n",
    "                 step_size: int = 1,\n",
    "                 scaler_type: str ='identity',\n",
    "                 random_seed: int = 1,\n",
    "                 num_workers_loader: int = 0,\n",
    "                 drop_last_loader: bool = False,\n",
    "                 **trainer_kwargs):\n",
    "        # Inherit BaseModel class\n",
    "        super(FCGAGA, self).__init__(h=h,\n",
    "                                     input_size=input_size,\n",
    "                                     futr_exog_list=futr_exog_list,\n",
    "                                     hist_exog_list=hist_exog_list,\n",
    "                                     stat_exog_list=stat_exog_list,\n",
    "                                     loss=loss,\n",
    "                                     valid_loss=valid_loss,\n",
    "                                     max_steps=max_steps,\n",
    "                                     learning_rate=learning_rate,\n",
    "                                     num_lr_decays=num_lr_decays,\n",
    "                                     early_stop_patience_steps=early_stop_patience_steps,\n",
    "                                     val_check_steps=val_check_steps,\n",
    "                                     batch_size=batch_size,\n",
    "                                     windows_batch_size=windows_batch_size,\n",
    "                                     valid_batch_size=valid_batch_size,\n",
    "                                     inference_windows_batch_size=inference_windows_batch_size,\n",
    "                                     start_padding_enabled=start_padding_enabled,\n",
    "                                     step_size=step_size,\n",
    "                                     scaler_type=scaler_type,\n",
    "                                     num_workers_loader=num_workers_loader,\n",
    "                                     drop_last_loader=drop_last_loader,\n",
    "                                     random_seed=random_seed,\n",
    "                                     **trainer_kwargs)\n",
    "        \n",
    "        # Architecture\n",
    "        self.hist_input_size = len(self.hist_exog_list)\n",
    "        self.stat_input_size = len(self.stat_exog_list)\n",
    "\n",
    "        self.stacks = torch.nn.ModuleList()\n",
    "        self.n_stacks = n_stacks\n",
    "        for i in range(n_stacks):\n",
    "            fcgaga_layer = FcGagaLayer(h=h, input_size=input_size,\n",
    "                                       outputsize_multiplier=self.loss.outputsize_multiplier,\n",
    "                                       hist_input_size=self.hist_input_size,\n",
    "                                       stat_input_size=self.stat_input_size,\n",
    "                                       n_series=self.n_series,\n",
    "                                       n_blocks=n_blocks,\n",
    "                                       block_layers=block_layers,\n",
    "                                       hidden_units=hidden_units,\n",
    "                                       node_id_dim=n_s_hidden,\n",
    "                                       epsilon=epsilon)\n",
    "            self.stacks.append(fcgaga_layer)\n",
    "\n",
    "    def forward(self, windows_batch):\n",
    "        # Parse windows_batch\n",
    "        insample_y = windows_batch['insample_y']\n",
    "        hist_exog  = windows_batch['hist_exog']\n",
    "        stat_exog  = windows_batch['stat_exog']\n",
    "\n",
    "        # Reshape data for FC-GAGA\n",
    "        insample_y = torch.reshape(insample_y, (-1, self.n_series, self.input_size)) # [B*N,T] -> [B,N,T]\n",
    "        hist_exog = torch.reshape(hist_exog, (-1, self.n_series, self.hist_input_size * self.input_size))  # [B*N,X,T] -> [B,N,X,T]\n",
    "        stat_exog = torch.reshape(stat_exog, (-1, self.n_series, self.stat_input_size)) # [B*N,S] -> [B,N,S]\n",
    "\n",
    "        # FC-GAGA's forward\n",
    "        _, forecast = self.stacks[0](insample_y=insample_y, hist_exog=hist_exog, stat_exog=stat_exog)\n",
    "        for i, stack in enumerate(self.stacks[1:]):\n",
    "            _, stack_forecast = stack(insample_y=insample_y,\n",
    "                                      hist_exog=hist_exog,\n",
    "                                      stat_exog=stat_exog)\n",
    "            forecast = forecast + stack_forecast\n",
    "        forecast = forecast / (self.n_stacks + 1)\n",
    "\n",
    "        # Adapting output's domain, output tuple distribution parameters\n",
    "        forecast = torch.reshape(forecast, (-1, self.h, self.loss.outputsize_multiplier)) # [B,N,H,D] -> [B*N,H,D]\n",
    "        output = self.loss.domain_map(forecast)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57a831f-94bc-4616-b579-c114c3fc57c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_doc(FCGAGA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9013b63-f65b-4a92-913c-b696e6e69914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_doc(FCGAGA.fit, name='FCGAGA.fit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66184ee-7a71-4598-976c-c79b83089a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_doc(FCGAGA.predict, name='FCGAGA.predict')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuralforecast",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
