{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp models.dilated_rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dilated RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Dilated Recurrent Neural Network (`DilatedRNN`) addresses common challenges of modeling long sequences like vanishing gradients, computational efficiency, and improved model flexibility to model complex relationships while maintaining its parsimony. The `DilatedRNN` builds a deep stack of RNN layers using skip conditions on the temporal and the network's depth dimensions. The temporal dilated recurrent skip connections offer the capability to focus on multi-resolution inputs.The predictions are obtained by transforming the hidden states into contexts $\\mathbf{c}_{[t+1:t+H]}$, that are decoded and adapted into $\\mathbf{\\hat{y}}_{[t+1:t+H],[q]}$ through MLPs.\n",
    "\n",
    "\\begin{align}\n",
    " \\mathbf{h}_{t} &= \\textrm{DilatedRNN}([\\mathbf{y}_{t},\\mathbf{x}^{(h)}_{t},\\mathbf{x}^{(s)}], \\mathbf{h}_{t-1})\\\\\n",
    "\\mathbf{c}_{[t+1:t+H]}&=\\textrm{Linear}([\\mathbf{h}_{t}, \\mathbf{x}^{(f)}_{[:t+H]}]) \\\\ \n",
    "\\hat{y}_{\\tau,[q]}&=\\textrm{MLP}([\\mathbf{c}_{\\tau},\\mathbf{x}^{(f)}_{\\tau}])\n",
    "\\end{align}\n",
    "\n",
    "where $\\mathbf{h}_{t}$, is the hidden state for time $t$, $\\mathbf{y}_{t}$ is the input at time $t$ and $\\mathbf{h}_{t-1}$ is the hidden state of the previous layer at $t-1$, $\\mathbf{x}^{(s)}$ are static exogenous inputs, $\\mathbf{x}^{(h)}_{t}$ historic exogenous, $\\mathbf{x}^{(f)}_{[:t+H]}$ are future exogenous available at the time of the prediction.\n",
    "\n",
    "**References**<br>-[Shiyu Chang, et al. \"Dilated Recurrent Neural Networks\".](https://arxiv.org/abs/1710.02224)<br>-[Yao Qin, et al. \"A Dual-Stage Attention-Based recurrent neural network for time series prediction\".](https://arxiv.org/abs/1704.02971)<br>-[Kashif Rasul, et al. \"Zalando Research: PyTorch Dilated Recurrent Neural Networks\".](https://arxiv.org/abs/1710.02224)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Figure 1. Three layer DilatedRNN with dilation 1, 2, 4.](imgs_models/dilated_rnn.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import logging\n",
    "import warnings\n",
    "from nbdev.showdoc import show_doc\n",
    "from neuralforecast.utils import generate_series\n",
    "from neuralforecast.common._model_checks import check_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from typing import List, Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from neuralforecast.losses.pytorch import MAE\n",
    "from neuralforecast.common._base_model import BaseModel\n",
    "from neuralforecast.common._modules import MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "class LSTMCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout=0.):\n",
    "        super(LSTMCell, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.weight_ih = nn.Parameter(torch.randn(4 * hidden_size, input_size))\n",
    "        self.weight_hh = nn.Parameter(torch.randn(4 * hidden_size, hidden_size))\n",
    "        self.bias_ih = nn.Parameter(torch.randn(4 * hidden_size))\n",
    "        self.bias_hh = nn.Parameter(torch.randn(4 * hidden_size))\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, inputs, hidden):\n",
    "        hx, cx = hidden[0].squeeze(0), hidden[1].squeeze(0)\n",
    "        gates = (torch.matmul(inputs, self.weight_ih.t()) + self.bias_ih +\n",
    "                         torch.matmul(hx, self.weight_hh.t()) + self.bias_hh)\n",
    "        ingate, forgetgate, cellgate, outgate = gates.chunk(4, 1)\n",
    "\n",
    "        ingate = torch.sigmoid(ingate)\n",
    "        forgetgate = torch.sigmoid(forgetgate)\n",
    "        cellgate = torch.tanh(cellgate)\n",
    "        outgate = torch.sigmoid(outgate)\n",
    "\n",
    "        cy = (forgetgate * cx) + (ingate * cellgate)\n",
    "        hy = outgate * torch.tanh(cy)\n",
    "\n",
    "        return hy, (hy, cy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "class ResLSTMCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout=0.):\n",
    "        super(ResLSTMCell, self).__init__()\n",
    "        self.register_buffer('input_size', torch.Tensor([input_size]))\n",
    "        self.register_buffer('hidden_size', torch.Tensor([hidden_size]))\n",
    "        self.weight_ii = nn.Parameter(torch.randn(3 * hidden_size, input_size))\n",
    "        self.weight_ic = nn.Parameter(torch.randn(3 * hidden_size, hidden_size))\n",
    "        self.weight_ih = nn.Parameter(torch.randn(3 * hidden_size, hidden_size))\n",
    "        self.bias_ii = nn.Parameter(torch.randn(3 * hidden_size))\n",
    "        self.bias_ic = nn.Parameter(torch.randn(3 * hidden_size))\n",
    "        self.bias_ih = nn.Parameter(torch.randn(3 * hidden_size))\n",
    "        self.weight_hh = nn.Parameter(torch.randn(1 * hidden_size, hidden_size))\n",
    "        self.bias_hh = nn.Parameter(torch.randn(1 * hidden_size))\n",
    "        self.weight_ir = nn.Parameter(torch.randn(hidden_size, input_size))\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, inputs, hidden):\n",
    "        hx, cx = hidden[0].squeeze(0), hidden[1].squeeze(0)\n",
    "\n",
    "        ifo_gates = (torch.matmul(inputs, self.weight_ii.t()) + self.bias_ii +\n",
    "                                  torch.matmul(hx, self.weight_ih.t()) + self.bias_ih +\n",
    "                                  torch.matmul(cx, self.weight_ic.t()) + self.bias_ic)\n",
    "        ingate, forgetgate, outgate = ifo_gates.chunk(3, 1)\n",
    "\n",
    "        cellgate = torch.matmul(hx, self.weight_hh.t()) + self.bias_hh\n",
    "\n",
    "        ingate = torch.sigmoid(ingate)\n",
    "        forgetgate = torch.sigmoid(forgetgate)\n",
    "        cellgate = torch.tanh(cellgate)\n",
    "        outgate = torch.sigmoid(outgate)\n",
    "\n",
    "        cy = (forgetgate * cx) + (ingate * cellgate)\n",
    "        ry = torch.tanh(cy)\n",
    "\n",
    "        if self.input_size == self.hidden_size:\n",
    "            hy = outgate * (ry + inputs)\n",
    "        else:\n",
    "            hy = outgate * (ry + torch.matmul(inputs, self.weight_ir.t()))\n",
    "        return hy, (hy, cy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "class ResLSTMLayer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout=0.):\n",
    "        super(ResLSTMLayer, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.cell = ResLSTMCell(input_size, hidden_size, dropout=0.)\n",
    "\n",
    "    def forward(self, inputs, hidden):\n",
    "        inputs = inputs.unbind(0)\n",
    "        outputs = []\n",
    "        for i in range(len(inputs)):\n",
    "                out, hidden = self.cell(inputs[i], hidden)\n",
    "                outputs += [out]\n",
    "        outputs = torch.stack(outputs)\n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "class AttentiveLSTMLayer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout=0.0):\n",
    "        super(AttentiveLSTMLayer, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        attention_hsize = hidden_size\n",
    "        self.attention_hsize = attention_hsize\n",
    "\n",
    "        self.cell = LSTMCell(input_size, hidden_size)\n",
    "        self.attn_layer = nn.Sequential(nn.Linear(2 * hidden_size + input_size, attention_hsize),\n",
    "                                        nn.Tanh(),\n",
    "                                        nn.Linear(attention_hsize, 1))\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, inputs, hidden):\n",
    "        inputs = inputs.unbind(0)\n",
    "        outputs = []\n",
    "\n",
    "        for t in range(len(inputs)):\n",
    "            # attention on windows\n",
    "            hx, cx = (tensor.squeeze(0) for tensor in hidden)\n",
    "            hx_rep = hx.repeat(len(inputs), 1, 1)\n",
    "            cx_rep = cx.repeat(len(inputs), 1, 1)\n",
    "            x = torch.cat((inputs, hx_rep, cx_rep), dim=-1)\n",
    "            l = self.attn_layer(x)\n",
    "            beta = self.softmax(l)\n",
    "            context = torch.bmm(beta.permute(1, 2, 0),\n",
    "                                inputs.permute(1, 0, 2)).squeeze(1)\n",
    "            out, hidden = self.cell(context, hidden)\n",
    "            outputs += [out]\n",
    "        outputs = torch.stack(outputs)\n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "class DRNN(nn.Module):\n",
    "\n",
    "    def __init__(self, n_input, n_hidden, n_layers, dilations, dropout=0, cell_type='GRU', batch_first=True):\n",
    "        super(DRNN, self).__init__()\n",
    "\n",
    "        self.dilations = dilations\n",
    "        self.cell_type = cell_type\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "        layers = []\n",
    "        if self.cell_type == \"GRU\":\n",
    "            cell = nn.GRU\n",
    "        elif self.cell_type == \"RNN\":\n",
    "            cell = nn.RNN\n",
    "        elif self.cell_type == \"LSTM\":\n",
    "            cell = nn.LSTM\n",
    "        elif self.cell_type == \"ResLSTM\":\n",
    "            cell = ResLSTMLayer\n",
    "        elif self.cell_type == \"AttentiveLSTM\":\n",
    "            cell = AttentiveLSTMLayer\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        for i in range(n_layers):\n",
    "            if i == 0:\n",
    "                c = cell(n_input, n_hidden, dropout=dropout)\n",
    "            else:\n",
    "                c = cell(n_hidden, n_hidden, dropout=dropout)\n",
    "            layers.append(c)\n",
    "        self.cells = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, inputs, hidden=None):\n",
    "        if self.batch_first:\n",
    "            inputs = inputs.transpose(0, 1)\n",
    "        outputs = []\n",
    "        for i, (cell, dilation) in enumerate(zip(self.cells, self.dilations)):\n",
    "            if hidden is None:\n",
    "                inputs, _ = self.drnn_layer(cell, inputs, dilation)\n",
    "            else:\n",
    "                inputs, hidden[i] = self.drnn_layer(cell, inputs, dilation, hidden[i])\n",
    "\n",
    "            outputs.append(inputs[-dilation:])\n",
    "\n",
    "        if self.batch_first:\n",
    "            inputs = inputs.transpose(0, 1)\n",
    "        return inputs, outputs\n",
    "\n",
    "    def drnn_layer(self, cell, inputs, rate, hidden=None):\n",
    "        n_steps = len(inputs)\n",
    "        batch_size = inputs[0].size(0)\n",
    "        hidden_size = cell.hidden_size\n",
    "\n",
    "        inputs, dilated_steps = self._pad_inputs(inputs, n_steps, rate)\n",
    "        dilated_inputs = self._prepare_inputs(inputs, rate)\n",
    "\n",
    "        if hidden is None:\n",
    "            dilated_outputs, hidden = self._apply_cell(dilated_inputs, cell, batch_size, rate, hidden_size)\n",
    "        else:\n",
    "            hidden = self._prepare_inputs(hidden, rate)\n",
    "            dilated_outputs, hidden = self._apply_cell(dilated_inputs, cell, batch_size, rate, hidden_size,\n",
    "                                                       hidden=hidden)\n",
    "\n",
    "        splitted_outputs = self._split_outputs(dilated_outputs, rate)\n",
    "        outputs = self._unpad_outputs(splitted_outputs, n_steps)\n",
    "\n",
    "        return outputs, hidden\n",
    "\n",
    "    def _apply_cell(self, dilated_inputs, cell, batch_size, rate, hidden_size, hidden=None):\n",
    "        if hidden is None:\n",
    "            hidden = torch.zeros(batch_size * rate, hidden_size,\n",
    "                                 dtype=dilated_inputs.dtype,\n",
    "                                 device=dilated_inputs.device)\n",
    "            hidden = hidden.unsqueeze(0)\n",
    "            \n",
    "            if self.cell_type in ['LSTM', 'ResLSTM', 'AttentiveLSTM']:\n",
    "                hidden = (hidden, hidden)\n",
    "                \n",
    "        dilated_outputs, hidden = cell(dilated_inputs, hidden) # compatibility hack\n",
    "\n",
    "        return dilated_outputs, hidden\n",
    "\n",
    "    def _unpad_outputs(self, splitted_outputs, n_steps):\n",
    "        return splitted_outputs[:n_steps]\n",
    "\n",
    "    def _split_outputs(self, dilated_outputs, rate):\n",
    "        batchsize = dilated_outputs.size(1) // rate\n",
    "\n",
    "        blocks = [dilated_outputs[:, i * batchsize: (i + 1) * batchsize, :] for i in range(rate)]\n",
    "\n",
    "        interleaved = torch.stack((blocks)).transpose(1, 0)\n",
    "        interleaved = interleaved.reshape(dilated_outputs.size(0) * rate,\n",
    "                                       batchsize,\n",
    "                                       dilated_outputs.size(2))\n",
    "        return interleaved\n",
    "\n",
    "    def _pad_inputs(self, inputs, n_steps, rate):\n",
    "        iseven = (n_steps % rate) == 0\n",
    "\n",
    "        if not iseven:\n",
    "            dilated_steps = n_steps // rate + 1\n",
    "\n",
    "            zeros_ = torch.zeros(dilated_steps * rate - inputs.size(0),\n",
    "                                 inputs.size(1),\n",
    "                                 inputs.size(2), \n",
    "                                 dtype=inputs.dtype,\n",
    "                                 device=inputs.device)\n",
    "            inputs = torch.cat((inputs, zeros_))\n",
    "        else:\n",
    "            dilated_steps = n_steps // rate\n",
    "\n",
    "        return inputs, dilated_steps\n",
    "\n",
    "    def _prepare_inputs(self, inputs, rate):\n",
    "        dilated_inputs = torch.cat([inputs[j::rate, :, :] for j in range(rate)], 1)\n",
    "        return dilated_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class DilatedRNN(BaseModel):\n",
    "    \"\"\" DilatedRNN\n",
    "\n",
    "    **Parameters:**<br>\n",
    "    `h`: int, forecast horizon.<br>\n",
    "    `input_size`: int, maximum sequence length for truncated train backpropagation. Default -1 uses all history.<br>\n",
    "    `inference_input_size`: int, maximum sequence length for truncated inference. Default -1 uses all history.<br>\n",
    "    `cell_type`: str, type of RNN cell to use. Options: 'GRU', 'RNN', 'LSTM', 'ResLSTM', 'AttentiveLSTM'.<br>\n",
    "    `dilations`: int list, dilations betweem layers.<br>\n",
    "    `encoder_hidden_size`: int=200, units for the RNN's hidden state size.<br>\n",
    "    `context_size`: int=10, size of context vector for each timestamp on the forecasting window.<br>\n",
    "    `decoder_hidden_size`: int=200, size of hidden layer for the MLP decoder.<br>\n",
    "    `decoder_layers`: int=2, number of layers for the MLP decoder.<br>\n",
    "    `futr_exog_list`: str list, future exogenous columns.<br>\n",
    "    `hist_exog_list`: str list, historic exogenous columns.<br>\n",
    "    `stat_exog_list`: str list, static exogenous columns.<br>\n",
    "    `loss`: PyTorch module, instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
    "    `valid_loss`: PyTorch module=`loss`, instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
    "    `max_steps`: int, maximum number of training steps.<br>\n",
    "    `learning_rate`: float, Learning rate between (0, 1).<br>\n",
    "    `num_lr_decays`: int, Number of learning rate decays, evenly distributed across max_steps.<br>\n",
    "    `early_stop_patience_steps`: int, Number of validation iterations before early stopping.<br>\n",
    "    `val_check_steps`: int, Number of training steps between every validation loss check.<br>\n",
    "    `batch_size`: int=32, number of different series in each batch.<br>\n",
    "    `valid_batch_size`: int=None, number of different series in each validation and test batch.<br>\n",
    "    `step_size`: int=1, step size between each window of temporal data.<br>\n",
    "    `scaler_type`: str='robust', type of scaler for temporal inputs normalization see [temporal scalers](https://nixtla.github.io/neuralforecast/common.scalers.html).<br>\n",
    "    `random_seed`: int=1, random_seed for pytorch initializer and numpy generators.<br>\n",
    "    `num_workers_loader`: int=os.cpu_count(), workers to be used by `TimeSeriesDataLoader`.<br>\n",
    "    `drop_last_loader`: bool=False, if True `TimeSeriesDataLoader` drops last non-full batch.<br>\n",
    "    `alias`: str, optional,  Custom name of the model.<br>\n",
    "    `optimizer`: Subclass of 'torch.optim.Optimizer', optional, user specified optimizer instead of the default choice (Adam).<br>\n",
    "    `optimizer_kwargs`: dict, optional, list of parameters used by the user specified `optimizer`.<br>\n",
    "    `lr_scheduler`: Subclass of 'torch.optim.lr_scheduler.LRScheduler', optional, user specified lr_scheduler instead of the default choice (StepLR).<br>\n",
    "    `lr_scheduler_kwargs`: dict, optional, list of parameters used by the user specified `lr_scheduler`.<br> \n",
    "    `**trainer_kwargs`: int,  keyword trainer arguments inherited from [PyTorch Lighning's trainer](https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.trainer.trainer.Trainer.html?highlight=trainer).<br>    \n",
    "    \"\"\"\n",
    "    # Class attributes\n",
    "    EXOGENOUS_FUTR = True\n",
    "    EXOGENOUS_HIST = True\n",
    "    EXOGENOUS_STAT = True\n",
    "    MULTIVARIATE = False    # If the model produces multivariate forecasts (True) or univariate (False)\n",
    "    RECURRENT = False       # If the model produces forecasts recursively (True) or direct (False)\n",
    "\n",
    "    def __init__(self,\n",
    "                 h: int,\n",
    "                 input_size: int,\n",
    "                 inference_input_size: int = -1,\n",
    "                 cell_type: str = 'LSTM',\n",
    "                 dilations: List[List[int]] = [[1, 2], [4, 8]],\n",
    "                 encoder_hidden_size: int = 128,\n",
    "                 context_size: int = 10,\n",
    "                 decoder_hidden_size: int = 128,\n",
    "                 decoder_layers: int = 2,\n",
    "                 futr_exog_list = None,\n",
    "                 hist_exog_list = None,\n",
    "                 stat_exog_list = None,\n",
    "                 exclude_insample_y = False,\n",
    "                 loss = MAE(),\n",
    "                 valid_loss = None,\n",
    "                 max_steps: int = 1000,\n",
    "                 learning_rate: float = 1e-3,\n",
    "                 num_lr_decays: int = 3,\n",
    "                 early_stop_patience_steps: int =-1,\n",
    "                 val_check_steps: int = 100,\n",
    "                 batch_size = 32,\n",
    "                 valid_batch_size: Optional[int] = None,\n",
    "                 windows_batch_size = 128,\n",
    "                 inference_windows_batch_size = 1024,\n",
    "                 start_padding_enabled = False,\n",
    "                 step_size: int = 1,\n",
    "                 scaler_type: str = 'robust',\n",
    "                 random_seed: int = 1,\n",
    "                 num_workers_loader: int = 0,\n",
    "                 drop_last_loader: bool = False,\n",
    "                 optimizer = None,\n",
    "                 optimizer_kwargs = None,\n",
    "                 lr_scheduler = None,\n",
    "                 lr_scheduler_kwargs = None,\n",
    "                 **trainer_kwargs):\n",
    "        super(DilatedRNN, self).__init__(\n",
    "            h=h,\n",
    "            input_size=input_size,\n",
    "            futr_exog_list=futr_exog_list,\n",
    "            hist_exog_list=hist_exog_list,\n",
    "            stat_exog_list=stat_exog_list,\n",
    "            exclude_insample_y = exclude_insample_y,\n",
    "            loss=loss,\n",
    "            valid_loss=valid_loss,\n",
    "            max_steps=max_steps,\n",
    "            learning_rate=learning_rate,\n",
    "            num_lr_decays=num_lr_decays,\n",
    "            early_stop_patience_steps=early_stop_patience_steps,\n",
    "            val_check_steps=val_check_steps,\n",
    "            batch_size=batch_size,\n",
    "            valid_batch_size=valid_batch_size,\n",
    "            windows_batch_size=windows_batch_size,\n",
    "            inference_windows_batch_size=inference_windows_batch_size,\n",
    "            start_padding_enabled=start_padding_enabled,\n",
    "            step_size=step_size,\n",
    "            scaler_type=scaler_type,\n",
    "            random_seed=random_seed,\n",
    "            num_workers_loader=num_workers_loader,\n",
    "            drop_last_loader=drop_last_loader,\n",
    "            optimizer=optimizer,\n",
    "            optimizer_kwargs=optimizer_kwargs,\n",
    "            lr_scheduler=lr_scheduler,\n",
    "            lr_scheduler_kwargs=lr_scheduler_kwargs,\n",
    "            **trainer_kwargs\n",
    "        )\n",
    "\n",
    "        # Dilated RNN\n",
    "        self.cell_type = cell_type\n",
    "        self.dilations = dilations\n",
    "        self.encoder_hidden_size = encoder_hidden_size\n",
    "        \n",
    "        # Context adapter\n",
    "        self.context_size = context_size\n",
    "\n",
    "        # MLP decoder\n",
    "        self.decoder_hidden_size = decoder_hidden_size\n",
    "        self.decoder_layers = decoder_layers\n",
    "\n",
    "        # RNN input size (1 for target variable y)\n",
    "        input_encoder = 1 + self.hist_exog_size + self.stat_exog_size + self.futr_exog_size\n",
    "\n",
    "        # Instantiate model\n",
    "        layers = []\n",
    "        for grp_num in range(len(self.dilations)):\n",
    "            if grp_num > 0:\n",
    "                input_encoder = self.encoder_hidden_size\n",
    "            layer = DRNN(input_encoder,\n",
    "                         self.encoder_hidden_size,\n",
    "                         n_layers=len(self.dilations[grp_num]),\n",
    "                         dilations=self.dilations[grp_num],\n",
    "                         cell_type=self.cell_type)\n",
    "            layers.append(layer)\n",
    "\n",
    "        self.rnn_stack = nn.Sequential(*layers)\n",
    "\n",
    "        # Context adapter\n",
    "        self.context_adapter = nn.Linear(in_features=self.input_size,\n",
    "                                         out_features=h)\n",
    "\n",
    "        # Decoder MLP\n",
    "        self.mlp_decoder = MLP(in_features=self.encoder_hidden_size + self.futr_exog_size,\n",
    "                               out_features=self.loss.outputsize_multiplier,\n",
    "                               hidden_size=self.decoder_hidden_size,\n",
    "                               num_layers=self.decoder_layers,\n",
    "                               activation='ReLU',\n",
    "                               dropout=0.0)\n",
    "\n",
    "    def forward(self, windows_batch):\n",
    "        \n",
    "        # Parse windows_batch\n",
    "        encoder_input = windows_batch['insample_y']                         # [B, L, 1]\n",
    "        futr_exog     = windows_batch['futr_exog']                          # [B, L + h, F]\n",
    "        hist_exog     = windows_batch['hist_exog']                          # [B, L, X]\n",
    "        stat_exog     = windows_batch['stat_exog']                          # [B, S]\n",
    "\n",
    "        # Concatenate y, historic and static inputs              \n",
    "        batch_size, seq_len = encoder_input.shape[:2]\n",
    "        if self.hist_exog_size > 0:\n",
    "            encoder_input = torch.cat((encoder_input, hist_exog), dim=2)    # [B, L, 1] + [B, L, X] -> [B, L, 1 + X]\n",
    "\n",
    "        if self.stat_exog_size > 0:\n",
    "            stat_exog = stat_exog.unsqueeze(1).repeat(1, seq_len, 1)        # [B, S] -> [B, L, S]\n",
    "            encoder_input = torch.cat((encoder_input, stat_exog), dim=2)    # [B, L, 1 + X] + [B, L, S] -> [B, L, 1 + X + S]\n",
    "\n",
    "        if self.futr_exog_size > 0:\n",
    "            encoder_input = torch.cat((encoder_input, \n",
    "                                       futr_exog[:, :seq_len]), dim=2)      # [B, L, 1 + X + S] + [B, L, F] -> [B, L, 1 + X + S + F]\n",
    "\n",
    "        # DilatedRNN forward\n",
    "        for layer_num in range(len(self.rnn_stack)):\n",
    "            residual = encoder_input\n",
    "            output, _ = self.rnn_stack[layer_num](encoder_input)\n",
    "            if layer_num > 0:\n",
    "                output += residual\n",
    "            encoder_input = output\n",
    "\n",
    "        # Context adapter\n",
    "        output = output.permute(0, 2, 1)                                    # [B, L, C] -> [B, C, L]\n",
    "        context = self.context_adapter(output)                              # [B, C, L] -> [B, C, h]\n",
    "\n",
    "        # Residual connection with futr_exog\n",
    "        if self.futr_exog_size > 0:\n",
    "            futr_exog_futr = futr_exog[:, seq_len:].permute(0, 2, 1)        # [B, h, F] -> [B, F, h]\n",
    "            context = torch.cat((context, futr_exog_futr), \n",
    "                                dim=1)                                      # [B, C, h] + [B, F, h] = [B, C + F, h]\n",
    "\n",
    "        # Final forecast\n",
    "        context = context.permute(0, 2, 1)                                  # [B, C + F, h] -> [B, h, C + F]\n",
    "        output = self.mlp_decoder(context)                                  # [B, h, C + F] -> [B, h, n_output]\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/models/dilated_rnn.py#L289){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DilatedRNN\n",
       "\n",
       ">      DilatedRNN (h:int, input_size:int, inference_input_size:int=-1,\n",
       ">                  cell_type:str='LSTM', dilations:List[List[int]]=[[1, 2], [4,\n",
       ">                  8]], encoder_hidden_size:int=200, context_size:int=10,\n",
       ">                  decoder_hidden_size:int=200, decoder_layers:int=2,\n",
       ">                  futr_exog_list=None, hist_exog_list=None,\n",
       ">                  stat_exog_list=None, exclude_insample_y=False, loss=MAE(),\n",
       ">                  valid_loss=None, max_steps:int=1000,\n",
       ">                  learning_rate:float=0.001, num_lr_decays:int=3,\n",
       ">                  early_stop_patience_steps:int=-1, val_check_steps:int=100,\n",
       ">                  batch_size=32, valid_batch_size:Optional[int]=None,\n",
       ">                  windows_batch_size=1024, inference_windows_batch_size=1024,\n",
       ">                  start_padding_enabled=False, step_size:int=1,\n",
       ">                  scaler_type:str='robust', random_seed:int=1,\n",
       ">                  num_workers_loader:int=0, drop_last_loader:bool=False,\n",
       ">                  optimizer=None, optimizer_kwargs=None, lr_scheduler=None,\n",
       ">                  lr_scheduler_kwargs=None, **trainer_kwargs)\n",
       "\n",
       "*DilatedRNN\n",
       "\n",
       "**Parameters:**<br>\n",
       "`h`: int, forecast horizon.<br>\n",
       "`input_size`: int, maximum sequence length for truncated train backpropagation. Default -1 uses all history.<br>\n",
       "`inference_input_size`: int, maximum sequence length for truncated inference. Default -1 uses all history.<br>\n",
       "`cell_type`: str, type of RNN cell to use. Options: 'GRU', 'RNN', 'LSTM', 'ResLSTM', 'AttentiveLSTM'.<br>\n",
       "`dilations`: int list, dilations betweem layers.<br>\n",
       "`encoder_hidden_size`: int=200, units for the RNN's hidden state size.<br>\n",
       "`context_size`: int=10, size of context vector for each timestamp on the forecasting window.<br>\n",
       "`decoder_hidden_size`: int=200, size of hidden layer for the MLP decoder.<br>\n",
       "`decoder_layers`: int=2, number of layers for the MLP decoder.<br>\n",
       "`futr_exog_list`: str list, future exogenous columns.<br>\n",
       "`hist_exog_list`: str list, historic exogenous columns.<br>\n",
       "`stat_exog_list`: str list, static exogenous columns.<br>\n",
       "`loss`: PyTorch module, instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
       "`valid_loss`: PyTorch module=`loss`, instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
       "`max_steps`: int, maximum number of training steps.<br>\n",
       "`learning_rate`: float, Learning rate between (0, 1).<br>\n",
       "`num_lr_decays`: int, Number of learning rate decays, evenly distributed across max_steps.<br>\n",
       "`early_stop_patience_steps`: int, Number of validation iterations before early stopping.<br>\n",
       "`val_check_steps`: int, Number of training steps between every validation loss check.<br>\n",
       "`batch_size`: int=32, number of different series in each batch.<br>\n",
       "`valid_batch_size`: int=None, number of different series in each validation and test batch.<br>\n",
       "`step_size`: int=1, step size between each window of temporal data.<br>\n",
       "`scaler_type`: str='robust', type of scaler for temporal inputs normalization see [temporal scalers](https://nixtla.github.io/neuralforecast/common.scalers.html).<br>\n",
       "`random_seed`: int=1, random_seed for pytorch initializer and numpy generators.<br>\n",
       "`num_workers_loader`: int=os.cpu_count(), workers to be used by `TimeSeriesDataLoader`.<br>\n",
       "`drop_last_loader`: bool=False, if True `TimeSeriesDataLoader` drops last non-full batch.<br>\n",
       "`alias`: str, optional,  Custom name of the model.<br>\n",
       "`optimizer`: Subclass of 'torch.optim.Optimizer', optional, user specified optimizer instead of the default choice (Adam).<br>\n",
       "`optimizer_kwargs`: dict, optional, list of parameters used by the user specified `optimizer`.<br>\n",
       "`lr_scheduler`: Subclass of 'torch.optim.lr_scheduler.LRScheduler', optional, user specified lr_scheduler instead of the default choice (StepLR).<br>\n",
       "`lr_scheduler_kwargs`: dict, optional, list of parameters used by the user specified `lr_scheduler`.<br> \n",
       "`**trainer_kwargs`: int,  keyword trainer arguments inherited from [PyTorch Lighning's trainer](https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.trainer.trainer.Trainer.html?highlight=trainer).<br>*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/models/dilated_rnn.py#L289){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DilatedRNN\n",
       "\n",
       ">      DilatedRNN (h:int, input_size:int, inference_input_size:int=-1,\n",
       ">                  cell_type:str='LSTM', dilations:List[List[int]]=[[1, 2], [4,\n",
       ">                  8]], encoder_hidden_size:int=200, context_size:int=10,\n",
       ">                  decoder_hidden_size:int=200, decoder_layers:int=2,\n",
       ">                  futr_exog_list=None, hist_exog_list=None,\n",
       ">                  stat_exog_list=None, exclude_insample_y=False, loss=MAE(),\n",
       ">                  valid_loss=None, max_steps:int=1000,\n",
       ">                  learning_rate:float=0.001, num_lr_decays:int=3,\n",
       ">                  early_stop_patience_steps:int=-1, val_check_steps:int=100,\n",
       ">                  batch_size=32, valid_batch_size:Optional[int]=None,\n",
       ">                  windows_batch_size=1024, inference_windows_batch_size=1024,\n",
       ">                  start_padding_enabled=False, step_size:int=1,\n",
       ">                  scaler_type:str='robust', random_seed:int=1,\n",
       ">                  num_workers_loader:int=0, drop_last_loader:bool=False,\n",
       ">                  optimizer=None, optimizer_kwargs=None, lr_scheduler=None,\n",
       ">                  lr_scheduler_kwargs=None, **trainer_kwargs)\n",
       "\n",
       "*DilatedRNN\n",
       "\n",
       "**Parameters:**<br>\n",
       "`h`: int, forecast horizon.<br>\n",
       "`input_size`: int, maximum sequence length for truncated train backpropagation. Default -1 uses all history.<br>\n",
       "`inference_input_size`: int, maximum sequence length for truncated inference. Default -1 uses all history.<br>\n",
       "`cell_type`: str, type of RNN cell to use. Options: 'GRU', 'RNN', 'LSTM', 'ResLSTM', 'AttentiveLSTM'.<br>\n",
       "`dilations`: int list, dilations betweem layers.<br>\n",
       "`encoder_hidden_size`: int=200, units for the RNN's hidden state size.<br>\n",
       "`context_size`: int=10, size of context vector for each timestamp on the forecasting window.<br>\n",
       "`decoder_hidden_size`: int=200, size of hidden layer for the MLP decoder.<br>\n",
       "`decoder_layers`: int=2, number of layers for the MLP decoder.<br>\n",
       "`futr_exog_list`: str list, future exogenous columns.<br>\n",
       "`hist_exog_list`: str list, historic exogenous columns.<br>\n",
       "`stat_exog_list`: str list, static exogenous columns.<br>\n",
       "`loss`: PyTorch module, instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
       "`valid_loss`: PyTorch module=`loss`, instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
       "`max_steps`: int, maximum number of training steps.<br>\n",
       "`learning_rate`: float, Learning rate between (0, 1).<br>\n",
       "`num_lr_decays`: int, Number of learning rate decays, evenly distributed across max_steps.<br>\n",
       "`early_stop_patience_steps`: int, Number of validation iterations before early stopping.<br>\n",
       "`val_check_steps`: int, Number of training steps between every validation loss check.<br>\n",
       "`batch_size`: int=32, number of different series in each batch.<br>\n",
       "`valid_batch_size`: int=None, number of different series in each validation and test batch.<br>\n",
       "`step_size`: int=1, step size between each window of temporal data.<br>\n",
       "`scaler_type`: str='robust', type of scaler for temporal inputs normalization see [temporal scalers](https://nixtla.github.io/neuralforecast/common.scalers.html).<br>\n",
       "`random_seed`: int=1, random_seed for pytorch initializer and numpy generators.<br>\n",
       "`num_workers_loader`: int=os.cpu_count(), workers to be used by `TimeSeriesDataLoader`.<br>\n",
       "`drop_last_loader`: bool=False, if True `TimeSeriesDataLoader` drops last non-full batch.<br>\n",
       "`alias`: str, optional,  Custom name of the model.<br>\n",
       "`optimizer`: Subclass of 'torch.optim.Optimizer', optional, user specified optimizer instead of the default choice (Adam).<br>\n",
       "`optimizer_kwargs`: dict, optional, list of parameters used by the user specified `optimizer`.<br>\n",
       "`lr_scheduler`: Subclass of 'torch.optim.lr_scheduler.LRScheduler', optional, user specified lr_scheduler instead of the default choice (StepLR).<br>\n",
       "`lr_scheduler_kwargs`: dict, optional, list of parameters used by the user specified `lr_scheduler`.<br> \n",
       "`**trainer_kwargs`: int,  keyword trainer arguments inherited from [PyTorch Lighning's trainer](https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.trainer.trainer.Trainer.html?highlight=trainer).<br>*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DilatedRNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### DilatedRNN.fit\n",
       "\n",
       ">      DilatedRNN.fit (dataset, val_size=0, test_size=0, random_seed=None,\n",
       ">                      distributed_config=None)\n",
       "\n",
       "*Fit.\n",
       "\n",
       "The `fit` method, optimizes the neural network's weights using the\n",
       "initialization parameters (`learning_rate`, `windows_batch_size`, ...)\n",
       "and the `loss` function as defined during the initialization.\n",
       "Within `fit` we use a PyTorch Lightning `Trainer` that\n",
       "inherits the initialization's `self.trainer_kwargs`, to customize\n",
       "its inputs, see [PL's trainer arguments](https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.trainer.trainer.Trainer.html?highlight=trainer).\n",
       "\n",
       "The method is designed to be compatible with SKLearn-like classes\n",
       "and in particular to be compatible with the StatsForecast library.\n",
       "\n",
       "By default the `model` is not saving training checkpoints to protect\n",
       "disk memory, to get them change `enable_checkpointing=True` in `__init__`.\n",
       "\n",
       "**Parameters:**<br>\n",
       "`dataset`: NeuralForecast's `TimeSeriesDataset`, see [documentation](https://nixtla.github.io/neuralforecast/tsdataset.html).<br>\n",
       "`val_size`: int, validation size for temporal cross-validation.<br>\n",
       "`random_seed`: int=None, random_seed for pytorch initializer and numpy generators, overwrites model.__init__'s.<br>\n",
       "`test_size`: int, test size for temporal cross-validation.<br>*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### DilatedRNN.fit\n",
       "\n",
       ">      DilatedRNN.fit (dataset, val_size=0, test_size=0, random_seed=None,\n",
       ">                      distributed_config=None)\n",
       "\n",
       "*Fit.\n",
       "\n",
       "The `fit` method, optimizes the neural network's weights using the\n",
       "initialization parameters (`learning_rate`, `windows_batch_size`, ...)\n",
       "and the `loss` function as defined during the initialization.\n",
       "Within `fit` we use a PyTorch Lightning `Trainer` that\n",
       "inherits the initialization's `self.trainer_kwargs`, to customize\n",
       "its inputs, see [PL's trainer arguments](https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.trainer.trainer.Trainer.html?highlight=trainer).\n",
       "\n",
       "The method is designed to be compatible with SKLearn-like classes\n",
       "and in particular to be compatible with the StatsForecast library.\n",
       "\n",
       "By default the `model` is not saving training checkpoints to protect\n",
       "disk memory, to get them change `enable_checkpointing=True` in `__init__`.\n",
       "\n",
       "**Parameters:**<br>\n",
       "`dataset`: NeuralForecast's `TimeSeriesDataset`, see [documentation](https://nixtla.github.io/neuralforecast/tsdataset.html).<br>\n",
       "`val_size`: int, validation size for temporal cross-validation.<br>\n",
       "`random_seed`: int=None, random_seed for pytorch initializer and numpy generators, overwrites model.__init__'s.<br>\n",
       "`test_size`: int, test size for temporal cross-validation.<br>*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DilatedRNN.fit, name='DilatedRNN.fit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### DilatedRNN.predict\n",
       "\n",
       ">      DilatedRNN.predict (dataset, test_size=None, step_size=1,\n",
       ">                          random_seed=None, **data_module_kwargs)\n",
       "\n",
       "*Predict.\n",
       "\n",
       "Neural network prediction with PL's `Trainer` execution of `predict_step`.\n",
       "\n",
       "**Parameters:**<br>\n",
       "`dataset`: NeuralForecast's `TimeSeriesDataset`, see [documentation](https://nixtla.github.io/neuralforecast/tsdataset.html).<br>\n",
       "`test_size`: int=None, test size for temporal cross-validation.<br>\n",
       "`step_size`: int=1, Step size between each window.<br>\n",
       "`random_seed`: int=None, random_seed for pytorch initializer and numpy generators, overwrites model.__init__'s.<br>\n",
       "`**data_module_kwargs`: PL's TimeSeriesDataModule args, see [documentation](https://pytorch-lightning.readthedocs.io/en/1.6.1/extensions/datamodules.html#using-a-datamodule).*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### DilatedRNN.predict\n",
       "\n",
       ">      DilatedRNN.predict (dataset, test_size=None, step_size=1,\n",
       ">                          random_seed=None, **data_module_kwargs)\n",
       "\n",
       "*Predict.\n",
       "\n",
       "Neural network prediction with PL's `Trainer` execution of `predict_step`.\n",
       "\n",
       "**Parameters:**<br>\n",
       "`dataset`: NeuralForecast's `TimeSeriesDataset`, see [documentation](https://nixtla.github.io/neuralforecast/tsdataset.html).<br>\n",
       "`test_size`: int=None, test size for temporal cross-validation.<br>\n",
       "`step_size`: int=1, Step size between each window.<br>\n",
       "`random_seed`: int=None, random_seed for pytorch initializer and numpy generators, overwrites model.__init__'s.<br>\n",
       "`**data_module_kwargs`: PL's TimeSeriesDataModule args, see [documentation](https://pytorch-lightning.readthedocs.io/en/1.6.1/extensions/datamodules.html#using-a-datamodule).*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DilatedRNN.predict, name='DilatedRNN.predict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DilatedRNN: checking forecast AirPassengers dataset\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "# Unit tests for models\n",
    "logging.getLogger(\"pytorch_lightning\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"lightning_fabric\").setLevel(logging.ERROR)\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    check_model(DilatedRNN, [\"airpassengers\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ospra\\OneDrive\\Nixtla\\Repositories\\neuralforecast\\neuralforecast\\common\\_base_model.py:134: UserWarning: Input size too small. Automatically setting input size to 3 * horizon = 36\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c575af1dd4b545f1a017aa6edc64a115",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e680f1712194b2fa69c3669284867db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e398bdd3c29d4cb1a2cdf258edb3d0c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "372f289b0dd64143a31120abab6a85fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ospra\\miniconda3\\envs\\neuralforecast\\lib\\site-packages\\utilsforecast\\processing.py:384: FutureWarning: 'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n",
      "  freq = pd.tseries.frequencies.to_offset(freq)\n",
      "c:\\Users\\ospra\\miniconda3\\envs\\neuralforecast\\lib\\site-packages\\utilsforecast\\processing.py:438: FutureWarning: 'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n",
      "  freq = pd.tseries.frequencies.to_offset(freq)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e0bf257d4b2449eaa43ab1859bde8f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ospra\\OneDrive\\Nixtla\\Repositories\\neuralforecast\\neuralforecast\\core.py:213: FutureWarning: In a future version the predictions will have the id as a column. You can set the `NIXTLA_ID_AS_COL` environment variable to adopt the new behavior and to suppress this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| eval: false\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from neuralforecast import NeuralForecast\n",
    "from neuralforecast.models import DilatedRNN\n",
    "from neuralforecast.losses.pytorch import DistributionLoss\n",
    "from neuralforecast.utils import AirPassengersPanel, AirPassengersStatic\n",
    "\n",
    "Y_train_df = AirPassengersPanel[AirPassengersPanel.ds<AirPassengersPanel['ds'].values[-12]] # 132 train\n",
    "Y_test_df = AirPassengersPanel[AirPassengersPanel.ds>=AirPassengersPanel['ds'].values[-12]].reset_index(drop=True) # 12 test\n",
    "\n",
    "fcst = NeuralForecast(\n",
    "    models=[DilatedRNN(h=12,\n",
    "                       input_size=-1,\n",
    "                       loss=DistributionLoss(distribution='Normal', level=[80, 90]),\n",
    "                       scaler_type='robust',\n",
    "                       encoder_hidden_size=100,\n",
    "                       max_steps=200,\n",
    "                       futr_exog_list=['y_[lag12]'],\n",
    "                       hist_exog_list=None,\n",
    "                       stat_exog_list=['airline1'],\n",
    "    )\n",
    "    ],\n",
    "    freq='M'\n",
    ")\n",
    "fcst.fit(df=Y_train_df, static_df=AirPassengersStatic)\n",
    "forecasts = fcst.predict(futr_df=Y_test_df)\n",
    "\n",
    "Y_hat_df = forecasts.reset_index(drop=False).drop(columns=['unique_id','ds'])\n",
    "plot_df = pd.concat([Y_test_df, Y_hat_df], axis=1)\n",
    "plot_df = pd.concat([Y_train_df, plot_df])\n",
    "\n",
    "plot_df = plot_df[plot_df.unique_id=='Airline1'].drop('unique_id', axis=1)\n",
    "plt.plot(plot_df['ds'], plot_df['y'], c='black', label='True')\n",
    "plt.plot(plot_df['ds'], plot_df['DilatedRNN-median'], c='blue', label='median')\n",
    "plt.fill_between(x=plot_df['ds'][-12:], \n",
    "                 y1=plot_df['DilatedRNN-lo-90'][-12:].values, \n",
    "                 y2=plot_df['DilatedRNN-hi-90'][-12:].values,\n",
    "                 alpha=0.4, label='level 90')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
