{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp models.deepnpts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepNPTS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Non-Parametric Time Series Forecaster (`DeepNPTS`) is a non-parametric baseline model for time-series forecasting. This model generates predictions by sampling from the empirical distribution according to a tunable strategy. This strategy is learned by exploiting the information across multiple related time series. This model provides a strong, simple baseline for time series forecasting.\n",
    "\n",
    "\n",
    "**References**<br>\n",
    "[Rangapuram, Syama Sundar, Jan Gasthaus, Lorenzo Stella, Valentin Flunkert, David Salinas, Yuyang Wang, and Tim Januschowski (2023). \"Deep Non-Parametric Time Series Forecaster\". arXiv.](https://arxiv.org/abs/2312.14657)<br>\n",
    "\n",
    "\n",
    ":::{.callout-warning collapse=\"false\"}\n",
    "#### Exogenous Variables, Losses, and Parameters Availability\n",
    "\n",
    "Given the sampling procedure during inference, DeepNPTS only supports `DistributionLoss` as training loss.\n",
    "\n",
    "Note that DeepNPTS generates a non-parametric forecast distribution using Monte Carlo. We use this sampling procedure also during validation to make it closer to the inference procedure. Therefore, only the `MQLoss` is available for validation.\n",
    "\n",
    "Aditionally, Monte Carlo implies that historic exogenous variables are not available for the model.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import neuralforecast.losses.pytorch as losses\n",
    "from typing import Optional\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "from neuralforecast.common._base_windows import BaseWindows\n",
    "from neuralforecast.losses.pytorch import MQLoss, GMM, PMM, NBMM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "from fastcore.test import test_eq\n",
    "from nbdev.showdoc import show_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "logging.getLogger(\"pytorch_lightning\").setLevel(logging.ERROR)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. DeepNPTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class DeepNPTS(BaseWindows):\n",
    "    \"\"\" DeepNPTS\n",
    "\n",
    "    Deep Non-Parametric Time Series Forecaster (`DeepNPTS`) is a baseline model for time-series forecasting. This model generates predictions by sampling from the empirical distribution according to a learnable strategy. The strategy is learned by exploiting the information across multiple related time series. \n",
    "\n",
    "    **Parameters:**<br>\n",
    "    `h`: int, Forecast horizon. <br>\n",
    "    `input_size`: int, autorregresive inputs size, y=[1,2,3,4] input_size=2 -> y_[t-2:t]=[1,2].<br>\n",
    "    `hidden_size`: int=32, hidden size of dense layers.<br>\n",
    "    `batch_norm`: bool=True, if True, applies Batch Normalization after each dense layer in the network.<br>\n",
    "    `dropout`: float=0.1, dropout.<br>\n",
    "    `n_layers`: int=2, number of dense layers.<br>\n",
    "    `trajectory_samples`: int=100, number of Monte Carlo trajectories during inference.<br>\n",
    "    `stat_exog_list`: str list, static exogenous columns.<br>\n",
    "    `hist_exog_list`: str list, historic exogenous columns.<br>\n",
    "    `futr_exog_list`: str list, future exogenous columns.<br>\n",
    "    `exclude_insample_y`: bool=False, the model skips the autoregressive features y[t-input_size:t] if True.<br>\n",
    "    `loss`: PyTorch module, instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
    "    `valid_loss`: PyTorch module=`loss`, instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
    "    `max_steps`: int=1000, maximum number of training steps.<br>\n",
    "    `learning_rate`: float=1e-3, Learning rate between (0, 1).<br>\n",
    "    `num_lr_decays`: int=-1, Number of learning rate decays, evenly distributed across max_steps.<br>\n",
    "    `early_stop_patience_steps`: int=-1, Number of validation iterations before early stopping.<br>\n",
    "    `val_check_steps`: int=100, Number of training steps between every validation loss check.<br>\n",
    "    `batch_size`: int=32, number of different series in each batch.<br>\n",
    "    `valid_batch_size`: int=None, number of different series in each validation and test batch, if None uses batch_size.<br>\n",
    "    `windows_batch_size`: int=1024, number of windows to sample in each training batch, default uses all.<br>\n",
    "    `inference_windows_batch_size`: int=-1, number of windows to sample in each inference batch, -1 uses all.<br>\n",
    "    `start_padding_enabled`: bool=False, if True, the model will pad the time series with zeros at the beginning, by input size.<br>\n",
    "    `step_size`: int=1, step size between each window of temporal data.<br>\n",
    "    `scaler_type`: str='identity', type of scaler for temporal inputs normalization see [temporal scalers](https://nixtla.github.io/neuralforecast/common.scalers.html).<br>\n",
    "    `random_seed`: int, random_seed for pytorch initializer and numpy generators.<br>\n",
    "    `num_workers_loader`: int=os.cpu_count(), workers to be used by `TimeSeriesDataLoader`.<br>\n",
    "    `drop_last_loader`: bool=False, if True `TimeSeriesDataLoader` drops last non-full batch.<br>\n",
    "    `alias`: str, optional,  Custom name of the model.<br>\n",
    "    `optimizer`: Subclass of 'torch.optim.Optimizer', optional, user specified optimizer instead of the default choice (Adam).<br>\n",
    "    `optimizer_kwargs`: dict, optional, list of parameters used by the user specified `optimizer`.<br>\n",
    "    `**trainer_kwargs`: int,  keyword trainer arguments inherited from [PyTorch Lighning's trainer](https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.trainer.trainer.Trainer.html?highlight=trainer).<br>    \n",
    "\n",
    "    **References**<br>\n",
    "    - [Rangapuram, Syama Sundar, Jan Gasthaus, Lorenzo Stella, Valentin Flunkert, David Salinas, Yuyang Wang, and Tim Januschowski (2023). \"Deep Non-Parametric Time Series Forecaster\". arXiv.](https://arxiv.org/abs/2312.14657)<br>\n",
    "\n",
    "    \"\"\"\n",
    "    # Class attributes\n",
    "    SAMPLING_TYPE = 'windows'\n",
    "    \n",
    "    def __init__(self,\n",
    "                 h,\n",
    "                 input_size: int = -1,\n",
    "                 hidden_size: int = 32,\n",
    "                 batch_norm: bool = True,\n",
    "                 dropout: float = 0.1,\n",
    "                 n_layers: int = 2,\n",
    "                 trajectory_samples: int = 100,\n",
    "                 futr_exog_list = None,\n",
    "                 hist_exog_list = None,\n",
    "                 stat_exog_list = None,\n",
    "                 exclude_insample_y = False,\n",
    "                 loss = GMM(),\n",
    "                 valid_loss = MQLoss(level=[80, 90]),\n",
    "                 max_steps: int = 1000,\n",
    "                 learning_rate: float = 1e-5,\n",
    "                 num_lr_decays: int = 3,\n",
    "                 early_stop_patience_steps: int =-1,\n",
    "                 val_check_steps: int = 100,\n",
    "                 batch_size: int = 32,\n",
    "                 valid_batch_size: Optional[int] = None,\n",
    "                 windows_batch_size: int = 1024,\n",
    "                 inference_windows_batch_size: int = -1,\n",
    "                 start_padding_enabled = False,\n",
    "                 step_size: int = 1,\n",
    "                 scaler_type: str = 'standard',\n",
    "                 random_seed: int = 1,\n",
    "                 num_workers_loader = 0,\n",
    "                 drop_last_loader = False,\n",
    "                 optimizer = None,\n",
    "                 optimizer_kwargs = None,\n",
    "                 **trainer_kwargs):\n",
    "\n",
    "        if hist_exog_list is not None:\n",
    "            raise Exception('DeepNPTS does not support historical exogenous variables.')\n",
    "\n",
    "        if exclude_insample_y:\n",
    "            raise Exception('DeepNPTS has no possibility for excluding y.')\n",
    "        \n",
    "        supported_losses = (losses.GMM,\n",
    "                            losses.PMM,\n",
    "                            losses.NBMM)\n",
    "\n",
    "        if not isinstance(loss, supported_losses):\n",
    "            raise Exception('DeepNPTS only supports GMM, PMM or NBMM as loss function.')               \n",
    "        \n",
    "        if not isinstance(valid_loss, losses.MQLoss):\n",
    "            raise Exception('DeepNPTS only supports MQLoss as validation loss.')\n",
    "    \n",
    "        # Overwrite n_components, it has to be the input_size in DeepNPTS\n",
    "        loss.n_components = input_size\n",
    "        \n",
    "        # Inherit BaseWindows class\n",
    "        super(DeepNPTS, self).__init__(h=h,\n",
    "                                    input_size=input_size,\n",
    "                                    futr_exog_list=futr_exog_list,\n",
    "                                    hist_exog_list=hist_exog_list,\n",
    "                                    stat_exog_list=stat_exog_list,\n",
    "                                    exclude_insample_y = exclude_insample_y,\n",
    "                                    loss=loss,\n",
    "                                    valid_loss=valid_loss,\n",
    "                                    max_steps=max_steps,\n",
    "                                    learning_rate=learning_rate,\n",
    "                                    num_lr_decays=num_lr_decays,\n",
    "                                    early_stop_patience_steps=early_stop_patience_steps,\n",
    "                                    val_check_steps=val_check_steps,\n",
    "                                    batch_size=batch_size,\n",
    "                                    windows_batch_size=windows_batch_size,\n",
    "                                    valid_batch_size=valid_batch_size,\n",
    "                                    inference_windows_batch_size=inference_windows_batch_size,\n",
    "                                    start_padding_enabled=start_padding_enabled,\n",
    "                                    step_size=step_size,\n",
    "                                    scaler_type=scaler_type,\n",
    "                                    num_workers_loader=num_workers_loader,\n",
    "                                    drop_last_loader=drop_last_loader,\n",
    "                                    random_seed=random_seed,\n",
    "                                    optimizer=optimizer,\n",
    "                                    optimizer_kwargs=optimizer_kwargs,\n",
    "                                    **trainer_kwargs)\n",
    "\n",
    "        self.h = h\n",
    "        self.h_backup = self.h                                  # Used because h=1 during training\n",
    "        self.use_softmax = True\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dropout = dropout\n",
    "        self.trajectory_samples = trajectory_samples\n",
    "\n",
    "        self.futr_exog_size = len(self.futr_exog_list)\n",
    "        self.stat_exog_size = len(self.stat_exog_list)\n",
    "\n",
    "        input_dim = input_size * (1 + self.futr_exog_size) + self.stat_exog_size\n",
    "        # Create DeepNPTSNetwork\n",
    "        modules = []       \n",
    "        for i in range(n_layers):\n",
    "            modules.append(nn.Linear(input_dim if i == 0 else hidden_size, hidden_size))\n",
    "            modules.append(nn.ReLU())\n",
    "            if batch_norm:\n",
    "                modules.append(nn.BatchNorm1d(hidden_size))\n",
    "            if dropout > 0.0:\n",
    "                modules.append(nn.Dropout(dropout))\n",
    "\n",
    "        self.deepnptsnetwork = nn.Sequential(*modules)\n",
    "        self.deepnptsnetwork.apply(partial(self._init_weights, scale=0.07))\n",
    "\n",
    "        # Add output layers for Mixture distribution        \n",
    "        output_modules = []\n",
    "        if dropout > 0.0:\n",
    "            output_modules.append(nn.Dropout(self.dropout))\n",
    "        \n",
    "        if isinstance(loss, GMM):\n",
    "            output_modules.append(nn.Linear(hidden_size, input_size + 1))\n",
    "        elif isinstance(loss, PMM):\n",
    "            output_modules.append(nn.Linear(hidden_size, input_size))\n",
    "        elif isinstance(loss, NBMM):\n",
    "            output_modules.append(nn.Linear(hidden_size, input_size))\n",
    "\n",
    "        self.output_layer = nn.Sequential(*output_modules)\n",
    "        self.output_layer.apply(self._init_weights)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _init_weights(module, scale=1.0):\n",
    "        if type(module) == nn.Linear:\n",
    "            nn.init.uniform_(module.weight, -scale, scale)\n",
    "            nn.init.zeros_(module.bias)\n",
    "\n",
    "    def _domain_map(self, o_t, insample_y):\n",
    "        if isinstance(self.loss, GMM):\n",
    "            weights = o_t[:, :-1]                                             #   [B, L + 1] -> [B, L]\n",
    "            kernel_width = o_t[:, -1:]                                        #   [B, L + 1] -> [B, 1]\n",
    "            kernel_width = torch.repeat_interleave(input=kernel_width,\n",
    "                                                   repeats=weights.shape[1],\n",
    "                                                   dim=-1)                    #   [B, 1] -> [B, L]\n",
    "            output = torch.cat([insample_y, kernel_width, weights], dim=-1)   #   [B, L] + [B, L] + [B, L] = [B, 3 * L]\n",
    "            output = output.unsqueeze(1)                                      #   [B, 3 * L] = [B, 1, 3 * L]\n",
    "        elif isinstance(self.loss, PMM):\n",
    "            weights = o_t                                                     #   [B, L] -> [B, L]\n",
    "            output = torch.cat([insample_y, weights], dim=-1)                 #   [B, L] + [B, L] = [B, 2 * L]\n",
    "            output = output.unsqueeze(1)                                      #   [B, 2 * L] = [B, 1, 2 * L]        \n",
    "        elif isinstance(self.loss, NBMM):\n",
    "            weights = torch.ones_like(o_t)                                    #   [B, L] -> [B, L]\n",
    "            output = torch.cat([insample_y, o_t, weights], dim=-1)            #   [B, L] + [B, L] + [B, L] = [B, 3 * L]\n",
    "            output = output.unsqueeze(1)                                      #   [B, 3 * L] = [B, 1, 3 * \n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        \n",
    "        return output\n",
    "\n",
    "    # Override BaseWindows method\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \n",
    "        # Only train one-step ahead\n",
    "        self.h = 1\n",
    "        self.quantiles = self.loss.quantiles\n",
    "\n",
    "        # Create and normalize windows [Ws, L+H, C]\n",
    "        y_idx = batch[\"y_idx\"]\n",
    "        windows = self._create_windows(batch, step=\"train\")\n",
    "        original_outsample_y = torch.clone(windows[\"temporal\"][:, -self.h :, y_idx])\n",
    "        windows = self._normalization(windows=windows, y_idx=y_idx)\n",
    "\n",
    "        # Parse windows\n",
    "        (\n",
    "            insample_y,\n",
    "            insample_mask,\n",
    "            outsample_y,\n",
    "            outsample_mask,\n",
    "            _,\n",
    "            futr_exog,\n",
    "            stat_exog,\n",
    "        ) = self._parse_windows(batch, windows)\n",
    "\n",
    "        windows_batch = dict(\n",
    "            insample_y=insample_y,  # [Ws, L]\n",
    "            insample_mask=insample_mask,  # [Ws, L]\n",
    "            futr_exog=futr_exog,  # [Ws, L+H]\n",
    "            hist_exog=None,  \n",
    "            stat_exog=stat_exog, # [Ws, 1]\n",
    "            y_idx=y_idx # [Ws, 1]\n",
    "        )  \n",
    "\n",
    "        # Model Predictions\n",
    "        output = self.train_forward(windows_batch)\n",
    "\n",
    "        _, y_loc, y_scale = self._inv_normalization(\n",
    "            y_hat=outsample_y, \n",
    "            temporal_cols=batch[\"temporal_cols\"], \n",
    "            y_idx=y_idx\n",
    "        )\n",
    "        # outsample_y = original_insample_y\n",
    "        outsample_y = original_outsample_y\n",
    "        distr_args = self.loss.scale_decouple(\n",
    "            output=output, loc=y_loc, scale=y_scale\n",
    "        )\n",
    "        loss = self.loss(y=outsample_y, distr_args=distr_args, mask=outsample_mask)\n",
    "\n",
    "        if torch.isnan(loss):\n",
    "            print(\"Model Parameters\", self.hparams)\n",
    "            print(\"insample_y\", torch.isnan(insample_y).sum())\n",
    "            print(\"outsample_y\", torch.isnan(outsample_y).sum())\n",
    "            print(\"output\", torch.isnan(output).sum())\n",
    "            raise Exception(\"Loss is NaN, training stopped.\")\n",
    "\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, on_epoch=True)\n",
    "        self.train_trajectories.append((self.global_step, float(loss)))\n",
    "\n",
    "        self.h = self.h_backup \n",
    "        \n",
    "        return loss\n",
    "\n",
    "    # Override BaseWindows method\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "\n",
    "        self.h = self.h_backup\n",
    "        self.quantiles = self.valid_loss.quantiles\n",
    "\n",
    "        if self.val_size == 0:\n",
    "            return np.nan\n",
    "\n",
    "        # TODO: Hack to compute number of windows\n",
    "        windows = self._create_windows(batch, step=\"val\")\n",
    "        n_windows = len(windows[\"temporal\"])\n",
    "        y_idx = batch[\"y_idx\"]\n",
    "\n",
    "        # Number of windows in batch\n",
    "        windows_batch_size = self.inference_windows_batch_size\n",
    "        if windows_batch_size < 0:\n",
    "            windows_batch_size = n_windows\n",
    "        n_batches = int(np.ceil(n_windows / windows_batch_size))\n",
    "\n",
    "        valid_losses = []\n",
    "        batch_sizes = []\n",
    "        for i in range(n_batches):\n",
    "            # Create and normalize windows [Ws, L+H, C]\n",
    "            w_idxs = np.arange(\n",
    "                i * windows_batch_size, min((i + 1) * windows_batch_size, n_windows)\n",
    "            )\n",
    "            windows = self._create_windows(batch, step=\"val\", w_idxs=w_idxs)\n",
    "            original_outsample_y = torch.clone(windows[\"temporal\"][:, -self.h:, 0])\n",
    "            windows = self._normalization(windows=windows, y_idx=y_idx)\n",
    "\n",
    "            # Parse windows\n",
    "            (\n",
    "                insample_y,\n",
    "                insample_mask,\n",
    "                _,\n",
    "                outsample_mask,\n",
    "                _,\n",
    "                futr_exog,\n",
    "                stat_exog,\n",
    "            ) = self._parse_windows(batch, windows)\n",
    "            \n",
    "            windows_batch = dict(\n",
    "                insample_y=insample_y,  # [Ws, L]\n",
    "                insample_mask=insample_mask,  # [Ws, L]\n",
    "                futr_exog=futr_exog,  # [Ws, L+H]\n",
    "                hist_exog=None,  # [Ws, L]\n",
    "                stat_exog=stat_exog,\n",
    "                y_idx=y_idx,\n",
    "            )  # [Ws, 1]\n",
    "\n",
    "            # Model Predictions\n",
    "            output_batch = self(windows_batch)\n",
    "            # Monte Carlo already returns y_hat with mean and quantiles\n",
    "            output_batch = output_batch[:,:, 1:] # Remove mean\n",
    "            valid_loss_batch = self.valid_loss(y=original_outsample_y, y_hat=output_batch, mask=outsample_mask)\n",
    "            valid_losses.append(valid_loss_batch)\n",
    "            batch_sizes.append(len(output_batch))\n",
    "\n",
    "        valid_loss = torch.stack(valid_losses)\n",
    "        batch_sizes = torch.tensor(batch_sizes, device=valid_loss.device)\n",
    "        valid_loss = torch.sum(valid_loss * batch_sizes) / torch.sum(batch_sizes)\n",
    "\n",
    "        if torch.isnan(valid_loss):\n",
    "            raise Exception(\"Loss is NaN, training stopped.\")\n",
    "\n",
    "        self.log(\"valid_loss\", valid_loss, prog_bar=True, on_epoch=True)\n",
    "        self.validation_step_outputs.append(valid_loss)\n",
    "        return valid_loss\n",
    "\n",
    "    # Override BaseWindows method\n",
    "    def predict_step(self, batch, batch_idx):\n",
    "\n",
    "        self.h == self.h_backup\n",
    "        self.quantiles = self.loss.quantiles\n",
    "\n",
    "        # TODO: Hack to compute number of windows\n",
    "        windows = self._create_windows(batch, step='predict')\n",
    "        n_windows = len(windows['temporal'])\n",
    "        y_idx = batch['y_idx']\n",
    "\n",
    "        # Number of windows in batch\n",
    "        windows_batch_size = self.inference_windows_batch_size\n",
    "        if windows_batch_size < 0:\n",
    "            windows_batch_size = n_windows\n",
    "        n_batches = int(np.ceil(n_windows/windows_batch_size))\n",
    "\n",
    "        y_hats = []\n",
    "        for i in range(n_batches):\n",
    "            # Create and normalize windows [Ws, L+H, C]\n",
    "            w_idxs = np.arange(i*windows_batch_size, \n",
    "                    min((i+1)*windows_batch_size, n_windows))\n",
    "            windows = self._create_windows(batch, step='predict', w_idxs=w_idxs)\n",
    "            windows = self._normalization(windows=windows, y_idx=y_idx)\n",
    "\n",
    "            # Parse windows\n",
    "            insample_y, insample_mask, _, _, _, futr_exog, stat_exog = self._parse_windows(batch, windows)\n",
    "            windows_batch = dict(insample_y=insample_y, # [Ws, L]\n",
    "                                insample_mask=insample_mask, # [Ws, L]\n",
    "                                futr_exog=futr_exog, # [Ws, L+H]\n",
    "                                stat_exog=stat_exog,\n",
    "                                y_idx=y_idx)\n",
    "            \n",
    "            # Model Predictions\n",
    "            y_hat = self(windows_batch)\n",
    "            # Monte Carlo already returns y_hat with mean and quantiles\n",
    "            y_hats.append(y_hat)\n",
    "        y_hat = torch.cat(y_hats, dim=0)\n",
    "        return y_hat\n",
    "\n",
    "    def train_forward(self, windows_batch):\n",
    "        # Parse windows_batch\n",
    "        x_t           = windows_batch['insample_y'].unsqueeze(-1)       #   [B, L, 1]\n",
    "        futr_exog     = windows_batch['futr_exog']                      #   [B, L + h, F]\n",
    "        stat_exog     = windows_batch['stat_exog']                      #   [B, S]\n",
    "\n",
    "        batch_size, seq_len = x_t.shape[:2]                             #   B = batch_size, L = seq_len\n",
    "\n",
    "        # Concatenate x_t with future exogenous\n",
    "        if self.futr_exog_size > 0:      \n",
    "            futr_exog_t = futr_exog[:, :seq_len]                        #   [B, L + h, F] -> [B, L, F]\n",
    "            x_t = torch.cat((x_t, futr_exog_t), dim=2)                  #   [B, L, 1] + [B, L, F] -> [B, L, 1 + F]            \n",
    "        \n",
    "        x_t = x_t.reshape(batch_size, -1)                               #   [B, L, 1 + F] -> [B, L * (1 + F)]\n",
    "\n",
    "        # Concatenate x_t with static exogenous\n",
    "        if self.stat_exog_size > 0:\n",
    "            x_t = torch.cat((x_t, stat_exog), dim=1)                    #   [B, L * (1 + F)] + [B, S] -> [B, L * (1 + F) + S]\n",
    "\n",
    "        # Run through DeepNPTSNetwork\n",
    "        h_t = self.deepnptsnetwork(x_t)                                 #   [B, L * (1 + F) + S] -> [B, hidden_size]\n",
    "        o_t = self.output_layer(h_t)                                    #   [B, hidden_size] -> [B, L + 1]\n",
    "\n",
    "        output = self._domain_map(o_t, windows_batch['insample_y'])     #   [B, L + 1], [B, L] -> [B, 3 * L]\n",
    "        output = self.loss.domain_map(output)                           #   [B, 3 * L] -> ([B, L], [B, L], [B, L])\n",
    "\n",
    "        return output\n",
    "\n",
    "    def forward(self, windows_batch):\n",
    "        # Parse windows_batch\n",
    "        insample_y_t  = windows_batch['insample_y'].unsqueeze(-1)       #   [B, L, 1]\n",
    "        futr_exog     = windows_batch['futr_exog']                      #   [B, L + h, F]\n",
    "        stat_exog     = windows_batch['stat_exog']                      #   [B, S]\n",
    "        y_idx         = windows_batch['y_idx']\n",
    "\n",
    "        batch_size, seq_len = insample_y_t.shape[:2]                    #   B = batch_size, L = seq_len\n",
    "        device = insample_y_t.device\n",
    "        dtype = insample_y_t.dtype\n",
    "\n",
    "        # Repeat insample_y for trajectory samples\n",
    "        insample_y_t = torch.repeat_interleave(input=insample_y_t, \n",
    "                                                repeats=self.trajectory_samples, \n",
    "                                                dim=0)                  #   [B, L, 1] -> [B * n_samples, L, 1]\n",
    "       \n",
    "        # Input x_t is insample_y at time t\n",
    "        x_t = insample_y_t\n",
    "\n",
    "        # Repeat futr_exog if available for trajectory samples and add to x_t        \n",
    "        if self.futr_exog_size > 0:      \n",
    "            futr_exog = torch.repeat_interleave(input=futr_exog, \n",
    "                                                repeats=self.trajectory_samples, \n",
    "                                                dim=0)                  #   [B, L + h, F] -> [B * n_samples, L + h, F]            \n",
    "            x_t = torch.cat((x_t, futr_exog[:, :seq_len]), dim=2)       #   [B * n_samples, L, 1] + [B * n_samples, L, F] -> [B * n_samples, L, 1 + F]            \n",
    "        \n",
    "        x_t = x_t.reshape(batch_size * self.trajectory_samples, -1)     #   [B * n_samples, L, 1 + F] -> [B * n_samples, L * (1 + F)]\n",
    "\n",
    "        # Repeat stat_exog if available for trajectory samples and add to x_t\n",
    "        if self.stat_exog_size > 0:\n",
    "            stat_exog = torch.repeat_interleave(\n",
    "                                      input=stat_exog, \n",
    "                                      repeats=self.trajectory_samples, \n",
    "                                      dim=0)                            #   [B, S] -> [B * n_samples, S] \n",
    "            x_t = torch.cat((x_t, stat_exog), dim=1)                    #   [B * n_samples, L * (1 + F)] + [B * n_samples, S] -> [B * n_samples, L * (1 + F) + S]\n",
    "\n",
    "        # Scales for inverse normalization\n",
    "        y_scale = self.scaler.x_scale[:, :, y_idx]\n",
    "        y_loc = self.scaler.x_shift[:, :, y_idx]\n",
    "        y_scale = torch.repeat_interleave(input=y_scale, \n",
    "                                          repeats=self.trajectory_samples, \n",
    "                                          dim=0)\n",
    "        y_loc = torch.repeat_interleave(input=y_loc, \n",
    "                                        repeats=self.trajectory_samples, \n",
    "                                        dim=0)\n",
    "        # Create forecasts tensor\n",
    "        forecasts = torch.zeros((batch_size, \n",
    "                                self.h,\n",
    "                                len(self.quantiles) + 1), \n",
    "                              device=device, \n",
    "                              dtype=dtype)\n",
    "        \n",
    "        # Recursive predictions\n",
    "        for t in range(self.h):\n",
    "            # Run input throught DeepNPTSNetwork\n",
    "            h_t = self.deepnptsnetwork(x_t)                              #   [B * n_samples, L * (1 + F) + S] -> [B, hidden_size]\n",
    "            o_t = self.output_layer(h_t)                                 #   [B * n_samples, hidden_size] -> [B * n_samples, L (+ 1)]\n",
    "            output = self._domain_map(o_t, insample_y_t.squeeze(-1))     #   [B * n_samples, L + 1], [B * n_samples, L] -> [B * n_samples, 3 * L]\n",
    "            output = self.loss.domain_map(output)                        #   [B * n_samples, 3 * L] -> ([B * n_samples, L], [B * n_samples, L], [B * n_samples, L])\n",
    "\n",
    "            # Inverse normalization\n",
    "            distr_args = self.loss.scale_decouple(output=output, \n",
    "                                                  loc=y_loc, \n",
    "                                                  scale=y_scale)\n",
    "\n",
    "            # Sample and create probabilistic outputs\n",
    "            samples_t_flat, _, _ = self.loss.sample(distr_args=distr_args, \n",
    "                                                    num_samples=1)\n",
    "\n",
    "            samples_t_flat = samples_t_flat.squeeze()\n",
    "            samples_t = samples_t_flat.reshape(batch_size, \n",
    "                                      self.trajectory_samples)          #   [B * n_samples] -> [B, n_samples]  \n",
    "            \n",
    "            samples_t_mean = torch.mean(samples_t, dim=-1)              #   [B, n_samples] -> [B]  \n",
    "            quantiles_t = torch.quantile(input=samples_t, \n",
    "                                      q=self.quantiles, \n",
    "                                      dim=-1)                           #   [B, n_samples] -> [Q, B]\n",
    "            forecasts[:, t, 0] = samples_t_mean\n",
    "            forecasts[:, t, 1:] = quantiles_t.permute(1, 0)\n",
    "\n",
    "            insample_y_t_next = self.scaler.scaler(samples_t_flat, \n",
    "                                                y_loc.squeeze(), \n",
    "                                                y_scale.squeeze())      #   [B * n_samples] -> [B * n_samples]\n",
    "            insample_y_t_next = insample_y_t_next.unsqueeze(-1)\\\n",
    "                                                 .unsqueeze(-1)         #   [B * n_samples] -> [B * n_samples, 1, 1]\n",
    "\n",
    "            # Update insample_y_t           \n",
    "            insample_y_t = torch.cat([insample_y_t[:, 1:], \n",
    "                                      insample_y_t_next], \n",
    "                                      dim=1)                            #   [B * n_samples, L - 1, 1] + [B * n_samples, 1, 1] -> [B * n_samples, L, 1]\n",
    "            \n",
    "            # Update input\n",
    "            x_t = insample_y_t\n",
    "            # Concatenate x_t with future exogenous\n",
    "            if self.futr_exog_size > 0:      \n",
    "                x_t = torch.cat((x_t, \n",
    "                                 futr_exog[:, t:seq_len + t]), \n",
    "                                 dim=2)                                 #   [B * n_samples, L, 1] + [B * n_samples, L, F] -> [B * n_samples, L, 1 + F]            \n",
    "            \n",
    "            x_t = x_t.reshape(batch_size * self.trajectory_samples\n",
    "                              , -1)                                     #   [B * n_samples, L, 1 + F] -> [B * n_samples, L * (1 + F)]\n",
    "\n",
    "            # Concatenate x_t with static exogenous\n",
    "            if self.stat_exog_size > 0:\n",
    "                x_t = torch.cat((x_t, stat_exog), dim=1)                #   [B * n_samples, L * (1 + F)] + [B * n_samples, S] -> [B * n_samples, L * (1 + F) + S]\n",
    "       \n",
    "        return forecasts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/models/deepnpts.py#L20){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DeepNPTS\n",
       "\n",
       ">      DeepNPTS (h, input_size:int=-1, hidden_size:int=32, batch_norm:bool=True,\n",
       ">                dropout:float=0.5, n_layers:int=2, trajectory_samples:int=100,\n",
       ">                futr_exog_list=None, hist_exog_list=None, stat_exog_list=None,\n",
       ">                exclude_insample_y=False, loss=GMM(), valid_loss=MQLoss(),\n",
       ">                max_steps:int=1000, learning_rate:float=0.001,\n",
       ">                num_lr_decays:int=3, early_stop_patience_steps:int=-1,\n",
       ">                val_check_steps:int=100, batch_size:int=32,\n",
       ">                valid_batch_size:Optional[int]=None,\n",
       ">                windows_batch_size:int=1024,\n",
       ">                inference_windows_batch_size:int=-1,\n",
       ">                start_padding_enabled=False, step_size:int=1,\n",
       ">                scaler_type:str='standard', random_seed:int=1,\n",
       ">                num_workers_loader=0, drop_last_loader=False, optimizer=None,\n",
       ">                optimizer_kwargs=None, **trainer_kwargs)\n",
       "\n",
       "DeepNPTS\n",
       "\n",
       "Deep Non-Parametric Time Series Forecaster (`DeepNPTS`) is a baseline model for time-series forecasting. This model generates predictions by sampling from the empirical distribution according to a learnable strategy. The strategy is learned by exploiting the information across multiple related time series. \n",
       "\n",
       "**Parameters:**<br>\n",
       "`h`: int, Forecast horizon. <br>\n",
       "`input_size`: int, autorregresive inputs size, y=[1,2,3,4] input_size=2 -> y_[t-2:t]=[1,2].<br>\n",
       "`hidden_size`: int=32, hidden size of dense layers.<br>\n",
       "`batch_norm`: bool=True, if True, applies Batch Normalization after each dense layer in the network.<br>\n",
       "`dropout`: float=0.1, dropout.<br>\n",
       "`n_layers`: int=2, number of dense layers.<br>\n",
       "`trajectory_samples`: int=100, number of Monte Carlo trajectories during inference.<br>\n",
       "`stat_exog_list`: str list, static exogenous columns.<br>\n",
       "`hist_exog_list`: str list, historic exogenous columns.<br>\n",
       "`futr_exog_list`: str list, future exogenous columns.<br>\n",
       "`exclude_insample_y`: bool=False, the model skips the autoregressive features y[t-input_size:t] if True.<br>\n",
       "`loss`: PyTorch module, instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
       "`valid_loss`: PyTorch module=`loss`, instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
       "`max_steps`: int=1000, maximum number of training steps.<br>\n",
       "`learning_rate`: float=1e-3, Learning rate between (0, 1).<br>\n",
       "`num_lr_decays`: int=-1, Number of learning rate decays, evenly distributed across max_steps.<br>\n",
       "`early_stop_patience_steps`: int=-1, Number of validation iterations before early stopping.<br>\n",
       "`val_check_steps`: int=100, Number of training steps between every validation loss check.<br>\n",
       "`batch_size`: int=32, number of different series in each batch.<br>\n",
       "`valid_batch_size`: int=None, number of different series in each validation and test batch, if None uses batch_size.<br>\n",
       "`windows_batch_size`: int=1024, number of windows to sample in each training batch, default uses all.<br>\n",
       "`inference_windows_batch_size`: int=-1, number of windows to sample in each inference batch, -1 uses all.<br>\n",
       "`start_padding_enabled`: bool=False, if True, the model will pad the time series with zeros at the beginning, by input size.<br>\n",
       "`step_size`: int=1, step size between each window of temporal data.<br>\n",
       "`scaler_type`: str='identity', type of scaler for temporal inputs normalization see [temporal scalers](https://nixtla.github.io/neuralforecast/common.scalers.html).<br>\n",
       "`random_seed`: int, random_seed for pytorch initializer and numpy generators.<br>\n",
       "`num_workers_loader`: int=os.cpu_count(), workers to be used by `TimeSeriesDataLoader`.<br>\n",
       "`drop_last_loader`: bool=False, if True `TimeSeriesDataLoader` drops last non-full batch.<br>\n",
       "`alias`: str, optional,  Custom name of the model.<br>\n",
       "`optimizer`: Subclass of 'torch.optim.Optimizer', optional, user specified optimizer instead of the default choice (Adam).<br>\n",
       "`optimizer_kwargs`: dict, optional, list of parameters used by the user specified `optimizer`.<br>\n",
       "`**trainer_kwargs`: int,  keyword trainer arguments inherited from [PyTorch Lighning's trainer](https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.trainer.trainer.Trainer.html?highlight=trainer).<br>    \n",
       "\n",
       "**References**<br>\n",
       "- [Rangapuram, Syama Sundar, Jan Gasthaus, Lorenzo Stella, Valentin Flunkert, David Salinas, Yuyang Wang, and Tim Januschowski (2023). \"Deep Non-Parametric Time Series Forecaster\". arXiv.](https://arxiv.org/abs/2312.14657)<br>"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/models/deepnpts.py#L20){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DeepNPTS\n",
       "\n",
       ">      DeepNPTS (h, input_size:int=-1, hidden_size:int=32, batch_norm:bool=True,\n",
       ">                dropout:float=0.5, n_layers:int=2, trajectory_samples:int=100,\n",
       ">                futr_exog_list=None, hist_exog_list=None, stat_exog_list=None,\n",
       ">                exclude_insample_y=False, loss=GMM(), valid_loss=MQLoss(),\n",
       ">                max_steps:int=1000, learning_rate:float=0.001,\n",
       ">                num_lr_decays:int=3, early_stop_patience_steps:int=-1,\n",
       ">                val_check_steps:int=100, batch_size:int=32,\n",
       ">                valid_batch_size:Optional[int]=None,\n",
       ">                windows_batch_size:int=1024,\n",
       ">                inference_windows_batch_size:int=-1,\n",
       ">                start_padding_enabled=False, step_size:int=1,\n",
       ">                scaler_type:str='standard', random_seed:int=1,\n",
       ">                num_workers_loader=0, drop_last_loader=False, optimizer=None,\n",
       ">                optimizer_kwargs=None, **trainer_kwargs)\n",
       "\n",
       "DeepNPTS\n",
       "\n",
       "Deep Non-Parametric Time Series Forecaster (`DeepNPTS`) is a baseline model for time-series forecasting. This model generates predictions by sampling from the empirical distribution according to a learnable strategy. The strategy is learned by exploiting the information across multiple related time series. \n",
       "\n",
       "**Parameters:**<br>\n",
       "`h`: int, Forecast horizon. <br>\n",
       "`input_size`: int, autorregresive inputs size, y=[1,2,3,4] input_size=2 -> y_[t-2:t]=[1,2].<br>\n",
       "`hidden_size`: int=32, hidden size of dense layers.<br>\n",
       "`batch_norm`: bool=True, if True, applies Batch Normalization after each dense layer in the network.<br>\n",
       "`dropout`: float=0.1, dropout.<br>\n",
       "`n_layers`: int=2, number of dense layers.<br>\n",
       "`trajectory_samples`: int=100, number of Monte Carlo trajectories during inference.<br>\n",
       "`stat_exog_list`: str list, static exogenous columns.<br>\n",
       "`hist_exog_list`: str list, historic exogenous columns.<br>\n",
       "`futr_exog_list`: str list, future exogenous columns.<br>\n",
       "`exclude_insample_y`: bool=False, the model skips the autoregressive features y[t-input_size:t] if True.<br>\n",
       "`loss`: PyTorch module, instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
       "`valid_loss`: PyTorch module=`loss`, instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
       "`max_steps`: int=1000, maximum number of training steps.<br>\n",
       "`learning_rate`: float=1e-3, Learning rate between (0, 1).<br>\n",
       "`num_lr_decays`: int=-1, Number of learning rate decays, evenly distributed across max_steps.<br>\n",
       "`early_stop_patience_steps`: int=-1, Number of validation iterations before early stopping.<br>\n",
       "`val_check_steps`: int=100, Number of training steps between every validation loss check.<br>\n",
       "`batch_size`: int=32, number of different series in each batch.<br>\n",
       "`valid_batch_size`: int=None, number of different series in each validation and test batch, if None uses batch_size.<br>\n",
       "`windows_batch_size`: int=1024, number of windows to sample in each training batch, default uses all.<br>\n",
       "`inference_windows_batch_size`: int=-1, number of windows to sample in each inference batch, -1 uses all.<br>\n",
       "`start_padding_enabled`: bool=False, if True, the model will pad the time series with zeros at the beginning, by input size.<br>\n",
       "`step_size`: int=1, step size between each window of temporal data.<br>\n",
       "`scaler_type`: str='identity', type of scaler for temporal inputs normalization see [temporal scalers](https://nixtla.github.io/neuralforecast/common.scalers.html).<br>\n",
       "`random_seed`: int, random_seed for pytorch initializer and numpy generators.<br>\n",
       "`num_workers_loader`: int=os.cpu_count(), workers to be used by `TimeSeriesDataLoader`.<br>\n",
       "`drop_last_loader`: bool=False, if True `TimeSeriesDataLoader` drops last non-full batch.<br>\n",
       "`alias`: str, optional,  Custom name of the model.<br>\n",
       "`optimizer`: Subclass of 'torch.optim.Optimizer', optional, user specified optimizer instead of the default choice (Adam).<br>\n",
       "`optimizer_kwargs`: dict, optional, list of parameters used by the user specified `optimizer`.<br>\n",
       "`**trainer_kwargs`: int,  keyword trainer arguments inherited from [PyTorch Lighning's trainer](https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.trainer.trainer.Trainer.html?highlight=trainer).<br>    \n",
       "\n",
       "**References**<br>\n",
       "- [Rangapuram, Syama Sundar, Jan Gasthaus, Lorenzo Stella, Valentin Flunkert, David Salinas, Yuyang Wang, and Tim Januschowski (2023). \"Deep Non-Parametric Time Series Forecaster\". arXiv.](https://arxiv.org/abs/2312.14657)<br>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DeepNPTS, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### DeepNPTS.fit\n",
       "\n",
       ">      DeepNPTS.fit (dataset, val_size=0, test_size=0, random_seed=None,\n",
       ">                    distributed_config=None)\n",
       "\n",
       "Fit.\n",
       "\n",
       "The `fit` method, optimizes the neural network's weights using the\n",
       "initialization parameters (`learning_rate`, `windows_batch_size`, ...)\n",
       "and the `loss` function as defined during the initialization.\n",
       "Within `fit` we use a PyTorch Lightning `Trainer` that\n",
       "inherits the initialization's `self.trainer_kwargs`, to customize\n",
       "its inputs, see [PL's trainer arguments](https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.trainer.trainer.Trainer.html?highlight=trainer).\n",
       "\n",
       "The method is designed to be compatible with SKLearn-like classes\n",
       "and in particular to be compatible with the StatsForecast library.\n",
       "\n",
       "By default the `model` is not saving training checkpoints to protect\n",
       "disk memory, to get them change `enable_checkpointing=True` in `__init__`.\n",
       "\n",
       "**Parameters:**<br>\n",
       "`dataset`: NeuralForecast's `TimeSeriesDataset`, see [documentation](https://nixtla.github.io/neuralforecast/tsdataset.html).<br>\n",
       "`val_size`: int, validation size for temporal cross-validation.<br>\n",
       "`random_seed`: int=None, random_seed for pytorch initializer and numpy generators, overwrites model.__init__'s.<br>\n",
       "`test_size`: int, test size for temporal cross-validation.<br>"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### DeepNPTS.fit\n",
       "\n",
       ">      DeepNPTS.fit (dataset, val_size=0, test_size=0, random_seed=None,\n",
       ">                    distributed_config=None)\n",
       "\n",
       "Fit.\n",
       "\n",
       "The `fit` method, optimizes the neural network's weights using the\n",
       "initialization parameters (`learning_rate`, `windows_batch_size`, ...)\n",
       "and the `loss` function as defined during the initialization.\n",
       "Within `fit` we use a PyTorch Lightning `Trainer` that\n",
       "inherits the initialization's `self.trainer_kwargs`, to customize\n",
       "its inputs, see [PL's trainer arguments](https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.trainer.trainer.Trainer.html?highlight=trainer).\n",
       "\n",
       "The method is designed to be compatible with SKLearn-like classes\n",
       "and in particular to be compatible with the StatsForecast library.\n",
       "\n",
       "By default the `model` is not saving training checkpoints to protect\n",
       "disk memory, to get them change `enable_checkpointing=True` in `__init__`.\n",
       "\n",
       "**Parameters:**<br>\n",
       "`dataset`: NeuralForecast's `TimeSeriesDataset`, see [documentation](https://nixtla.github.io/neuralforecast/tsdataset.html).<br>\n",
       "`val_size`: int, validation size for temporal cross-validation.<br>\n",
       "`random_seed`: int=None, random_seed for pytorch initializer and numpy generators, overwrites model.__init__'s.<br>\n",
       "`test_size`: int, test size for temporal cross-validation.<br>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DeepNPTS.fit, name='DeepNPTS.fit', title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### DeepNPTS.predict\n",
       "\n",
       ">      DeepNPTS.predict (dataset, test_size=None, step_size=1, random_seed=None,\n",
       ">                        **data_module_kwargs)\n",
       "\n",
       "Predict.\n",
       "\n",
       "Neural network prediction with PL's `Trainer` execution of `predict_step`.\n",
       "\n",
       "**Parameters:**<br>\n",
       "`dataset`: NeuralForecast's `TimeSeriesDataset`, see [documentation](https://nixtla.github.io/neuralforecast/tsdataset.html).<br>\n",
       "`test_size`: int=None, test size for temporal cross-validation.<br>\n",
       "`step_size`: int=1, Step size between each window.<br>\n",
       "`random_seed`: int=None, random_seed for pytorch initializer and numpy generators, overwrites model.__init__'s.<br>\n",
       "`**data_module_kwargs`: PL's TimeSeriesDataModule args, see [documentation](https://pytorch-lightning.readthedocs.io/en/1.6.1/extensions/datamodules.html#using-a-datamodule)."
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### DeepNPTS.predict\n",
       "\n",
       ">      DeepNPTS.predict (dataset, test_size=None, step_size=1, random_seed=None,\n",
       ">                        **data_module_kwargs)\n",
       "\n",
       "Predict.\n",
       "\n",
       "Neural network prediction with PL's `Trainer` execution of `predict_step`.\n",
       "\n",
       "**Parameters:**<br>\n",
       "`dataset`: NeuralForecast's `TimeSeriesDataset`, see [documentation](https://nixtla.github.io/neuralforecast/tsdataset.html).<br>\n",
       "`test_size`: int=None, test size for temporal cross-validation.<br>\n",
       "`step_size`: int=1, Step size between each window.<br>\n",
       "`random_seed`: int=None, random_seed for pytorch initializer and numpy generators, overwrites model.__init__'s.<br>\n",
       "`**data_module_kwargs`: PL's TimeSeriesDataModule args, see [documentation](https://pytorch-lightning.readthedocs.io/en/1.6.1/extensions/datamodules.html#using-a-datamodule)."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DeepNPTS.predict, name='DeepNPTS.predict', title_level=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralforecast import NeuralForecast\n",
    "from neuralforecast.losses.pytorch import MQLoss, DistributionLoss, GMM\n",
    "from neuralforecast.tsdataset import TimeSeriesDataset\n",
    "from neuralforecast.utils import AirPassengers, AirPassengersPanel, AirPassengersStatic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b74158f17d254e4884139ee5c48e5706",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ed78b67d4e041668812b75248ad7757",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb8b8f816727459d8f31378b98cbb587",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3611a5d5430a4343a01bf607e8740356",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5a35ce7aeb54711bd449e66b6c10966",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d23824a7adb04bb19b4c33063a0fd1eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "842c02d1c3a7414eaa582f00ea3da2e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bc7f5e925a54c19aa32229733b5c53d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4ae0935430c4fe39c9d1ec1712a833e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAigAAAGdCAYAAAA44ojeAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAACUd0lEQVR4nOzdd3hUVfrA8e+U9EpCSAgECL0E6YsUBZcmiI39iQqiLOjqoigC4iK6BgsIK8IKawdBWSy7imtBJICAiEiXJkV6SQiBkF4mmfv7Y7yXTDKTzExmJpPwfp7HR+bOnXvPPQTm5T3vOUenKIqCEEIIIYQP0dd0A4QQQgghypMARQghhBA+RwIUIYQQQvgcCVCEEEII4XMkQBFCCCGEz5EARQghhBA+RwIUIYQQQvgcCVCEEEII4XOMNd0AV5jNZs6fP09YWBg6na6mmyOEEEIIByiKQk5ODvHx8ej1ledIamWAcv78eRISEmq6GUIIIYRwwZkzZ2jcuHGl59TKACUsLAywPGB4eHgNt8ZzTCYTa9asYfDgwfj5+dV0c3ya9JVzpL+cI/3lOOkr51xr/ZWdnU1CQoL2PV4ZpwOUnJwcnnvuOVauXEl6ejpdunThn//8Jz169AAs6ZuZM2fyzjvvkJmZSc+ePfnXv/5Fhw4dtGsUFRUxdepUPvroIwoKChgwYABvvPFGldGUSh3WCQ8Pr/MBSnBwMOHh4dfED251SF85R/rLOdJfjpO+cs612l+OlGc4XST74IMPkpKSwocffsi+ffsYPHgwAwcO5Ny5cwDMnTuX1157jUWLFrF9+3bi4uIYNGgQOTk52jUmTZrEypUr+fjjj9m8eTO5ubkMHz6c0tJSZ5sjhBBCiDrIqQCloKCAzz77jLlz53LjjTfSsmVLkpOTSUxM5M0330RRFBYsWMCMGTMYMWIESUlJLFu2jPz8fFasWAFAVlYWixcvZt68eQwcOJAuXbqwfPly9u3bx9q1az3ykEIIIYSoXZwa4ikpKaG0tJTAwECr40FBQWzevJkTJ06QlpbG4MGDtfcCAgLo168fW7Zs4eGHH2bnzp2YTCarc+Lj40lKSmLLli0MGTKkwn2LioooKirSXmdnZwOW1JjJZHLmEWoV9dnq8jO6i/SVc6S/nCP95TjpK+dca/3lzHM6FaCEhYXRq1cvXnzxRdq1a0dsbCwfffQRP//8M61atSItLQ2A2NhYq8/FxsZy6tQpANLS0vD396devXoVzlE/X97s2bOZOXNmheNr1qwhODjYmUeolVJSUmq6CbWG9JVzpL+cI/3lOG/0lV6vr3Kqam1gNBr5/vvva7oZblNaWoqiKDbfy8/Pd/g6ThfJfvjhh4wbN45GjRphMBjo2rUro0aNYteuXdo55YtfFEWpsiCmsnOmT5/O5MmTtddqFfDgwYPrfJFsSkoKgwYNuqaKp1whfeUc6S/nSH85zht9ZTKZuHDhAgUFBR65vjcpikJhYSGBgYF1Zl0vnU5Hw4YNCQkJqfCeOgLiCKcDlBYtWrBx40by8vLIzs6mYcOG3H333SQmJhIXFwdYsiQNGzbUPpOenq5lVeLi4iguLiYzM9Mqi5Kenk7v3r1t3jMgIICAgIAKx/38/K6Jvyyuled0B+kr50h/OUf6y3Ge6iuz2czx48cxGAw0atQIf3//Wv3Fbjabyc3NJTQ0tE5kgxRF4eLFi6SlpdGqVSsMBoPV+878TLi8DkpISAghISFkZmby3XffMXfuXC1ISUlJoUuXLgAUFxezceNG5syZA0C3bt3w8/MjJSWFkSNHApCamsr+/fuZO3euq80RQghxDSguLsZsNpOQkFAnhvjNZjPFxcUEBgbWiQAFICYmhpMnT2IymSoEKM5wOkD57rvvUBSFNm3a8Ntvv/HUU0/Rpk0b/vznP6PT6Zg0aRKzZs2iVatWtGrVilmzZhEcHMyoUaMAiIiIYPz48UyZMoXo6GiioqKYOnUqHTt2ZODAgS4/iBBCiGtHXfkyr4vcldFyOkDJyspi+vTpnD17lqioKP70pz/x8ssva2mbadOmUVBQwIQJE7SF2tasWWO1atz8+fMxGo2MHDlSW6ht6dKl1Yq0hBBCCFF3OB2gjBw5UhuasUWn05GcnExycrLdcwIDA1m4cCELFy509vZCCCGEuAZIjkwIIYQQPkcCFCGEEMLDdDqdzf8MBgP16tXjz3/+c0030efUyt2MhRBCiNokNTVV+/Unn3zC3//+dw4fPozZbCYnJ4cGDRpYnW8yma75Ke2SQRFCCFGrKYpCXl5ejfxnb8XU8uLi4rT/IiIi0Ol02uuioiKioqL49NNP6d+/P4GBgSxfvpzk5GQ6d+5sdZ0FCxbQrFkzq2d///33adeuHYGBgbRt25Y33njDjb1bcySDIoQQolbLz88nNDS0Ru6dm5trc8VUVzz99NPMmzeP999/n4CAAN55550qP/POu+8yMzmZRYsW0aVLF3bv3s1DDz1ESEgIDzzwgFvaVVMkQBFCCCF8wKRJkxgxYoRTn3n5pZeYN2+e9rnExEQOHjzI22+/LQGKEEIIUZOCg4PJzc2tsXu7S/fu3Z06/+LFi5w5c4bx48fz0EMPacdLSkqIiIhwW7tqigQoQgghajWdTue2YZaaVP4Z9Hp9hRoXk8mk/dpsNgPw7rvv0rNnT6vz6sLCpxKgCCGEED4oJiaGtLQ0FEXRlo/fs2eP9n5sbCyNGjXi+PHjjB49uoZa6TkSoAghhBA+qH///ly8eJG5c+fyf//3f6xevZpvv/2W8PBw7ZwZz/2dKU9OIjw8nKFDh1JUVMSOHTvIzMxk8uTJNdj66pNpxkIIIYQPateuHW+88Qb/+te/6NSpE9u2bWPq1KlW5/x53Hjee+89li5dSseOHenXrx9Lly4lMTGxhlrtPpJBEUIIIbxo7NixjB07VnvdpEkTSktLbe7Q/Mgjj/DII49YHXvmmWe0XyvAqFGjGDVqlKeaW2MkgyKEEELUUo4uFFcbSYAihBBC1FJ1OD6RAEUIIYSorepwfCIBihBCCFFr1eEUigQoQgghRC2lUHfrUCRAEUIIIWopRam7wzwSoAghhBC1WR2NUCRAEUIIIWophTobn0iAIoQQQtRedTdEkQBFCCGEqK2UujuRRwIUIYQQopaqo7EJIAGKEEII4RX9+/dn4sSJTJo0iXr16hEbG8s777xDXl4e48aNIywsjBYtWvDtt99qnzl48CDDhg0jNDSU2NhYxowZQ0ZGhvb+2jXf0e/GG4mMjCQ6Oprhw4dz7Ngx7f2TJ0+i0+n4/PPPuemmmwgODqZTp0789NNPXn12V0iAIoQQonZTFMjLq5n/nBxfWbZsGfXr12fbtm1MnDiRRx99lLFjx9KrVy927drFkCFDGDNmDPn5+aSmptKvXz86d+7Mjh07WL16NRcuXGDkyJHa9fLy83niyUls376ddevWodfrufPOOzGbzVb3nTFjBlOnTmXPnj20bt2ae++9l5KSErd0v6fIbsZCCCFqt/x8CA2tmXvn5kJIiMOnd+rUiWeffRaA6dOn88orrxAdHc1DDz2EXq/n73//O2+++SZ79+5l1apVdO3alVmzZmmfX7JkCQkJCRw5coTWrVtz2+13EhJgxN9oyTcsXryYBg0acPDgQZKSkrTPTZ06lVtuuQWAmTNn0qFDB3777Tfatm3rjl7wCMmgCCGEEF5y3XXXab82GAxER0fTvn177VhsbCwA6enp7Ny5k++//57Q0FDtPzWgUIdxjh8/xv1jRtO8eXPCw8NJTEwE4PTp03bv27BhQ+0evkwyKEIIIWq34GBLJqOm7u0EPz8/q9c6nc7qmE6nA8BsNmM2m7n11luZM2dOheuoQcaou0bQpEkC7777LvHx8ZjNZpKSkiguLrZ737L38GUSoAghhKjddDqnhllqi65du/LZZ5/RrFkzjMaKX9eXLl3iyOFDvPHmmwy4qT8Amzdv9m4jPUiGeIQQQggf9Oijj3L58mXuvfdetm3bxvHjx1mzZg3jxo2jtLSUyMhIoqKiWfzee/z222+sX7+eyZMn13Sz3capAKWkpIRnn32WxMREgoKCaN68OS+88IJVmkhRFJKTk4mPjycoKIj+/ftz4MABq+sUFRUxceJE6tevT0hICLfddhtnz551zxMJIYQQdUB8fDw//vgjpaWlDBkyhKSkJJ544gkiIiLQ6/Xo9XreXfoBu3ftIikpiSeffJJ//OMfNd1st3FqiGfOnDm89dZbLFu2jA4dOrBjxw7+/Oc/ExERwRNPPAHA3Llzee2111i6dCmtW7fmpZdeYtCgQRw+fJiwsDAAJk2axFdffcXHH39MdHQ0U6ZMYfjw4ezcuRODweD+pxRCCCFq2IYNGyocO378ONnZ2VbHlDJTl1u1asXnn39u83pmRaHfTQPY9ctegvyvfp2X/XyzZs2sXgNERkZWOOaLnApQfvrpJ26//XZtqlKzZs346KOP2LFjB2DplAULFjBjxgxGjBgBWOZ8x8bGsmLFCh5++GGysrJYvHgxH374IQMHDgRg+fLlJCQksHbtWoYMGeLO5xNCCCHqJsXqf3WOU0M8ffv2Zd26dRw5cgSAX375hc2bNzNs2DAATpw4QVpaGoMHD9Y+ExAQQL9+/diyZQsAO3fuxGQyWZ0THx9PUlKSdo4QQgghKldXAxOVUxmUp59+mqysLNq2bYvBYKC0tJSXX36Ze++9F4C0tDTg6jxuVWxsLKdOndLO8ff3p169ehXOUT9fXlFREUVFRdprNR1mMpkwmUzOPEKtoj5bXX5Gd5G+co70l3Okvxzn6b4ymUwoiqJNw63t1KEW9Zm89VlPMpvNKIqCyWSqULbhzM+FUwHKJ598wvLly1mxYgUdOnRgz549TJo0ifj4eB544AHtPHWOtUpRlArHyqvsnNmzZzNz5swKx9esWUOwk3PQa6OUlJSabkKtIX3lHOkv50h/Oc5TfWU0GomLiyM3N7fCWh+1WU5OjkufMwCmEjAVuLc91VFcXExBQQGbNm2qsJx+fn6+w9dxKkB56qmn+Nvf/sY999wDQMeOHTl16hSzZ8/mgQceIC4uDrBkSdRFZMCyWp2aVYmLi6O4uJjMzEyrLEp6ejq9e/e2ed/p06dbTZ3Kzs4mISGBwYMHEx4e7swj1Comk4mUlBQGDRpUYXEfYU36yjnSX86R/nKcp/uqsLCQM2fOEBoaSmBgoNuv722KopCTk0NYWFiV/5Avr9SskF1YQoBRT7C/70wwKSwsJCgoiBtvvLHC71H5guDKOBWg5Ofno9dbl60YDAYttZSYmEhcXBwpKSl06dIFsERSGzdu1FbC69atG35+fqSkpGgbHqWmprJ//37mzp1r874BAQEEBARUOO7n53dN/GVxrTynO0hfOUf6yznSX47zVF+Vlpai0+m0aba1nfr9qT6TM0qVq8M6vtQXer1eWyG3/M+AMz8TTgUot956Ky+//DJNmjShQ4cO7N69m9dee41x48YBlg6eNGkSs2bNolWrVrRq1YpZs2YRHBzMqFGjAIiIiGD8+PFMmTKF6OhooqKimDp1Kh07dtRm9QghhBDCMXW1WNapAGXhwoU899xzTJgwgfT0dOLj43n44Yf5+9//rp0zbdo0CgoKmDBhApmZmfTs2ZM1a9Zoa6AAzJ8/H6PRyMiRIykoKGDAgAEsXbpU1kARQgghHFVXI5PfORWghIWFsWDBAhYsWGD3HJ1OR3JyMsnJyXbPCQwMZOHChSxcuNCZ2wshhBDid2p8UgvWXHOJ7wxaCSGEEMIJtiOT/v37M2nSJO11s2bNKk0s+CrZzVgIIYSohbQMShVjPdu3byekFu72LAGKEEIIURupS91XMcQTExPj+bZ4gAzxCCGEEF7Qv39/Jk6cyKRJk6hXrx6xsbG888475OXlMW7cOMLCwmjRogXffvut9pmDBw8ybNgwQkNDiY2NZcyYMWRkZACW+CQvL49HHhxHaGgoDRs2ZN68eRXuW36I57XXXqNjx46EhISQkJDAhAkTyM3N1d5funQpkZGRfPfdd7Rr147Q0FBuvvlmUlNTPdY3tkiAIoQQolZTFMjLq5n/nC1QXbZsGfXr12fbtm1MnDiRRx99lLFjx9KrVy927drFkCFDGDNmDPn5+aSmptKvXz86d+7Mjh07WL16NRcuXNDWEANIfnY6P2zayMqVK1mzZg0bNmxg586dlbZBr9fz+uuvs3//fpYtW8b69euZNm2a1Tn5+fm8+uqrfPjhh2zatInTp08zdepU5x62mmSIRwghRK2Wnw+hoTVz79xccKa8o1OnTjz77LOAZZX0V155hejoaB566CH0ej1///vfefPNN9m7dy+rVq2ia9euzJo1S/v8kiVLSEhI4MiRI9SLieXfHyzljXeWMGjQIMASADVu3LjSNpQtoE1MTOTFF1/kr3/9K2+88YZ23GQy8dZbb9GiRQsAHnvsMV544QXHH9QNJEARQgghvOS6667Tfm0wGIiOjqZ9+/baMXVbmPT0dHbu3Mn3339PqI3o69ixY0Rl51BcXEyPnj2141FRUbRp06bSNnz//ffMmjWLgwcPkp2dTUlJCYWFheTl5WnFtMHBwVpwAtCwYUPS09Nde2gXSYAihBCiVgsOtmQyaurezii/1Lu6JHzZ14C2W/Ott96qbRVTVsOGDdl38NffXzk+znTq1CmGDRvGI488wosvvkhUVBSbN29m/PjxVjsN22qn4uUFVyRAEUIIUavpdM4Ns9QWXbt25bPPPqNZs2YYjRW/rpu3aImfnx/bt22jY5uW6HQ6MjMzOXLkCP369bN5zR07dlBSUsK8efO0/Xs+/fRTjz6Hq6RIVgghhPBBjz76KJcvX+bee+9l27ZtHD9+nDVr1jBu3DhKS0sJDQll9P1jSX52OuvWrWP//v2MHTu20o0DW7RoQUlJCQsXLuT48eN8+OGHvPXWW158KsdJgCKEEEL4oPj4eH788UdKS0sZMmQISUlJPPHEE0RERKDX61GA5Jdm06tPX26//XYGDhxI37596datm91rdu7cmddee405c+aQlJTEv//9b2bPnu29h3KCDPEIIYQQXrBhw4YKx44fP052drbVsbK1Hq1ateLzzz+3e83Q0FDefPd96v3bH73eUr/y1FNPWZ1z8uRJq9dPPvkkTz75pNWxMWPGaL8eO3YsY8eOtXr/jjvu8HoNimRQhBBCiFpIsfPrukICFCGEEKI2KpPRqGo/ntpIAhQhhBCiFlLsvqgbJEARQgghark6GJ9IgCKEEELURnUxKClLAhQhhBCiNioToXh5go1XSIAihBBC1Hp1L0KRAEUIIYSohWSasRBCCCF8T9lxnToYoUiAIoQQQnhB//79mTRpktuu50gGZezYsdxxxx1uu6c3yVL3Qggh6oQVP5/26v1G9Wzi1ft5yqeffsqsWbM4cuQIMTExPPbYYxWWy9+4cSOTJ0/mwIEDxMfHM23aNB555BGPtksCFCGEEKIWssqguDjE8+233zJ69GgWLlzI4MGD+fXXX3nwwQcJCgriscceA+DEiRMMGzaMhx56iOXLl/Pjjz8yYcIEYmJi+NOf/lT9B7FDhniEEEKIGlBcXMzTTz9N+/btCQsLo2fPntqGgllZWQQFBbF69Wqrz3z++eeEhISQm5sLCqSeP8f4B+4jPrY+0dHR3H777RU2B6zMhx9+yB133MEjjzxC8+bNueWWW3j66aeZM2eOtjngW2+9RZMmTViwYAHt2rXjwQcfZNy4cbz66qvu6gqbJEARQgghasCf//xntmzZwnvvvceePXu46667uPnmmzl69CgRERHccsst/Pvf/7b6zIoVK7j99tsJDQ0lLz+f24cNITQkhO/WrWfz5s2EhoZy8803U1xc7FAbioqKCAwMtDoWFBTE2bNnOXXqFAA//fQTgwcPtjpnyJAh7NixA5PJVI0eqJwEKEIIIYSXHTt2jI8++ohPPvmE3r1706JFC6ZOnUrfvn15//33ARg9ejRffPEF+fn5AGRnZ/PNN99w3333AbDyv5+i1+tZ8K+36NChI+3ateP999/n9OnTWiamKkOGDOHzzz9n3bp1mM1mjhw5woIFCwBITU0FIC0tjdjYWKvPxcbGUlJSQkZGhht6wzapQRFCCCG8bNeuXSiKQtu2ba2OFxUVER0dDcAtt9yC0Wjkyy+/5J577uGzzz4jLCxMy2bs2b2LE8eP0axhfatrFBYWcuzYMYfaMf7BBzl27BjDhw/HZDIRHh7OE088QXJyMgaDQTtPp9NZfU4d/il/3J0kQBFCCCG8zGw2YzAY2L59OwUFBYSGhqLXWwY1QkNDAfD39+f//u//WLFiBffccw8rVqzg7rvvxmg0oigKZrOZTl268tZ7S/H30xPif/UrPSYmxuG2zJkzh1mzZpGWlkZMTAzr1q0DoFmzZgDExcWRlpZm9Zn09HSMRqMWTHmCBChCCCGEl3Xp0oXS0lLS09Pp1KkT4eHhWoBS1ujRoxk8eDAHDhzg+++/58UXX9Teu65TZ774/L/ExMRQPyqS0EA/p9thVsAAGAwGGjVqBMBHH31Er169aNCgAQC9evXiq6++svrcmjVr6N69O35+zt/TUVKDIoQQQnhZ69atGT16NGPHjuWrr77ixIkTbN++nTlz5rBq1SrtvH79+hEbG8vo0aNp1qwZ119/PWCZYvx/d99LVFQ0993zf/y4eTMnTpxg48aNPPHEE5w9e9ahdly8eJG33nqLQ4cOsWfPHp544gn+85//aHUoAI888ginTp1i8uTJ/PrrryxZsoTFixczdepUd3ZJBU4FKM2aNUOn01X479FHHwUsY1LJycnEx8cTFBRE//79OXDggNU1ioqKmDhxIvXr1yckJITbbrvN4Y4UQggh6or333+fMWPG8Oyzz9KuXTtuu+02fv75ZxISErRzdDod9957L7/88gujR4+++mEFgoOD+eq7tTRunMDoe0fSrl07xo0bR0FBAeHh4Q61wazAsmXL6N69O3369OHAgQNs2LCBP/zhD9o5iYmJrFq1ig0bNtC5c2defPFFXn/9dY+ugQKA4oT09HQlNTVV+y8lJUUBlO+//15RFEV55ZVXlLCwMOWzzz5T9u3bp9x9991Kw4YNlezsbO0ajzzyiNKoUSMlJSVF2bVrl3LTTTcpnTp1UkpKShxuR1ZWlgIoWVlZzjS/1ikuLla++OILpbi4uKab4vOkr5wj/eUc6S/HebqvCgoKlIMHDyoFBQUeub63lZaWKpmZmUppaalTnyspNSsZOYXaf1n5rvV3fpHJpc9VprLfI2e+v53KoMTExBAXF6f99/XXX9OiRQv69euHoigsWLCAGTNmMGLECJKSkli2bBn5+fmsWLECsCw8s3jxYubNm8fAgQPp0qULy5cvZ9++faxdu9b90ZcQQghRJ7lnd0CzD28y6HINSnFxMcuXL2fcuHHodDpOnDhBWlqa1WIuAQEB9OvXjy1btgCwc+dOTCaT1Tnx8fEkJSVp5wghhBCicuXjClfjDLOra+R7gcuzeL744guuXLnC2LFjAbQpSLYWc1FXo0tLS8Pf35969epVOKf8FKayioqKKCoq0l5nZ2cDYDKZPLqKXU1Tn60uP6O7SF85R/rLOdJfjvN0X5lMJm2Krdls9sg9vEn5PUBQn8nhz5VLfTj7eZXZ7NrnKr+mGUVRMJlMVmupgHM/Fy4HKIsXL2bo0KHEx8dbHbe1mEtVC7lUdc7s2bOZOXNmheNr1qwhODjYiVbXTikpKTXdhFpD+so50l/Okf5ynKf6ymg0EhcXR25ursPLudcGOTk5Tn/GUO51tovd4ern7CkuLqagoIBNmzZRUlJi9Z66Kq4jXApQTp06xdq1a/n888+1Y3FxcYAlS9KwYUPteHp6upZViYuLo7i4mMzMTKssSnp6Or1797Z7v+nTpzN58mTtdXZ2NgkJCQwePNjhSuXayGQykZKSwqBBgzw617wukL5yjvSXc6S/HOfpviosLOTMmTOEhoZW2EOmNlIUhZycHMLCwpxalbXErJBTePXL36CD8KCr/V2qKJSWKvgb7VdyKEBOgcnqc+5QWFhIUFAQN954Y4XfI3UExBEuBSjvv/8+DRo04JZbbtGOJSYmEhcXR0pKCl26dAEsUdTGjRuZM2cOAN26dcPPz4+UlBRGjhwJWNb6379/P3PnzrV7v4CAAAICAioc9/Pzuyb+srhWntMdpK+cI/3lHOkvx3mqr0pLS7UlLmwtbFbbqMMrzj6PrtywjIL150tKzBSXmgn0t3/NUrNS4XPuoP7+2PoZcOZnwukAxWw28/777/PAAw9gNF79uE6nY9KkScyaNYtWrVrRqlUrZs2aRXBwMKNGjQIgIiKC8ePHM2XKFKKjo4mKimLq1Kl07NiRgQMHOtsUIYQQ1xj1Cy4/P5+goKAabk3NqapI1qwomErNlJrNGOwEIAqKm+YCWVOH3srXnzjL6QBl7dq1nD59mnHjxlV4b9q0aRQUFDBhwgQyMzPp2bMna9asISwsTDtn/vz5GI1GRo4cSUFBAQMGDGDp0qXVfhAhhBB1n8FgIDIykvT0dMCyWJknN6zzNLPZTHFxMYWFhU5lMopLSikuKtVe63RQaLiaVSkoLqHYZCbbbCLI3/ZXvanUTFFRidXnqstsNnPx4kWCg4OtkhiucPrTgwcP1qqOy9PpdCQnJ5OcnGz384GBgSxcuJCFCxc6e2shhBBCq3lUg5TaTFEUCgoKCAoKcq4GpdRMUYl1YJEZcPUrvaiklJJSBZ0Ogu0EKKVmhUJTKZn+RtwZ4+n1epo0aVLtwFE2CxRCCFGr6HQ6GjZsSIMGDWr91G+TycSmTZu48cYbnarPOJuZz+HTV6yODW3dEIPeEhT8fOISF/Mty3Nc3yia+qEV6zhTrxRw6FQmg1vG4m903yiGv7+/W+paJEARQghRKxkMhlpfHmAwGCgpKSEwMNC5omJDCcXlvsL9/AO0WTt5JXrt/TNZJTSuH1HhEsrv1zD6BxBoJ8tSk2p/CbQQQghxjbFValF2Vdj84qv1KWcu52MqrVhnUvL7Ym+lPrrevQQoQgghRC1jK6ZQA42SUjPFZepTyq+ZolKDFglQhBBCCOEWtoKK0t8zKPmm0grvFZVUPFZS+ntAIwGKEEIIIdzB1iZ/5t8DjYLiisGIqaTi+SVmyaAIIYQQwo1sBShqoJFvI0CxlUEpLpEMihBCCCHcqNIhnuKK9Sbl10yBMhmUUglQhBBCCOEGtpIe6vY8toZ4im3N4tFqUNy3kqw7SYAihBBC1DI2h3gU+0M8xTYyKMUyi0cIIYQQ7mS2EVSYK6lBsRWgyCweIYQQQrhVaSVFsgWmijUoNgMUmcUjhBBCCHeyVTZSYlYwmxUKTfaHc8oylcpKskIIIYRwI1s1KIqiUGAqxcZbdhZqswQtMsQjhBBCCLewFaCUmBWb9SdQcYinpNSszQSSDIoQQggh3MLeQm22phhb3ruaMQHrrIlMMxZCCCGEW9goKcGsKOTbKJBVmcosyFZ2d2PJoAghhBDCLexlUOwN8YB1HUpJadkMim8GKMaaboAQQgghnGNzHRTF9gweVdk6FFOZYR1Z6l4IIYQQbmEr6VFqtr1Im6rsfjxlh3tsraniCyRAEUIIIWoZewu12dooUFV2LZQSqUERQgghhLspdgKUQpP9DEqxnQyKr9agSIAihBBC1DK2sh75xSU2Z/eorAOUshkUmWYshBBCCDewFaDkFtkf3oHyQzyKzV/7EglQhBBCiFrGVl1rZTN4AIpMdmbxyBCPEEIIIdzB1jooKkVReG/23/hwwQtWtSrFpbbXQTErtqct1zRZB0UIIYSoRRRFsTnNWJWRepbv//cRAF37DqBD9z6AdQ1KSblilVJFQY/O/Y2tBsmgCCGEELVIVcmOrMsZ2q//t/Rf2q/LroNSXD5AKXdRW7sfe5sEKEIIIUQtUlXNSNkA5cCOH/lt/26gfAbF+hrlpxpXVc/iDRKgCCGEELVIZfUnYB2gAPxv6SLAsvaJWpNSfgfj8svdSwZFCCGEEE6pKkDJzrQEKG069UCn07Fr81pOH/0VuDrMU1whg2IdsBTVxgzKuXPnuO+++4iOjiY4OJjOnTuzc+dO7X1FUUhOTiY+Pp6goCD69+/PgQMHrK5RVFTExIkTqV+/PiEhIdx2222cPXu2+k8jhBBC1HGO1qC06fwH/vDHWwD48oM3gKu1JxWKZCvUoNSyACUzM5M+ffrg5+fHt99+y8GDB5k3bx6RkZHaOXPnzuW1115j0aJFbN++nbi4OAYNGkROTo52zqRJk1i5ciUff/wxmzdvJjc3l+HDh1NaWvMpJSGEEMKXVVWDkp15CYDwetHc9sAEALau+5pL6alaHUpVNSi+MMTj1DTjOXPmkJCQwPvvv68da9asmfZrRVFYsGABM2bMYMSIEQAsW7aM2NhYVqxYwcMPP0xWVhaLFy/mww8/ZODAgQAsX76chIQE1q5dy5AhQ9zwWEIIIUTdZGsfnrKyLl8EICKqPs1ad6Bh0xaknjrGhTMnKS65DkVRKgQkvphBcSpA+fLLLxkyZAh33XUXGzdupFGjRkyYMIGHHnoIgBMnTpCWlsbgwYO1zwQEBNCvXz+2bNnCww8/zM6dOzGZTFbnxMfHk5SUxJYtW2wGKEVFRRQVFWmvs7OzATCZTJhMJueeuBZRn60uP6O7SF85R/rLOdJfjpO+co4r/VVUbAKz/QyHlkGJrAfmUsIiIkkFcjIzKCgsJj9AX+HzxcXW36cFhcUe+T105ppOBSjHjx/nzTffZPLkyTzzzDNs27aNxx9/nICAAO6//37S0tIAiI2NtfpcbGwsp06dAiAtLQ1/f3/q1atX4Rz18+XNnj2bmTNnVji+Zs0agoODnXmEWiklJaWmm1BrSF85R/rLOdJfjpO+co6z/RVSyXvZGZbv0jjlMiHpe4kMNABgOneA/T9vYL+Nz+9Ph/3ljq361akmOSQ/P9/hc50KUMxmM927d2fWrFkAdOnShQMHDvDmm29y//33a+fpdNar0SmKUuFYeZWdM336dCZPnqy9zs7OJiEhgcGDBxMeHu7MI9QqJpOJlJQUBg0ahJ+fX003x6dJXzlH+ss50l+Ok75yjiv9lZ5TxMbDF22+V1pSotV8+rfsTV69aIIaNAW2cckcTLPOfWkcFcR3+y9Yfa5zQiStYkO11+sOXWBAW+tkgzuoIyCOcCpAadiwIe3bt7c61q5dOz777DMA4uLiAEuWpGHDhto56enpWlYlLi6O4uJiMjMzrbIo6enp9O7d2+Z9AwICCAgIqHDcz8/vmvgDcK08pztIXzlH+ss50l+Ok75yjjP9pdOXgN5g873s7Izfz9ETGhkNegOhEVEA5GRdoQQdis5Q4fOKXq/dv6ikFHQGj/z+OXNNp2bx9OnTh8OHD1sdO3LkCE2bNgUgMTGRuLg4q1RVcXExGzdu1IKPbt264efnZ3VOamoq+/fvtxugCCGEEMKisnVQ1CnG4ZHR6A2WICQs0hKg5GZfoajEXGEGD1gXyfpCgSw4mUF58skn6d27N7NmzWLkyJFs27aNd955h3feeQewDO1MmjSJWbNm0apVK1q1asWsWbMIDg5m1KhRAERERDB+/HimTJlCdHQ0UVFRTJ06lY4dO2qzeoQQQghhm7mS+KHsFGNVWEQkADlXLlNcYsZUWvECZWf1+MIibeBkgNKjRw9WrlzJ9OnTeeGFF0hMTGTBggWMHj1aO2fatGkUFBQwYcIEMjMz6dmzJ2vWrCEsLEw7Z/78+RiNRkaOHElBQQEDBgxg6dKlGAy2U1ZCCCHEtaa4xIy/seJAhyMZlIio+tqx0MirQzz2AhTrDErNr4ECTgYoAMOHD2f48OF239fpdCQnJ5OcnGz3nMDAQBYuXMjChQudvb0QQghxTTibmU/zmNAKx0srCVCy1SGeshmUcEu9Z27WZYpLzRXWQAHrhdt8ZYhH9uIRQgghfEyhqZTUrEKb71W2UJvNDIo6xJOV6VAGpdDkGxkUCVCEEEIIH1NUYuZCtu0AxUZ8oVFrUCKiYrRjapFsfk42BYXFtotkFcmgCCGEEKIKRSWlFJrMXMkvrvBe5TUolvVRwqOuDvGEhEVov87OukJ+ccUMSWmZyltfKZKVAEUIIYTwMWqQkGYji1LZZoFXMyhXh3gMRiMh4ZYgJTcrk/zikgqfs65BkSEeIYQQQtigBglpNupQHFoHpV59q+OhvxfK5mRlkltUMUDxxXVQJEARQgghfEzh7xmU9JwizOUyJvYSKIqi2MygAIRFqjN5MimwMcRTIgGKEEIIIaqiBgklpQoZeUVW79nLoOTnZFNaYtktWC2MVYVGXA1QbAU4VhkUmcUjhBBCCFvKBgnp2dYBSrGdDIdaIBscGo5/QKDVe2G/Byg5VzJtflYNUMxmBZONWT41QQIUIYQQwseUHWYpW4dyOC2H4xfzbH4my87wDlzNoORkVR6g+MrwDkiAIoQQQrjF0aNHWbFiRaULqTmq7EyajNwiSkrN7D17hZ2nbAcYYHsVWVVYxNXVZG0p0QIU3xjeAReWuhdCCCGEtfT0dPr27Ut6ejpNmzalT58+1bpe2UyGWYHvD1/kYk5RJZ+ArMzfAxQbGZSwMvvx2FNSapYMihBCCFFXKIrCX/7yF9LT0wE4efJkta9Zfrn5qoITuJpBibCRQQkNjwQsRbL2lJgVn1nmHiRAEUIIIaplyZIl/O9//9NeZ2RkVOt6plJzpcvZ26PWoISXWeZeVVUNCljqUCSDIoQQQtQBx44d44knngAgOtqSubh06VK1rulqkFBZBkUd4qkqg+Iry9yDBChCCCGEy8aPH09eXh79+vXjr3/9K1D9DIqr65Co04xtz+KJBCA3+wpms+0gxGxWfKpIVgIUIYQQwgXZ2dls3LgRgPfff58GDRoA1Q9QCl3NoGhDPDaKZH8f4lHMZvJzsm1+vkSGeIQQQoja7/z58wCEh4eTmJhI/fqWwKDmMijqEE/FAMXo509gcChgf5inVDIoQgghRO2nBijx8fEA7gtQXMhiFBUWUJhvWcAtPLpigAIQ9vswT47dtVDM2h5AvkACFCGEEMIF9gKUmiiSVQtk/fwDCPo9U1JeVWuhSAZFCCGEqAPKByjqLJ6MjIxqrSbrylokZetPdDqdzXO0qcZX7K8mK7N4hBBCiFouNTUVqJhBKS4uJjc31+XrupJBuXLp9xk8NqYYq8LK7GhsS6Gp1OZOxzVFAhQhhBDCBeUzKMHBwQQFBQHVq0NxpUj2UrqlLVENGto9R82g5GZfsfl+frHvDO+ABChCCCGES9QApWHDq0GBOwplXZlmfCnN0pb6cY3snlPVEE9eUYnT9/UkCVCEEEIIF5TPoIB7CmVdyqBcqDpACdMClCs235cMihBCCFHLKYpiM0ApWyjrCrNZwVTqfCFIRto5y/1j4+2eY8mgzGbX5o9IP2eo8L5kUIQQQoha7sqVKxQWFgLuHeJxdSVXLUCpLIMSWQ8Ygbk0gsN7Ayq870pg5EkSoAghhBBOUmfw1KtXTyuMBXcEKM4Ps5SUmLiSccFy/0oyKMFh9YCmAGRfrphB8TUSoAghhBBOsjW8A9UPUFxZyTUzPQ1FUfDzDyCskmnG5pJYwJI5uXLJ97/+fb+FQgghhI+pKkBxtUjWlQxKxoWrU4z1evtf6/m5V5fAv5xuPxBKP2dg3owInn/e6aa4lQQoQgghhJNsTTGG6hfJulKDcun3+pPKZvAAZGaEaL8uOGF7R2OAjDQja78M5pNPnG6KW0mAIoQQQjjJc0M8LmRQtADFfv0JwMWzV7/ycy5XkmnJs7wXEeF0U9zKqQAlOTkZnU5n9V9cXJz2vqIoJCcnEx8fT1BQEP379+fAgQNW1ygqKmLixInUr1+fkJAQbrvtNs6ePeuepxFCCCG8wFMBiksZlN+HeKJjq8igHC3Ufn0lL9jueQW5tTBAAejQoQOpqanaf/v27dPemzt3Lq+99hqLFi1i+/btxMXFMWjQIHJycrRzJk2axMqVK/n444/ZvHkzubm5DB8+nNJS31ogRgghhG/65ptv6NWrF4cOHaqxNjgSoLiyYaArm/VdnWJceQbl0qmr7ckqjsBs51YFeZbNBmtdgGI0GomLi9P+i4mJASzZkwULFjBjxgxGjBhBUlISy5YtIz8/nxUrVgCQlZXF4sWLmTdvHgMHDqRLly4sX76cffv2sXbtWvc+mRBCiDonJyeHcePGsXXrVj799NMaa0f5jQJVag1KSUmJ1T/OHeVKkay2imwVGZS0jKtZk1KM5GXbDgHyfSSDYnT2A0ePHiU+Pp6AgAB69uzJrFmzaN68OSdOnCAtLY3Bgwdr5wYEBNCvXz+2bNnCww8/zM6dOzGZTFbnxMfHk5SUxJYtWxgyZIjNexYVFVFUVKS9zs62FPeYTCZMJpOzj1BrqM9Wl5/RXaSvnCP95RzpL8d5uq/mzJlDeno6ABcvXqyR35Oyq8jGxMRYtcFoNBISEkJeXh6pqalWa6TYUr6/CopMYHY8SFEU5eoQT4NYu58tKtRxsTASgAAKKSKQrAwIC694fnF6PhBJ2Om9mEztHG6LI5z5/XIqQOnZsycffPABrVu35sKFC7z00kv07t2bAwcOkJaWBkBsbKzVZ2JjYzl16hQAaWlp+Pv7U69evQrnqJ+3Zfbs2cycObPC8TVr1hAcbH8cra5ISUmp6SbUGtJXzpH+co70l+M80VeXL1/m1Vdf1V7v27ePVatWuf0+VcnOzqa4uBiAPXv2VKi1DA4OJi8vjy+//JLWrVs7dM2y/RVSyXnl5eTkUJifB0CC7hIB6bk2z7t0IhhoRgRXiOc8v9KeouOnCAmtWCujnG0CxBP80xpWrTrhRGuqlp+f7/C5TgUoQ4cO1X7dsWNHevXqRYsWLVi2bBnXX389ADqdzuoziqJUOFZeVedMnz6dyZMna6+zs7NJSEhg8ODBhIeHO/MItYrJZCIlJYVBgwbh5+dX083xadJXzpH+co70l+M82VcTJkygqKgIo9FISUkJQUFBDBs2zK33cIRaexkdHc3tt99e4f3GjRtz8eJF2rRpY/W9aUv5/vrql/NOLdZ2+spBAMLrRVOS0AN7u+lc3HwFgOacIIIr/Ep7Tl+pT4sGFetWcgstV6kfbXR7/6ojII5weoinrJCQEDp27MjRo0e54447AEuWpOy88PT0dC2rEhcXR3FxMZmZmVZZlPT0dHr37m33PgEBAQQEVNw3wM/P75r4y+JaeU53kL5yjvSXc6S/HOfuvjp48CBLliwBYPLkycydO5fLly/XyO/HxYsXAUuJgq37q7WZV65ccbh9Wn/pDaC3/w/2n1KC+WhRJI+/nEHLpGIupVtGH+rHNbJ81o6c/VkANAq/TGC2pf0Xf00HfdMK5xbkWQKkyHp6t/evM9er1jooRUVF/PrrrzRs2JDExETi4uKs0lTFxcVs3LhRCz66deuGn5+f1Tmpqans37+/0gBFCCHEte3555/HbDZz5513cuuttwKur9ZaXfZm8Kiqu5psZX5eF8ylC0Y2r7YMBF26UPUuxgCXjltqPxrEFRNquAxA1inbw0HaOijR1cphVJtTd586dSq33norTZo0IT09nZdeeons7GweeOABdDodkyZNYtasWbRq1YpWrVoxa9YsgoODGTVqFAARERGMHz+eKVOmEB0dTVRUFFOnTqVjx44MHDjQIw8ohBCi9tu0aRMATz31FJGRkYDvByiuroVSmcwMS5bk6O+7EWek/V4gW8UqsukXLOdHtfCj8Fw25EHuRdtDSfkFlixHRIy/W9rsKqcClLNnz3LvvfeSkZFBTEwM119/PVu3bqVpU0uKaNq0aRQUFDBhwgQyMzPp2bMna9asISwsTLvG/PnzMRqNjBw5koKCAgYMGMDSpUsxGHx/Z0UhhBDel5OTo83cad++vTar88qVK5SUlGA0evdf+vamGKuqu9x9ZXLPFgEBnP7NSGGB7uoqspVlUMxmzmZb2hTWsR5+vxRbApQc2zOMcosCAQiPrXwGkqc59bv68ccfV/q+TqcjOTmZ5ORku+cEBgaycOFCFi5c6MythRBCXKOOHz8OWL74IyIiKCm5Wgp65coVLWPhLTWVQVEUuJxlCR7MZj0n9+q0IZ7K9uEJPXOSE0oPy687NyBwjQHOQ36R7YVOckyWwCQi3pn5RO4ne/EIIYTwaceOHQOgRYsWgGWtEXUGZ00M89jbKFDlqQAlN1tPsXJ12CVzxX5tDZRmAYH0/vtEEtZXnHat2/kb+YSgw0x0Y4Xo5pa+y1NiyM+1nlVjNkOu2RKYRDQOq3Atb5IARQghhE8rH6DA1WGUmgxQPFkke/KIH/t+DrQ6duWi9Vf28e1mgi5eoD5w//xkmq35kuvefpXy8vZYZu3EBWXi5w/hzSMByCaWsyd+szq3MP/qDKKIppEut98dJEARQgjh03wpQDGbzVXWoLgjg/LqlBjmPBnD5fSr9Zm5py2b/RmxzMjZZu7B60CKTke9MycBCD99HEOB9WJol3/fJDAuxrKgW1CzUABMBHDy8Bmrc/NzLAGKP0UENvbu0Fl5EqAIIYTwab4UoGRkZGg1MHFxcTbPKVsk68qGgaZiyLxoRDHrOH/qaqlo/nHLcMz1xu34GUvJIIbraElnRaGgXn2Kw8LRKQqRxw5fvZjZzMWzlmtEN7V85ZfGNyAcy7ooJw+lW927OMMSzESQBb+v51JTJEARQgjh03wpQFGHdxo0aGB30TG1baWlpWRlZTl9j9ysq1mTi+evBii5Zyyzl+JDL5HY3pJF+YleZBmMrF/4bzI6dAGg3tGD2mfCTx3jtMmS6YlsY9kapiAmllguAJD122Wre5ekWbIs4bocqOGtZCRAEUII4bOKi4u1/dxsBSiXL1+2+TlPOXHCsjdNQkKC3XMCAwMJDbUMo7gyzJN95epXc3qZACU7zbKxX1RkMQktrwDwKr15/k9jyGrZll0N+pNLCPWOXA1Qog/s4TjNAWjQ2LLuidnPn/p6S7uKz13diBfAdNEyPBRmzHO63e4mAYoQQgiNoijs2LGDOXPmMHjwYBo3bsxHH31UY+05deoUZrOZoKAgq1kzUVFRgPczKL/9ZikqbdWqVaXnVadQNqdMgFI2g5KZYcnYRNYv5bf9bwBwyP8mujw2nXUrQ7n1y2TuYzn1jl7dvDDqwB4O0waABo2uTs+OCswBQMkJpLBMzYopwxKwhPkXOt1ud5MARQghhOall16iR48e/O1vfyMlJYVz587x73//u8baU3Z4p+ymsjU1xHP06FHA8QDFlQxKzpWrQzwZZ64+8+Vsy/ok2cUnOXXkXQBKTK3ZvTmSpa9a9rf7jiEEHT2OrtSSbcnbk0Eq8RgNpTRpZdKuFR1mCUACaEDqqWPa8cLLll2aQwOKnW63u0mAIoQQQvPtt98C0K9fPx566CEATp8+XWPtsVV/AjUXoDibQVFXwHVG7qWrhbUZZ68GKBcLLOuX7D7wLZBGSHgmiqLj9WfrYy61nFdIELuKOhJ25gSGwkL2nLTUn7RslUdA4NXrRkZaghU9sZw7cVQ7XnTFEtiEBtvbF9l7JEARQggBWKbQ7tu3D4A33niDJ598EkCrAakJvhagqBmUli1bVnqeWqNy8uRJp++Rn3p1eCUzN4jCAh2KWSGtxDKrJrX0FB269+G6npYhH8Wso02nQrreYBmq2Ug/6h09SL0j+9ms9AGg9R+sZxOFR1telxLL+TIZlALLyA/Bobb36fEmCVCEEEIAli/T3Nxc/P39ad26NU2aNAEgOzvbpdko7lBVgOLNItmCggLOnj0LVJ1BUdurtt8Z+ekmq9cZqUZKz2dTiGWI5zypDL7rAdp3s9SLxDY28eScDDr2tAQ2G+lH5JGDRB/Yw0b6AdC2i3UxbGisZRipkFgup6dpxwtzLZmY4HCnm+12NbuXshBCCJ+hZk/at2+P0WjEaDQSHR3NpUuXOH36NB07dvR6m3wpg6K2JSIiQru/PdUJUPIyrLMX6ecNRBZZnjOcK2RTSJOW7YiKzcNghOuuLyAs0ky734OQLfQm9PArZAcUcpwW6HVmWl1nHaCENLbsbpxLLH4XrwYoefmWwCUosuY38JUMihBCCAD27t0LwHXXXacdU7MoNVGHoiiKtlFg+QBFncVTUFBAQUGBV9pTtkC2bMGuLdUJUNQiWT2WepArRwrIP2ZZpC2K8wQEBVO/YWOMRug3PI969S0BTaNEE+GhReQTwolfAzi0z7KOScvGmQSHWA/xBDW1TIO+TCyXywYoBZZho6Coms9fSIAihBACuBqglM2U1GSAkpqaSkFBAQaDgaZNm1q9Fx4ejtFo+RL1VhZFLZCtqv4ErgYoFy9eJCcnx6n7ZOVYNgRMYj8A2b/mkHvGMnwTTCqNm7dBr6/49a3XQ5sultk3W3Ou4+crSQC07llxNduA3/fjKSCEnAtX25dbbNn/J7B+gFNt9gQJUIQQQgBXh3h8JYOiZh+aNGlSYdVWnU7n9bVQHJ1iDJYASp3J42wW5UqBJfPRg+0AXDptJjvVkiUxkkpCizZ2P9u2u2X2zUb6sYkbLe39Q8UARd8wkmAsi7GFFoRQmG/5dY7Jcu+AmJpdRRYkQBFCCAHk5+drX8C2ApSamMljr/5E5e06FEenGKtcGeZRFMgsDgOgfUPL0EvqxRAyL1mGfRTO06RlW7ufb9vlaqHsr7QHoE2nooon6nQ0MFjWaIkmlsyMCxgKC8lRLEM/AXESoAghhPABBw8exGw2U79+fWJjY7Xj6tBKTWZQqgpQvDWTx9EpxipXApT8XB0limXoKr6PJVA5m9+AS1mWgKGQVBJa2A9QmrQwERZQQAGW81vUTycswvaU4Wh/y8ys0N/rUAKyLpNFBAABDSRAEUII4QPKDu+ULQD1hSEeewGKN4d4nJlirHIlQFH34Qkhl8CbLYFIDmEcybcEilmkklBJBkVvgKRWV/uj7XX5ds+tF2J5L4hYMtPT8L98SQtQQsKc34XZ3SRAEUIIYXMGD1wNUM6dO0dJiXdXF1WHVHxhiMeZKcYqNdPiTICSl2Ypco3hIsWJCcQaLCvRHsBS8FoUnk9YRL1Kr9Gq19Wv9hb9Au2eVy/Ccq8AYsnMSIf0bEp/X31EAhQhhBA+wdYMHoDY2Fj8/Pwwm82cP3/eq23ypRoUZ6YYq1zJoBSessyoidZfpiQklEbhmVbvxyWGVXmNVr2vfrW37mY/qIxoYik8NtKEzItpmC7kApbpzYFBEqAIIYSoYYqi2M2g6PV6bdl2bw7zZGdna7UlzZs3t3mONwMUZwtk4WqAcvr0aYqLHdt8L/+cZU2XKH9LoNKgofWqss3aVJ29adammD/ekcOt92dpa6TYEpZkycTkkEj2hfOUXLTcO9RYwLf/+ZCtW7eiKDUXqEiAIoQQ17gLFy6QkZGBXq+nffv2Fd6viToUNVsTHh5OWJjtrEFNZFAcLZAFS/YpJCQEs9ns8J486hBPRLAlWIhscXU9kkCySWybWOU19HoY/7dM7plge3sCw+/f/GEdIgE4RSJxZ05QfMky2yfYmMfCF5/mzjvvdDhb5AkSoAghxDVOzZ60bNmS4OCKszfUmTzenGp87tw5ABo1amT3HG/O4nElg6LT6bTsj6PDPLmXLBmPiHBL5iS8w9VNcUKqWAPFUYF+linLDRpZVqo9QwLN0zMovGy5Z6DeMtTTqVOnat+rOiRAEUKIa5y94R1VTWZQ4uPj7Z7jzVk8rmRQwPk6lOxMy9dyWKQlUKmfcPU9P1KJb+bc/W0JMFoClIjoUvz1JkoxEp8bQWGW5Z56nWVZfQlQhBBC1ChbK8iWVRMBijMZFE8HKK5MMVY5HaBkWwpXQ+tbvp4bNLxa5JoXnI1/gP1ZOY4K8LNcW6+H2GjLCrJRNCPviiWjYi61ZKQkQBFCCFGj7M3gUflqBqXsEI/ZbL8YtLpcmWKscjZAycwPAiAozhKoRDUoRaezBA4h9Uudurc9gcarOxVHN7WEAaUkYrpsqX8pLL4I2A9YvUUCFCGEuMadOHECsJ8dqMkAxZEMitlsJivLdkGoOxw8eBBwboqxypkARVEUMossBcFB8ZZAxWAE/0DLkvQN4t2zw3Cg39Wv/vpNLM9zgkQKiyybFBaaL2P086dNm+rXu1SHBChCCHENy8nJ0b7c1enE5anHs7OzPRoIlKUO8VSWQQkICCAkJATwbKHs119/DUDfvn2d/qwaoBw/frzSLI/JZGLggAFcKo0EICDBUhyrKAqK2TIE17xdiNP3L0+nu1qDAhATbxlCOkGitopsEVk0bdm6wgaN3iYBihBC1ACTycTq1avJzc2t0XaotRXh4eGEh4fbPCckJETbmddbM3kcGeIBz9ehmEwmvvrqKwBGjBjh9OebNGmCwWCgsLCQ1NRUu+fNnz+f3d9vJotIAPwTLcHChbOnKC56CJ3+QYaNauj8A5Rj0OswGq5mgWwFKIVk0bx1xenm3iYBihBC1IDFixczdOhQ+vXrx5UrV2qsHWqA0rhx40rP8+YwT9lVaysb4gHPz+TZsGEDV65coUGDBvTu3dvpz/v5+dGsWTPA/jDP+fPnefHFF4nFEmzpKSUwypK92PfzJuAkbTsfICSs+hkUP4MOg/5qgNKgTICSze9ZG7JIbNOh2veqLglQhBCiBmzcuBGAXbt2MWzYsBrLpKgBir3hHZU3A5SMjAxKSkrQ6XTExcVVeq6nMyiff/45AHfccQcGg6GKs22rrA5FURTeeOMNioqKaBZmOS+SS+h//3bet+0HADr2vNGle5dn0Ovx01/96o9paCm8TaMhaah9nU1i63ZuuV91VCtAmT17NjqdjkmTJmnHFEUhOTmZ+Ph4goKC6N+/PwcOHLD6XFFRERMnTqR+/fqEhIRw2223aX9IhBDiWrBr1y4ADAYDP/30E7fddhsFBQVeb8eZM2cA38qgqNmTBg0aVFkH4c4A5eLFi/z3v//FZLIsWFZaWsrKlSsB14Z3VJUFKEuXLmX//v0EBQUxdsRfAAjlImlnTlJSYuLAji0AXOemAMVPr8NQZognJNyMf6Bl9s4+1FlcWTRvU4uHeLZv384777xTYRrS3Llzee2111i0aBHbt28nLi6OQYMGkZOTo50zadIkVq5cyccff8zmzZvJzc1l+PDhlJa6ZwqVEEL4suzsbI4cOQLAl19+SVhYGN9//z2PPvqo19vii0M8jhTIqtwZoMyYMYO77rqLCRMmALB161YuXLhAREQEN910k8vXVWdHHT582Op4dnY2Tz/9NADJycmQbxnCCSSDvT9v5Nj+PRTm5xIWGUXT1u4ZcjHodRjLDPHodBDdwLLEfc7vQzyhEUbCI6Pccr/qcClAyc3NZfTo0bz77rvUq3d122dFUViwYAEzZsxgxIgRJCUlsWzZMvLz81mxYgUAWVlZLF68mHnz5jFw4EC6dOnC8uXL2bdvH2vXrnXPUwkhhA/bs2cPYBlWGTZsmPb34zfffOP1tjg7xOPonjLV4Wj9CUBMTAxApQWojlKz/e+99x4rVqzQhnduvfVW/P39Xb6uur+ROl1ZtW3bNq5cuUJMTAwTJ04kI9WSuTFykX0//8Deny3DgEl/6Ite756KDD+D3qoGBSC2sfXsokbN6rvlXtXl0qTqRx99lFtuuYWBAwfy0ksvacdPnDhBWloagwcP1o4FBATQr18/tmzZwsMPP8zOnTsxmUxW58THx5OUlMSWLVsYMmRIhfsVFRVRVFSkvc7OtizDazKZtFRcXaQ+W11+RneRvnKO9Jdz3N1f27dvB6BLly6YTCat+DI9PZ2LFy8SGRnplvs4Qs2IxMXFVfp8aoBy/PjxSs9zR1852ia4uvT8wYMHq/37U3aG0sMPP0xoaChgCVCqc201g3LkyBHy8/O1Yavdu3cDliEgRVG4eMEyiqCQwcGdW7h8wRKodezRF8zuGWHQK6XozKVW14tLsN6xOKFFHIq51CN/PzhzTacDlI8//phdu3Zpf8DKSktLAyw7OJYVGxur/canpaXh7+9vlXlRz1E/X97s2bOZOXNmheNr1qyxubFVXZOSklLTTag1pK+cI/3lHHf1lzptNSQkhFWrVgFQr149MjMzWbZsmdPLqVeHmhE5fvy41hZb1CLeCxcu8PnnnxMYWPmS69Xpq59//lm7Z2Vtgqvrn+zZs4dvvvnG5d13TSaTlrlp3rw5x48fJzc3F39/fxRFqbIdlVEUhcDAQAoLC1myZImWrVq9ejUAzZo1IyUlhXMXLJkMnfEKhfl5nDxiyej0bB5NSPpel+9f1pV02HwEys4HahyWCL9PbwZoG6+j+MROVp1wyy2t5OfnO3yuUwHKmTNneOKJJ1izZk2lP5zlf0AURanyh6ayc6ZPn87kyZO119nZ2SQkJDB48GC78/brApPJREpKCoMGDarxBXN8nfSVc6S/nOPu/po+fToAd999N8OGDQMsy8xv2rSJmJgY7Zin5ebmkpdn2Ytl1KhRVf59OnHiRDIzM2nVqpXdZfHd0VfvvPMOAP3796+yL4qKipgyZQr5+fl06tSpyloae06cOKEFEuvWraNHjx5kZGQwdOjQahXIqjp27Mj27dutfn/Vf3g3a9aMQYMG8VGxJagLbRQMvydzGie2JrDdTeRVuwUWzWNCSIoP58tfrg6JRbQJsjqn8fU34p/YjoHtYst/vNrUERBHOBWg7Ny5k/T0dLp166YdKy0tZdOmTSxatEgrAEpLS6Nhw6sLyqSnp2tZlbi4OIqLi8nMzLTKoqSnp9udYx4QEEBAQECF435+ftfEX67XynO6g/SVc6S/nOOO/srLy9P+ruzZs6d2vTZt2rBp0yaOHz/utd+TCxcuABAWFubQHjMtWrRgx44dnDlzhq5du1Z6bnX6Ss1kJCQkVHkNPz8/Wrduza+//sqRI0dITEys1j2bNGlCs2bN+Pzzz3n++ed59tln3fL70aFDB7Zv387hw4fx8/OjpKREq0lp2rQpfjodFwstQ0oN2jfSApSOPW8AvWvTm23x9/cjMMDf6poNGpUd4skhvlkLdHqDR34OnbmmU1U3AwYMYN++fezZs0f7r3v37owePZo9e/bQvHlz4uLirFJ7xcXFbNy4UQs+unXrhp+fn9U5qamp7N+/36VFcIQQojb55ZdfMJvNNGzY0GqNj9atWwNos3u8wdECWVXz5s0Bxze+c5UzRbIASUlJAOzfv9/le6plCE2bNgXghhtuYP369XTv3t3la5bVoYNlFo4alBw9epSioiKCg4Mt/4A3GslI6m9pw/VXZ+y4a/0TlVGvw2iw/uqvX2bHZIMxH4PRPXv+VJdTrQgLC9N+EFQhISFER0drxydNmsSsWbNo1aoVrVq1YtasWQQHBzNq1CjAshvk+PHjmTJlCtHR0URFRTF16lQ6duzIwIED3fRYQgjhm9T1T8pnIGoyQHF0WKTsvjKeUlxcTHp6OuDYNGOwfPn/5z//cUuAohYDu1v5mTz79ln210lKStJm6Fy8ZMlqxDeL4Oa7x3Hh7Cnad+vl1naoM3iMeh0lZkvmJDBIISgkj4K8EIKCSyr7uFe5PUyaNm0aBQUFTJgwgczMTHr27MmaNWsICwvTzpk/fz5Go5GRI0dSUFDAgAEDWLp0qcur9AkhRG2hBihlh8rBOkBxpG7PHRxdpE3ljQyKOlnCz89P2/+nKuo/kMsvCuoMdeaQmkFxNzVAOXz4MCUlJezdayl6VduuKJBh2bSYsEgzY5583iPt8Ps9e2IoE6AAxDc1cOwgxCbUAzI8cm9nVTtA2bBhg9VrnU5HcnKyZdEZOwIDA1m4cCELFy6s7u2FEKJW2blzJ1Axg9K8eXP0ej25ubkV6vg8xdkhHm9kUMpuEuhokFY2QDGbzS6tGVJ+iMfdmjRpQnBwMPn5+Rw7dkwLUMoWG//4I3zxUwYR0Z5btFRdpM1o0FFUJlnSoFEpxw5CSJjnA2NHyV48QgjhJYWFhdq/8stnUPz9/bUCT28N8zg7xKNmUE6ePOmxlb+dWUVW1aJFC/z9/cnPz3d5t2VPD/Ho9XqrYZ7yAYpOBz16QLe+hdgqAYmPrHxat6OMvwdvxnJBnLqrcVCIucJnaooEKEII4SV79+6ltLSUmJgYmwWg3q5DUYd4HM2gNG7cGD8/P4qLi7VAwt2cLZAFMBqNtGtn2dzOlToURVE8PsQDV4d5fvrpJy0gKl/XaU/nhEjCAqtflaHuw1N+NdmufQsIr1dKlz6F1b6Hu0iAIoQQXlK2QNbW8IW3AxRnMygGg4FmzZoBnhvmcSWDAtWbyZOenk5RURE6nc7ldVQcoQYon376KWAJwqKiHNvzJjTASJcmkdVug1+ZItmyWnUs5o1V57hhmLtWXKk+CVCEEMJL7BXIqrwZoOTl5ZGZmQk4HqCA5wtlXcmgwNVpvK4UyqrZjPj4eI+uQaMGKOr9ym+2a4+/UY/RoKdxvWDiIiquCeYMQ5kalPLKxszeKNKuigQoQgjhJeoCbfbS+t4MUNTsSVhYGBEREQ5/ztOFsmWLZJ1RnQyKpwtkVWqAorK3Gm95wf5XZ7h2bVKP6sQO6hoo5WtQyqsf6vrmiO4iAYoQQniJ+qWufsmXpwYox44do6TEs+tRODu8o/J0BkUd4nE2g6IGKL/++qvTfeeN+hOwLGkfFHR1WXlHMyhBZQKUyGB/mkS5vgedOrRTvgalvAZh7inKrQ4JUIQQwgsKCwu1L1/1S768xo0bExgYiMlkcnk2iqOcXQNF5asZlKZNmxIcHExxcbHTwZOnZ/CoDAYDbdu21V47GqAE+1mvERbo5/qaYerQjq0hnrIahFdvKMkdJEARQtRp58+f56abbuK///1vjbbj1KlTKIpS6b43er1e28nY08M8zq6BovJkBiU3N1fbTM7ZAEWv12t1KM4O83hriAeuDvP4+fnRpk0bhz4TEmA9e6eq7Edlrk4ztn+NyGA/Aow1v3CqBChCiDptxYoVbNiwgUcffZTCwpqbQqlmHJo3b15pAaK36lCqO8Rz+fJlrly54tK9N2/eTHR0NIsXL7Y6vmPHDsCyJUrZ1ccd5eiKsiaTiUuXLmmvayJAadu2Lf7+jtV5lB3iAbAXW1RVm6LTlV3q3v7Xf6wPZE9AAhQhRB136NAhwDKVdPny5TXWDjVAqWq3XW8FKM6ugaIKDQ2lQYMGgOvDPJ988gmXL19mypQpXL58WTs+c+ZMAO6++26XrutoBuXBBx+kYcOGbNu2Dbhag+LpIR6AO++8k+joaMaMGePwZ4IrBCi2IxE/Q+Vf6WWzJpUN8fhC/QlIgCKEqOPUAAVg3rx5mM01s1Jm2QxKZdwZoCiKwnvvvWfzC9vVDApUvw5F3SgvKyuLOXPmALB+/Xo2bNiAv78/zz77rEvXLb9jsC2ZmZl89NFHmEwmFi5cSE5Ojjbd2hsZlHbt2pGRkcFTTz3l8GeC/R0b4vGroq6kbFBS2RCPL9SfgAQoQog6Tg1Q9Ho9hw4dYtWqVTXSDmcDFHVKcnWsW7eOhx56iN69e2vDJwAnTpzQakhcCVDUZ3AlQFEURQtQAF5//XXOnTvHc889B8Bf/vIXp7M6KrWm47fffrO7FP///vc/TCYTAP/973+1Jefr1avn0rCSNziaQQkwVpVBufq+vSCnno/Un4AEKEKIOiwjI0OrNXjkkUcASxalJjgaoKhfsmfOnCEvr3qreqq1GDk5OQwdOpRDhw6xd+9eevfuTV5eHm3atHG4ULOs6hTKpqWlcfnyZfR6PX/4wx8oLCzktttuY8uWLQQGBvLMM884fU1VkyZNCAgIoKioyO4sKHUVV7DMrFIzON4Y3nGFn0FXYejG3kiOU0M8dmpQfCV7AhKgCCHqMDV70rRpU/72t79hNBrZsGGDtqOwtyiK4nCAEh0dTf369YHqD/P89ttvgCV7lJGRwcCBA7nxxhtJS0ujY8eOrF+/3qWVU6szxKNmT1q1aqUFi+oKuxMmTKjWLs4Gg6HSWVCZmZmsXbsWsGRqAL766ivAO8M7rig/vAP2V3mtMkAp8769GhRfqT8BCVCEEHWYGqC0bduWhIQErfjS21mUjIwMcnNz0el0Dn0RqmtllK2fcYUaoMyaNYu2bdty7tw5srKyuOGGG9i0aZPTU3lV1cmgqPUwSUlJ9O3bl1tuuQWA4OBgnn76aZfaU5aaEbI1RKYO73Ts2JGXXnrJKjjz3QCl4nCLwR1FsnaGeCSDIoQQXlA2QAF44oknAPjiiy88vlJrWWqmoVGjRgQGVv0vVHcHKD179iQlJYUbbriBsWPH8t133xEZGenydVu2bAlYpucWFBQ49Vk1g6Iu8z5v3jw6derEq6++qs0Oqo7KanjU4Z277rqLmJgYbrvtNu09Xx3isRmg2Aku/KuoQSn7OVvX8KX6E4Dq790shBA+qnyA0q1bN8LDw8nOzmb//v107tzZK+1wdHhH1a5dO6B6AUpJSQknT54ELAFF48aN2bRpk8vXKysuLo6oqCguX77Mr7/+SteuXR3+bPkApU2bNuzZs8ct7VKvBxUDlMzMTFJSUgBLgAIwbtw4PvvsM8CXMyi2hnhsn2vQ6zDoodTORDWrWTw2si31w3wnewKSQRFC1GG//vorcDVA0ev19OjRA0BbA8MbnA1Q3JFBOX36NCUlJQQGBro8lGOPTqfTlmkvOyOnrIKCAubNm8ewYcO0YKG0tFSbAmxvw8TqshegqFmzjh07av07ZMgQmjVr5tH2VFf5RdrAfgZFrwNDJQuwlR0CsjXEE+hD2ROQAEUIUUcVFhZy4sQJAKv9T/7whz8AtSNAOXLkiN3pslVRh3eaN2+Ovoqda12hZkDKByjFxcV8++23tGvXjqlTp/Ltt9/y8ssvA5Z+KCgoICgoyO6GidWlDvGcO3eO3Nxc7fh//vMf4Gr2BCxFtevWrWP9+vVa1srX2BrisTfNWK/T2Z3hA1UP8QT4+VZI4FutEUIINzl69CiKohAREUFsbKx2vGfPngD8/PPPXmuLswFK06ZNCQgIoLCw0OVNA9UARa0XcTd7Acqjjz7K22+/zfnz57XZSF999RUmk0k7t3379hgMnvnXelRUlHbfo0ePApZp1ursnbIBClh+T2666SaPtMUdnAlQLEM89r/WqyqSrWodFW/zrdYIIYSblK0/KTstU82gHDhwgJycHK+0xdkAxWAwaJkAV4d51Bk2ng5Q1IXOwJI9UQtRZ8+ezenTp4mJieHKlSts3LjRagaPJ5Uf5lm3bh0mk4mWLVtaZdNqA1s1KPaHeHSVrhBbdu0To0FfoZbFlwpkQQIUIUQdpX6xl0/dN2zYkMaNG6Moirb+hicVFxdr+95UtQ9PWdWtQ1EzKJ4aSlGDjLS0NDIyMgDLZn8FBQWEh4fz5JNPEhQUpM2U+eKLLyoUyHpK+QDl22+/BWDo0KEeva+7GQ06mzNz7MUgBn3lOx2XX/uk/LmSQRFCCC8oP4OnLG8O85w6dQpFUQgKCrIaaqqKuwIUT2VQQkNDtYyQGnhs3LgRsAzhqHUvd955J2AJUNRsizcDFEVRtADl5ptv9uh93c3W8A6A3uUMiq7S14F+kkERQgiPqyxA8WahbNnhHXsrgNpSnQDFbDZr9/VUgAIVh3k2bNgAWA/hDBgwgNDQUM6dO6et7uqtIZ4jR45w8OBBzpw5Q0BAAP379/fofd3NXoBib6E2vU5nN3iBisvbSwZFCCG8zGw2+2SA4ozqBCjnz5+nsLAQo9Ho0QXIyk41NplM/Pjjj4B1ABIYGGg1tBIVFVWt5ewdUXaxNjV70r9/f4KDgz16X3cL8rO9VFllRbKVZlDKDfH4lVv6vrLgpiZIgCKEqHPOnTtHfn4+RqPRZmDQrVs3dDodZ86cITU11aNtcTVAUb9kL168qG146Ch1eKdZs2YYjZ5bj7PsTJ5du3aRl5dHVFRUhaBIHeZRP+NMJskVLVq0wGAwkJuby/vvvw/UvvoTgJAAe0M8ts/X6aqoQdHbr0HxteEdkABFCFEHqVmHli1b2twMLywsjA4dOgDuy6JkZmbanBWkFmo6G6CEhoaSkJBgdQ1Hebr+RKUGKPv372f9+vUA9O3bt8K6K8OGDdN+H7yxIJq/v79WkKwuDFcbAxRnh3gMep3d96Di6rFlAxZfG94BCVCEEHVQZcM7KncO8+Tk5NCyZUsaNWrESy+9RF5eHhcuXOCuu+7Sdst1pTDU0WEes9nM+vXryc7OBjw/xVjVsmVLAgICyM/PZ+nSpQDceOONFc6LiIhgyJAhwNUCZU9T61DAMntK3eW4NgmyMcUYKl+ozd4uxVAxu2KQAEUIIbxLXeK+7JdUee4MUHbu3Mnly5fJycnhueeeo0WLFrRv357//ve/GAwGnnvuOZcKNB0NUJ588kkGDBjAfffdB3h+irHKaDRqmSi1ANZWgALw3nvvsXz5ckaNGuXRNqnUITKwZE88PazkCSEuzOKpfKl7+zUovrYGCkiAIoSog9S0vvrlaYv6L/lt27ZhNtvZXc1Bu3fvBizDFy1atODChQtcvnyZzp07s337dl544QWXviDVAEUNuGxZtGgRr7/+OmBZsXX79u1eG+IB68xQZGSk3UxRbGwso0eP9tgKsuWVDU5r4/BOWKCR0AD79UO2lrSvqki20gyKjy1zDxKgCCHqIDVAad++vd1zOnToQGBgINnZ2doXuqvUAGXkyJEcPHiQ9957j3/9619s27aNLl26uHzdqjIoq1at4oknngDQClNfeOGFGgtQbrjhBq8FIFVRAxR/f3+fXsreFqNBx42tYmzuOKyyNcyj19kf/gHw09fhGpQ333yT6667jvDwcMLDw+nVq5c2hQtAURSSk5OJj48nKCiI/v37c+DAAatrFBUVMXHiROrXr09ISAi33XYbZ8+edc/TCCGueRcvXuTixYtA5TUofn5+dOrUCbCsgFod6oq0Xbp0wd/fn/HjxzNhwgSbBbrOUNt//PhxioqKrN47cOAAd999N2azmXHjxpGSkoJer+frr78mNzcXnU7n1Mq1rlKnGgM+tc5I3759eeCBB5g7dy4hISE13RyH6XTQq3k0EcGV/+zYDFD09mtQ9LqKQ0PGujTE07hxY1555RV27NjBjh07+OMf/8jtt9+uBSFz587ltddeY9GiRWzfvp24uDgGDRpkVdk+adIkVq5cyccff8zmzZvJzc1l+PDhLu/YKYQQZanDIc2aNavyi6l79+6ApYbEVQUFBVqGozrZElsaNmxIRESE1bouqtmzZ5Obm0v//v158803ad26Nffee6/2fkJCAgEBAW5tjy1lMyj9+vXz+P0cZTQaWbp0qZZhqi2S4iNIiKp6vRZb04kNOp3daca2jtepDMqtt97KsGHDaN26Na1bt+bll18mNDSUrVu3oigKCxYsYMaMGYwYMYKkpCSWLVtGfn4+K1asACArK4vFixczb948Bg4cSJcuXVi+fDn79u3TdpoUQtROeXl59O/fn8cff7zaNR3Vof6DqbL6E1W3bt2A6mVQ9u3bR2lpKTExMcTHx7t8HVt0Op3WxvLFvOqiaDNmzMDf3x+AZ599Vqt18cbwDlhqS/7yl79w77330rlzZ6/csy5rFRvm0Hm2CmX1OvvTjP1sDBf5eg2Kyyv4lJaW8p///Ie8vDx69erFiRMnSEtLY/Dgwdo5AQEB9OvXjy1btvDwww+zc+dOTCaT1Tnx8fEkJSWxZcsWbRpaeUVFRVbpTXUqnclkwmQyufoIPk99trr8jO4ifeUcT/TXmjVr2LhxIxs3biQkJIQXXnjBbdd2hrpjbps2bap8PnWIZ9euXRQVFVVYv0NVWX+pwU3nzp0pKSlxud32dOvWjfXr17N161bGjh0LWDboO3nyJDqdjs6dO2vtatGiBSNHjuSTTz5x6PndZdGiRYBlurP8WXSOq/2lU0rBbD3yYC41WY6ZK45I6NFVuIeuzLlGndkrv2fO3MPpAGXfvn306tWLwsJCQkNDWblyJe3bt2fLli0AFTbDio2N5dSpU4DlD5W/vz/16tWrcE5aWprde86ePZuZM2dWOL5mzZpat3SxK1JSUmq6CbWG9JVz3Nlfn3zyifbrV155hfz8fP74xz+67fqO+uGHHwDLX4SrVq2q9NzS0lL8/f3Jzc3lvffeo3HjxpWeb6u/vvzySwDCw8OrvJ8r1KLT9evXa9dXNzlMSEjQMimq4cOHYzAY6Natm0fa4yj5s+gcV/qr/ADm2jV7bR4HUIBVp+1f44d1Tt/eJfn5+Q6f63SA0qZNG/bs2cOVK1f47LPPeOCBB7QdLIEKU+kURalyel1V50yfPp3Jkydrr7Ozs0lISGDw4MGEh4c7+wi1hslkIiUlhUGDBlW72K6uk75yjif66+233wYshZ2HDh3izTff5LbbbrO7LoanPPLIIwDcc8899OjRo8rzu3btytatWwkNDWXYsGE2z6msv15++WUARowYYffz1dG5c2deeeUVTp8+zY033khoaKgWlAwYMMDmPe+++263t8NR8mfROa7217pDF7ica52N+FPXRlzKL2bDoYsVzq8f6s9NbRtYHTuXWcCWY5fQ6eD/ulUenLuLOgLiCKcDFH9/f21ss3v37mzfvp1//vOfPP3004AlS1J2I6j09HQtqxIXF0dxcTGZmZlWWZT09HR69+5t954BAQE2i738/PyuiT8A18pzuoP0lXPc1V+KomgzWZYsWcKCBQv49NNPueeeezh16pTXMp2XL1/WsrEdO3Z06Nl69OjB1q1b2bNnDw888ECl55bvr5KSEvbt26ddxxM/e02bNqVRo0acO3eOffv2ceONN7J9+3YA+vTp47M/7/Jn0TnO9pfB4Ad661qvgAB/AksAfcUZObauHxBQAnoDAX56r/1eOXOfalfFKIpCUVERiYmJxMXFWaWpiouL2bhxoxZ8dOvWDT8/P6tzUlNT2b9/f6UBihDCt507d44LFy5gMBjo3LkzS5cuJS4ujoyMjGpP4XWGuv5JkyZNCAtzrNhQLUJ1ZSbP4cOHKSwsJCwszKOrtpZd9bakpEQLUK6//nqP3VP4tvI1r+pre0WytqYfq0WyvjjFGJwMUJ555hl++OEHTp48yb59+5gxYwYbNmxg9OjR6HQ6Jk2axKxZs1i5ciX79+9n7NixBAcHa0sbR0REMH78eKZMmcK6devYvXs39913Hx07dmTgwIEeeUAhhOepQUhSUhJBQUEEBQXRp08fALZu3eq1djiyQFt56lTjXbt2Ob3cgZo16tSpk90CW3dQA5Sff/6ZAwcOkJeXR3h4OO3atfPYPYVvK18Woa6LYrCzDorRxs+neswXpxiDk0M8Fy5cYMyYMaSmphIREcF1113H6tWrGTRoEADTpk2joKCACRMmkJmZSc+ePVmzZo3Vv2Tmz5+P0Whk5MiRFBQUMGDAAJYuXeozqw8KIZynZh/UbARY/nX/2Wef+XyA0rZtW4KDg8nLy+PIkSOVfumbzWbWrl1Lp06diImJ0VaQdff6J+WVXZZf7c8//OEPHg2KhG8rnylRAxR7S91XmkHxwSnG4GSAsnjx4krf1+l0JCcnk5ycbPecwMBAFi5cyMKFC525tRDCh6kZFDUbAVeHH3766SeHiuXdwZE9eMozGAx06dKFH3/8kR07dtgNUHJzcxkxYgSrVq2iUaNGWhYYPB+gdOvWDZ1Ox+nTp/niiy8AGd651tnbV8feUve2FmpTNw+sE0M8QghRnqIoNgOUrl27YjQaSUtL48yZM15pi7pImzMZFKi6DmXv3r1MnTpVm7Z77tw5+vXrpz23pwOUssM5q1evBiRAudaVj0PU1/YyKOX34YGyNSi+GQr4ZquEELXGmTNnyMjIwGg0Wi17HhwcrC2E5o1hnitXrnD+/HkAp2sz1MDKVkHv1q1bueGGG0hLS6NZs2Z89913XHfddVy4cIHc3Fz8/f2dDohcoQ7z2Hstri3lh3i0DIpeh60YxfZS95YQINBPMihCiDpI/VLv2LEjgYGBVu+p/8r3RoCi7sHTqFEjIiIinPqsGqDs3r27QqHsu+++S0FBAR06dOCnn35i8ODBrF+/nq5duwKWzfLUpeY9SS2UBcsy9vXr1/f4PYXvKr/UfdmhHVvL4PvZqUHR6SSDIoTwkIsXL2orOdcEW8M7Km8GKK4UyKpat25NSEgI+fn5FTbl27NnD2DZiyw6OhqA6Oho1q1bx/Tp0/nnP/9ZvYY7qGyA0qtXL6/cU/iu8rUmZV/bGuaxt4mgUa/z2SJZ32yVEMIhx44do1OnTvTp06fCZnLeYmsGj0oNUNS9btzNZDJx6NAhvv32W1auXAk4VyCrMhgMWkakbB2KyWTS9vZJTEy0+kxkZCSzZs3y2hpOZTNUUn8i7BXJ2noPbG8WCJbZPVIkK4Rwq9OnTzNgwABSU1MB+P77773eBnsFsqoWLVoQHR1NUVERv/zyi1vuefbsWV5//XVuvfVWoqKiaNeuHcOGDeObb74BLEMurrC1s/GhQ4coLi4mPDycBg0a2PuoV/j5+fGnP/2JsLAwjyypL2qX8jFI2de2AhR7GRSDXu+zQzwu72YshKg5qampDBgwgFOnTmEwGCgtLdU2kPOmkydPcvnyZfz9/UlKSqrwvk6n4/rrr+ebb75h69atVsMUriguLqZLly5kZGRox0JDQ2nevDmJiYlcd911Lu9DY6tQVh3e6dSpk1emSVdl2bJlmEymCrU+4tpTYYhHX/kQj611UNRzfTVA8c1WCSHsMpvNDBs2jN9++43ExEQ+/PBDgBoZ4lG/zK+77jqb+2WBe+tQdu/eTUZGBuHh4cyZM4ddu3aRlZXFL7/8whdffMELL7zg8r4/agZlz549lJSUaL8G17My7mYwGCQ4EYCNIR5d2SEe+6vGlhfop8doZ/inpkkGRYhaZsOGDezZs4eIiAjWrVtHgwYN0Ov1nDt3jnPnztGoUSOvtUXdVbeyzIg7AxS1GLh///5Mmzat2tcrq3Xr1oSGhpKbm8uvv/5Kx44drTIoQvgSZ4tk7WVQgv19NwzwzbBJCGHXBx98AMA999xDYmIiISEh2vCKt7MoGzZsAKBfv352z+nRowc6nY4TJ05w4cKFat1PDVA8UZiq1+utCmUVRZEARfis8kmPsgkSW9OM/e1kSUIDJEARQrhBXl4en332GQD333+/drzsbrfecvnyZfbu3QtUHqBERERoM2t++OEHl++nKIoWoHhqmm3ZOpSzZ89y+fJljEajbMonfI6zGRR7dSbB/r45gwckQBGiVvniiy/Izc2lRYsWVl/SZTeT85YffvgBRVFo27YtsbGxlZ570003AdWbaXT69GnOnz+P0Wi0OWPIHcouea/OOmrXrp3UfQifUz5AqWyasZ9BZ7fIO0QyKEIId1ALYseMGWP1F46aQdm+fXuFlVA9ZePGjUDl2RPVH//4RwDWr1/v8v1++uknwLLvjauFsFVRA589e/ZoBcCdO3f2yL2EqI7yQUhl04z9K5mlIwGKEKLaUlNTSUlJAeC+++6zeq99+/YEBweTk5PD4cOHvdIeNUDp379/lef269cPnU7HoUOHtHVbnOXp4R2wLCEfHh5OYWEhK1asACRAEb6psqXuywcolU0jDvbRfXhAAhQhao0VK1ZgNpvp06cPLVq0sHqv7LCHN4Z5rly5wu7duwHHMij16tXTvujVwlpnebJAVlW2UPbo0aOABCjCN5UvMzFUsg5KZRkUWwW1vkICFCFqCXX2zpgxY2y+rw7zeGPBts2bN6MoCq1ataJhw4YOfaY6wzx5eXnajBpPLy1fvr5FZvAIX1R+N+PKMij+Bt/NklRGAhQhHPT9999z6623cvLkSa/fe9u2bezduxd/f39Gjhxp8xxvzuRxZnhH5UyhbHFxMePHj+fvf/87iqJotTWNGjUiISHBpTY7quyeQo0bN9Y2CBTCl5QverWexWP91e6rmwFWxXerY4TwIZcuXeLuu+/m4sWLtGvXjrlz53r1/s8++ywA9957L/Xq1bN5jhqg7N27l4KCAoKCgjzWHkfWPynvhhtuwGAwcOzYMU6fPk2TJk3snrtgwQKWLFkCQHx8PJcvXwY8nz0B6wyKDO8IX1X5ZoHW59pbA8XX1c5WC+FlU6ZM4eLFi8DV2STuUlhYyJYtWzh69CjFxcUV3l+3bh0pKSn4+fmRnJxs9zpNmjQhNjaWkpISrT7EE7Kzs9m1axfgXIASHh6ufflXlkU5f/48L774ovb6iSeeYPny5YB3ApQWLVoQEREBSIAifFfFIZ4y75XLoFRWg+LLamerhfCilJQUli1bpr3evn27zUDCVU8//TR9+vShdevWBAUF0apVK20GiaIoPPPMMwA88sgjNGvWzO511I35wPVC1LLmz5/PkCFDOHPmjNXxzZs3YzabadGiBY0bN3bqmo7UoUybNo3c3Fyuv/567rzzToqLi/n1118B7wQoOp1Oa6czAZgQ3qSrsJJsmQyKzvEiWV9WO1sthJfk5eXx8MMPA/DYY48RFRVFUVGRVrDpDuvWrQMsM3HMZjO//fYbo0eP5vHHH+fTTz9l27ZthISEMGPGjCqvdfPNNwPwzTffVLtds2fPZs2aNQwePFjbPfjcuXM8/fTTgHP1J6qydSiKolR4f/Pmzfz73/9Gp9OxaNEi3n//fVq2bAlAYGCg1zIa77zzDps2bWLgwIFeuZ8QziofhFhtFmgoXyRbO7/qa2erhfCS5ORkTpw4QUJCArNmzdL+Ba9Oea2ugoICDh06BMCJEydITU3V6k0WLlzIvffeC8DkyZOrXK0VYNiwYYBlYz41qHDFxYsXtSGtQ4cOMXToUHbu3EmfPn3Yv38/DRs21AIVZ/Tp0wc/Pz/OnDnDsWPHrN4rLS1l4sSJADz44IN069aNiIgI/vvf/xIXF8fo0aPx9/d3+ZmcUb9+fW644Qav3EsIVziz1H1l66D4strZaiG8YOfOnbz22msAvPnmm4SFhWmLhLkrQNm3bx+lpaXExMTQqFEj4uLiePHFF/nyyy+JiIhAURSioqKYMmWKQ9dr0qQJ1113HWazmdWrV7vcLnVIpX79+tSvX58dO3bQvXt3Tp06RatWrdiyZQutWrVy+rrBwcHaMJSaOVKtXLmSPXv2EBkZycsvv6wd79SpE+fOneO9995z+XmEqGv0FYZ4rv7amZVkfVntbLUQHmYymXjwwQcxm83cc8893HLLLcDVGgh3FcqqxaxdunSxmjZ46623smPHDsaNG8cnn3yiFW06Qm1rdYZ51AClR48erF69mrCwMMAyBXfz5s2V1sJUZdCgQQAVAqj//e9/ADz00EPExMRYvacv/7exENe4ytZBqZhBkXVQhKgz5s+fz549e4iKiuKf//yndrxHjx4YDAbOnj1boXjUFWUDlPJatmzJ4sWLna6DGD58OGAJAEpKSlxq18GDBwHLEvrdunXjhx9+4B//+Afff/89DRo0cOmaKnUYau3atVqxcWlpKatWrbJqvxDCvso2Cyy/OqxfuZqU2kICFCHK+e2333j++ecBeO2116y+kENCQrSVRd0xzKNO17UVoLiqZ8+eREdHc+XKFZfbWDZAAcswy9SpU7VMSnV06dKFBg0akJuby48//ghYamYuX75MZGSkV2bqCFHb6fU6ysYoZX9tLLcmilGKZIWoG/76179SWFjIwIEDuf/++yu8765hnpKSEvbt2weg7f/iDgaDgaFDhwLw9ddfu3SN8gGKO+n1em220bfffgtcHY4aOnQoRqOsHymEI6zWPrGz1H1trT8BCVCEsHLq1CnWrl2L0Wjk7bffrrCcNOC2mTyHDh2isLCQsLCwCpv/VZc6TOJKHUpWVhbnz58HoF27dm5tl0oNoNQARQ2k1PoZIUTV7O2/UzZYqa31JyABihBW1H1sOnXqRPPmzW2eo87k2b17NwUFBS7fS60/6dSpk9uLQIcMGYLBYODgwYOcOHHCqc+qBbKNGjVyqjjXGYMHD0av17N//35+/PFH9u3bZ5VZEUJUrWyAUvYfU2WHdGrrGiggAYoQVtQARd3XxpamTZvSsGFDSkpK2LFjh8v3qqxAtroiIyPp27cv4Pwwjzq846nsCUBUVBQ9e/YELEvZgyXwk435hHCc9f471tletQ5FhniEqCN+/vlnAO3L0xadTqcN82zevNnle3kyQIGrs2XKrzdSFU/Wn5SlDvPs3LkTkNk7QjirsuXt9ddagDJ79mx69OhBWFgYDRo04I477uDw4cNW5yiKQnJyMvHx8QQFBdG/f38OHDhgdU5RURETJ06kfv36hISEcNttt3H27NnqP40Q1VBSUqJ9WVaWQQEYMGAAAB988IHNJduroiiKtly+pwIUdR8Zde8cR3krQFEDKJXUnwjhHL2dWTxwDWZQNm7cyKOPPsrWrVtJSUmhpKSEwYMHk5eXp50zd+5cXnvtNRYtWsT27duJi4tj0KBB5OTkaOdMmjSJlStX8vHHH7N582Zyc3MZPnw4paWl7nsyIZx08OBB8vPzCQsLo02bNpWeO3r0aEJDQzl06BBr1651+l4nT57kypUr+Pn5eSwQ6Nq1K8HBwVy6dEmrKymruLiYjz/+mCFDhjBixAiKiooA7wUo6nRjsKyAm5SU5NH7CVHX2Ju5U/b1NVODsnr1asaOHUuHDh3o1KkT77//PqdPn9b+1akoCgsWLGDGjBmMGDGCpKQkli1bRn5+vrY7a1ZWFosXL2bevHkMHDiQLl26sHz5cvbt2+fSX/RCuIs6vNOjR48qi1bDw8MZO3YsYNkzx1nq+idJSUke21/Gz89PK+j94YcftOOKovDyyy8zbtw47r//ftasWcPKlSv55JNPyM3N5dSpU4DnAxS9Xq9lUYYPH25zxpQQwr6yQzz2Fm6rrfvwAFRrwYGsrCzAUvAGls3O0tLSGDx4sHZOQEAA/fr1Y8uWLTz88MPs3LkTk8lkdU58fDxJSUls2bKFIUOGVLhPUVGR9q87gOzsbMCyHLnJZKrOI/g09dnq8jOqPv74YxYtWqRl0YKCgvjHP/5Bt27dHPq8O/pq69atAHTv3t2h6zz88MMsWrSIr7/+msOHD9ud9WOLWlzbqVMnj/7+9u7dm3Xr1rFx40bGjx8PWFZwnTlzJgCNGzemTZs2rFu3jgULFtC6dWsAGjRoQHh4uMd/9mbOnElcXByPP/64T/+cX0t/FqtL+so51ekvnbkUzJa/M82lJeovAdBjec+I4lO/F860xeUARVEUJk+eTN++fbXUbFpaGkCFXVdjY2O1f5WlpaXh7+9PvXr1Kpyjfr682bNna3+hlrVmzRqCg4NdfYRaIyUlpaab4FHqLrZqwKt64IEHmDNnjlP/sq5OX61fvx6w/MteXXa9Kp07d2bPnj08/fTT/PnPf7Z73sWLF9m7dy9nz55FURQtQPHz83P4Xq7w8/MDLP3yzTffoNPpmD9/PgB//OMfefTRR8nLy+OHH35g9+7d2gq6MTExHm1XWddff702e8rX1fU/i+4kfeUcV/sr5Pf/r1q11+Z7e9Oh4js1Jz8/3+FzXQ5QHnvsMfbu3WtzFkP5LxRFUar8kqnsnOnTpzN58mTtdXZ2NgkJCQwePJjw8HAXWl87mEwmUlJSGDRokPZFUxd9++23ZGVlERMTw7vvvovJZOL+++/nyJEj+Pv7a5vLVcaRvlIUhV27dtGpU6cKq5Xm5uZy+vRpAB555BHi4+MdaruiKNx5551s3LiRpUuXEhISYvX+/Pnzeffdd/ntt99sfn7s2LH06NHDoXu5on///rzwwgtcunSJDh06UL9+fUaNGgXAzTffzM0334yfnx8bNmxgyZIl2l+Sffv2rVDEei27Vv4suoP0lXOq01+bj14kNasIo0HHnV0a2XxvYPsG1Av2zDCyK9QREEe4FKBMnDiRL7/8kk2bNtG4cWPteFxcHGDJkjRs2FA7np6ermVV4uLiKC4uJjMz0yqLkp6ebncPjoCAAAICAioc9/Pzuyb+ANT151Trk0aNGsXtt98OWFZpnT9/Pi+//DJDhw51OItSWV+9/PLLPPvss4wbN47Fixdbvbdv3z7MZjONGjWiadOmDrf91ltvpXnz5hw/fpxPP/2Uv/zlL9p7p0+f5umnnwYsy8/36NGDHj16aDUnrVq1olevXh6tvYiIiKB79+5s3bqVrVu3oigK+fn5tGzZklatWmn9NWnSJJYsWaLNSEpKSqrTP3Ouqut/Ft1J+so5rvSXwegH+hL0Bn2Fzxr9LO8FBwbg5+c720c484xOVc8oisJjjz3G559/zvr160lMTLR6PzExkbi4OKtUVXFxMRs3btSCj27duuHn52d1TmpqKvv375dNwq5BWVlZ/O9//wNgzJgx2vGnnnqKwMBAtmzZ4vQ6HracPHmSl156CYAlS5awYcMGq/fVIYbK1j+xxWAw8OijjwLwr3/9y2rK8SeffAJYFiC7dOkSP/30E6+//jqvvvoqr776Kg8//LBXCkNvuOEGADZt2sQHH3wAwH333Wd1744dO3LTTTdprz1dICuEqD61ENbWRJ1rbhbPo48+yvLly1mxYgVhYWGkpaWRlpamLfet0+mYNGkSs2bNYuXKlezfv5+xY8cSHByspZUjIiIYP348U6ZMYd26dezevZv77ruPjh07Or2tvKj9PvvsMwoLC2nfvr3VhnkNGzbUshEzZ850aa2RsiZPnkxhYaGWvXjkkUesCq/VGTxVrX9iy9ixYwkICGDv3r3ajDaAjz76CLDU0nhqyXhHqAHK119/rdXZqH8ey1JXdAUJUISoDdSZO+Vn8AAY9Xp0umtoHZQ333yTrKws+vfvT8OGDbX/1H8pAkybNo1JkyYxYcIEunfvzrlz51izZo3VNu3z58/njjvuYOTIkfTp04fg4GC++uorDIbau6mRcI36L/oxY8ZUyCY8/fTTBAQEsHnz5goZD2ekpKSwcuVKDAYD69ato0GDBhw+fJhXX31VO8eRJe7tiYqK4k9/+hMA7733HmDZCHD37t0YjUb+7//+z+W2u0Pfvn3R6XSkpaWhKAo33ngjzZo1q3De8OHDue+++3j44YcrFLoLIXyPOsvYVoBi0Nfu7Am4MMRj6z91PQiwZFGSk5NJTU2lsLCQjRs3VliAKTAwkIULF3Lp0iXy8/P56quvSEhIcMsDidrj1KlTbNy4EZ1Ox+jRoyu8Hx8fz0MPPQSgzTxxVnFxMY8//jhgKezu27evdq2XXnqJp59+ml69enH69Gl0Oh3du3d36T4PPvggYKmnycvL07IngwcPrvH9ZerVq2f1Z/D++++3eZ7BYODDDz/krbfekjVJhKgFrg7x2ApQ9LU6ewKyF4+oQcuXLwfgpptushug/vWvfwUsiwRevnzZ6XvMnz+fQ4cOERMTQ3JyMgD33nsvAwcOpLCwkLlz52rrn9x9991WmT5n9OvXjxYtWpCTk8Onn36qBSi2hlJqgjrMExgYWOMZHSGEe+i0IZ6K7xn1OglQhChv7969xMfH8/rrr9s959SpU7zzzjuA/X/Rg6UW4rrrrsNkMvHZZ5851Y6NGzcyY8YMAObMmUNkZCRg+UP9zjvvMGDAAEaPHs2SJUs4efKkFlS4Qq/XawuhPffccxw9epSgoCBtVlJNGzFiBGAZSqvJehghhPuomRPbQzwSoAhRweLFi0lNTeWZZ54hIyOjwvsbNmyge/funD59mkaNGmlfnvbce++9AE4FEGfOnOGuu+6itLSUUaNGWQ1DgmXG2dq1a1m+fDl//vOfnZpabM8DDzyAwWDg3LlzgGUKcmhoaLWv6w4DBgzg+PHjLFq0qKabIoRwE3UvHltDPEa9joBrqQZFCEd89913AOTl5TFv3jyr99544w0GDhxIRkYGXbt2ZcuWLVUOq9xzzz2AJbA5f/58lfcvLCzkT3/6ExcvXqRTp068++67XqmpiI+Pt9qR11eGd1SJiYke2/dHCOF9ukqKZPV6HQF+tfsrvna3Xjjt8OHDjB07lnvuuYd77rmH0aNHW20kV12nTp3i8OHD2utFixZx6dIlAJYtW8ajjz6qZTV++OEHmjRpUuU1mzVrRu/evVEUxWrGmC0nT57klltuYfv27URFRbFy5UqvboegFvVGRkZy8803e+2+QohrjzbEYyeD4lfLMyi+s7yc8Iqnn35aWxhNtXbtWg4dOlRhfyRXqAvw9erVi4KCAvbs2cP8+fMZOHCg9uX91FNPOb3Hzr333suWLVv46KOPePLJJyu8bzab+frrrxk9ejR5eXkEBgbyySefVFhM0NNuueUW/vWvf9G+fXubqx8LIYS7qJkTg50alABj7V66o3aHV8IpGRkZfPPNN4Bliu0///lPWrduTXp6ulZMWl1r1qwBYMiQIfz9738H4PXXX2fEiBGYTCbuuusuXnnlFaeHXO666y70ej3bt2+3ua/NmDFjeO+998jLy+OGG27gl19+qZGF/3Q6HRMmTKB///5ev7cQ4tqiJkhszeKRIllRq3z88ceUlJTQtWtXZsyYweOPP87bb78NwFtvvaWtpuqq0tJS1q5dC1jW/7j99tu57rrryMnJITMzk549e7Js2TL0eud/7GJjYxkwYABQsVj20KFD/Oc//0Gv1/P666+zYcMGWrduXa1nEUIIX6etJGtzHRQJUEQt8uGHHwLW03r79+/PAw88gKIoPPzww5SUlLh8/R07dpCZmUlERAQ9evRAr9czc+ZMAJo2bcr//vc/goKCXL6+WnT6wQcfYDabtePvv/8+YNnn6ZFHHnEpABJCiNqmqqXur6mVZEXtdfjwYbZt24bBYNCm7ar+8Y9/EBUVxS+//MLChQtdvoc6vDNgwACMRkt50x133MHGjRvZtm1btZdP/9Of/kRERAS//fabVkdjMplYtmyZdl8hhLhWVLVZoGRQRK2gZk9uvvlmGjRoYPVeTEwMc+bMASzBStnshDPUAGXw4MFWx2+88cYK93RFWFgYEyZMACwLrymKwrfffsuFCxdo0KCBy8vUCyFEbaQO7diq6bMUydbur/ja3XrhELPZbHN4p6wxY8YQGhpKamqq1Y68jsrOzuann34CKgYo7vTEE08QEBDAzz//zKZNm1iyZAkA9913n5a1EUKIa4FaemJrFo9Rr5MhHuH7fvjhB06fPk14eDi33nqrzXMCAgK0dTvKT0N2xPfff09paSmtWrXy6NTe2NhYbVXYv/3tb3z99ddA5cvlCyFEXVTZSrIBRr3N4tnaRAIUDzGZTCxevJjXXntN+++XX36pkbZ88MEHAIwcObLSIlV135gvv/zSqevv27eP6dOnA57NnqimTp2KXq9n69atlJaWcv3119O+fXuP31cIIXzJ1SGeiu/VhR3JJSfuIS+++CIvvvii1bHIyEgOHTpU7WJRZ5SUlPDFF18AMHr06ErPHTZsGAaDgX379nHixIkqMyGKovDGG28wZcoUioqKiI2NZeLEie5qul0tW7bkT3/6E//5z38AGDdunMfvKYQQvkZfSQalLpAMigccP36cuXPnApasxH333UeLFi24cuUKU6ZM8WpbNm/ezOXLl4mOjqZv376VnhsVFcUNN9wAVD3Mc/LkSYYMGcJjjz1GUVERw4YNY+/evbRp08Ztba/M008/DUBoaCh33323V+4phBC+xFDJSrJ1gQQoHvDkk09SVFTEwIEDWblyJR9++CEff/wxOp2Of//736xbt84t91EUhU8//bTSBdbUQOPWW291qIi0qmEes9nMwoULSUpKIiUlhcDAQBYsWMDXX3/tlpk6jurWrRtr1qxh/fr1hIeHe+2+QgjhK9Qln+rCcI4tEqC42erVq/nyyy8xGo28/vrr2g9O9+7defTRRwH461//SmFhYbXv9Z///Ie7776b3r17M3fuXBRFsXpfURRteEcNPKpy2223AbBp0yYuX75c4f1p06bx+OOPk5eXR9++ffnll1944oknauQPyKBBg+jRo4fX7yuEEL5AhniEw4qKinj88ccBePzxx2nXrp3V+y+99BINGzbk6NGj2rojriotLeX5558HLFmNp59+mtGjR5Ofn6+ds2/fPk6ePElgYCCDBg1y6LrNmzcnKSmJ0tJSvv32W6v3SkpKtFVbX3nlFTZu3ChLygshRA3RdjOum/GJBCju9Prrr3P06FFiY2O14KGsiIgIFixYAMDs2bO5cOGCy/f66KOPtB2IX331VYxGIx999BF//OMfteyMOrwzaNAgQkJCHL62mkUpX4eydetWLl++TL169ZgyZYosKS+EEDVITVzbWuq+LpBvGAcoikJycjJ/+ctf+PHHHysMpQBkZmYya9YswBJ82KuLuOuuu/jDH/5AUVERb7zxhkvtKSkp0fa4eeqpp5gyZQrr1q0jKiqKn3/+mSeeeAJAG9654447nLq+Ohz07bffUlBQoB1Xd0K++eabZVE0IYSoYYZKNgusCyRAccA333zDzJkzeffdd+nbty/XXXcdS5YssQpU/vGPf3DlyhU6dOhQ6aJhOp2OqVOnAvCvf/3LakjGUR988AG//fYbMTEx2rTeG2+8kY8++gidTsc777zDrFmz2LVrFzqdjuHDhzt1/e7du9O0aVNyc3P5/PPPtePqomjOXk8IIYT7aXvxSAbl2lRSUsK0adMA6NKlC0FBQezfv5/x48fz97//HYDU1FRt6GbWrFkYDIZKr3nnnXeSmJjIpUuXtEXUHFVcXMwLL7wAWKbahoaGau8NHjxYG1qaMWMGAL1793Z6do1er2f8+PEAvPfeewCcOnWK/fv3o9frtRVnhRBC1BydTodOd3U2T11TRx/Lfd5//31+/fVXoqOj+f777zl//rwWBLz00kvMnj2bl156iYKCAnr16mV3KfmyjEYjkyZNAuC1116jtLTU4fa8+eabnDp1iri4OP76179WeP+5555jyJAh2mtHZ++UN3bsWPR6PRs2bODo0aPa8E7v3r2Jiopy6ZpCCCHcy6DTSQ3KtSgvL0/Lkjz33HNEREQQGRlJcnKyNgvnmWee4a233gIstSeOTrcdN24ckZGRHD16lK+++sqhz5w5c4Znn30WgJkzZxIcHFzhHL1ez/Lly2natCkBAQH83//9n0PXLi8hIUHLlCxZskSGd4QQwgfpdDLEc02aN28eaWlpNG/evEK2Ytq0aVbTfG+++Wb69evn8LVDQ0N55JFHtPtURVEUHnvsMXJzc+nTpw8PPvig3XPr16/Pnj17OHToULU27lPvsWTJEr7//nsAbrnlFpevJ4QQwr0Mel2dLZKVqRh27N+/X1uufvbs2fj7+1c45/nnn0en07FixQqHgozyJk6cyLx589i8eTNbtmyhd+/e2nvff/89kyZNonHjxjRq1IjTp0/z5Zdf4ufnx9tvv13lFN/IyEgiIyOdblNZw4cPp0GDBqSnpwPQtGlTOnToUK1rCiGEcB+9TifroFxL1q5dS58+fcjLy6NXr17cddddNs/T6XQ8//zzHD582KXddOPj47UZP2rhK1gKYR966CH27t3LqlWr6NGjB/fccw9gKYz1VpDg5+fH2LFjtdfDhw+vs0sqCyFEbaTX62Ql2WvFkiVLGDp0KNnZ2dxwww189dVXHv1SfuaZZzAYDHz33XfanjpvvfUWx44dIzY2lj59+uDn54fJZKJVq1ba7BxvUWfzgAzvCCGEr5Ei2WvE559/zvjx4ykpKWHUqFGkpKQQHR3t0Xs2b96cMWPGAJYsypUrV7RsyvPPP89TTz3FyZMn+eCDD1i7di2BgYEebU95rVu3Zvr06dx9990MGDDAq/cWQghROb2u7q4kKzUoZQwfPpybbrqJPn368MILL3htOGPGjBl8+OGHrFq1ivvuu49Lly7Rtm1bxo4dy5o1a4iJidGCmJqgrpArhBDCt8gQTxmbNm3i1ltvJT4+Hp1Opy2nrlKXhY+PjycoKIj+/ftz4MABq3OKioqYOHEi9evXJyQkhNtuu42zZ89W60Hcwd/fn++++44XX3zRq7UWLVu2ZPTo0cDV5eTnzJkjy8kLIYSolEGKZK/Ky8ujU6dOLFq0yOb7c+fO5bXXXmPRokVs376duLg4Bg0aRE5OjnbOpEmTWLlyJR9//DGbN28mNzeX4cOHO7Vgmaf4+fnVyH1nzJihzcy58cYbHVrwTQghxLXNYNDV2ckLTgcoQ4cO5aWXXmLEiBEV3lMUhQULFjBjxgxGjBhBUlISy5YtIz8/nxUrVgCQlZXF4sWLmTdvHgMHDqRLly4sX76cffv2sXbt2uo/US3VunVrHn/8cSIiIpg/f36d/YETQgjhPn51dZ173FyDcuLECdLS0hg8eLB2LCAggH79+rFlyxYefvhhdu7ciclksjonPj6epKQktmzZYrVMu6qoqIiioiLtdXZ2NgAmkwmTyeTOR6hRc+fO1dZeKftsdekZPUX6yjnSX86R/nKc9JVzqttfekprVV8701a3BihpaWkAxMbGWh2PjY3l1KlT2jn+/v7Uq1evwjnq58ubPXs2M2fOrHB8zZo1Npd7r2tSUlJqugm1hvSVc6S/nCP95TjpK+dUp79WHXRjQzwsPz/f4XM9UoVZfnhCUZQqhywqO2f69OlMnjxZe52dnU1CQgKDBw8mPDy8+g32USaTiZSUFAYNGlRjtTG1hfSVc6S/nCP95TjpK+dUt78OnM+iQ3yEB1rmGeoIiCPcGqDExcUBlixJw4YNtePp6elaViUuLo7i4mIyMzOtsijp6elWS72XFRAQQEBAQIXjfn5+18QfgGvlOd1B+so50l/Okf5ynPSVc1ztL38//1rVz8601a3VNYmJicTFxVmlqoqLi9m4caMWfHTr1g0/Pz+rc1JTU9m/f7/dAEUIIYQQFfkZ6u6ECqczKLm5ufz222/a6xMnTrBnzx6ioqJo0qQJkyZNYtasWbRq1YpWrVoxa9YsgoODGTVqFAARERGMHz+eKVOmEB0dTVRUFFOnTqVjx44MHDjQfU8mhBBC1HF+BpnFo9mxYwc33XST9lqtDXnggQdYunQp06ZNo6CggAkTJpCZmUnPnj1Zs2YNYWFh2mfmz5+P0Whk5MiRFBQUMGDAAJYuXYrBYHDDIwkhhBDXBqNkUK7q378/iqLYfV+n05GcnExycrLdcwIDA1m4cCELFy509vZCCCGE+F1dzqDU3ScTQggh6jhjXV3nHglQhBBCiFrLKBkUIYQQQvgafwlQhBBCCOFr6nKRrAQoQgghRC0lRbJCCCGEEF4kAYoQQgghfI4EKEIIIYTwORKgCCGEEMLnSIAihBBCCJ8jAYoQQgghfI4EKEIIIYTwORKgCCGEEMLnSIAihBBCCJ8jAYoQQgghfI4EKEIIIYTwORKgCCGEEMLnSIAihBBCCJ8jAYoQQgghfI4EKEIIIYTwOcaaboArFEUBIDs7u4Zb4lkmk4n8/Hyys7Px8/Or6eb4NOkr50h/OUf6y3HSV8651vpL/d5Wv8crUysDlJycHAASEhJquCVCCCGEcFZOTg4RERGVnqNTHAljfIzZbOb8+fOEhYWh0+lqujkek52dTUJCAmfOnCE8PLymm+PTpK+cI/3lHOkvx0lfOeda6y9FUcjJySE+Ph69vvIqk1qZQdHr9TRu3Limm+E14eHh18QPrjtIXzlH+ss50l+Ok75yzrXUX1VlTlRSJCuEEEIInyMBihBCCCF8jgQoPiwgIIDnn3+egICAmm6Kz5O+co70l3OkvxwnfeUc6S/7amWRrBBCCCHqNsmgCCGEEMLnSIAihBBCCJ8jAYoQQgghfI4EKEIIIYTwORKgeNCmTZu49dZbiY+PR6fT8cUXX1i9f+HCBcaOHUt8fDzBwcHcfPPNHD161Oqc/v37o9PprP675557rM7JzMxkzJgxREREEBERwZgxY7hy5YqHn879vNFfJ0+eZPz48SQmJhIUFESLFi14/vnnKS4u9sYjupW3fr5URUVFdO7cGZ1Ox549ezz0VJ7hzb765ptv6NmzJ0FBQdSvX58RI0Z48tE8wlv9deTIEW6//Xbq169PeHg4ffr04fvvv/f047mdO/oL4KeffuKPf/wjISEhREZG0r9/fwoKCrT368rf9Y6SAMWD8vLy6NSpE4sWLarwnqIo3HHHHRw/fpz//e9/7N69m6ZNmzJw4EDy8vKszn3ooYdITU3V/nv77bet3h81ahR79uxh9erVrF69mj179jBmzBiPPpsneKO/Dh06hNls5u233+bAgQPMnz+ft956i2eeecbjz+du3vr5Uk2bNo34+HiPPIuneauvPvvsM8aMGcOf//xnfvnlF3788UdGjRrl0WfzBG/11y233EJJSQnr169n586ddO7cmeHDh5OWlubR53M3d/TXTz/9xM0338zgwYPZtm0b27dv57HHHrNaDr6u/F3vMEV4BaCsXLlSe3348GEFUPbv368dKykpUaKiopR3331XO9avXz/liSeesHvdgwcPKoCydetW7dhPP/2kAMqhQ4fc+gze5Kn+smXu3LlKYmJidZtcozzdX6tWrVLatm2rHDhwQAGU3bt3u7H13uWpvjKZTEqjRo2U9957zxPNrjGe6q+LFy8qgLJp0ybtWHZ2tgIoa9eudeszeJOr/dWzZ0/l2WeftXvduvp3fWUkg1JDioqKAAgMDNSOGQwG/P392bx5s9W5//73v6lfvz4dOnRg6tSp2m7OYIm6IyIi6Nmzp3bs+uuvJyIigi1btnj4KbzHXf1lS1ZWFlFRUe5vdA1yZ39duHCBhx56iA8//JDg4GDPN97L3NVXu3bt4ty5c+j1erp06ULDhg0ZOnQoBw4c8M6DeIm7+is6Opp27drxwQcfkJeXR0lJCW+//TaxsbF069bNOw/jBY70V3p6Oj///DMNGjSgd+/exMbG0q9fP6v+vFb+ri9LApQa0rZtW5o2bcr06dPJzMykuLiYV155hbS0NFJTU7XzRo8ezUcffcSGDRt47rnn+Oyzz6zGtNPS0mjQoEGF6zdo0KDWpUkr467+Ku/YsWMsXLiQRx55xBuP4TXu6i9FURg7diyPPPII3bt3r4lH8Th39dXx48cBSE5O5tlnn+Xrr7+mXr169OvXj8uXL3v9uTzFXf2l0+lISUlh9+7dhIWFERgYyPz581m9ejWRkZE18GSe4Uh/lf3Zeeihh1i9ejVdu3ZlwIABWq3KtfJ3vZWaTuFcKyiX9lMURdmxY4fSqVMnBVAMBoMyZMgQZejQocrQoUPtXmfHjh0KoOzcuVNRFEV5+eWXldatW1c4r2XLlsrs2bPd+gze5Kn+KuvcuXNKy5YtlfHjx7u7+V7nqf765z//qfTu3VspKSlRFEVRTpw4UeeGeBTFPX3173//WwGUt99+WzunsLBQqV+/vvLWW2955Fm8wVP9ZTabldtuu00ZOnSosnnzZmXnzp3KX//6V6VRo0bK+fPnPflIHuVKf/34448KoEyfPt3qcx07dlT+9re/KYpSd/+ur4xkUGpQt27d2LNnD1euXCE1NZXVq1dz6dIlEhMT7X6ma9eu+Pn5aVF1XFwcFy5cqHDexYsXiY2N9Vjba4I7+kt1/vx5brrpJnr16sU777zj6abXCHf01/r169m6dSsBAQEYjUZatmwJQPfu3XnggQe88hze4I6+atiwIQDt27fXzgkICKB58+acPn3asw/gZe762fr666/5+OOP6dOnD127duWNN94gKCiIZcuWeetRvKKq/rL1swPQrl077WfnWvq7XiUBig+IiIggJiaGo0ePsmPHDm6//Xa75x44cACTyaT9QPfq1YusrCy2bdumnfPzzz+TlZVF7969Pd72mlCd/gI4d+4c/fv3p2vXrrz//vtWVfJ1UXX66/XXX+eXX35hz5497Nmzh1WrVgHwySef8PLLL3ul/d5Unb7q1q0bAQEBHD58WDvHZDJx8uRJmjZt6vG214Tq9Fd+/v+3b8cuycRxHMd/j6CIcA4GDiYVtLQYtOXiICINTrc6NLTXkKNLU1v9Ce26uCg4OTg4OJyjkiJF0CDWEDxgQ59niCTpeSAyr9Pn/YKbzuPu++Xnjw/35X4bY8yH/5/P5zMvLy+Le+gf9K9+bW1tmVgsNrN2jHn9DPtt7fyPez0jngV6enqS4zhyHEfGGF1cXMhxHN3c3EiSSqWSGo2GBoOBKpWKNjc3Zdv29Pp+v6+zszO1220Nh0NVq1Xt7Oxob29v+spdkg4ODrS7u6tWq6VWq6VEIqFcLud6vfNyo19vY510Oq27uzvd399Pj2Xj1vp6b1lHPG716uTkROvr66rX6+p2uzo6OlI0GtXDw4PrNc/DjX6NRiOtra3Jtm11Oh31ej0VCgX5/X51Op0fqfur5u2XJF1eXiocDqtcLuv6+lrFYlHBYFD9fn/6m1XZ6z+LgLJAjUZDxpgPx+HhoaTX+X48Hpff79fGxoaKxaImk8n0+tvbW6VSKUUiEQUCAW1vb+v4+Fjj8XjmPuPxWPl8XpZlybIs5fN5PT4+uljp93CjX1dXV3+9xzJmdbfW13vLGlDc6tXz87NOT08VjUZlWZYymczM56XLwq1+tdttZbNZRSIRWZal/f191Wo1N0v9FvP26835+bni8bhCoZCSyaSazebM+VXZ6z/rlyQt5t0MAADA16z28B0AACwlAgoAAPAcAgoAAPAcAgoAAPAcAgoAAPAcAgoAAPAcAgoAAPAcAgoAAPAcAgoAAPAcAgoAAPAcAgoAAPAcAgoAAPCcPzzH1+IGXFZcAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| eval: false\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from neuralforecast import NeuralForecast\n",
    "#from neuralforecast.models import DeepAR\n",
    "from neuralforecast.losses.pytorch import DistributionLoss, HuberMQLoss\n",
    "from neuralforecast.utils import AirPassengers, AirPassengersPanel, AirPassengersStatic\n",
    "\n",
    "#AirPassengersPanel['y'] = AirPassengersPanel['y'] + 10\n",
    "Y_train_df = AirPassengersPanel[AirPassengersPanel.ds<AirPassengersPanel['ds'].values[-12]] # 132 train\n",
    "Y_test_df = AirPassengersPanel[AirPassengersPanel.ds>=AirPassengersPanel['ds'].values[-12]].reset_index(drop=True) # 12 test\n",
    "\n",
    "nf = NeuralForecast(\n",
    "    models=[DeepNPTS(h=12,\n",
    "                   input_size=12,\n",
    "                   trajectory_samples=100,\n",
    "                   loss=GMM(),\n",
    "                #    learning_rate=1e-5,\n",
    "                   n_layers = 2,\n",
    "                   dropout=0.0,\n",
    "                   stat_exog_list=['airline1'],\n",
    "                   futr_exog_list=['trend'],\n",
    "                   max_steps=1000,\n",
    "                   val_check_steps=10,\n",
    "                   early_stop_patience_steps=3,\n",
    "                   scaler_type='robust',\n",
    "                   enable_progress_bar=True),\n",
    "    ],\n",
    "    freq='M'\n",
    ")\n",
    "nf.fit(df=Y_train_df, static_df=AirPassengersStatic, val_size=12)\n",
    "Y_hat_df = nf.predict(futr_df=Y_test_df)\n",
    "\n",
    "# Plot quantile predictions\n",
    "Y_hat_df = Y_hat_df.reset_index(drop=False).drop(columns=['unique_id','ds'])\n",
    "plot_df = pd.concat([Y_test_df, Y_hat_df], axis=1)\n",
    "plot_df = pd.concat([Y_train_df, plot_df])\n",
    "\n",
    "plot_df = plot_df[plot_df.unique_id=='Airline1'].drop('unique_id', axis=1)\n",
    "plt.plot(plot_df['ds'], plot_df['y'], c='black', label='True')\n",
    "plt.plot(plot_df['ds'], plot_df['DeepNPTS'], c='red', label='mean')\n",
    "plt.plot(plot_df['ds'], plot_df['DeepNPTS-median'], c='blue', label='median')\n",
    "plt.fill_between(x=plot_df['ds'][-12:], \n",
    "                 y1=plot_df['DeepNPTS-lo-90'][-12:].values, \n",
    "                 y2=plot_df['DeepNPTS-hi-90'][-12:].values,\n",
    "                 alpha=0.4, label='level 90')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
