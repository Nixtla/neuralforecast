{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp models.deepnpts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepNPTS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Non-Parametric Time Series Forecaster (`DeepNPTS`) is a non-parametric baseline model for time-series forecasting. This model generates predictions by sampling from the empirical distribution according to a tunable strategy. This strategy is learned by exploiting the information across multiple related time series. This model provides a strong, simple baseline for time series forecasting. \n",
    "\n",
    "\n",
    "**References**<br>\n",
    "[Rangapuram, Syama Sundar, Jan Gasthaus, Lorenzo Stella, Valentin Flunkert, David Salinas, Yuyang Wang, and Tim Januschowski (2023). \"Deep Non-Parametric Time Series Forecaster\". arXiv.](https://arxiv.org/abs/2312.14657)<br>\n",
    "\n",
    "\n",
    ":::{.callout-warning collapse=\"false\"}\n",
    "#### Losses\n",
    "\n",
    "This implementation differs from the original work in that a weighted sum of the empirical distribution is returned as forecast. Therefore, it only supports point losses.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import neuralforecast.losses.pytorch as losses\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "from neuralforecast.common._base_windows import BaseWindows\n",
    "from neuralforecast.common._modules import RevIN\n",
    "from neuralforecast.losses.pytorch import MAE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "from fastcore.test import test_eq\n",
    "from nbdev.showdoc import show_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "logging.getLogger(\"pytorch_lightning\").setLevel(logging.ERROR)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class CustomConv1d(nn.Module):\n",
    "    \"\"\"\n",
    "    Forward- and backward looking Conv1D\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, padding=0, dilation=1, mode='backward', groups=1):\n",
    "        super().__init__()\n",
    "        k = np.sqrt(1 / (in_channels * kernel_size))\n",
    "        weight_data = -k + 2 * k * torch.rand((out_channels, in_channels // groups, kernel_size))\n",
    "        bias_data = -k + 2 * k * torch.rand((out_channels))\n",
    "        self.weight = nn.Parameter(weight_data, requires_grad=True)\n",
    "        self.bias = nn.Parameter(bias_data, requires_grad=True)  \n",
    "        self.dilation = dilation\n",
    "        self.groups = groups\n",
    "        if mode == 'backward':\n",
    "            self.padding_left = padding\n",
    "            self.padding_right= 0\n",
    "        elif mode == 'forward':\n",
    "            self.padding_left = 0\n",
    "            self.padding_right= padding            \n",
    "\n",
    "    def forward(self, x):\n",
    "        xp = F.pad(x, (self.padding_left, self.padding_right))\n",
    "        return F.conv1d(xp, self.weight, self.bias, dilation=self.dilation, groups=self.groups)\n",
    "\n",
    "class TCNCell(nn.Module):\n",
    "    \"\"\"\n",
    "    Temporal Convolutional Network Cell, consisting of CustomConv1D modules.\n",
    "    \"\"\"    \n",
    "    def __init__(self, in_channels, out_channels, kernel_size, padding, dilation, mode, groups, dropout):\n",
    "        super().__init__()\n",
    "        self.conv1 = CustomConv1d(in_channels, out_channels, kernel_size, padding, dilation, mode, groups)\n",
    "        self.conv2 = CustomConv1d(out_channels, in_channels + 1, 1)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h_prev, out_prev = x\n",
    "        h = self.drop(F.gelu(self.conv1(h_prev)))\n",
    "        h = self.conv2(h)\n",
    "        h_next = h[:, :-1]\n",
    "        out_next = h[:, -1]\n",
    "        return (h_prev + h_next, out_prev + out_next)\n",
    "    \n",
    "#| export\n",
    "class MLPResidual(nn.Module):\n",
    "    \"\"\"\n",
    "    MLPResidual\n",
    "    \"\"\"   \n",
    "    def __init__(self, input_dim, hidden_size, output_dim, dropout, layernorm):\n",
    "        super().__init__()\n",
    "        self.layernorm = layernorm\n",
    "        if layernorm:\n",
    "            self.norm = nn.LayerNorm(output_dim)\n",
    "\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.lin1 = nn.Linear(input_dim, hidden_size)\n",
    "        self.lin2 = nn.Linear(hidden_size, output_dim)\n",
    "        self.skip = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # MLP dense\n",
    "        x = F.gelu(self.lin1(input))                                            \n",
    "        x = self.drop(x)\n",
    "        x = self.lin2(x)\n",
    "\n",
    "        # Skip connection\n",
    "        x_skip = self.skip(input)\n",
    "\n",
    "        # Combine\n",
    "        x = x + x_skip\n",
    "\n",
    "        if self.layernorm:\n",
    "            return self.norm(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class DeepNPTS(BaseWindows):\n",
    "    \"\"\" DeepNPTS\n",
    "\n",
    "    Deep Non-Parametric Time Series Forecaster (`DeepNPTS`) is a baseline model for time-series forecasting. This model generates predictions by (weighted) sampling from the empirical distribution according to a learnable strategy. The strategy is learned by exploiting the information across multiple related time series.\n",
    "\n",
    "    **Parameters:**<br>\n",
    "    `h`: int, Forecast horizon. <br>\n",
    "    `input_size`: int, autorregresive inputs size, y=[1,2,3,4] input_size=2 -> y_[t-2:t]=[1,2].<br>\n",
    "    `hidden_size`: int=32, hidden size of dense layers.<br>\n",
    "    `batch_norm`: bool=True, if True, applies Batch Normalization after each dense layer in the network.<br>\n",
    "    `dropout`: float=0.1, dropout.<br>\n",
    "    `n_layers`: int=2, number of dense layers.<br>\n",
    "    `stat_exog_list`: str list, static exogenous columns.<br>\n",
    "    `hist_exog_list`: str list, historic exogenous columns.<br>\n",
    "    `futr_exog_list`: str list, future exogenous columns.<br>\n",
    "    `exclude_insample_y`: bool=False, the model skips the autoregressive features y[t-input_size:t] if True.<br>\n",
    "    `loss`: PyTorch module, instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
    "    `valid_loss`: PyTorch module=`loss`, instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
    "    `max_steps`: int=1000, maximum number of training steps.<br>\n",
    "    `learning_rate`: float=1e-3, Learning rate between (0, 1).<br>\n",
    "    `num_lr_decays`: int=-1, Number of learning rate decays, evenly distributed across max_steps.<br>\n",
    "    `early_stop_patience_steps`: int=-1, Number of validation iterations before early stopping.<br>\n",
    "    `val_check_steps`: int=100, Number of training steps between every validation loss check.<br>\n",
    "    `batch_size`: int=32, number of different series in each batch.<br>\n",
    "    `valid_batch_size`: int=None, number of different series in each validation and test batch, if None uses batch_size.<br>\n",
    "    `windows_batch_size`: int=1024, number of windows to sample in each training batch, default uses all.<br>\n",
    "    `inference_windows_batch_size`: int=-1, number of windows to sample in each inference batch, -1 uses all.<br>\n",
    "    `start_padding_enabled`: bool=False, if True, the model will pad the time series with zeros at the beginning, by input size.<br>\n",
    "    `step_size`: int=1, step size between each window of temporal data.<br>\n",
    "    `scaler_type`: str='identity', type of scaler for temporal inputs normalization see [temporal scalers](https://nixtla.github.io/neuralforecast/common.scalers.html).<br>\n",
    "    `random_seed`: int, random_seed for pytorch initializer and numpy generators.<br>\n",
    "    `drop_last_loader`: bool=False, if True `TimeSeriesDataLoader` drops last non-full batch.<br>\n",
    "    `alias`: str, optional,  Custom name of the model.<br>\n",
    "    `optimizer`: Subclass of 'torch.optim.Optimizer', optional, user specified optimizer instead of the default choice (Adam).<br>\n",
    "    `optimizer_kwargs`: dict, optional, list of parameters used by the user specified `optimizer`.<br>\n",
    "    `lr_scheduler`: Subclass of 'torch.optim.lr_scheduler.LRScheduler', optional, user specified lr_scheduler instead of the default choice (StepLR).<br>\n",
    "    `lr_scheduler_kwargs`: dict, optional, list of parameters used by the user specified `lr_scheduler`.<br>\n",
    "    `dataloader_kwargs`: dict, optional, list of parameters passed into the PyTorch Lightning dataloader by the `TimeSeriesDataLoader`. <br>\n",
    "    `**trainer_kwargs`: int,  keyword trainer arguments inherited from [PyTorch Lighning's trainer](https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.trainer.trainer.Trainer.html?highlight=trainer).<br>    \n",
    "\n",
    "    **References**<br>\n",
    "    - [Rangapuram, Syama Sundar, Jan Gasthaus, Lorenzo Stella, Valentin Flunkert, David Salinas, Yuyang Wang, and Tim Januschowski (2023). \"Deep Non-Parametric Time Series Forecaster\". arXiv.](https://arxiv.org/abs/2312.14657)<br>\n",
    "\n",
    "    \"\"\"\n",
    "    # Class attributes\n",
    "    SAMPLING_TYPE = 'windows'\n",
    "    EXOGENOUS_FUTR = True\n",
    "    EXOGENOUS_HIST = True\n",
    "    EXOGENOUS_STAT = True\n",
    "    \n",
    "    def __init__(self,\n",
    "                 h,\n",
    "                 input_size: int = -1,\n",
    "                 hidden_size: int = 32,\n",
    "                 batch_norm: bool = True,\n",
    "                 dropout: float = 0.1,\n",
    "                 n_layers: int = 2,\n",
    "                 futr_exog_list = None,\n",
    "                 hist_exog_list = None,\n",
    "                 stat_exog_list = None,\n",
    "                 exclude_insample_y = False,\n",
    "                 loss = MAE(),\n",
    "                 valid_loss = MAE(),\n",
    "                 max_steps: int = 1000,\n",
    "                 learning_rate: float = 1e-3,\n",
    "                 num_lr_decays: int = 3,\n",
    "                 early_stop_patience_steps: int =-1,\n",
    "                 val_check_steps: int = 100,\n",
    "                 batch_size: int = 32,\n",
    "                 valid_batch_size: Optional[int] = None,\n",
    "                 windows_batch_size: int = 1024,\n",
    "                 inference_windows_batch_size: int = 1024,\n",
    "                 start_padding_enabled = False,\n",
    "                 step_size: int = 1,\n",
    "                 scaler_type: str = 'standard',\n",
    "                 random_seed: int = 1,\n",
    "                 drop_last_loader = False,\n",
    "                 optimizer = None,\n",
    "                 optimizer_kwargs = None,\n",
    "                 lr_scheduler = None,\n",
    "                 lr_scheduler_kwargs = None,\n",
    "                 dataloader_kwargs = None,\n",
    "                 **trainer_kwargs):\n",
    "\n",
    "        if exclude_insample_y:\n",
    "            raise Exception('DeepNPTS has no possibility for excluding y.')\n",
    "\n",
    "        if not isinstance(loss, losses.BasePointLoss):\n",
    "            raise Exception('DeepNPTS only supports point loss functions (MAE, MSE, etc) as loss function.')               \n",
    "    \n",
    "        if not isinstance(valid_loss, losses.BasePointLoss):\n",
    "            raise Exception('DeepNPTS only supports point loss functions (MAE, MSE, etc) as valid loss function.')   \n",
    "            \n",
    "        # Inherit BaseWindows class\n",
    "        super(DeepNPTS, self).__init__(h=h,\n",
    "                                    input_size=input_size,\n",
    "                                    futr_exog_list=futr_exog_list,\n",
    "                                    hist_exog_list=hist_exog_list,\n",
    "                                    stat_exog_list=stat_exog_list,\n",
    "                                    exclude_insample_y = exclude_insample_y,\n",
    "                                    loss=loss,\n",
    "                                    valid_loss=valid_loss,\n",
    "                                    max_steps=max_steps,\n",
    "                                    learning_rate=learning_rate,\n",
    "                                    num_lr_decays=num_lr_decays,\n",
    "                                    early_stop_patience_steps=early_stop_patience_steps,\n",
    "                                    val_check_steps=val_check_steps,\n",
    "                                    batch_size=batch_size,\n",
    "                                    windows_batch_size=windows_batch_size,\n",
    "                                    valid_batch_size=valid_batch_size,\n",
    "                                    inference_windows_batch_size=inference_windows_batch_size,\n",
    "                                    start_padding_enabled=start_padding_enabled,\n",
    "                                    step_size=step_size,\n",
    "                                    scaler_type=scaler_type,\n",
    "                                    drop_last_loader=drop_last_loader,\n",
    "                                    random_seed=random_seed,\n",
    "                                    optimizer=optimizer,\n",
    "                                    optimizer_kwargs=optimizer_kwargs,\n",
    "                                    lr_scheduler=lr_scheduler,\n",
    "                                    lr_scheduler_kwargs=lr_scheduler_kwargs,\n",
    "                                    dataloader_kwargs=dataloader_kwargs,\n",
    "                                    **trainer_kwargs)\n",
    "\n",
    "        self.h = h\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.revin_layer = RevIN(1, affine=True)\n",
    "\n",
    "        # Create DeepNPTSNetwork\n",
    "        self.deepnptsnetwork =  MLPResidual(input_dim = input_size,\n",
    "                                            hidden_size = hidden_size,\n",
    "                                          output_dim=h * input_size * self.loss.outputsize_multiplier,\n",
    "                                          dropout=dropout,\n",
    "                                          layernorm=False)\n",
    "\n",
    "        self.global_mlp =  MLPResidual(input_dim = input_size,\n",
    "                                            hidden_size = hidden_size,\n",
    "                                          output_dim=h * self.loss.outputsize_multiplier,\n",
    "                                          dropout=dropout,\n",
    "                                          layernorm=False)\n",
    "\n",
    "        self.attn_network = MLPResidual(input_dim = input_size,\n",
    "                                            hidden_size = hidden_size,\n",
    "                                          output_dim=2,\n",
    "                                          dropout=dropout,\n",
    "                                          layernorm=False)\n",
    "\n",
    "    def forward(self, windows_batch):\n",
    "        # Parse windows_batch\n",
    "        insample_y = windows_batch['insample_y']\n",
    "        x             = insample_y                                     #   [B, L]\n",
    "        batch_size, seq_len = x.shape[:2]                              #   B = batch_size, L = seq_len\n",
    "        \n",
    "\n",
    "        x = self.revin_layer(x.unsqueeze(-1), mode=\"norm\")                                                  #   [B, L]  -> [B, L]\n",
    "        x = x.squeeze(-1)\n",
    "\n",
    "        # Run through DeepNPTSNetwork\n",
    "        weights = self.deepnptsnetwork(x)                              #   [B, L]  -> [B, L * h * n_outputs]\n",
    "\n",
    "        # Apply softmax for weighted input predictions\n",
    "        weights = weights.reshape(batch_size, seq_len, -1)             #   [B, L * h * n_outputs] -> [B, L, h]\n",
    "        x = F.softmax(weights, dim=1) * insample_y.unsqueeze(-1)       #   [B, L, h * n_outputs] * [B, L, 1] = [B, L, h * n_outputs]\n",
    "        base_fcst = torch.sum(x, dim=1)                                #   [B, L, h * n_outputs] -> [B, h * n_outputs]\n",
    "        base_fcst = base_fcst.unsqueeze(1)                             #   [B, h * n_outputs] -> [B, 1, h * n_outputs]\n",
    "\n",
    "        mlp_fcst = self.global_mlp(insample_y)                         #   [B, L]  -> [B, h * n_outputs]\n",
    "        mlp_fcst = mlp_fcst.unsqueeze(1)                               #   [B, h * n_outputs] -> [B, 1, h * n_outputs]\n",
    "\n",
    "        # fcst = base_fcst + mlp_fcst                                   #   [B, h * n_outputs] + [B, h * n_outputs] -> [B, h * n_outputs]\n",
    "\n",
    "        # Attention mechanism\n",
    "        weights = self.attn_network(insample_y)                        #   [B, L]  -> [B, 2]\n",
    "        weights = F.softmax(weights, dim=1)                            #   [B, 2]  -> [B, 2]\n",
    "        fcsts = torch.cat([base_fcst, mlp_fcst], dim=1)                #   [B, 1, h * n_outputs] + [B, 1, h * n_outputs] -> [B, 2, h * n_outputs]\n",
    "\n",
    "        fcst = torch.sum(fcsts * weights.unsqueeze(-1), dim=1)         #   [B, 2, h * n_outputs] * [B, 2, 1] -> [B, h * n_outputs]\n",
    "\n",
    "        fcst = self.revin_layer(fcst.unsqueeze(-1), mode=\"denorm\")        #   [B, h * n_outputs, 1] -> [B, h * n_outputs, 1]\n",
    "\n",
    "\n",
    "        fcst = fcst.reshape(batch_size, self.h, -1)                    #   [B, h * n_outputs] -> [B, h, n_outputs]\n",
    "\n",
    "\n",
    "        forecast = self.loss.domain_map(fcst)                         #   [B, h, 1] -> [B, h, 1]\n",
    "\n",
    "        return forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/models/deepnpts.py#L90){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DeepNPTS\n",
       "\n",
       ">      DeepNPTS (h, input_size:int=-1, hidden_size:int=32, batch_norm:bool=True,\n",
       ">                dropout:float=0.1, n_layers:int=2, futr_exog_list=None,\n",
       ">                hist_exog_list=None, stat_exog_list=None,\n",
       ">                exclude_insample_y=False, loss=MAE(), valid_loss=MAE(),\n",
       ">                max_steps:int=1000, learning_rate:float=0.001,\n",
       ">                num_lr_decays:int=3, early_stop_patience_steps:int=-1,\n",
       ">                val_check_steps:int=100, batch_size:int=32,\n",
       ">                valid_batch_size:Optional[int]=None,\n",
       ">                windows_batch_size:int=1024,\n",
       ">                inference_windows_batch_size:int=1024,\n",
       ">                start_padding_enabled=False, step_size:int=1,\n",
       ">                scaler_type:str='standard', random_seed:int=1,\n",
       ">                drop_last_loader=False, optimizer=None, optimizer_kwargs=None,\n",
       ">                lr_scheduler=None, lr_scheduler_kwargs=None,\n",
       ">                dataloader_kwargs=None, **trainer_kwargs)\n",
       "\n",
       "*DeepNPTS\n",
       "\n",
       "Deep Non-Parametric Time Series Forecaster (`DeepNPTS`) is a baseline model for time-series forecasting. This model generates predictions by (weighted) sampling from the empirical distribution according to a learnable strategy. The strategy is learned by exploiting the information across multiple related time series.\n",
       "\n",
       "**Parameters:**<br>\n",
       "`h`: int, Forecast horizon. <br>\n",
       "`input_size`: int, autorregresive inputs size, y=[1,2,3,4] input_size=2 -> y_[t-2:t]=[1,2].<br>\n",
       "`hidden_size`: int=32, hidden size of dense layers.<br>\n",
       "`batch_norm`: bool=True, if True, applies Batch Normalization after each dense layer in the network.<br>\n",
       "`dropout`: float=0.1, dropout.<br>\n",
       "`n_layers`: int=2, number of dense layers.<br>\n",
       "`stat_exog_list`: str list, static exogenous columns.<br>\n",
       "`hist_exog_list`: str list, historic exogenous columns.<br>\n",
       "`futr_exog_list`: str list, future exogenous columns.<br>\n",
       "`exclude_insample_y`: bool=False, the model skips the autoregressive features y[t-input_size:t] if True.<br>\n",
       "`loss`: PyTorch module, instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
       "`valid_loss`: PyTorch module=`loss`, instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
       "`max_steps`: int=1000, maximum number of training steps.<br>\n",
       "`learning_rate`: float=1e-3, Learning rate between (0, 1).<br>\n",
       "`num_lr_decays`: int=-1, Number of learning rate decays, evenly distributed across max_steps.<br>\n",
       "`early_stop_patience_steps`: int=-1, Number of validation iterations before early stopping.<br>\n",
       "`val_check_steps`: int=100, Number of training steps between every validation loss check.<br>\n",
       "`batch_size`: int=32, number of different series in each batch.<br>\n",
       "`valid_batch_size`: int=None, number of different series in each validation and test batch, if None uses batch_size.<br>\n",
       "`windows_batch_size`: int=1024, number of windows to sample in each training batch, default uses all.<br>\n",
       "`inference_windows_batch_size`: int=-1, number of windows to sample in each inference batch, -1 uses all.<br>\n",
       "`start_padding_enabled`: bool=False, if True, the model will pad the time series with zeros at the beginning, by input size.<br>\n",
       "`step_size`: int=1, step size between each window of temporal data.<br>\n",
       "`scaler_type`: str='identity', type of scaler for temporal inputs normalization see [temporal scalers](https://nixtla.github.io/neuralforecast/common.scalers.html).<br>\n",
       "`random_seed`: int, random_seed for pytorch initializer and numpy generators.<br>\n",
       "`drop_last_loader`: bool=False, if True `TimeSeriesDataLoader` drops last non-full batch.<br>\n",
       "`alias`: str, optional,  Custom name of the model.<br>\n",
       "`optimizer`: Subclass of 'torch.optim.Optimizer', optional, user specified optimizer instead of the default choice (Adam).<br>\n",
       "`optimizer_kwargs`: dict, optional, list of parameters used by the user specified `optimizer`.<br>\n",
       "`lr_scheduler`: Subclass of 'torch.optim.lr_scheduler.LRScheduler', optional, user specified lr_scheduler instead of the default choice (StepLR).<br>\n",
       "`lr_scheduler_kwargs`: dict, optional, list of parameters used by the user specified `lr_scheduler`.<br>\n",
       "`dataloader_kwargs`: dict, optional, list of parameters passed into the PyTorch Lightning dataloader by the `TimeSeriesDataLoader`. <br>\n",
       "`**trainer_kwargs`: int,  keyword trainer arguments inherited from [PyTorch Lighning's trainer](https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.trainer.trainer.Trainer.html?highlight=trainer).<br>    \n",
       "\n",
       "**References**<br>\n",
       "- [Rangapuram, Syama Sundar, Jan Gasthaus, Lorenzo Stella, Valentin Flunkert, David Salinas, Yuyang Wang, and Tim Januschowski (2023). \"Deep Non-Parametric Time Series Forecaster\". arXiv.](https://arxiv.org/abs/2312.14657)<br>*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/models/deepnpts.py#L90){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DeepNPTS\n",
       "\n",
       ">      DeepNPTS (h, input_size:int=-1, hidden_size:int=32, batch_norm:bool=True,\n",
       ">                dropout:float=0.1, n_layers:int=2, futr_exog_list=None,\n",
       ">                hist_exog_list=None, stat_exog_list=None,\n",
       ">                exclude_insample_y=False, loss=MAE(), valid_loss=MAE(),\n",
       ">                max_steps:int=1000, learning_rate:float=0.001,\n",
       ">                num_lr_decays:int=3, early_stop_patience_steps:int=-1,\n",
       ">                val_check_steps:int=100, batch_size:int=32,\n",
       ">                valid_batch_size:Optional[int]=None,\n",
       ">                windows_batch_size:int=1024,\n",
       ">                inference_windows_batch_size:int=1024,\n",
       ">                start_padding_enabled=False, step_size:int=1,\n",
       ">                scaler_type:str='standard', random_seed:int=1,\n",
       ">                drop_last_loader=False, optimizer=None, optimizer_kwargs=None,\n",
       ">                lr_scheduler=None, lr_scheduler_kwargs=None,\n",
       ">                dataloader_kwargs=None, **trainer_kwargs)\n",
       "\n",
       "*DeepNPTS\n",
       "\n",
       "Deep Non-Parametric Time Series Forecaster (`DeepNPTS`) is a baseline model for time-series forecasting. This model generates predictions by (weighted) sampling from the empirical distribution according to a learnable strategy. The strategy is learned by exploiting the information across multiple related time series.\n",
       "\n",
       "**Parameters:**<br>\n",
       "`h`: int, Forecast horizon. <br>\n",
       "`input_size`: int, autorregresive inputs size, y=[1,2,3,4] input_size=2 -> y_[t-2:t]=[1,2].<br>\n",
       "`hidden_size`: int=32, hidden size of dense layers.<br>\n",
       "`batch_norm`: bool=True, if True, applies Batch Normalization after each dense layer in the network.<br>\n",
       "`dropout`: float=0.1, dropout.<br>\n",
       "`n_layers`: int=2, number of dense layers.<br>\n",
       "`stat_exog_list`: str list, static exogenous columns.<br>\n",
       "`hist_exog_list`: str list, historic exogenous columns.<br>\n",
       "`futr_exog_list`: str list, future exogenous columns.<br>\n",
       "`exclude_insample_y`: bool=False, the model skips the autoregressive features y[t-input_size:t] if True.<br>\n",
       "`loss`: PyTorch module, instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
       "`valid_loss`: PyTorch module=`loss`, instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
       "`max_steps`: int=1000, maximum number of training steps.<br>\n",
       "`learning_rate`: float=1e-3, Learning rate between (0, 1).<br>\n",
       "`num_lr_decays`: int=-1, Number of learning rate decays, evenly distributed across max_steps.<br>\n",
       "`early_stop_patience_steps`: int=-1, Number of validation iterations before early stopping.<br>\n",
       "`val_check_steps`: int=100, Number of training steps between every validation loss check.<br>\n",
       "`batch_size`: int=32, number of different series in each batch.<br>\n",
       "`valid_batch_size`: int=None, number of different series in each validation and test batch, if None uses batch_size.<br>\n",
       "`windows_batch_size`: int=1024, number of windows to sample in each training batch, default uses all.<br>\n",
       "`inference_windows_batch_size`: int=-1, number of windows to sample in each inference batch, -1 uses all.<br>\n",
       "`start_padding_enabled`: bool=False, if True, the model will pad the time series with zeros at the beginning, by input size.<br>\n",
       "`step_size`: int=1, step size between each window of temporal data.<br>\n",
       "`scaler_type`: str='identity', type of scaler for temporal inputs normalization see [temporal scalers](https://nixtla.github.io/neuralforecast/common.scalers.html).<br>\n",
       "`random_seed`: int, random_seed for pytorch initializer and numpy generators.<br>\n",
       "`drop_last_loader`: bool=False, if True `TimeSeriesDataLoader` drops last non-full batch.<br>\n",
       "`alias`: str, optional,  Custom name of the model.<br>\n",
       "`optimizer`: Subclass of 'torch.optim.Optimizer', optional, user specified optimizer instead of the default choice (Adam).<br>\n",
       "`optimizer_kwargs`: dict, optional, list of parameters used by the user specified `optimizer`.<br>\n",
       "`lr_scheduler`: Subclass of 'torch.optim.lr_scheduler.LRScheduler', optional, user specified lr_scheduler instead of the default choice (StepLR).<br>\n",
       "`lr_scheduler_kwargs`: dict, optional, list of parameters used by the user specified `lr_scheduler`.<br>\n",
       "`dataloader_kwargs`: dict, optional, list of parameters passed into the PyTorch Lightning dataloader by the `TimeSeriesDataLoader`. <br>\n",
       "`**trainer_kwargs`: int,  keyword trainer arguments inherited from [PyTorch Lighning's trainer](https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.trainer.trainer.Trainer.html?highlight=trainer).<br>    \n",
       "\n",
       "**References**<br>\n",
       "- [Rangapuram, Syama Sundar, Jan Gasthaus, Lorenzo Stella, Valentin Flunkert, David Salinas, Yuyang Wang, and Tim Januschowski (2023). \"Deep Non-Parametric Time Series Forecaster\". arXiv.](https://arxiv.org/abs/2312.14657)<br>*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DeepNPTS, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### DeepNPTS.fit\n",
       "\n",
       ">      DeepNPTS.fit (dataset, val_size=0, test_size=0, random_seed=None,\n",
       ">                    distributed_config=None)\n",
       "\n",
       "*Fit.\n",
       "\n",
       "The `fit` method, optimizes the neural network's weights using the\n",
       "initialization parameters (`learning_rate`, `windows_batch_size`, ...)\n",
       "and the `loss` function as defined during the initialization.\n",
       "Within `fit` we use a PyTorch Lightning `Trainer` that\n",
       "inherits the initialization's `self.trainer_kwargs`, to customize\n",
       "its inputs, see [PL's trainer arguments](https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.trainer.trainer.Trainer.html?highlight=trainer).\n",
       "\n",
       "The method is designed to be compatible with SKLearn-like classes\n",
       "and in particular to be compatible with the StatsForecast library.\n",
       "\n",
       "By default the `model` is not saving training checkpoints to protect\n",
       "disk memory, to get them change `enable_checkpointing=True` in `__init__`.\n",
       "\n",
       "**Parameters:**<br>\n",
       "`dataset`: NeuralForecast's `TimeSeriesDataset`, see [documentation](https://nixtla.github.io/neuralforecast/tsdataset.html).<br>\n",
       "`val_size`: int, validation size for temporal cross-validation.<br>\n",
       "`random_seed`: int=None, random_seed for pytorch initializer and numpy generators, overwrites model.__init__'s.<br>\n",
       "`test_size`: int, test size for temporal cross-validation.<br>*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### DeepNPTS.fit\n",
       "\n",
       ">      DeepNPTS.fit (dataset, val_size=0, test_size=0, random_seed=None,\n",
       ">                    distributed_config=None)\n",
       "\n",
       "*Fit.\n",
       "\n",
       "The `fit` method, optimizes the neural network's weights using the\n",
       "initialization parameters (`learning_rate`, `windows_batch_size`, ...)\n",
       "and the `loss` function as defined during the initialization.\n",
       "Within `fit` we use a PyTorch Lightning `Trainer` that\n",
       "inherits the initialization's `self.trainer_kwargs`, to customize\n",
       "its inputs, see [PL's trainer arguments](https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.trainer.trainer.Trainer.html?highlight=trainer).\n",
       "\n",
       "The method is designed to be compatible with SKLearn-like classes\n",
       "and in particular to be compatible with the StatsForecast library.\n",
       "\n",
       "By default the `model` is not saving training checkpoints to protect\n",
       "disk memory, to get them change `enable_checkpointing=True` in `__init__`.\n",
       "\n",
       "**Parameters:**<br>\n",
       "`dataset`: NeuralForecast's `TimeSeriesDataset`, see [documentation](https://nixtla.github.io/neuralforecast/tsdataset.html).<br>\n",
       "`val_size`: int, validation size for temporal cross-validation.<br>\n",
       "`random_seed`: int=None, random_seed for pytorch initializer and numpy generators, overwrites model.__init__'s.<br>\n",
       "`test_size`: int, test size for temporal cross-validation.<br>*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DeepNPTS.fit, name='DeepNPTS.fit', title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### DeepNPTS.predict\n",
       "\n",
       ">      DeepNPTS.predict (dataset, test_size=None, step_size=1, random_seed=None,\n",
       ">                        **data_module_kwargs)\n",
       "\n",
       "*Predict.\n",
       "\n",
       "Neural network prediction with PL's `Trainer` execution of `predict_step`.\n",
       "\n",
       "**Parameters:**<br>\n",
       "`dataset`: NeuralForecast's `TimeSeriesDataset`, see [documentation](https://nixtla.github.io/neuralforecast/tsdataset.html).<br>\n",
       "`test_size`: int=None, test size for temporal cross-validation.<br>\n",
       "`step_size`: int=1, Step size between each window.<br>\n",
       "`random_seed`: int=None, random_seed for pytorch initializer and numpy generators, overwrites model.__init__'s.<br>\n",
       "`**data_module_kwargs`: PL's TimeSeriesDataModule args, see [documentation](https://pytorch-lightning.readthedocs.io/en/1.6.1/extensions/datamodules.html#using-a-datamodule).*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### DeepNPTS.predict\n",
       "\n",
       ">      DeepNPTS.predict (dataset, test_size=None, step_size=1, random_seed=None,\n",
       ">                        **data_module_kwargs)\n",
       "\n",
       "*Predict.\n",
       "\n",
       "Neural network prediction with PL's `Trainer` execution of `predict_step`.\n",
       "\n",
       "**Parameters:**<br>\n",
       "`dataset`: NeuralForecast's `TimeSeriesDataset`, see [documentation](https://nixtla.github.io/neuralforecast/tsdataset.html).<br>\n",
       "`test_size`: int=None, test size for temporal cross-validation.<br>\n",
       "`step_size`: int=1, Step size between each window.<br>\n",
       "`random_seed`: int=None, random_seed for pytorch initializer and numpy generators, overwrites model.__init__'s.<br>\n",
       "`**data_module_kwargs`: PL's TimeSeriesDataModule args, see [documentation](https://pytorch-lightning.readthedocs.io/en/1.6.1/extensions/datamodules.html#using-a-datamodule).*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DeepNPTS.predict, name='DeepNPTS.predict', title_level=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]tensor([[0.5454, 0.4546],\n",
      "        [0.5454, 0.4546]], device='cuda:0')\n",
      "Epoch 0:   0%|          | 0/1 [00:00<?, ?it/s]                             tensor([[0.7522, 0.2478],\n",
      "        [0.7419, 0.2581],\n",
      "        [0.6385, 0.3615],\n",
      "        ...,\n",
      "        [0.3312, 0.6688],\n",
      "        [0.6657, 0.3343],\n",
      "        [0.6703, 0.3297]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=2.180, train_loss_epoch=2.180]        tensor([[0.7600, 0.2400],\n",
      "        [0.7123, 0.2877],\n",
      "        [0.9057, 0.0943],\n",
      "        ...,\n",
      "        [0.8442, 0.1558],\n",
      "        [0.8686, 0.1314],\n",
      "        [0.8475, 0.1525]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=2.130, train_loss_epoch=2.130]        tensor([[0.3228, 0.6772],\n",
      "        [0.6885, 0.3115],\n",
      "        [0.4179, 0.5821],\n",
      "        ...,\n",
      "        [0.7409, 0.2591],\n",
      "        [0.5288, 0.4712],\n",
      "        [0.3024, 0.6976]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 3:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=2.060, train_loss_epoch=2.060]        tensor([[0.9483, 0.0517],\n",
      "        [0.7874, 0.2126],\n",
      "        [0.8301, 0.1699],\n",
      "        ...,\n",
      "        [0.3886, 0.6114],\n",
      "        [0.3086, 0.6914],\n",
      "        [0.8321, 0.1679]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 4:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=2.030, train_loss_epoch=2.030]        tensor([[0.8816, 0.1184],\n",
      "        [0.6818, 0.3182],\n",
      "        [0.5086, 0.4914],\n",
      "        ...,\n",
      "        [0.9598, 0.0402],\n",
      "        [0.3252, 0.6748],\n",
      "        [0.4640, 0.5360]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 5:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=2.080, train_loss_epoch=2.080]        tensor([[0.8440, 0.1560],\n",
      "        [0.3820, 0.6180],\n",
      "        [0.8794, 0.1206],\n",
      "        ...,\n",
      "        [0.8177, 0.1823],\n",
      "        [0.6147, 0.3853],\n",
      "        [0.3354, 0.6646]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 6:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=2.000, train_loss_epoch=2.000]        tensor([[0.9111, 0.0889],\n",
      "        [0.8060, 0.1940],\n",
      "        [0.4990, 0.5010],\n",
      "        ...,\n",
      "        [0.6639, 0.3361],\n",
      "        [0.5464, 0.4536],\n",
      "        [0.7311, 0.2689]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 7:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=2.000, train_loss_epoch=2.000]        tensor([[0.8317, 0.1683],\n",
      "        [0.8950, 0.1050],\n",
      "        [0.8967, 0.1033],\n",
      "        ...,\n",
      "        [0.7948, 0.2052],\n",
      "        [0.8563, 0.1437],\n",
      "        [0.7318, 0.2682]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 8:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=1.950, train_loss_epoch=1.950]        tensor([[0.9052, 0.0948],\n",
      "        [0.4277, 0.5723],\n",
      "        [0.3197, 0.6803],\n",
      "        ...,\n",
      "        [0.7213, 0.2787],\n",
      "        [0.2927, 0.7073],\n",
      "        [0.9428, 0.0572]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 9:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=1.940, train_loss_epoch=1.940]        tensor([[0.8628, 0.1372],\n",
      "        [0.4888, 0.5112],\n",
      "        [0.9657, 0.0343],\n",
      "        ...,\n",
      "        [0.6857, 0.3143],\n",
      "        [0.8645, 0.1355],\n",
      "        [0.5355, 0.4645]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 10:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=1.910, train_loss_epoch=1.910]       tensor([[0.9227, 0.0773],\n",
      "        [0.8650, 0.1350],\n",
      "        [0.9192, 0.0808],\n",
      "        ...,\n",
      "        [0.8984, 0.1016],\n",
      "        [0.9741, 0.0259],\n",
      "        [0.8832, 0.1168]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 11:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=1.890, train_loss_epoch=1.890]        tensor([[0.9093, 0.0907],\n",
      "        [0.9674, 0.0326],\n",
      "        [0.8816, 0.1184],\n",
      "        ...,\n",
      "        [0.9671, 0.0329],\n",
      "        [0.8554, 0.1446],\n",
      "        [0.9399, 0.0601]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 12:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=1.840, train_loss_epoch=1.840]        tensor([[0.3073, 0.6927],\n",
      "        [0.6646, 0.3354],\n",
      "        [0.8664, 0.1336],\n",
      "        ...,\n",
      "        [0.5747, 0.4253],\n",
      "        [0.8492, 0.1508],\n",
      "        [0.9272, 0.0728]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 13:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=1.800, train_loss_epoch=1.800]        tensor([[0.9836, 0.0164],\n",
      "        [0.9064, 0.0936],\n",
      "        [0.5858, 0.4142],\n",
      "        ...,\n",
      "        [0.6765, 0.3235],\n",
      "        [0.7468, 0.2532],\n",
      "        [0.9265, 0.0735]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 14:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=1.810, train_loss_epoch=1.810]        tensor([[0.4785, 0.5215],\n",
      "        [0.5414, 0.4586],\n",
      "        [0.8894, 0.1106],\n",
      "        ...,\n",
      "        [0.5916, 0.4084],\n",
      "        [0.3029, 0.6971],\n",
      "        [0.3595, 0.6405]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 15:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=1.760, train_loss_epoch=1.760]        tensor([[0.3123, 0.6877],\n",
      "        [0.3524, 0.6476],\n",
      "        [0.9105, 0.0895],\n",
      "        ...,\n",
      "        [0.8887, 0.1113],\n",
      "        [0.2600, 0.7400],\n",
      "        [0.8790, 0.1210]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 16:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=1.720, train_loss_epoch=1.720]        tensor([[0.8575, 0.1425],\n",
      "        [0.7628, 0.2372],\n",
      "        [0.9131, 0.0869],\n",
      "        ...,\n",
      "        [0.9521, 0.0479],\n",
      "        [0.9080, 0.0920],\n",
      "        [0.8573, 0.1427]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 17:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=1.720, train_loss_epoch=1.720]        tensor([[0.8520, 0.1480],\n",
      "        [0.9395, 0.0605],\n",
      "        [0.8973, 0.1027],\n",
      "        ...,\n",
      "        [0.9580, 0.0420],\n",
      "        [0.9520, 0.0480],\n",
      "        [0.5679, 0.4321]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 18:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=1.660, train_loss_epoch=1.660]        tensor([[0.9184, 0.0816],\n",
      "        [0.9308, 0.0692],\n",
      "        [0.8927, 0.1073],\n",
      "        ...,\n",
      "        [0.8271, 0.1729],\n",
      "        [0.8874, 0.1126],\n",
      "        [0.2941, 0.7059]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 19:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=1.650, train_loss_epoch=1.650]        tensor([[0.5428, 0.4572],\n",
      "        [0.6677, 0.3323],\n",
      "        [0.9262, 0.0738],\n",
      "        ...,\n",
      "        [0.8906, 0.1094],\n",
      "        [0.9570, 0.0430],\n",
      "        [0.7994, 0.2006]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 20:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=1.670, train_loss_epoch=1.670]        tensor([[0.8712, 0.1288],\n",
      "        [0.8761, 0.1239],\n",
      "        [0.5494, 0.4506],\n",
      "        ...,\n",
      "        [0.9619, 0.0381],\n",
      "        [0.9072, 0.0928],\n",
      "        [0.9525, 0.0475]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 21:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=1.580, train_loss_epoch=1.580]        tensor([[0.9800, 0.0200],\n",
      "        [0.4091, 0.5909],\n",
      "        [0.8643, 0.1357],\n",
      "        ...,\n",
      "        [0.7363, 0.2637],\n",
      "        [0.9346, 0.0654],\n",
      "        [0.3706, 0.6294]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 22:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=1.590, train_loss_epoch=1.590]        tensor([[0.9556, 0.0444],\n",
      "        [0.1509, 0.8491],\n",
      "        [0.9290, 0.0710],\n",
      "        ...,\n",
      "        [0.9617, 0.0383],\n",
      "        [0.9317, 0.0683],\n",
      "        [0.9465, 0.0535]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 23:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=1.560, train_loss_epoch=1.560]        tensor([[0.9046, 0.0954],\n",
      "        [0.9363, 0.0637],\n",
      "        [0.4852, 0.5148],\n",
      "        ...,\n",
      "        [0.8465, 0.1535],\n",
      "        [0.9336, 0.0664],\n",
      "        [0.9383, 0.0617]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 24:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=1.530, train_loss_epoch=1.530]        tensor([[0.7834, 0.2166],\n",
      "        [0.9764, 0.0236],\n",
      "        [0.9013, 0.0987],\n",
      "        ...,\n",
      "        [0.3852, 0.6148],\n",
      "        [0.9133, 0.0867],\n",
      "        [0.3592, 0.6408]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 25:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=1.490, train_loss_epoch=1.490]        tensor([[0.9966, 0.0034],\n",
      "        [0.9982, 0.0018],\n",
      "        [0.8596, 0.1404],\n",
      "        ...,\n",
      "        [0.8530, 0.1470],\n",
      "        [0.8178, 0.1822],\n",
      "        [0.4463, 0.5537]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 26:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=1.490, train_loss_epoch=1.490]        tensor([[0.3743, 0.6257],\n",
      "        [0.1994, 0.8006],\n",
      "        [0.8996, 0.1004],\n",
      "        ...,\n",
      "        [0.9827, 0.0173],\n",
      "        [0.3390, 0.6610],\n",
      "        [0.9527, 0.0473]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 27:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=1.430, train_loss_epoch=1.430]        tensor([[0.8171, 0.1829],\n",
      "        [0.1686, 0.8314],\n",
      "        [0.9405, 0.0595],\n",
      "        ...,\n",
      "        [0.9742, 0.0258],\n",
      "        [0.8926, 0.1074],\n",
      "        [0.2995, 0.7005]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 28:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=1.390, train_loss_epoch=1.390]        tensor([[0.9503, 0.0497],\n",
      "        [0.9582, 0.0418],\n",
      "        [0.9420, 0.0580],\n",
      "        ...,\n",
      "        [0.8966, 0.1034],\n",
      "        [0.8256, 0.1744],\n",
      "        [0.3929, 0.6071]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 29:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=1.360, train_loss_epoch=1.360]        tensor([[0.1167, 0.8833],\n",
      "        [0.9261, 0.0739],\n",
      "        [0.7751, 0.2249],\n",
      "        ...,\n",
      "        [0.8929, 0.1071],\n",
      "        [0.9782, 0.0218],\n",
      "        [0.9479, 0.0521]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 30:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=1.340, train_loss_epoch=1.340]        tensor([[0.9393, 0.0607],\n",
      "        [0.8083, 0.1917],\n",
      "        [0.8330, 0.1670],\n",
      "        ...,\n",
      "        [0.9161, 0.0839],\n",
      "        [0.1620, 0.8380],\n",
      "        [0.9720, 0.0280]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 31:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=1.330, train_loss_epoch=1.330]        tensor([[0.8144, 0.1856],\n",
      "        [0.8994, 0.1006],\n",
      "        [0.9455, 0.0545],\n",
      "        ...,\n",
      "        [0.3243, 0.6757],\n",
      "        [0.9881, 0.0119],\n",
      "        [0.1653, 0.8347]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 32:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=1.300, train_loss_epoch=1.300]        tensor([[0.9243, 0.0757],\n",
      "        [0.9346, 0.0654],\n",
      "        [0.8504, 0.1496],\n",
      "        ...,\n",
      "        [0.9565, 0.0435],\n",
      "        [0.1184, 0.8816],\n",
      "        [0.9807, 0.0193]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 33:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=1.290, train_loss_epoch=1.290]        tensor([[0.2468, 0.7532],\n",
      "        [0.1936, 0.8064],\n",
      "        [0.7906, 0.2094],\n",
      "        ...,\n",
      "        [0.2970, 0.7030],\n",
      "        [0.2063, 0.7937],\n",
      "        [0.8940, 0.1060]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 34:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=1.250, train_loss_epoch=1.250]        tensor([[9.8828e-01, 1.1724e-02],\n",
      "        [7.7836e-01, 2.2164e-01],\n",
      "        [9.4561e-01, 5.4394e-02],\n",
      "        ...,\n",
      "        [9.6423e-01, 3.5767e-02],\n",
      "        [9.9932e-01, 6.8387e-04],\n",
      "        [7.9423e-01, 2.0577e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 35:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=1.210, train_loss_epoch=1.210]        tensor([[0.9989, 0.0011],\n",
      "        [0.9043, 0.0957],\n",
      "        [0.9529, 0.0471],\n",
      "        ...,\n",
      "        [0.8765, 0.1235],\n",
      "        [0.9744, 0.0256],\n",
      "        [0.3019, 0.6981]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 36:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=1.220, train_loss_epoch=1.220]        tensor([[0.9855, 0.0145],\n",
      "        [0.9524, 0.0476],\n",
      "        [0.9549, 0.0451],\n",
      "        ...,\n",
      "        [0.9811, 0.0189],\n",
      "        [0.7746, 0.2254],\n",
      "        [0.3158, 0.6842]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 37:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=1.170, train_loss_epoch=1.170]        tensor([[0.6443, 0.3557],\n",
      "        [0.9635, 0.0365],\n",
      "        [0.7416, 0.2584],\n",
      "        ...,\n",
      "        [0.6896, 0.3104],\n",
      "        [0.7121, 0.2879],\n",
      "        [0.9347, 0.0653]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 38:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=1.170, train_loss_epoch=1.170]        tensor([[0.9551, 0.0449],\n",
      "        [0.6953, 0.3047],\n",
      "        [0.3052, 0.6948],\n",
      "        ...,\n",
      "        [0.7681, 0.2319],\n",
      "        [0.7982, 0.2018],\n",
      "        [0.2742, 0.7258]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 39:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=1.140, train_loss_epoch=1.140]        tensor([[0.9096, 0.0904],\n",
      "        [0.9114, 0.0886],\n",
      "        [0.1134, 0.8866],\n",
      "        ...,\n",
      "        [0.2673, 0.7327],\n",
      "        [0.9538, 0.0462],\n",
      "        [0.1062, 0.8938]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 40:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=1.140, train_loss_epoch=1.140]        tensor([[0.2369, 0.7631],\n",
      "        [0.9134, 0.0866],\n",
      "        [0.9344, 0.0656],\n",
      "        ...,\n",
      "        [0.9487, 0.0513],\n",
      "        [0.9411, 0.0589],\n",
      "        [0.9571, 0.0429]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 41:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=1.100, train_loss_epoch=1.100]        tensor([[0.9783, 0.0217],\n",
      "        [0.9226, 0.0774],\n",
      "        [0.9025, 0.0975],\n",
      "        ...,\n",
      "        [0.9146, 0.0854],\n",
      "        [0.3034, 0.6966],\n",
      "        [0.9389, 0.0611]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 42:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=1.070, train_loss_epoch=1.070]        tensor([[0.6888, 0.3112],\n",
      "        [0.6241, 0.3759],\n",
      "        [0.8882, 0.1118],\n",
      "        ...,\n",
      "        [0.6798, 0.3202],\n",
      "        [0.9521, 0.0479],\n",
      "        [0.9846, 0.0154]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 43:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=1.080, train_loss_epoch=1.080]        tensor([[0.8169, 0.1831],\n",
      "        [0.9877, 0.0123],\n",
      "        [0.5785, 0.4215],\n",
      "        ...,\n",
      "        [0.7623, 0.2377],\n",
      "        [0.3009, 0.6991],\n",
      "        [0.8638, 0.1362]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 44:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=1.080, train_loss_epoch=1.080]        tensor([[0.3373, 0.6627],\n",
      "        [0.2872, 0.7128],\n",
      "        [0.9353, 0.0647],\n",
      "        ...,\n",
      "        [0.8513, 0.1487],\n",
      "        [0.0942, 0.9058],\n",
      "        [0.6521, 0.3479]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 45:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=1.030, train_loss_epoch=1.030]        tensor([[0.8891, 0.1109],\n",
      "        [0.5713, 0.4287],\n",
      "        [0.0676, 0.9324],\n",
      "        ...,\n",
      "        [0.2738, 0.7262],\n",
      "        [0.7625, 0.2375],\n",
      "        [0.8390, 0.1610]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 46:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=1.010, train_loss_epoch=1.010]        tensor([[0.7765, 0.2235],\n",
      "        [0.9878, 0.0122],\n",
      "        [0.1157, 0.8843],\n",
      "        ...,\n",
      "        [0.8528, 0.1472],\n",
      "        [0.1219, 0.8781],\n",
      "        [0.9738, 0.0262]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 47:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=1.010, train_loss_epoch=1.010]        tensor([[0.9791, 0.0209],\n",
      "        [0.7796, 0.2204],\n",
      "        [0.9929, 0.0071],\n",
      "        ...,\n",
      "        [0.0374, 0.9626],\n",
      "        [0.9307, 0.0693],\n",
      "        [0.5196, 0.4804]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 48:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.998, train_loss_epoch=0.998]        tensor([[0.9854, 0.0146],\n",
      "        [0.1465, 0.8535],\n",
      "        [0.0736, 0.9264],\n",
      "        ...,\n",
      "        [0.9856, 0.0144],\n",
      "        [0.9727, 0.0273],\n",
      "        [0.1256, 0.8744]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 49:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.983, train_loss_epoch=0.983]        tensor([[0.9661, 0.0339],\n",
      "        [0.9590, 0.0410],\n",
      "        [0.9782, 0.0218],\n",
      "        ...,\n",
      "        [0.8436, 0.1564],\n",
      "        [0.0473, 0.9527],\n",
      "        [0.8458, 0.1542]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 50:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.975, train_loss_epoch=0.975]        tensor([[0.1133, 0.8867],\n",
      "        [0.9557, 0.0443],\n",
      "        [0.6234, 0.3766],\n",
      "        ...,\n",
      "        [0.9231, 0.0769],\n",
      "        [0.8039, 0.1961],\n",
      "        [0.1493, 0.8507]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 51:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.959, train_loss_epoch=0.959]        tensor([[0.9454, 0.0546],\n",
      "        [0.8597, 0.1403],\n",
      "        [0.1477, 0.8523],\n",
      "        ...,\n",
      "        [0.6497, 0.3503],\n",
      "        [0.1496, 0.8504],\n",
      "        [0.9606, 0.0394]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 52:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.940, train_loss_epoch=0.940]        tensor([[0.8910, 0.1090],\n",
      "        [0.9734, 0.0266],\n",
      "        [0.7309, 0.2691],\n",
      "        ...,\n",
      "        [0.9589, 0.0411],\n",
      "        [0.5588, 0.4412],\n",
      "        [0.0969, 0.9031]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 53:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.916, train_loss_epoch=0.916]        tensor([[0.1387, 0.8613],\n",
      "        [0.9905, 0.0095],\n",
      "        [0.9476, 0.0524],\n",
      "        ...,\n",
      "        [0.9936, 0.0064],\n",
      "        [0.9415, 0.0585],\n",
      "        [0.9741, 0.0259]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 54:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.930, train_loss_epoch=0.930]        tensor([[0.7456, 0.2544],\n",
      "        [0.9781, 0.0219],\n",
      "        [0.9807, 0.0193],\n",
      "        ...,\n",
      "        [0.9704, 0.0296],\n",
      "        [0.0974, 0.9026],\n",
      "        [0.7457, 0.2543]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 55:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.906, train_loss_epoch=0.906]        tensor([[0.8553, 0.1447],\n",
      "        [0.7748, 0.2252],\n",
      "        [0.9932, 0.0068],\n",
      "        ...,\n",
      "        [0.8264, 0.1736],\n",
      "        [0.1089, 0.8911],\n",
      "        [0.1836, 0.8164]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 56:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.894, train_loss_epoch=0.894]        tensor([[0.9257, 0.0743],\n",
      "        [0.1282, 0.8718],\n",
      "        [0.7864, 0.2136],\n",
      "        ...,\n",
      "        [0.1908, 0.8092],\n",
      "        [0.9892, 0.0108],\n",
      "        [0.3549, 0.6451]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 57:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.877, train_loss_epoch=0.877]        tensor([[0.6761, 0.3239],\n",
      "        [0.8293, 0.1707],\n",
      "        [0.9139, 0.0861],\n",
      "        ...,\n",
      "        [0.1033, 0.8967],\n",
      "        [0.0328, 0.9672],\n",
      "        [0.9934, 0.0066]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 58:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.866, train_loss_epoch=0.866]        tensor([[0.1217, 0.8783],\n",
      "        [0.8929, 0.1071],\n",
      "        [0.9666, 0.0334],\n",
      "        ...,\n",
      "        [0.0557, 0.9443],\n",
      "        [0.4273, 0.5727],\n",
      "        [0.4383, 0.5617]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 59:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.882, train_loss_epoch=0.882]        tensor([[0.7418, 0.2582],\n",
      "        [0.5853, 0.4147],\n",
      "        [0.2573, 0.7427],\n",
      "        ...,\n",
      "        [0.9901, 0.0099],\n",
      "        [0.3850, 0.6150],\n",
      "        [0.6441, 0.3559]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 60:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.875, train_loss_epoch=0.875]        tensor([[0.9952, 0.0048],\n",
      "        [0.3445, 0.6555],\n",
      "        [0.9132, 0.0868],\n",
      "        ...,\n",
      "        [0.0974, 0.9026],\n",
      "        [0.9772, 0.0228],\n",
      "        [0.9771, 0.0229]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 61:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.852, train_loss_epoch=0.852]        tensor([[0.6868, 0.3132],\n",
      "        [0.9724, 0.0276],\n",
      "        [0.7082, 0.2918],\n",
      "        ...,\n",
      "        [0.8823, 0.1177],\n",
      "        [0.7250, 0.2750],\n",
      "        [0.9101, 0.0899]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 62:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.856, train_loss_epoch=0.856]        tensor([[0.5270, 0.4730],\n",
      "        [0.7334, 0.2666],\n",
      "        [0.7217, 0.2783],\n",
      "        ...,\n",
      "        [0.9659, 0.0341],\n",
      "        [0.9609, 0.0391],\n",
      "        [0.9319, 0.0681]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 63:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.820, train_loss_epoch=0.820]        tensor([[3.4174e-01, 6.5826e-01],\n",
      "        [1.3579e-01, 8.6421e-01],\n",
      "        [9.9987e-01, 1.2858e-04],\n",
      "        ...,\n",
      "        [9.7443e-01, 2.5569e-02],\n",
      "        [7.0672e-01, 2.9328e-01],\n",
      "        [4.8789e-01, 5.1211e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 64:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.812, train_loss_epoch=0.812]        tensor([[0.9725, 0.0275],\n",
      "        [0.7525, 0.2475],\n",
      "        [0.7688, 0.2312],\n",
      "        ...,\n",
      "        [0.3077, 0.6923],\n",
      "        [0.0388, 0.9612],\n",
      "        [0.8759, 0.1241]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 65:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.817, train_loss_epoch=0.817]        tensor([[0.5823, 0.4177],\n",
      "        [0.7114, 0.2886],\n",
      "        [0.6899, 0.3101],\n",
      "        ...,\n",
      "        [0.2706, 0.7294],\n",
      "        [0.0891, 0.9109],\n",
      "        [0.0964, 0.9036]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 66:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.820, train_loss_epoch=0.820]        tensor([[0.7824, 0.2176],\n",
      "        [0.9536, 0.0464],\n",
      "        [0.9056, 0.0944],\n",
      "        ...,\n",
      "        [0.1213, 0.8787],\n",
      "        [0.5082, 0.4918],\n",
      "        [0.3724, 0.6276]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 67:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.819, train_loss_epoch=0.819]        tensor([[0.7679, 0.2321],\n",
      "        [0.1139, 0.8861],\n",
      "        [0.9763, 0.0237],\n",
      "        ...,\n",
      "        [0.3614, 0.6386],\n",
      "        [0.5661, 0.4339],\n",
      "        [0.0787, 0.9213]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 68:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.823, train_loss_epoch=0.823]        tensor([[0.9100, 0.0900],\n",
      "        [0.9615, 0.0385],\n",
      "        [0.2109, 0.7891],\n",
      "        ...,\n",
      "        [0.6222, 0.3778],\n",
      "        [0.0650, 0.9350],\n",
      "        [0.3587, 0.6413]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 69:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.816, train_loss_epoch=0.816]        tensor([[0.7260, 0.2740],\n",
      "        [0.9884, 0.0116],\n",
      "        [0.0663, 0.9337],\n",
      "        ...,\n",
      "        [0.7679, 0.2321],\n",
      "        [0.7210, 0.2790],\n",
      "        [0.5444, 0.4556]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 70:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.808, train_loss_epoch=0.808]        tensor([[0.9874, 0.0126],\n",
      "        [0.8207, 0.1793],\n",
      "        [0.9648, 0.0352],\n",
      "        ...,\n",
      "        [0.9907, 0.0093],\n",
      "        [0.8937, 0.1063],\n",
      "        [0.9651, 0.0349]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 71:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.802, train_loss_epoch=0.802]        tensor([[0.9936, 0.0064],\n",
      "        [0.9915, 0.0085],\n",
      "        [0.5982, 0.4018],\n",
      "        ...,\n",
      "        [0.1263, 0.8737],\n",
      "        [0.9638, 0.0362],\n",
      "        [0.9861, 0.0139]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 72:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.815, train_loss_epoch=0.815]        tensor([[6.8375e-02, 9.3163e-01],\n",
      "        [9.6797e-01, 3.2031e-02],\n",
      "        [9.9989e-01, 1.0852e-04],\n",
      "        ...,\n",
      "        [9.9896e-01, 1.0405e-03],\n",
      "        [9.4099e-01, 5.9014e-02],\n",
      "        [7.9103e-01, 2.0897e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 73:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.808, train_loss_epoch=0.808]        tensor([[0.2515, 0.7485],\n",
      "        [0.7228, 0.2772],\n",
      "        [0.0480, 0.9520],\n",
      "        ...,\n",
      "        [0.7737, 0.2263],\n",
      "        [0.9614, 0.0386],\n",
      "        [0.3182, 0.6818]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 74:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.804, train_loss_epoch=0.804]        tensor([[0.9453, 0.0547],\n",
      "        [0.1795, 0.8205],\n",
      "        [0.6733, 0.3267],\n",
      "        ...,\n",
      "        [0.9177, 0.0823],\n",
      "        [0.5504, 0.4496],\n",
      "        [0.9360, 0.0640]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 75:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.805, train_loss_epoch=0.805]        tensor([[9.8349e-01, 1.6509e-02],\n",
      "        [6.9230e-01, 3.0770e-01],\n",
      "        [9.2197e-01, 7.8029e-02],\n",
      "        ...,\n",
      "        [7.0783e-01, 2.9217e-01],\n",
      "        [9.9987e-01, 1.2772e-04],\n",
      "        [9.5095e-01, 4.9047e-02]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 76:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.786, train_loss_epoch=0.786]        tensor([[0.0539, 0.9461],\n",
      "        [0.8271, 0.1729],\n",
      "        [0.7442, 0.2558],\n",
      "        ...,\n",
      "        [0.1168, 0.8832],\n",
      "        [0.9658, 0.0342],\n",
      "        [0.9889, 0.0111]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 77:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.793, train_loss_epoch=0.793]        tensor([[0.6681, 0.3319],\n",
      "        [0.5984, 0.4016],\n",
      "        [0.5716, 0.4284],\n",
      "        ...,\n",
      "        [0.9873, 0.0127],\n",
      "        [0.7449, 0.2551],\n",
      "        [0.8747, 0.1253]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 78:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.780, train_loss_epoch=0.780]        tensor([[0.5910, 0.4090],\n",
      "        [0.9604, 0.0396],\n",
      "        [0.9814, 0.0186],\n",
      "        ...,\n",
      "        [0.9959, 0.0041],\n",
      "        [0.9839, 0.0161],\n",
      "        [0.9959, 0.0041]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 79:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.791, train_loss_epoch=0.791]        tensor([[0.9780, 0.0220],\n",
      "        [0.5821, 0.4179],\n",
      "        [0.8290, 0.1710],\n",
      "        ...,\n",
      "        [0.6426, 0.3574],\n",
      "        [0.5630, 0.4370],\n",
      "        [0.6596, 0.3404]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 80:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.795, train_loss_epoch=0.795]        tensor([[0.3288, 0.6712],\n",
      "        [0.7145, 0.2855],\n",
      "        [0.6566, 0.3434],\n",
      "        ...,\n",
      "        [0.1584, 0.8416],\n",
      "        [0.9930, 0.0070],\n",
      "        [0.1685, 0.8315]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 81:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.773, train_loss_epoch=0.773]        tensor([[0.1376, 0.8624],\n",
      "        [0.8571, 0.1429],\n",
      "        [0.8203, 0.1797],\n",
      "        ...,\n",
      "        [0.9869, 0.0131],\n",
      "        [0.9908, 0.0092],\n",
      "        [0.7329, 0.2671]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 82:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.767, train_loss_epoch=0.767]        tensor([[0.9679, 0.0321],\n",
      "        [0.9807, 0.0193],\n",
      "        [0.7906, 0.2094],\n",
      "        ...,\n",
      "        [0.9934, 0.0066],\n",
      "        [0.9633, 0.0367],\n",
      "        [0.6589, 0.3411]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 83:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.782, train_loss_epoch=0.782]        tensor([[0.3638, 0.6362],\n",
      "        [0.3875, 0.6125],\n",
      "        [0.7460, 0.2540],\n",
      "        ...,\n",
      "        [0.1430, 0.8570],\n",
      "        [0.9035, 0.0965],\n",
      "        [0.8192, 0.1808]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 84:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.772, train_loss_epoch=0.772]        tensor([[0.9863, 0.0137],\n",
      "        [0.9282, 0.0718],\n",
      "        [0.9952, 0.0048],\n",
      "        ...,\n",
      "        [0.6440, 0.3560],\n",
      "        [0.7549, 0.2451],\n",
      "        [0.9931, 0.0069]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 85:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.766, train_loss_epoch=0.766]        tensor([[0.5385, 0.4615],\n",
      "        [0.9732, 0.0268],\n",
      "        [0.8237, 0.1763],\n",
      "        ...,\n",
      "        [0.9839, 0.0161],\n",
      "        [0.9603, 0.0397],\n",
      "        [0.9966, 0.0034]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 86:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.772, train_loss_epoch=0.772]        tensor([[0.7333, 0.2667],\n",
      "        [0.6141, 0.3859],\n",
      "        [0.9894, 0.0106],\n",
      "        ...,\n",
      "        [0.9433, 0.0567],\n",
      "        [0.9967, 0.0033],\n",
      "        [0.9761, 0.0239]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 87:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.781, train_loss_epoch=0.781]        tensor([[0.6561, 0.3439],\n",
      "        [0.6748, 0.3252],\n",
      "        [0.9954, 0.0046],\n",
      "        ...,\n",
      "        [0.6445, 0.3555],\n",
      "        [0.9774, 0.0226],\n",
      "        [0.9133, 0.0867]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 88:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.759, train_loss_epoch=0.759]        tensor([[0.8262, 0.1738],\n",
      "        [0.7020, 0.2980],\n",
      "        [0.6775, 0.3225],\n",
      "        ...,\n",
      "        [0.6685, 0.3315],\n",
      "        [0.0500, 0.9500],\n",
      "        [0.1336, 0.8664]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 89:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.760, train_loss_epoch=0.760]        tensor([[0.2065, 0.7935],\n",
      "        [0.1858, 0.8142],\n",
      "        [0.1532, 0.8468],\n",
      "        ...,\n",
      "        [0.6537, 0.3463],\n",
      "        [0.9952, 0.0048],\n",
      "        [0.6429, 0.3571]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 90:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.752, train_loss_epoch=0.752]        tensor([[0.5828, 0.4172],\n",
      "        [0.8062, 0.1938],\n",
      "        [0.1494, 0.8506],\n",
      "        ...,\n",
      "        [0.1474, 0.8526],\n",
      "        [0.7077, 0.2923],\n",
      "        [0.8445, 0.1555]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 91:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.736, train_loss_epoch=0.736]        tensor([[0.2051, 0.7949],\n",
      "        [0.6441, 0.3559],\n",
      "        [0.1507, 0.8493],\n",
      "        ...,\n",
      "        [0.1981, 0.8019],\n",
      "        [0.9445, 0.0555],\n",
      "        [0.1356, 0.8644]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 92:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.749, train_loss_epoch=0.749]        tensor([[0.3928, 0.6072],\n",
      "        [0.5594, 0.4406],\n",
      "        [0.9921, 0.0079],\n",
      "        ...,\n",
      "        [0.5275, 0.4725],\n",
      "        [0.8415, 0.1585],\n",
      "        [0.1234, 0.8766]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 93:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.739, train_loss_epoch=0.739]        tensor([[0.7677, 0.2323],\n",
      "        [0.9494, 0.0506],\n",
      "        [0.6560, 0.3440],\n",
      "        ...,\n",
      "        [0.3696, 0.6304],\n",
      "        [0.6517, 0.3483],\n",
      "        [0.6901, 0.3099]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 94:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.743, train_loss_epoch=0.743]        tensor([[0.7889, 0.2111],\n",
      "        [0.1525, 0.8475],\n",
      "        [0.1201, 0.8799],\n",
      "        ...,\n",
      "        [0.9775, 0.0225],\n",
      "        [0.7954, 0.2046],\n",
      "        [0.1612, 0.8388]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 95:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.757, train_loss_epoch=0.757]        tensor([[0.1258, 0.8742],\n",
      "        [0.9948, 0.0052],\n",
      "        [0.9370, 0.0630],\n",
      "        ...,\n",
      "        [0.7640, 0.2360],\n",
      "        [0.9892, 0.0108],\n",
      "        [0.3258, 0.6742]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 96:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.753, train_loss_epoch=0.753]        tensor([[0.1582, 0.8418],\n",
      "        [0.7714, 0.2286],\n",
      "        [0.1573, 0.8427],\n",
      "        ...,\n",
      "        [0.5785, 0.4215],\n",
      "        [0.5562, 0.4438],\n",
      "        [0.9938, 0.0062]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 97:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.726, train_loss_epoch=0.726]        tensor([[0.6800, 0.3200],\n",
      "        [0.9380, 0.0620],\n",
      "        [0.7399, 0.2601],\n",
      "        ...,\n",
      "        [0.9175, 0.0825],\n",
      "        [0.4927, 0.5073],\n",
      "        [0.3614, 0.6386]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 98:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.727, train_loss_epoch=0.727]        tensor([[0.8021, 0.1979],\n",
      "        [0.9890, 0.0110],\n",
      "        [0.0640, 0.9360],\n",
      "        ...,\n",
      "        [0.6018, 0.3982],\n",
      "        [0.7993, 0.2007],\n",
      "        [0.1271, 0.8729]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 99:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.735, train_loss_epoch=0.735]        tensor([[0.8768, 0.1232],\n",
      "        [0.9839, 0.0161],\n",
      "        [0.9775, 0.0225],\n",
      "        ...,\n",
      "        [0.2150, 0.7850],\n",
      "        [0.8089, 0.1911],\n",
      "        [0.1542, 0.8458]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 99: 100%|| 1/1 [00:00<00:00, 51.95it/s, v_num=9165, train_loss_step=0.740, train_loss_epoch=0.735]tensor([[0.9806, 0.0194],\n",
      "        [0.9806, 0.0194]], device='cuda:0')\n",
      "Epoch 100:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.740, train_loss_epoch=0.740, valid_loss=35.80]       tensor([[0.9564, 0.0436],\n",
      "        [0.5864, 0.4136],\n",
      "        [0.6758, 0.3242],\n",
      "        ...,\n",
      "        [0.9731, 0.0269],\n",
      "        [0.9915, 0.0085],\n",
      "        [0.7742, 0.2258]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 101:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.714, train_loss_epoch=0.714, valid_loss=35.80]        tensor([[0.9875, 0.0125],\n",
      "        [0.8340, 0.1660],\n",
      "        [0.7437, 0.2563],\n",
      "        ...,\n",
      "        [0.9078, 0.0922],\n",
      "        [0.9888, 0.0112],\n",
      "        [0.9708, 0.0292]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 102:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.723, train_loss_epoch=0.723, valid_loss=35.80]        tensor([[0.6532, 0.3468],\n",
      "        [0.9602, 0.0398],\n",
      "        [0.7425, 0.2575],\n",
      "        ...,\n",
      "        [0.7506, 0.2494],\n",
      "        [0.2011, 0.7989],\n",
      "        [0.8909, 0.1091]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 103:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.727, train_loss_epoch=0.727, valid_loss=35.80]        tensor([[0.1515, 0.8485],\n",
      "        [0.8838, 0.1162],\n",
      "        [0.6880, 0.3120],\n",
      "        ...,\n",
      "        [0.9198, 0.0802],\n",
      "        [0.7501, 0.2499],\n",
      "        [0.8681, 0.1319]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 104:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.723, train_loss_epoch=0.723, valid_loss=35.80]        tensor([[0.6516, 0.3484],\n",
      "        [0.9859, 0.0141],\n",
      "        [0.6768, 0.3232],\n",
      "        ...,\n",
      "        [0.5063, 0.4937],\n",
      "        [0.9618, 0.0382],\n",
      "        [0.1389, 0.8611]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 105:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.712, train_loss_epoch=0.712, valid_loss=35.80]        tensor([[8.7682e-01, 1.2318e-01],\n",
      "        [5.6702e-01, 4.3298e-01],\n",
      "        [9.6740e-01, 3.2597e-02],\n",
      "        ...,\n",
      "        [3.2983e-01, 6.7017e-01],\n",
      "        [9.9973e-01, 2.7087e-04],\n",
      "        [3.3219e-01, 6.6781e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 106:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.737, train_loss_epoch=0.737, valid_loss=35.80]        tensor([[8.8999e-01, 1.1001e-01],\n",
      "        [7.9139e-01, 2.0861e-01],\n",
      "        [6.7146e-01, 3.2854e-01],\n",
      "        ...,\n",
      "        [2.1254e-01, 7.8746e-01],\n",
      "        [9.9972e-01, 2.8270e-04],\n",
      "        [9.0541e-01, 9.4594e-02]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 107:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.718, train_loss_epoch=0.718, valid_loss=35.80]        tensor([[9.7342e-01, 2.6578e-02],\n",
      "        [9.5300e-01, 4.6999e-02],\n",
      "        [9.9959e-01, 4.1258e-04],\n",
      "        ...,\n",
      "        [8.9372e-01, 1.0628e-01],\n",
      "        [6.7503e-01, 3.2497e-01],\n",
      "        [9.7059e-01, 2.9408e-02]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 108:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.715, train_loss_epoch=0.715, valid_loss=35.80]        tensor([[0.9549, 0.0451],\n",
      "        [0.8967, 0.1033],\n",
      "        [0.9892, 0.0108],\n",
      "        ...,\n",
      "        [0.9755, 0.0245],\n",
      "        [0.9405, 0.0595],\n",
      "        [0.6007, 0.3993]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 109:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.713, train_loss_epoch=0.713, valid_loss=35.80]        tensor([[0.6319, 0.3681],\n",
      "        [0.9698, 0.0302],\n",
      "        [0.9875, 0.0125],\n",
      "        ...,\n",
      "        [0.9179, 0.0821],\n",
      "        [0.1709, 0.8291],\n",
      "        [0.3791, 0.6209]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 110:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.713, train_loss_epoch=0.713, valid_loss=35.80]        tensor([[9.9967e-01, 3.3357e-04],\n",
      "        [9.9324e-01, 6.7607e-03],\n",
      "        [6.7451e-01, 3.2549e-01],\n",
      "        ...,\n",
      "        [9.0944e-01, 9.0561e-02],\n",
      "        [1.7425e-01, 8.2575e-01],\n",
      "        [6.3770e-01, 3.6230e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 111:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.715, train_loss_epoch=0.715, valid_loss=35.80]        tensor([[0.9698, 0.0302],\n",
      "        [0.5473, 0.4527],\n",
      "        [0.6288, 0.3712],\n",
      "        ...,\n",
      "        [0.7451, 0.2549],\n",
      "        [0.3360, 0.6640],\n",
      "        [0.1930, 0.8070]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 112:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.700, train_loss_epoch=0.700, valid_loss=35.80]        tensor([[0.8694, 0.1306],\n",
      "        [0.7221, 0.2779],\n",
      "        [0.9815, 0.0185],\n",
      "        ...,\n",
      "        [0.2324, 0.7676],\n",
      "        [0.1670, 0.8330],\n",
      "        [0.5614, 0.4386]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 113:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.699, train_loss_epoch=0.699, valid_loss=35.80]        tensor([[0.9509, 0.0491],\n",
      "        [0.3569, 0.6431],\n",
      "        [0.6299, 0.3701],\n",
      "        ...,\n",
      "        [0.9558, 0.0442],\n",
      "        [0.7932, 0.2068],\n",
      "        [0.7880, 0.2120]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 114:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.710, train_loss_epoch=0.710, valid_loss=35.80]        tensor([[0.2974, 0.7026],\n",
      "        [0.9698, 0.0302],\n",
      "        [0.8322, 0.1678],\n",
      "        ...,\n",
      "        [0.7500, 0.2500],\n",
      "        [0.4998, 0.5002],\n",
      "        [0.5946, 0.4054]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 115:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.700, train_loss_epoch=0.700, valid_loss=35.80]        tensor([[0.5621, 0.4379],\n",
      "        [0.2090, 0.7910],\n",
      "        [0.9741, 0.0259],\n",
      "        ...,\n",
      "        [0.5903, 0.4097],\n",
      "        [0.9772, 0.0228],\n",
      "        [0.8596, 0.1404]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 116:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.696, train_loss_epoch=0.696, valid_loss=35.80]        tensor([[0.5739, 0.4261],\n",
      "        [0.7971, 0.2029],\n",
      "        [0.9736, 0.0264],\n",
      "        ...,\n",
      "        [0.9710, 0.0290],\n",
      "        [0.6366, 0.3634],\n",
      "        [0.1810, 0.8190]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 117:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.691, train_loss_epoch=0.691, valid_loss=35.80]        tensor([[0.8651, 0.1349],\n",
      "        [0.0805, 0.9195],\n",
      "        [0.9616, 0.0384],\n",
      "        ...,\n",
      "        [0.9254, 0.0746],\n",
      "        [0.9898, 0.0102],\n",
      "        [0.7233, 0.2767]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 118:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.697, train_loss_epoch=0.697, valid_loss=35.80]        tensor([[0.8591, 0.1409],\n",
      "        [0.7278, 0.2722],\n",
      "        [0.7429, 0.2571],\n",
      "        ...,\n",
      "        [0.9490, 0.0510],\n",
      "        [0.4636, 0.5364],\n",
      "        [0.7742, 0.2258]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 119:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.712, train_loss_epoch=0.712, valid_loss=35.80]        tensor([[0.5632, 0.4368],\n",
      "        [0.6365, 0.3635],\n",
      "        [0.8808, 0.1192],\n",
      "        ...,\n",
      "        [0.9716, 0.0284],\n",
      "        [0.8995, 0.1005],\n",
      "        [0.8351, 0.1649]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 120:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.679, train_loss_epoch=0.679, valid_loss=35.80]        tensor([[0.7269, 0.2731],\n",
      "        [0.6288, 0.3712],\n",
      "        [0.9091, 0.0909],\n",
      "        ...,\n",
      "        [0.6010, 0.3990],\n",
      "        [0.5507, 0.4493],\n",
      "        [0.3956, 0.6044]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 121:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.700, train_loss_epoch=0.700, valid_loss=35.80]        tensor([[0.8305, 0.1695],\n",
      "        [0.4630, 0.5370],\n",
      "        [0.2924, 0.7076],\n",
      "        ...,\n",
      "        [0.8877, 0.1123],\n",
      "        [0.1968, 0.8032],\n",
      "        [0.9785, 0.0215]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 122:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.693, train_loss_epoch=0.693, valid_loss=35.80]        tensor([[0.9869, 0.0131],\n",
      "        [0.9904, 0.0096],\n",
      "        [0.9781, 0.0219],\n",
      "        ...,\n",
      "        [0.9594, 0.0406],\n",
      "        [0.7075, 0.2925],\n",
      "        [0.7391, 0.2609]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 123:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.678, train_loss_epoch=0.678, valid_loss=35.80]        tensor([[0.7141, 0.2859],\n",
      "        [0.1980, 0.8020],\n",
      "        [0.8849, 0.1151],\n",
      "        ...,\n",
      "        [0.7039, 0.2961],\n",
      "        [0.5370, 0.4630],\n",
      "        [0.5592, 0.4408]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 124:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.694, train_loss_epoch=0.694, valid_loss=35.80]        tensor([[0.8664, 0.1336],\n",
      "        [0.1957, 0.8043],\n",
      "        [0.3546, 0.6454],\n",
      "        ...,\n",
      "        [0.2022, 0.7978],\n",
      "        [0.5116, 0.4884],\n",
      "        [0.4127, 0.5873]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 125:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.690, train_loss_epoch=0.690, valid_loss=35.80]        tensor([[0.5090, 0.4910],\n",
      "        [0.5707, 0.4293],\n",
      "        [0.9277, 0.0723],\n",
      "        ...,\n",
      "        [0.9731, 0.0269],\n",
      "        [0.8927, 0.1073],\n",
      "        [0.3921, 0.6079]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 126:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.701, train_loss_epoch=0.701, valid_loss=35.80]        tensor([[0.9097, 0.0903],\n",
      "        [0.9437, 0.0563],\n",
      "        [0.6092, 0.3908],\n",
      "        ...,\n",
      "        [0.5248, 0.4752],\n",
      "        [0.6155, 0.3845],\n",
      "        [0.6785, 0.3215]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 127:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.679, train_loss_epoch=0.679, valid_loss=35.80]        tensor([[0.5157, 0.4843],\n",
      "        [0.9463, 0.0537],\n",
      "        [0.4132, 0.5868],\n",
      "        ...,\n",
      "        [0.8246, 0.1754],\n",
      "        [0.2085, 0.7915],\n",
      "        [0.7269, 0.2731]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 128:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.684, train_loss_epoch=0.684, valid_loss=35.80]        tensor([[0.3272, 0.6728],\n",
      "        [0.7503, 0.2497],\n",
      "        [0.7270, 0.2730],\n",
      "        ...,\n",
      "        [0.6221, 0.3779],\n",
      "        [0.7263, 0.2737],\n",
      "        [0.9776, 0.0224]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 129:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.693, train_loss_epoch=0.693, valid_loss=35.80]        tensor([[0.8813, 0.1187],\n",
      "        [0.2379, 0.7621],\n",
      "        [0.1315, 0.8685],\n",
      "        ...,\n",
      "        [0.6418, 0.3582],\n",
      "        [0.9790, 0.0210],\n",
      "        [0.8706, 0.1294]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 130:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.679, train_loss_epoch=0.679, valid_loss=35.80]        tensor([[0.5038, 0.4962],\n",
      "        [0.4056, 0.5944],\n",
      "        [0.1966, 0.8034],\n",
      "        ...,\n",
      "        [0.8934, 0.1066],\n",
      "        [0.9354, 0.0646],\n",
      "        [0.3310, 0.6690]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 131:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.660, train_loss_epoch=0.660, valid_loss=35.80]        tensor([[0.9550, 0.0450],\n",
      "        [0.6709, 0.3291],\n",
      "        [0.8548, 0.1452],\n",
      "        ...,\n",
      "        [0.7777, 0.2223],\n",
      "        [0.2080, 0.7920],\n",
      "        [0.2705, 0.7295]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 132:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.661, train_loss_epoch=0.661, valid_loss=35.80]        tensor([[0.1646, 0.8354],\n",
      "        [0.7036, 0.2964],\n",
      "        [0.1927, 0.8073],\n",
      "        ...,\n",
      "        [0.7261, 0.2739],\n",
      "        [0.9879, 0.0121],\n",
      "        [0.8395, 0.1605]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 133:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.654, train_loss_epoch=0.654, valid_loss=35.80]        tensor([[0.2528, 0.7472],\n",
      "        [0.9742, 0.0258],\n",
      "        [0.9882, 0.0118],\n",
      "        ...,\n",
      "        [0.5334, 0.4666],\n",
      "        [0.7881, 0.2119],\n",
      "        [0.6505, 0.3495]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 134:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.676, train_loss_epoch=0.676, valid_loss=35.80]        tensor([[0.9577, 0.0423],\n",
      "        [0.7750, 0.2250],\n",
      "        [0.6670, 0.3330],\n",
      "        ...,\n",
      "        [0.1425, 0.8575],\n",
      "        [0.9781, 0.0219],\n",
      "        [0.9130, 0.0870]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 135:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.645, train_loss_epoch=0.645, valid_loss=35.80]        tensor([[0.2365, 0.7635],\n",
      "        [0.6767, 0.3233],\n",
      "        [0.1094, 0.8906],\n",
      "        ...,\n",
      "        [0.1818, 0.8182],\n",
      "        [0.9772, 0.0228],\n",
      "        [0.8446, 0.1554]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 136:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.665, train_loss_epoch=0.665, valid_loss=35.80]        tensor([[0.6920, 0.3080],\n",
      "        [0.9536, 0.0464],\n",
      "        [0.9875, 0.0125],\n",
      "        ...,\n",
      "        [0.9603, 0.0397],\n",
      "        [0.9527, 0.0473],\n",
      "        [0.9978, 0.0022]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 137:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.680, train_loss_epoch=0.680, valid_loss=35.80]        tensor([[0.2509, 0.7491],\n",
      "        [0.7363, 0.2637],\n",
      "        [0.8127, 0.1873],\n",
      "        ...,\n",
      "        [0.9022, 0.0978],\n",
      "        [0.8763, 0.1237],\n",
      "        [0.8252, 0.1748]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 138:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.651, train_loss_epoch=0.651, valid_loss=35.80]        tensor([[0.8890, 0.1110],\n",
      "        [0.7750, 0.2250],\n",
      "        [0.7442, 0.2558],\n",
      "        ...,\n",
      "        [0.9051, 0.0949],\n",
      "        [0.2428, 0.7572],\n",
      "        [0.5867, 0.4133]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 139:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.678, train_loss_epoch=0.678, valid_loss=35.80]        tensor([[0.0891, 0.9109],\n",
      "        [0.8143, 0.1857],\n",
      "        [0.6684, 0.3316],\n",
      "        ...,\n",
      "        [0.7515, 0.2485],\n",
      "        [0.5822, 0.4178],\n",
      "        [0.7830, 0.2170]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 140:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.660, train_loss_epoch=0.660, valid_loss=35.80]        tensor([[0.9450, 0.0550],\n",
      "        [0.2252, 0.7748],\n",
      "        [0.6516, 0.3484],\n",
      "        ...,\n",
      "        [0.6205, 0.3795],\n",
      "        [0.2038, 0.7962],\n",
      "        [0.9487, 0.0513]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 141:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.658, train_loss_epoch=0.658, valid_loss=35.80]        tensor([[0.9708, 0.0292],\n",
      "        [0.7402, 0.2598],\n",
      "        [0.3834, 0.6166],\n",
      "        ...,\n",
      "        [0.8717, 0.1283],\n",
      "        [0.9854, 0.0146],\n",
      "        [0.7012, 0.2988]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 142:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.666, train_loss_epoch=0.666, valid_loss=35.80]        tensor([[0.2583, 0.7417],\n",
      "        [0.7310, 0.2690],\n",
      "        [0.2798, 0.7202],\n",
      "        ...,\n",
      "        [0.8668, 0.1332],\n",
      "        [0.9208, 0.0792],\n",
      "        [0.4118, 0.5882]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 143:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.661, train_loss_epoch=0.661, valid_loss=35.80]        tensor([[0.3860, 0.6140],\n",
      "        [0.8924, 0.1076],\n",
      "        [0.8406, 0.1594],\n",
      "        ...,\n",
      "        [0.2140, 0.7860],\n",
      "        [0.5346, 0.4654],\n",
      "        [0.7253, 0.2747]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 144:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.649, train_loss_epoch=0.649, valid_loss=35.80]        tensor([[0.6844, 0.3156],\n",
      "        [0.6487, 0.3513],\n",
      "        [0.6394, 0.3606],\n",
      "        ...,\n",
      "        [0.5280, 0.4720],\n",
      "        [0.8959, 0.1041],\n",
      "        [0.8717, 0.1283]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 145:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.631, train_loss_epoch=0.631, valid_loss=35.80]        tensor([[0.8913, 0.1087],\n",
      "        [0.2508, 0.7492],\n",
      "        [0.2073, 0.7927],\n",
      "        ...,\n",
      "        [0.3966, 0.6034],\n",
      "        [0.5977, 0.4023],\n",
      "        [0.7510, 0.2490]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 146:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.654, train_loss_epoch=0.654, valid_loss=35.80]        tensor([[0.9477, 0.0523],\n",
      "        [0.2143, 0.7857],\n",
      "        [0.4823, 0.5177],\n",
      "        ...,\n",
      "        [0.9639, 0.0361],\n",
      "        [0.9745, 0.0255],\n",
      "        [0.7936, 0.2064]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 147:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.651, train_loss_epoch=0.651, valid_loss=35.80]        tensor([[0.8369, 0.1631],\n",
      "        [0.2123, 0.7877],\n",
      "        [0.5455, 0.4545],\n",
      "        ...,\n",
      "        [0.8224, 0.1776],\n",
      "        [0.8563, 0.1437],\n",
      "        [0.9382, 0.0618]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 148:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.652, train_loss_epoch=0.652, valid_loss=35.80]        tensor([[0.7129, 0.2871],\n",
      "        [0.5311, 0.4689],\n",
      "        [0.8308, 0.1692],\n",
      "        ...,\n",
      "        [0.2940, 0.7060],\n",
      "        [0.8365, 0.1635],\n",
      "        [0.6443, 0.3557]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 149:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.660, train_loss_epoch=0.660, valid_loss=35.80]        tensor([[0.4306, 0.5694],\n",
      "        [0.4886, 0.5114],\n",
      "        [0.5488, 0.4512],\n",
      "        ...,\n",
      "        [0.7603, 0.2397],\n",
      "        [0.1608, 0.8392],\n",
      "        [0.6074, 0.3926]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 150:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.657, train_loss_epoch=0.657, valid_loss=35.80]        tensor([[0.6113, 0.3887],\n",
      "        [0.5821, 0.4179],\n",
      "        [0.7453, 0.2547],\n",
      "        ...,\n",
      "        [0.1535, 0.8465],\n",
      "        [0.8221, 0.1779],\n",
      "        [0.1921, 0.8079]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 151:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.654, train_loss_epoch=0.654, valid_loss=35.80]        tensor([[0.7168, 0.2832],\n",
      "        [0.7731, 0.2269],\n",
      "        [0.7647, 0.2353],\n",
      "        ...,\n",
      "        [0.9403, 0.0597],\n",
      "        [0.7605, 0.2395],\n",
      "        [0.6520, 0.3480]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 152:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.644, train_loss_epoch=0.644, valid_loss=35.80]        tensor([[0.6601, 0.3399],\n",
      "        [0.2063, 0.7937],\n",
      "        [0.7206, 0.2794],\n",
      "        ...,\n",
      "        [0.3624, 0.6376],\n",
      "        [0.9634, 0.0366],\n",
      "        [0.9596, 0.0404]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 153:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.644, train_loss_epoch=0.644, valid_loss=35.80]        tensor([[0.8167, 0.1833],\n",
      "        [0.4807, 0.5193],\n",
      "        [0.3861, 0.6139],\n",
      "        ...,\n",
      "        [0.8208, 0.1792],\n",
      "        [0.5523, 0.4477],\n",
      "        [0.4199, 0.5801]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 154:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.636, train_loss_epoch=0.636, valid_loss=35.80]        tensor([[0.7853, 0.2147],\n",
      "        [0.1175, 0.8825],\n",
      "        [0.6464, 0.3536],\n",
      "        ...,\n",
      "        [0.8974, 0.1026],\n",
      "        [0.8674, 0.1326],\n",
      "        [0.6534, 0.3466]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 155:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.645, train_loss_epoch=0.645, valid_loss=35.80]        tensor([[0.1240, 0.8760],\n",
      "        [0.6118, 0.3882],\n",
      "        [0.5851, 0.4149],\n",
      "        ...,\n",
      "        [0.9168, 0.0832],\n",
      "        [0.9600, 0.0400],\n",
      "        [0.9600, 0.0400]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 156:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.643, train_loss_epoch=0.643, valid_loss=35.80]        tensor([[0.3937, 0.6063],\n",
      "        [0.7859, 0.2141],\n",
      "        [0.6030, 0.3970],\n",
      "        ...,\n",
      "        [0.8946, 0.1054],\n",
      "        [0.5685, 0.4315],\n",
      "        [0.1924, 0.8076]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 157:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.647, train_loss_epoch=0.647, valid_loss=35.80]        tensor([[0.6372, 0.3628],\n",
      "        [0.5778, 0.4222],\n",
      "        [0.9747, 0.0253],\n",
      "        ...,\n",
      "        [0.2580, 0.7420],\n",
      "        [0.4249, 0.5751],\n",
      "        [0.7256, 0.2744]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 158:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.650, train_loss_epoch=0.650, valid_loss=35.80]        tensor([[0.7917, 0.2083],\n",
      "        [0.7613, 0.2387],\n",
      "        [0.3030, 0.6970],\n",
      "        ...,\n",
      "        [0.5530, 0.4470],\n",
      "        [0.8623, 0.1377],\n",
      "        [0.1581, 0.8419]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 159:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.655, train_loss_epoch=0.655, valid_loss=35.80]        tensor([[0.9544, 0.0456],\n",
      "        [0.4007, 0.5993],\n",
      "        [0.7535, 0.2465],\n",
      "        ...,\n",
      "        [0.9106, 0.0894],\n",
      "        [0.2554, 0.7446],\n",
      "        [0.8577, 0.1423]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 160:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.642, train_loss_epoch=0.642, valid_loss=35.80]        tensor([[0.4448, 0.5552],\n",
      "        [0.8870, 0.1130],\n",
      "        [0.9632, 0.0368],\n",
      "        ...,\n",
      "        [0.5835, 0.4165],\n",
      "        [0.6235, 0.3765],\n",
      "        [0.2683, 0.7317]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 161:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.624, train_loss_epoch=0.624, valid_loss=35.80]        tensor([[0.6206, 0.3794],\n",
      "        [0.6944, 0.3056],\n",
      "        [0.7659, 0.2341],\n",
      "        ...,\n",
      "        [0.2638, 0.7362],\n",
      "        [0.5952, 0.4048],\n",
      "        [0.4296, 0.5704]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 162:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.622, train_loss_epoch=0.622, valid_loss=35.80]        tensor([[0.6069, 0.3931],\n",
      "        [0.9750, 0.0250],\n",
      "        [0.7844, 0.2156],\n",
      "        ...,\n",
      "        [0.4451, 0.5549],\n",
      "        [0.2253, 0.7747],\n",
      "        [0.5687, 0.4313]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 163:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.646, train_loss_epoch=0.646, valid_loss=35.80]        tensor([[0.2821, 0.7179],\n",
      "        [0.5975, 0.4025],\n",
      "        [0.3711, 0.6289],\n",
      "        ...,\n",
      "        [0.9202, 0.0798],\n",
      "        [0.1189, 0.8811],\n",
      "        [0.9440, 0.0560]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 164:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.650, train_loss_epoch=0.650, valid_loss=35.80]        tensor([[0.8813, 0.1187],\n",
      "        [0.7040, 0.2960],\n",
      "        [0.8869, 0.1131],\n",
      "        ...,\n",
      "        [0.2415, 0.7585],\n",
      "        [0.6602, 0.3398],\n",
      "        [0.6389, 0.3611]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 165:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.638, train_loss_epoch=0.638, valid_loss=35.80]        tensor([[0.5927, 0.4073],\n",
      "        [0.7266, 0.2734],\n",
      "        [0.2274, 0.7726],\n",
      "        ...,\n",
      "        [0.9010, 0.0990],\n",
      "        [0.9664, 0.0336],\n",
      "        [0.9684, 0.0316]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 166:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.653, train_loss_epoch=0.653, valid_loss=35.80]        tensor([[0.7071, 0.2929],\n",
      "        [0.9123, 0.0877],\n",
      "        [0.9606, 0.0394],\n",
      "        ...,\n",
      "        [0.8952, 0.1048],\n",
      "        [0.8595, 0.1405],\n",
      "        [0.5713, 0.4287]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 167:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.619, train_loss_epoch=0.619, valid_loss=35.80]        tensor([[0.5486, 0.4514],\n",
      "        [0.2632, 0.7368],\n",
      "        [0.9660, 0.0340],\n",
      "        ...,\n",
      "        [0.8846, 0.1154],\n",
      "        [0.7685, 0.2315],\n",
      "        [0.7402, 0.2598]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 168:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.627, train_loss_epoch=0.627, valid_loss=35.80]        tensor([[0.4029, 0.5971],\n",
      "        [0.2431, 0.7569],\n",
      "        [0.2732, 0.7268],\n",
      "        ...,\n",
      "        [0.4332, 0.5668],\n",
      "        [0.6865, 0.3135],\n",
      "        [0.5949, 0.4051]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 169:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.635, train_loss_epoch=0.635, valid_loss=35.80]        tensor([[0.5356, 0.4644],\n",
      "        [0.2673, 0.7327],\n",
      "        [0.8055, 0.1945],\n",
      "        ...,\n",
      "        [0.7974, 0.2026],\n",
      "        [0.9005, 0.0995],\n",
      "        [0.8880, 0.1120]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 170:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.639, train_loss_epoch=0.639, valid_loss=35.80]        tensor([[0.5015, 0.4985],\n",
      "        [0.5884, 0.4116],\n",
      "        [0.7311, 0.2689],\n",
      "        ...,\n",
      "        [0.3262, 0.6738],\n",
      "        [0.5460, 0.4540],\n",
      "        [0.4886, 0.5114]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 171:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.635, train_loss_epoch=0.635, valid_loss=35.80]        tensor([[0.8880, 0.1120],\n",
      "        [0.9723, 0.0277],\n",
      "        [0.5512, 0.4488],\n",
      "        ...,\n",
      "        [0.2722, 0.7278],\n",
      "        [0.5955, 0.4045],\n",
      "        [0.6075, 0.3925]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 172:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.617, train_loss_epoch=0.617, valid_loss=35.80]        tensor([[0.6890, 0.3110],\n",
      "        [0.4467, 0.5533],\n",
      "        [0.7990, 0.2010],\n",
      "        ...,\n",
      "        [0.6056, 0.3944],\n",
      "        [0.9416, 0.0584],\n",
      "        [0.2004, 0.7996]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 173:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.627, train_loss_epoch=0.627, valid_loss=35.80]        tensor([[0.8928, 0.1072],\n",
      "        [0.2289, 0.7711],\n",
      "        [0.7022, 0.2978],\n",
      "        ...,\n",
      "        [0.5506, 0.4494],\n",
      "        [0.8900, 0.1100],\n",
      "        [0.2685, 0.7315]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 174:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.636, train_loss_epoch=0.636, valid_loss=35.80]        tensor([[0.9610, 0.0390],\n",
      "        [0.7013, 0.2987],\n",
      "        [0.9701, 0.0299],\n",
      "        ...,\n",
      "        [0.7949, 0.2051],\n",
      "        [0.4054, 0.5946],\n",
      "        [0.5134, 0.4866]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 175:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.630, train_loss_epoch=0.630, valid_loss=35.80]        tensor([[0.2972, 0.7028],\n",
      "        [0.8180, 0.1820],\n",
      "        [0.9660, 0.0340],\n",
      "        ...,\n",
      "        [0.9716, 0.0284],\n",
      "        [0.9316, 0.0684],\n",
      "        [0.7653, 0.2347]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 176:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.608, train_loss_epoch=0.608, valid_loss=35.80]        tensor([[0.5033, 0.4967],\n",
      "        [0.6020, 0.3980],\n",
      "        [0.7071, 0.2929],\n",
      "        ...,\n",
      "        [0.5798, 0.4202],\n",
      "        [0.9706, 0.0294],\n",
      "        [0.7679, 0.2321]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 177:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.629, train_loss_epoch=0.629, valid_loss=35.80]        tensor([[0.2306, 0.7694],\n",
      "        [0.9416, 0.0584],\n",
      "        [0.5953, 0.4047],\n",
      "        ...,\n",
      "        [0.7618, 0.2382],\n",
      "        [0.6190, 0.3810],\n",
      "        [0.3775, 0.6225]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 178:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.626, train_loss_epoch=0.626, valid_loss=35.80]        tensor([[0.8662, 0.1338],\n",
      "        [0.8177, 0.1823],\n",
      "        [0.5009, 0.4991],\n",
      "        ...,\n",
      "        [0.9628, 0.0372],\n",
      "        [0.9403, 0.0597],\n",
      "        [0.4602, 0.5398]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 179:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.615, train_loss_epoch=0.615, valid_loss=35.80]        tensor([[0.4444, 0.5556],\n",
      "        [0.3431, 0.6569],\n",
      "        [0.8879, 0.1121],\n",
      "        ...,\n",
      "        [0.7510, 0.2490],\n",
      "        [0.4256, 0.5744],\n",
      "        [0.8799, 0.1201]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 180:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.610, train_loss_epoch=0.610, valid_loss=35.80]        tensor([[0.4248, 0.5752],\n",
      "        [0.5899, 0.4101],\n",
      "        [0.9103, 0.0897],\n",
      "        ...,\n",
      "        [0.2742, 0.7258],\n",
      "        [0.6344, 0.3656],\n",
      "        [0.9631, 0.0369]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 181:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.621, train_loss_epoch=0.621, valid_loss=35.80]        tensor([[0.3071, 0.6929],\n",
      "        [0.2931, 0.7069],\n",
      "        [0.7305, 0.2695],\n",
      "        ...,\n",
      "        [0.7846, 0.2154],\n",
      "        [0.3812, 0.6188],\n",
      "        [0.9388, 0.0612]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 182:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.636, train_loss_epoch=0.636, valid_loss=35.80]        tensor([[0.2383, 0.7617],\n",
      "        [0.9958, 0.0042],\n",
      "        [0.9640, 0.0360],\n",
      "        ...,\n",
      "        [0.3892, 0.6108],\n",
      "        [0.1493, 0.8507],\n",
      "        [0.6073, 0.3927]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 183:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.625, train_loss_epoch=0.625, valid_loss=35.80]        tensor([[0.7547, 0.2453],\n",
      "        [0.7291, 0.2709],\n",
      "        [0.9452, 0.0548],\n",
      "        ...,\n",
      "        [0.4903, 0.5097],\n",
      "        [0.8843, 0.1157],\n",
      "        [0.7109, 0.2891]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 184:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.636, train_loss_epoch=0.636, valid_loss=35.80]        tensor([[0.8686, 0.1314],\n",
      "        [0.7232, 0.2768],\n",
      "        [0.2905, 0.7095],\n",
      "        ...,\n",
      "        [0.4095, 0.5905],\n",
      "        [0.4601, 0.5399],\n",
      "        [0.9325, 0.0675]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 185:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.616, train_loss_epoch=0.616, valid_loss=35.80]        tensor([[0.8266, 0.1734],\n",
      "        [0.4920, 0.5080],\n",
      "        [0.9963, 0.0037],\n",
      "        ...,\n",
      "        [0.8000, 0.2000],\n",
      "        [0.4364, 0.5636],\n",
      "        [0.8924, 0.1076]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 186:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.622, train_loss_epoch=0.622, valid_loss=35.80]        tensor([[0.5431, 0.4569],\n",
      "        [0.3154, 0.6846],\n",
      "        [0.8712, 0.1288],\n",
      "        ...,\n",
      "        [0.6384, 0.3616],\n",
      "        [0.6640, 0.3360],\n",
      "        [0.7544, 0.2456]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 187:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.622, train_loss_epoch=0.622, valid_loss=35.80]        tensor([[0.6225, 0.3775],\n",
      "        [0.7756, 0.2244],\n",
      "        [0.9254, 0.0746],\n",
      "        ...,\n",
      "        [0.4674, 0.5326],\n",
      "        [0.1965, 0.8035],\n",
      "        [0.8053, 0.1947]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 188:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.606, train_loss_epoch=0.606, valid_loss=35.80]        tensor([[0.4022, 0.5978],\n",
      "        [0.9571, 0.0429],\n",
      "        [0.8037, 0.1963],\n",
      "        ...,\n",
      "        [0.8756, 0.1244],\n",
      "        [0.2366, 0.7634],\n",
      "        [0.5731, 0.4269]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 189:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.597, train_loss_epoch=0.597, valid_loss=35.80]        tensor([[0.5982, 0.4018],\n",
      "        [0.9828, 0.0172],\n",
      "        [0.3249, 0.6751],\n",
      "        ...,\n",
      "        [0.4045, 0.5955],\n",
      "        [0.8145, 0.1855],\n",
      "        [0.7532, 0.2468]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 190:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.616, train_loss_epoch=0.616, valid_loss=35.80]        tensor([[0.7898, 0.2102],\n",
      "        [0.3307, 0.6693],\n",
      "        [0.8111, 0.1889],\n",
      "        ...,\n",
      "        [0.6450, 0.3550],\n",
      "        [0.8326, 0.1674],\n",
      "        [0.8801, 0.1199]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 191:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.598, train_loss_epoch=0.598, valid_loss=35.80]        tensor([[0.9708, 0.0292],\n",
      "        [0.9600, 0.0400],\n",
      "        [0.5602, 0.4398],\n",
      "        ...,\n",
      "        [0.4340, 0.5660],\n",
      "        [0.6427, 0.3573],\n",
      "        [0.5127, 0.4873]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 192:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.621, train_loss_epoch=0.621, valid_loss=35.80]        tensor([[0.7729, 0.2271],\n",
      "        [0.4095, 0.5905],\n",
      "        [0.8805, 0.1195],\n",
      "        ...,\n",
      "        [0.2269, 0.7731],\n",
      "        [0.8706, 0.1294],\n",
      "        [0.1453, 0.8547]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 193:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.595, train_loss_epoch=0.595, valid_loss=35.80]        tensor([[0.7525, 0.2475],\n",
      "        [0.9640, 0.0360],\n",
      "        [0.8897, 0.1103],\n",
      "        ...,\n",
      "        [0.4314, 0.5686],\n",
      "        [0.5150, 0.4850],\n",
      "        [0.8833, 0.1167]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 194:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.606, train_loss_epoch=0.606, valid_loss=35.80]        tensor([[0.6116, 0.3884],\n",
      "        [0.4810, 0.5190],\n",
      "        [0.4090, 0.5910],\n",
      "        ...,\n",
      "        [0.4145, 0.5855],\n",
      "        [0.7874, 0.2126],\n",
      "        [0.2349, 0.7651]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 195:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.610, train_loss_epoch=0.610, valid_loss=35.80]        tensor([[0.8811, 0.1189],\n",
      "        [0.9954, 0.0046],\n",
      "        [0.6293, 0.3707],\n",
      "        ...,\n",
      "        [0.7303, 0.2697],\n",
      "        [0.6154, 0.3846],\n",
      "        [0.4031, 0.5969]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 196:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.607, train_loss_epoch=0.607, valid_loss=35.80]        tensor([[0.8328, 0.1672],\n",
      "        [0.8305, 0.1695],\n",
      "        [0.7581, 0.2419],\n",
      "        ...,\n",
      "        [0.7342, 0.2658],\n",
      "        [0.3758, 0.6242],\n",
      "        [0.5600, 0.4400]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 197:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.620, train_loss_epoch=0.620, valid_loss=35.80]        tensor([[0.8677, 0.1323],\n",
      "        [0.9513, 0.0487],\n",
      "        [0.9969, 0.0031],\n",
      "        ...,\n",
      "        [0.5358, 0.4642],\n",
      "        [0.3142, 0.6858],\n",
      "        [0.9698, 0.0302]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 198:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.619, train_loss_epoch=0.619, valid_loss=35.80]        tensor([[0.7762, 0.2238],\n",
      "        [0.7641, 0.2359],\n",
      "        [0.5658, 0.4342],\n",
      "        ...,\n",
      "        [0.2648, 0.7352],\n",
      "        [0.9524, 0.0476],\n",
      "        [0.8852, 0.1148]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 199:   0%|          | 0/1 [00:00<?, ?it/s, v_num=9165, train_loss_step=0.586, train_loss_epoch=0.586, valid_loss=35.80]        tensor([[0.3786, 0.6214],\n",
      "        [0.7925, 0.2075],\n",
      "        [0.4801, 0.5199],\n",
      "        ...,\n",
      "        [0.5700, 0.4300],\n",
      "        [0.5738, 0.4262],\n",
      "        [0.4893, 0.5107]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 199: 100%|| 1/1 [00:00<00:00, 38.73it/s, v_num=9165, train_loss_step=0.588, train_loss_epoch=0.586, valid_loss=35.80]tensor([[0.9218, 0.0782],\n",
      "        [0.9218, 0.0782]], device='cuda:0')\n",
      "Epoch 199: 100%|| 1/1 [00:00<00:00, 15.38it/s, v_num=9165, train_loss_step=0.588, train_loss_epoch=0.588, valid_loss=27.40]\n",
      "Predicting DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]tensor([[0.9718, 0.0282],\n",
      "        [0.9718, 0.0282]], device='cuda:0')\n",
      "Predicting DataLoader 0: 100%|| 1/1 [00:00<00:00, 58.21it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| eval: false\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from neuralforecast import NeuralForecast\n",
    "# from neuralforecast.models import DeepNPTS\n",
    "from neuralforecast.utils import AirPassengersPanel, AirPassengersStatic\n",
    "\n",
    "Y_train_df = AirPassengersPanel[AirPassengersPanel.ds<AirPassengersPanel['ds'].values[-12]] # 132 train\n",
    "Y_test_df = AirPassengersPanel[AirPassengersPanel.ds>=AirPassengersPanel['ds'].values[-12]].reset_index(drop=True) # 12 test\n",
    "\n",
    "nf = NeuralForecast(\n",
    "    models=[DeepNPTS(h=12,\n",
    "                   input_size=24,\n",
    "                #    stat_exog_list=['airline1'],\n",
    "                #    futr_exog_list=['trend'],\n",
    "                   max_steps=200,\n",
    "                #    val_check_steps=10,\n",
    "                   early_stop_patience_steps=3,\n",
    "                   scaler_type='robust',\n",
    "                   enable_progress_bar=True),\n",
    "    ],\n",
    "    freq='M'\n",
    ")\n",
    "nf.fit(df=Y_train_df, static_df=AirPassengersStatic, val_size=12)\n",
    "Y_hat_df = nf.predict(futr_df=Y_test_df)\n",
    "\n",
    "# Plot quantile predictions\n",
    "Y_hat_df = Y_hat_df.reset_index(drop=False).drop(columns=['unique_id','ds'])\n",
    "plot_df = pd.concat([Y_test_df, Y_hat_df], axis=1)\n",
    "plot_df = pd.concat([Y_train_df, plot_df])\n",
    "\n",
    "plot_df = plot_df[plot_df.unique_id=='Airline1'].drop('unique_id', axis=1)\n",
    "plt.plot(plot_df['ds'], plot_df['y'], c='black', label='True')\n",
    "plt.plot(plot_df['ds'], plot_df['DeepNPTS'], c='red', label='mean')\n",
    "plt.grid()\n",
    "plt.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
