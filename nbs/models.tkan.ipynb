{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYTORCH_ENABLE_MPS_FALLBACK=1\n"
     ]
    }
   ],
   "source": [
    "%set_env PYTORCH_ENABLE_MPS_FALLBACK=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp models.tkan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TKAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from fastcore.test import test_eq\n",
    "from nbdev.showdoc import show_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# from typing import Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# from neuralforecast.losses.pytorch import MAE\n",
    "# from neuralforecast.common._base_windows import BaseWindows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class FixedSplineActivation(nn.Module):\n",
    "    def __init__(self, \n",
    "                 exponent: float = 1.0, \n",
    "                 max_exponent: \n",
    "                 float = 9.0) -> None:\n",
    "        \n",
    "        super(FixedSplineActivation, self).__init__()\n",
    "        \n",
    "        self.exponent = torch.tensor(exponent, dtype=torch.float32)\n",
    "        self.max_exponent = torch.tensor(max_exponent, dtype=torch.float32)\n",
    "        self.epsilon = torch.finfo(torch.float32).eps\n",
    "        self.exponent = torch.clamp(self.exponent, -self.max_exponent, self.max_exponent)\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n",
    "        inputs_safe = torch.clamp(inputs, self.epsilon, 1.0)\n",
    "        return torch.pow(inputs_safe, self.exponent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class PowerSplineActivation(nn.Module):\n",
    "    def __init__(self, \n",
    "                 initial_exponent: float = 1.0, \n",
    "                 epsilon: float = 1e-7, \n",
    "                 max_exponent: float = 9.0, \n",
    "                 trainable: bool = True) -> None:\n",
    "        \n",
    "        super(PowerSplineActivation, self).__init__()\n",
    "\n",
    "        self.initial_exponent = torch.tensor(initial_exponent, dtype=torch.float32)\n",
    "        self.epsilon = epsilon\n",
    "        self.max_exponent = torch.tensor(max_exponent, dtype=torch.float32)\n",
    "        self.trainable = trainable\n",
    "\n",
    "        if self.trainable:\n",
    "            self.exponent = nn.Parameter(torch.tensor(self.initial_exponent, dtype=torch.float32))\n",
    "            self.bias = nn.Parameter(torch.zeros(1, dtype=torch.float32))\n",
    "        else:\n",
    "            self.exponent = torch.tensor(self.initial_exponent, dtype=torch.float32)\n",
    "            self.bias = torch.zeros(1, dtype=torch.float32)\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n",
    "        clipped_exponent = torch.clamp(self.exponent, -self.max_exponent, self.max_exponent)\n",
    "        x_safe = torch.clamp(inputs + self.bias, self.epsilon, 1.0)\n",
    "        return torch.pow(x_safe, clipped_exponent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class LinspaceInitializer:\n",
    "    def __init__(self, start: float, stop: float, num: int):\n",
    "        self.start = start\n",
    "        self.stop = stop\n",
    "        self.num = num\n",
    "\n",
    "    def __call__(self, shape, dtype=torch.float32):\n",
    "        if torch.prod(torch.tensor(shape)) != self.num:\n",
    "            raise ValueError(f\"The shape {shape} does not match the number of elements {self.num}.\")\n",
    "        result = torch.linspace(self.start, self.stop, self.num, dtype=dtype)\n",
    "        return result.view(shape)\n",
    "\n",
    "class BSplineActivation(nn.Module):\n",
    "    def __init__(self, \n",
    "                 num_bases: int = 10, \n",
    "                 order: int = 3):\n",
    "        \n",
    "        super(BSplineActivation, self).__init__()\n",
    "\n",
    "        self.num_bases = num_bases\n",
    "        self.order = order\n",
    "        self.w = nn.Parameter(torch.nn.init.xavier_uniform_(torch.empty(1)))\n",
    "        \n",
    "        self.coefficients = nn.Parameter(torch.zeros(self.num_bases))\n",
    "        self.bases = nn.Parameter(LinspaceInitializer(0.0, 1.0, self.num_bases)((self.num_bases,)), requires_grad=False)\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n",
    "        x = inputs\n",
    "        silu = x * torch.sigmoid(x)  # SiLU activation\n",
    "        spline_output = self.compute_spline(x)\n",
    "        return self.w * (silu + spline_output)\n",
    "\n",
    "    def compute_spline(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        safe_x = torch.clamp(x, torch.finfo(torch.float32).eps, 1.0)\n",
    "        expanded_x = safe_x.unsqueeze(-1)\n",
    "        basis_function_values = torch.pow(expanded_x - self.bases, self.order)\n",
    "        spline_values = torch.sum(self.coefficients * basis_function_values, dim=-1)\n",
    "        return spline_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class DropoutRNNCell:\n",
    "\n",
    "    def __init__(self, dropout=0.0, recurrent_dropout=0.0, seed=None):\n",
    "        self.dropout = dropout\n",
    "        self.recurrent_dropout = recurrent_dropout\n",
    "        self.seed = seed\n",
    "        self._dropout_mask = None\n",
    "        self._recurrent_dropout_mask = None\n",
    "\n",
    "    def get_dropout_mask(self, step_input):\n",
    "        if self._dropout_mask is None and self.dropout > 0:\n",
    "            self._dropout_mask = F.dropout(torch.ones_like(step_input), p=self.dropout, training=True, inplace=False)\n",
    "        return self._dropout_mask\n",
    "\n",
    "    def get_recurrent_dropout_mask(self, step_input):\n",
    "        if self._recurrent_dropout_mask is None and self.recurrent_dropout > 0:\n",
    "            self._recurrent_dropout_mask = F.dropout(torch.ones_like(step_input), p=self.recurrent_dropout, training=True, inplace=False)\n",
    "        return self._recurrent_dropout_mask\n",
    "\n",
    "    def reset_dropout_mask(self):\n",
    "        \"\"\"Reset the cached dropout mask if any.\"\"\"\n",
    "        self._dropout_mask = None\n",
    "\n",
    "    def reset_recurrent_dropout_mask(self):\n",
    "        self._recurrent_dropout_mask = None\n",
    "\n",
    "class DropoutLSTMCell(nn.LSTMCell, DropoutRNNCell):\n",
    "    \"\"\"\n",
    "    LSTM cell with dropout applied to inputs and recurrent state.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, dropout=0.0, recurrent_dropout=0.0, seed=None):\n",
    "        nn.LSTMCell.__init__(self, input_size, hidden_size)\n",
    "        DropoutRNNCell.__init__(self, dropout, recurrent_dropout, seed)\n",
    "\n",
    "    def forward(self, input, hx=None):\n",
    "        self.reset_dropout_mask()\n",
    "        self.reset_recurrent_dropout_mask()\n",
    "        if self.training:\n",
    "            input = input * self.get_dropout_mask(input)\n",
    "        hx = tuple(h * self.get_recurrent_dropout_mask(h) for h in hx) if hx is not None else hx\n",
    "        return super(DropoutLSTMCell, self).forward(input, hx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class TKANCell(nn.Module, DropoutRNNCell):\n",
    "    def __init__(\n",
    "        self,\n",
    "        units,\n",
    "        tkan_activations=None,\n",
    "        activation=\"tanh\",\n",
    "        recurrent_activation=\"sigmoid\",\n",
    "        use_bias=True,\n",
    "        kernel_initializer=\"glorot_uniform\",\n",
    "        recurrent_initializer=\"orthogonal\",\n",
    "        bias_initializer=\"zeros\",\n",
    "        unit_forget_bias=True,\n",
    "        kernel_regularizer=None,\n",
    "        recurrent_regularizer=None,\n",
    "        bias_regularizer=None,\n",
    "        kernel_constraint=None,\n",
    "        recurrent_constraint=None,\n",
    "        bias_constraint=None,\n",
    "        dropout=0.0,\n",
    "        recurrent_dropout=0.0,\n",
    "        seed=None,\n",
    "    ):\n",
    "        nn.Module.__init__(self)\n",
    "        DropoutRNNCell.__init__(self, dropout, recurrent_dropout, seed)\n",
    "\n",
    "        if units <= 0:\n",
    "            raise ValueError(\n",
    "                \"Received an invalid value for argument `units`, \"\n",
    "                f\"expected a positive integer, got {units}.\"\n",
    "            )\n",
    "\n",
    "        self.units = units\n",
    "        self.activation = activation\n",
    "        self.recurrent_activation = recurrent_activation\n",
    "        self.use_bias = use_bias\n",
    "\n",
    "        self.kernel_initializer = kernel_initializer\n",
    "        self.recurrent_initializer = recurrent_initializer\n",
    "        self.bias_initializer = bias_initializer\n",
    "\n",
    "        self.kernel_regularizer = kernel_regularizer\n",
    "        self.recurrent_regularizer = recurrent_regularizer\n",
    "        self.bias_regularizer = bias_regularizer\n",
    "\n",
    "        self.kernel_constraint = kernel_constraint\n",
    "        self.recurrent_constraint = recurrent_constraint\n",
    "        self.bias_constraint = bias_constraint\n",
    "\n",
    "        self.unit_forget_bias = unit_forget_bias\n",
    "\n",
    "        self.tkan_activations = tkan_activations or [BSplineActivation(3)]\n",
    "        self.tkan_sub_layers = nn.ModuleList()\n",
    "        \n",
    "        for act in self.tkan_activations:\n",
    "            if act is None:\n",
    "                self.tkan_sub_layers.append(nn.Linear(1, 1, bias=False))\n",
    "            elif isinstance(act, (int, float)):\n",
    "                self.tkan_sub_layers.append(nn.Linear(1, 1, bias=False))\n",
    "            else:\n",
    "                self.tkan_sub_layers.append(nn.Linear(1, 1, bias=False))\n",
    "\n",
    "        self.state_size = [units, units] + [1 for _ in self.tkan_activations]\n",
    "        self.output_size = units\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        input_dim = input_shape[-1]\n",
    "        self.kernel = nn.Parameter(torch.empty((input_dim, self.units * 3)))\n",
    "        nn.init.xavier_uniform_(self.kernel)\n",
    "        self.recurrent_kernel = nn.Parameter(torch.empty((self.units, self.units * 3)))\n",
    "        nn.init.orthogonal_(self.recurrent_kernel)\n",
    "        self.sub_tkan_kernel = nn.Parameter(torch.empty((len(self.tkan_sub_layers), 2)))\n",
    "        nn.init.orthogonal_(self.sub_tkan_kernel)\n",
    "        self.sub_tkan_recurrent_kernel = nn.Parameter(torch.empty((len(self.tkan_sub_layers), input_shape[1] * 2)))\n",
    "        nn.init.orthogonal_(self.sub_tkan_recurrent_kernel)\n",
    "        self.aggregated_weight = nn.Parameter(torch.empty((len(self.tkan_sub_layers), self.units)))\n",
    "        nn.init.xavier_uniform_(self.aggregated_weight)\n",
    "        self.aggregated_bias = nn.Parameter(torch.zeros((self.units,)))\n",
    "        \n",
    "        if self.use_bias:\n",
    "            if self.unit_forget_bias:\n",
    "                self.bias = nn.Parameter(torch.cat([torch.zeros(self.units), torch.ones(self.units), torch.zeros(self.units)]))\n",
    "            else:\n",
    "                self.bias = nn.Parameter(torch.zeros((self.units * 3,)))\n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "    def forward(self, inputs, states):\n",
    "        self.reset_dropout_mask()\n",
    "        self.reset_recurrent_dropout_mask()\n",
    "        h_tm1, c_tm1 = states[:2]\n",
    "        sub_states = states[2:]\n",
    "\n",
    "        if self.training and self.dropout > 0.0:\n",
    "            inputs = inputs * self.get_dropout_mask(inputs)\n",
    "        if self.training and self.recurrent_dropout > 0.0:\n",
    "            h_tm1 = h_tm1 * self.get_recurrent_dropout_mask(h_tm1)\n",
    "\n",
    "        sub_outputs = []\n",
    "        new_sub_states = []\n",
    "\n",
    "        if self.use_bias:\n",
    "            x_i, x_f, x_c = torch.split(self.recurrent_activation(F.linear(inputs, self.kernel) + F.linear(h_tm1, self.recurrent_kernel) + self.bias), self.units, dim=-1)\n",
    "        else:\n",
    "            x_i, x_f, x_c = torch.split(self.recurrent_activation(F.linear(inputs, self.kernel) + F.linear(h_tm1, self.recurrent_kernel)), self.units, dim=-1)\n",
    "\n",
    "        for idx, (sub_layer, sub_state) in enumerate(zip(self.tkan_sub_layers, sub_states)):\n",
    "            sub_kernel_h, sub_kernel_x = torch.split(self.sub_tkan_recurrent_kernel[idx], 2)\n",
    "            agg_input = inputs * sub_kernel_x + sub_state * sub_kernel_h\n",
    "            sub_output = sub_layer(agg_input)\n",
    "            sub_recurrent_kernel_h, sub_recurrent_kernel_x = torch.split(self.sub_tkan_kernel[idx], 2)\n",
    "            new_sub_state = sub_recurrent_kernel_h * sub_output + sub_state * sub_recurrent_kernel_x\n",
    "\n",
    "            sub_outputs.append(sub_output)\n",
    "            new_sub_states.append(new_sub_state)\n",
    "\n",
    "        sub_outputs = torch.stack(sub_outputs)\n",
    "        aggregated_sub_output = sub_outputs.view(inputs.size(0), -1)\n",
    "        aggregated_input = torch.matmul(aggregated_sub_output, self.aggregated_weight) + self.aggregated_bias\n",
    "\n",
    "        xo = self.recurrent_activation(aggregated_input)\n",
    "\n",
    "        c = x_f * c_tm1 + x_i * x_c\n",
    "        h = xo * self.activation(c)\n",
    "\n",
    "        new_states = [h, c] + new_sub_states\n",
    "        return h, new_states\n",
    "\n",
    "    def get_initial_state(self, batch_size, dtype=torch.float32):\n",
    "        return [torch.zeros((batch_size, self.units), dtype=dtype),\n",
    "                torch.zeros((batch_size, self.units), dtype=dtype)] + [torch.zeros((batch_size, 1), dtype=dtype) for _ in range(len(self.tkan_sub_layers))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
