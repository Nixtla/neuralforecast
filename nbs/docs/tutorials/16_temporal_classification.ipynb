{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal Classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A logistic regression analyzes the relationship between a binary target variable and its predictor variables to estimate the probability of the dependent variable taking the value 1. In the presence of temporal data where observations along time aren't independent, the errors of the model will be correlated through time and incorporating autoregressive features or lags can capture temporal dependencies and enhance the predictive power of logistic regression.\n",
    "\n",
    "<br>NHITS's inputs are static exogenous $\\mathbf{x}^{(s)}$, historic exogenous $\\mathbf{x}^{(h)}_{[:t]}$, exogenous available at the time of the prediction $\\mathbf{x}^{(f)}_{[:t+H]}$ and autorregresive features $\\mathbf{y}_{[:t]}$, each of these inputs is further decomposed into categorical and continuous. The network uses a multi-quantile regression to model the following conditional probability:$$\\mathbb{P}(\\mathbf{y}_{[t+1:t+H]}|\\;\\mathbf{y}_{[:t]},\\; \\mathbf{x}^{(h)}_{[:t]},\\; \\mathbf{x}^{(f)}_{[:t+H]},\\; \\mathbf{x}^{(s)})$$\n",
    "\n",
    "In this notebook we show how to fit NeuralForecast methods for binary sequences regression. We will:\n",
    "- Installing NeuralForecast.\n",
    "- Loading binary sequence data.\n",
    "- Fit and predict temporal classifiers.\n",
    "- Plot and evaluate predictions.\n",
    "\n",
    "You can run these experiments using GPU with Google Colab.\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/Nixtla/neuralforecast/blob/main/nbs/examples/Temporal_Classifiers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installing NeuralForecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%capture\n",
    "#!pip install neuralforecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from neuralforecast import NeuralForecast\n",
    "from neuralforecast.models import MLP, NHITS, LSTM\n",
    "from neuralforecast.losses.pytorch import DistributionLoss, Accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading Binary Sequence Data\n",
    "\n",
    "The `core.NeuralForecast` class contains shared, `fit`, `predict` and other methods that take as inputs pandas DataFrames with columns `['unique_id', 'ds', 'y']`, where `unique_id` identifies individual time series from the dataset, `ds` is the date, and `y` is the target binary variable. \n",
    "\n",
    "In this motivation example we convert 8x8 digits images into 64-length sequences and define a classification problem, to identify when the pixels surpass certain threshold.\n",
    "We declare a pandas dataframe in long format, to match NeuralForecast's inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = datasets.load_digits()\n",
    "images = digits.images[:100]\n",
    "\n",
    "plt.imshow(images[0,:,:], cmap=plt.cm.gray, \n",
    "           vmax=16, interpolation=\"nearest\")\n",
    "\n",
    "pixels = np.reshape(images, (len(images), 64))\n",
    "ytarget = (pixels > 10) * 1\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "ax2 = ax1.twinx()\n",
    "ax1.plot(pixels[10])\n",
    "ax2.plot(ytarget[10], color='purple')\n",
    "ax1.set_xlabel('Pixel index')\n",
    "ax1.set_ylabel('Pixel value')\n",
    "ax2.set_ylabel('Pixel threshold', color='purple')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We flat the images and create an input dataframe\n",
    "# with 'unique_id' series identifier and 'ds' time stamp identifier.\n",
    "Y_df = pd.DataFrame.from_dict({\n",
    "            'unique_id': np.repeat(np.arange(100), 64),\n",
    "            'ds': np.tile(np.arange(64)+1910, 100),\n",
    "            'y': ytarget.flatten(), 'pixels': pixels.flatten()})\n",
    "Y_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fit and predict temporal classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `NeuralForecast.fit` method you can train a set of models to your dataset. You can define the forecasting `horizon` (12 in this example), and modify the hyperparameters of the model. For example, for the `NHITS` we changed the default hidden size for both encoder and decoders.\n",
    "\n",
    "See the `NHITS` and `MLP` [model documentation](https://nixtla.github.io/neuralforecast/models.mlp.html).\n",
    "\n",
    ":::{.callout-warning}\n",
    "For the moment Recurrent-based model family is not available to operate with Bernoulli distribution output.\n",
    "This affects the following methods `LSTM`, `GRU`, `DilatedRNN`, and `TCN`. This feature is work in progress.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "horizon = 12\n",
    "\n",
    "# Try different hyperparmeters to improve accuracy.\n",
    "models = [MLP(h=horizon,                           # Forecast horizon\n",
    "              input_size=2 * horizon,              # Length of input sequence\n",
    "              loss=DistributionLoss('Bernoulli'),  # Binary classification loss\n",
    "              valid_loss=Accuracy(),               # Accuracy validation signal\n",
    "              max_steps=500,                       # Number of steps to train\n",
    "              scaler_type='standard',              # Type of scaler to normalize data\n",
    "              hidden_size=64,                      # Defines the size of the hidden state of the LSTM\n",
    "              #early_stop_patience_steps=2,         # Early stopping regularization patience\n",
    "              val_check_steps=10,                  # Frequency of validation signal (affects early stopping)\n",
    "              ),\n",
    "          NHITS(h=horizon,                          # Forecast horizon\n",
    "                input_size=2 * horizon,             # Length of input sequence\n",
    "                loss=DistributionLoss('Bernoulli'), # Binary classification loss\n",
    "                valid_loss=Accuracy(),              # Accuracy validation signal                \n",
    "                max_steps=500,                      # Number of steps to train\n",
    "                n_freq_downsample=[2, 1, 1],        # Downsampling factors for each stack output\n",
    "                #early_stop_patience_steps=2,        # Early stopping regularization patience\n",
    "                val_check_steps=10,                 # Frequency of validation signal (affects early stopping)\n",
    "                )             \n",
    "          ]\n",
    "nf = NeuralForecast(models=models, freq='Y')\n",
    "Y_hat_df = nf.cross_validation(df=Y_df, n_windows=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By default NeuralForecast produces forecast intervals\n",
    "# In this case the lo-x and high-x levels represent the \n",
    "# low and high bounds of the prediction accumulating x% probability\n",
    "Y_hat_df = Y_hat_df.reset_index(drop=True)\n",
    "Y_hat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define classification threshold for final predictions\n",
    "# If (prob > threshold) -> 1\n",
    "Y_hat_df['NHITS'] = (Y_hat_df['NHITS'] > 0.5) * 1\n",
    "Y_hat_df['MLP'] = (Y_hat_df['MLP'] > 0.5) * 1\n",
    "Y_hat_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Plot and Evaluate Predictions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we plot the forecasts of both models againts the real values.\n",
    "And evaluate the accuracy of the `MLP` and `NHITS` temporal classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df = Y_hat_df[Y_hat_df.unique_id==10]\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize = (20, 7))\n",
    "plt.plot(plot_df.ds, plot_df.y, label='target signal')\n",
    "plt.plot(plot_df.ds, plot_df['MLP'] * 1.1, label='MLP prediction')\n",
    "plt.plot(plot_df.ds, plot_df['NHITS'] * .9, label='NHITS prediction')\n",
    "ax.set_title('Binary Sequence Forecast', fontsize=22)\n",
    "ax.set_ylabel('Pixel Threshold and Prediction', fontsize=20)\n",
    "ax.set_xlabel('Timestamp [t]', fontsize=20)\n",
    "ax.legend(prop={'size': 15})\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y, y_hat):\n",
    "    return np.mean(y==y_hat)\n",
    "\n",
    "mlp_acc = accuracy(y=Y_hat_df['y'], y_hat=Y_hat_df['MLP'])\n",
    "nhits_acc = accuracy(y=Y_hat_df['y'], y_hat=Y_hat_df['NHITS'])\n",
    "\n",
    "print(f'MLP Accuracy: {mlp_acc:.1%}')\n",
    "print(f'NHITS Accuracy: {nhits_acc:.1%}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- [Cox D. R. (1958). “The Regression Analysis of Binary Sequences.” Journal of the Royal Statistical Society B, 20(2), 215–242.](https://arxiv.org/abs/2201.12886)\n",
    "- [Cristian Challu, Kin G. Olivares, Boris N. Oreshkin, Federico Garza, Max Mergenthaler-Canseco, Artur Dubrawski (2023). NHITS: Neural Hierarchical Interpolation for Time Series Forecasting. Accepted at AAAI 2023.](https://arxiv.org/abs/2201.12886)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
