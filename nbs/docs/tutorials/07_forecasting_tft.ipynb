{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasting with TFT: Temporal Fusion Transformer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temporal Fusion Transformer (TFT) proposed by Lim et al. [1] is one of the most popular transformer-based model for time-series forecasting. In summary, TFT combines gating layers, an LSTM recurrent encoder, with multi-head attention layers for a multi-step forecasting strategy decoder. For more details on the Nixtla's TFT implementation visit [this link](https://nixtla.github.io/neuralforecast/models.tft.html).\n",
    "\n",
    "In this notebook we show how to train the TFT model on the Texas electricity market load data (ERCOT). Accurately forecasting electricity markets is of great interest, as it is useful for planning distribution and consumption.\n",
    "\n",
    "We will show you how to load the data, train the TFT performing automatic hyperparameter tuning, and produce forecasts. Then, we will show you how to perform multiple historical forecasts for cross validation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run these experiments using GPU with Google Colab.\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/Nixtla/neuralforecast/blob/main/nbs/examples/Forecasting_TFT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install neuralforecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load ERCOT Data\n",
    "\n",
    "The input to NeuralForecast is always a data frame in [long format](https://www.theanalysisfactor.com/wide-and-long-data/) with three columns: `unique_id`, `ds` and `y`:\n",
    "\n",
    "* The `unique_id` (string, int or category) represents an identifier for the series. \n",
    "\n",
    "* The `ds` (datestamp or int) column should be either an integer indexing time or a datestamp ideally like YYYY-MM-DD for a date or YYYY-MM-DD HH:MM:SS for a timestamp.\n",
    "\n",
    "* The `y` (numeric) represents the measurement we wish to forecast. \n",
    "We will rename the \n",
    "\n",
    "First, read the 2022 historic total demand of the ERCOT market. We processed the original data (available [here](https://www.ercot.com/gridinfo/load/load_hist)), by adding the missing hour due to daylight saving time, parsing the date to datetime format, and filtering columns of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_df = pd.read_csv('https://datasets-nixtla.s3.amazonaws.com/ERCOT-clean.csv')\n",
    "Y_df['ds'] = pd.to_datetime(Y_df['ds'])\n",
    "Y_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model training and forecast\n",
    "\n",
    "First, instantiate the `AutoTFT` model. The `AutoTFT` class will automatically perform hyperparamter tunning using [Tune library](https://docs.ray.io/en/latest/tune/index.html), exploring a user-defined or default search space. Models are selected based on the error on a validation set and the best model is then stored and used during inference. \n",
    "\n",
    "To instantiate `AutoTFT` you need to define:\n",
    "\n",
    "* `h`: forecasting horizon\n",
    "* `loss`: training loss\n",
    "* `config`: hyperparameter search space. If `None`, the `AutoTFT` class will use a pre-defined suggested hyperparameter space.\n",
    "* `num_samples`: number of configurations explored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import tune\n",
    "\n",
    "from neuralforecast.auto import AutoTFT\n",
    "from neuralforecast.core import NeuralForecast\n",
    "from neuralforecast.losses.pytorch import MAE\n",
    "\n",
    "import logging\n",
    "logging.getLogger(\"pytorch_lightning\").setLevel(logging.WARNING)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{.callout-tip}\n",
    "Increase the `num_samples` parameter to explore a wider set of configurations for the selected models. As a rule of thumb choose it to be bigger than `15`.  \n",
    "\n",
    "With `num_samples=3` this example should run in around 20 minutes.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon = 24\n",
    "models = [AutoTFT(h=horizon,\n",
    "                  loss=MAE(),\n",
    "                  config=None,\n",
    "                  num_samples=3)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{.callout-tip}\n",
    "All our models can be used for both point and probabilistic forecasting. For producing probabilistic outputs, simply modify the loss to one of our `DistributionLoss`. The complete list of losses is available in [this link](https://nixtla.github.io/neuralforecast/losses.pytorch.html) \n",
    ":::"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.callout-important collapse=\"true\"}\n",
    "\n",
    "TFT is a very large model and can require a lot of memory! If you are running out of GPU memory, try declaring your config search space and decrease the `hidden_size`, `n_heads`, and `windows_batch_size` parameters.\n",
    "\n",
    "This are all the parameters of the config: \n",
    "\n",
    "```python\n",
    "config = {\n",
    "      \"input_size\": tune.choice([horizon]),\n",
    "      \"hidden_size\": tune.choice([32]),\n",
    "      \"n_head\": tune.choice([2]),\n",
    "      \"learning_rate\": tune.loguniform(1e-4, 1e-1),\n",
    "      \"scaler_type\": tune.choice(['robust', 'standard']),\n",
    "      \"max_steps\": tune.choice([500, 1000]),\n",
    "      \"windows_batch_size\": tune.choice([32]),\n",
    "      \"check_val_every_n_epoch\": tune.choice([100]),\n",
    "      \"random_seed\": tune.randint(1, 20),\n",
    "}\n",
    "```\n",
    ":::"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `NeuralForecast` class has built-in methods to simplify the forecasting pipelines, such as `fit`, `predit`, and `cross_validation`. Instantiate a `NeuralForecast` object with the following required parameters:\n",
    "\n",
    "* `models`: a list of models.\n",
    "\n",
    "* `freq`: a string indicating the frequency of the data. (See [panda's available frequencies](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases).)\n",
    "\n",
    "Then, use the `fit` method to train the `AutoTFT` model on the ERCOT data. The total training time will depend on the hardware and the explored configurations, it should take between 10 and 30 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "nf = NeuralForecast(\n",
    "    models=models,\n",
    "    freq='H')\n",
    "\n",
    "nf.fit(df=Y_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, use the `predict` method to forecast the next 24 hours after the training data and plot the forecasts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_hat_df = nf.predict()\n",
    "Y_hat_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the results with matplot lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize = (10, 3))\n",
    "plot_df = pd.concat([Y_df.tail(24*5).reset_index(drop=True), Y_hat_df]).set_index('ds') # Concatenate the train and forecast dataframes\n",
    "plot_df[['y', 'AutoTFT']].plot(ax=ax, linewidth=2)\n",
    "\n",
    "ax.set_title('Load [MW]', fontsize=12)\n",
    "ax.set_ylabel('Monthly Passengers', fontsize=12)\n",
    "ax.set_xlabel('Date', fontsize=12)\n",
    "ax.legend(prop={'size': 10})\n",
    "ax.grid()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Cross validation for multiple historic forecasts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `cross_validation` method allows you to simulate multiple historic forecasts, greatly simplifying pipelines by replacing for loops with `fit` and `predict` methods. See [this tutorial](https://nixtla.github.io/statsforecast/examples/getting_started_complete.html) for an animation of how the windows are defined. \n",
    "\n",
    "With time series data, cross validation is done by defining a sliding window across the historical data and predicting the period following it. This form of cross validation allows us to arrive at a better estimation of our modelâ€™s predictive abilities across a wider range of temporal instances while also keeping the data in the training set contiguous as is required by our models. The `cross_validation` method will use the validation set for hyperparameter selection, and will then produce the forecasts for the test set.\n",
    "\n",
    "Use the `cross_validation` method to produce all the daily forecasts for September. Set the validation and test sizes. To produce daily forecasts set the forecasting set the step size between windows as 24, to only produce one forecast per day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "val_size  = 90*24 # 90 days x 24 hours\n",
    "test_size = 30*24 # 30 days x 24 hours\n",
    "fcst_df = nf.cross_validation(df=Y_df, val_size=val_size, test_size=test_size,\n",
    "                                n_windows=None, step_size=horizon)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we merge the forecasts with the `Y_df` dataset and plot the forecasts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_hat_df = fcst_df.reset_index(drop=True)\n",
    "Y_hat_df = Y_hat_df.drop(columns=['y','cutoff'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df = Y_df.merge(Y_hat_df, on=['unique_id','ds'], how='outer').tail(test_size+24*7)\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.plot(plot_df['ds'], plot_df['y'], c='black', label='True')\n",
    "plt.plot(plot_df['ds'], plot_df['AutoTFT'], c='blue', label='Forecast')\n",
    "plt.axvline(pd.to_datetime('2022-09-01'), color='red', linestyle='-.')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.plot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "In Challu et al [2] we demonstrate that the N-HiTS model outperforms the latest transformers by more than 20% with 50 times less computation.\n",
    "\n",
    "Learn how to use the N-HiTS and the NeuralForecast library in [this tutorial](https://nixtla.github.io/neuralforecast/examples/electricitypeakforecasting.html)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] [Lim, B., ArÄ±k, S. Ã–., Loeff, N., & Pfister, T. (2021). Temporal fusion transformers for interpretable multi-horizon time series forecasting. International Journal of Forecasting, 37(4), 1748-1764.](https://www.sciencedirect.com/science/article/pii/S0169207021000637).\n",
    "\n",
    "[2] [Cristian Challu, Kin G. Olivares, Boris N. Oreshkin, Federico Garza, Max Mergenthaler-Canseco, Artur Dubrawski (2021). N-HiTS: Neural Hierarchical Interpolation for Time Series Forecasting. Accepted at AAAI 2023.](https://arxiv.org/abs/2201.12886)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
