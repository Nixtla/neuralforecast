{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long-Horizon Forecasting with NHITS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Long-horizon forecasting is challenging because of the *volatility* of the predictions and the *computational complexity*. To solve this problem we created the [NHITS](https://arxiv.org/abs/2201.12886) model and made the code available [NeuralForecast library](https://nixtla.github.io/neuralforecast/models.nhits.html). `NHITS` specializes its partial outputs in the different frequencies of the time series through hierarchical interpolation and multi-rate input\n",
    "processing. \n",
    "\n",
    "In this notebook we show how to use `NHITS` on the [ETTm2](https://github.com/zhouhaoyi/ETDataset) benchmark dataset. This data set includes data points for 2 Electricity Transformers at 2 stations, including load, oil temperature.\n",
    "\n",
    "We will show you how to load data, train, and perform automatic hyperparameter tuning, **to achieve SoTA performance**, outperforming even the latest Transformer architectures for a fraction of their computational cost (50x faster)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run these experiments using GPU with Google Colab.\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/Nixtla/neuralforecast/blob/main/nbs/examples/LongHorizon_with_NHITS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installing NeuralForecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install neuralforecast datasetsforecast"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load ETTm2 Data\n",
    "\n",
    "The `LongHorizon` class will automatically download the complete ETTm2 dataset and process it.\n",
    "\n",
    "It return three Dataframes: `Y_df` contains the values for the target variables, `X_df` contains exogenous calendar features and `S_df` contains static features for each time-series (none for ETTm2). For this example we will only use `Y_df`.\n",
    "\n",
    "If you want to use your own data just replace `Y_df`. Be sure to use a long format and have a simmilar structure than our data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasetsforecast.long_horizon import LongHorizon\n",
    "\n",
    "# Change this to your own data to try the model\n",
    "Y_df, _, _ = LongHorizon.load(directory='./', group='ETTm2')\n",
    "Y_df['ds'] = pd.to_datetime(Y_df['ds'])\n",
    "\n",
    "# For this excercise we are going to take 20% of the DataSet\n",
    "n_time = len(Y_df.ds.unique())\n",
    "val_size = int(.2 * n_time)\n",
    "test_size = int(.2 * n_time)\n",
    "\n",
    "Y_df.groupby('unique_id').head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# We are going to plot the temperature of the transformer \n",
    "# and marking the validation and train splits\n",
    "u_id = 'HUFL'\n",
    "x_plot = pd.to_datetime(Y_df[Y_df.unique_id==u_id].ds)\n",
    "y_plot = Y_df[Y_df.unique_id==u_id].y.values\n",
    "\n",
    "x_val = x_plot[n_time - val_size - test_size]\n",
    "x_test = x_plot[n_time - test_size]\n",
    "\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.plot(x_plot, y_plot)\n",
    "plt.xlabel('Date', fontsize=17)\n",
    "plt.ylabel('HUFL [15 min temperature]', fontsize=17)\n",
    "\n",
    "plt.axvline(x_val, color='black', linestyle='-.')\n",
    "plt.axvline(x_test, color='black', linestyle='-.')\n",
    "plt.text(x_val, 5, '  Validation', fontsize=12)\n",
    "plt.text(x_test, 5, '  Test', fontsize=12)\n",
    "\n",
    "plt.grid()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Hyperparameter selection and forecasting\n",
    "\n",
    "The `AutoNHITS` class will automatically perform hyperparamter tunning using [Tune library](https://docs.ray.io/en/latest/tune/index.html), exploring a user-defined or default search space. Models are selected based on the error on a validation set and the best model is then stored and used during inference. \n",
    "\n",
    "The `AutoNHITS.default_config` attribute contains a suggested hyperparameter space. Here, we specify a different search space following the paper's hyperparameters. Notice that *1000 Stochastic Gradient Steps* are enough to achieve SoTA performance. Feel free to play around with this space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import tune\n",
    "from neuralforecast.auto import AutoNHITS\n",
    "from neuralforecast.core import NeuralForecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon = 96 # 24hrs = 4 * 15 min.\n",
    "\n",
    "# Use your own config or AutoNHITS.default_config\n",
    "nhits_config = {\n",
    "       \"learning_rate\": tune.choice([1e-3]),                                     # Initial Learning rate\n",
    "       \"max_steps\": tune.choice([1000]),                                         # Number of SGD steps\n",
    "       \"input_size\": tune.choice([5 * horizon]),                                 # input_size = multiplier * horizon\n",
    "       \"batch_size\": tune.choice([7]),                                           # Number of series in windows\n",
    "       \"windows_batch_size\": tune.choice([256]),                                 # Number of windows in batch\n",
    "       \"n_pool_kernel_size\": tune.choice([[2, 2, 2], [16, 8, 1]]),               # MaxPool's Kernelsize\n",
    "       \"n_freq_downsample\": tune.choice([[168, 24, 1], [24, 12, 1], [1, 1, 1]]), # Interpolation expressivity ratios\n",
    "       \"activation\": tune.choice(['ReLU']),                                      # Type of non-linear activation\n",
    "       \"n_blocks\":  tune.choice([[1, 1, 1]]),                                    # Blocks per each 3 stacks\n",
    "       \"mlp_units\":  tune.choice([[[512, 512], [512, 512], [512, 512]]]),        # 2 512-Layers per block for each stack\n",
    "       \"interpolation_mode\": tune.choice(['linear']),                            # Type of multi-step interpolation\n",
    "       \"val_check_steps\": tune.choice([100]),                                    # Compute validation every 100 epochs\n",
    "       \"random_seed\": tune.randint(1, 10),\n",
    "    }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{.callout-tip}\n",
    "Refer to https://docs.ray.io/en/latest/tune/index.html for more information on the different space options, such as lists and continous intervals.m\n",
    ":::"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To instantiate `AutoNHITS` you need to define:\n",
    "\n",
    "* `h`: forecasting horizon\n",
    "* `loss`: training loss. Use the `DistributionLoss` to produce probabilistic forecasts.\n",
    "* `config`: hyperparameter search space. If `None`, the `AutoNHITS` class will use a pre-defined suggested hyperparameter space.\n",
    "* `num_samples`: number of configurations explored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [AutoNHITS(h=horizon,\n",
    "                    config=nhits_config, \n",
    "                    num_samples=5)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model by instantiating a `NeuralForecast` object with the following required parameters:\n",
    "\n",
    "* `models`: a list of models.\n",
    "\n",
    "* `freq`: a string indicating the frequency of the data. (See [panda's available frequencies](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases).)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `cross_validation` method allows you to simulate multiple historic forecasts, greatly simplifying pipelines by replacing for loops with `fit` and `predict` methods.\n",
    "\n",
    "With time series data, cross validation is done by defining a sliding window across the historical data and predicting the period following it. This form of cross validation allows us to arrive at a better estimation of our modelâ€™s predictive abilities across a wider range of temporal instances while also keeping the data in the training set contiguous as is required by our models.\n",
    "\n",
    "The `cross_validation` method will use the validation set for hyperparameter selection, and will then produce the forecasts for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "nf = NeuralForecast(\n",
    "    models=models,\n",
    "    freq='15min')\n",
    "\n",
    "Y_hat_df = nf.cross_validation(df=Y_df, val_size=val_size,\n",
    "                               test_size=test_size, n_windows=None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate Results\n",
    "\n",
    "The `AutoNHITS` class contains a `results` tune attribute that stores information of each configuration explored. It contains the validation loss and best validation hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nf.models[0].results.get_best_result().config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = Y_hat_df.y.values\n",
    "y_hat = Y_hat_df['AutoNHITS'].values\n",
    "\n",
    "n_series = len(Y_df.unique_id.unique())\n",
    "\n",
    "y_true = y_true.reshape(n_series, -1, horizon)\n",
    "y_hat = y_hat.reshape(n_series, -1, horizon)\n",
    "\n",
    "print('Parsed results')\n",
    "print('2. y_true.shape (n_series, n_windows, n_time_out):\\t', y_true.shape)\n",
    "print('2. y_hat.shape  (n_series, n_windows, n_time_out):\\t', y_hat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=3, ncols=1, figsize=(10, 11))\n",
    "fig.tight_layout()\n",
    "\n",
    "series = ['HUFL','HULL','LUFL','LULL','MUFL','MULL','OT']\n",
    "series_idx = 3\n",
    "\n",
    "for idx, w_idx in enumerate([200, 300, 400]):\n",
    "  axs[idx].plot(y_true[series_idx, w_idx,:],label='True')\n",
    "  axs[idx].plot(y_hat[series_idx, w_idx,:],label='Forecast')\n",
    "  axs[idx].grid()\n",
    "  axs[idx].set_ylabel(series[series_idx]+f' window {w_idx}', \n",
    "                      fontsize=17)\n",
    "  if idx==2:\n",
    "    axs[idx].set_xlabel('Forecast Horizon', fontsize=17)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we compute the test errors for the two metrics of interest:\n",
    "\n",
    "$\\qquad MAE = \\frac{1}{Windows * Horizon} \\sum_{\\tau} |y_{\\tau} - \\hat{y}_{\\tau}| \\qquad$ and $\\qquad MSE = \\frac{1}{Windows * Horizon} \\sum_{\\tau} (y_{\\tau} - \\hat{y}_{\\tau})^{2} \\qquad$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralforecast.losses.numpy import mae, mse\n",
    "\n",
    "print('MAE: ', mae(y_hat, y_true))\n",
    "print('MSE: ', mse(y_hat, y_true))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference we can check the performance when compared\n",
    "to previous 'state-of-the-art' long-horizon Transformer-based forecasting methods from the [NHITS paper](https://arxiv.org/abs/2201.12886). To recover or improve the paper results try setting `hyperopt_max_evals=30` in [Hyperparameter Tuning](#cell-4).\n",
    "\n",
    "Mean Absolute Error (MAE):\n",
    "\n",
    "| Horizon   | NHITS        | AutoFormer | InFormer | ARIMA \n",
    "|---        |---           |---         |---       |---\n",
    "|  96       |  **0.255**   |   0.339    |  0.453   | 0.301 \n",
    "|  192      |  0.305       |   0.340    |  0.563   | 0.345 \n",
    "|  336      |  0.346       |   0.372    |  0.887   | 0.386 \n",
    "|  720      |  0.426       |   0.419    |  1.388   | 0.445 \n",
    "\n",
    "Mean Squared Error (MSE):\n",
    "\n",
    "| Horizon   | NHITS        | AutoFormer | InFormer | ARIMA \n",
    "|---        |---           |---         |---       |---\n",
    "|  96       |  **0.176**   |   0.255    |  0.365   | 0.225 \n",
    "|  192      |  0.245       |   0.281    |  0.533   | 0.298 \n",
    "|  336      |  0.295       |   0.339    |  1.363   | 0.370 \n",
    "|  720      |  0.401       |   0.422    |  3.379   | 0.478 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[Cristian Challu, Kin G. Olivares, Boris N. Oreshkin, Federico Garza, Max Mergenthaler-Canseco, Artur Dubrawski (2021). NHITS: Neural Hierarchical Interpolation for Time Series Forecasting. Accepted at AAAI 2023.](https://arxiv.org/abs/2201.12886)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
