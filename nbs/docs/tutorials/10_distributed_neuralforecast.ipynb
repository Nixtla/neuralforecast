{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "This notebook was ran in databricks using the following configuration:\n",
    "\n",
    "* Databricks Runtime Version: 14.3 LTS ML (Spark 3.5, GPU, Scala 2.12)\n",
    "* Worker and executors instance type: g4dn.xlarge\n",
    "* Cluster libraries:\n",
    "  * neuralforecast==1.7.0\n",
    "  * fugue\n",
    "  * protobuf<=3.20.1\n",
    "  * s3fs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from neuralforecast import NeuralForecast, DistributedConfig\n",
    "from neuralforecast.auto import AutoNHITS\n",
    "from neuralforecast.models import NHITS, LSTM\n",
    "from utilsforecast.evaluation import evaluate\n",
    "from utilsforecast.losses import mae, rmse, smape\n",
    "from utilsforecast.plotting import plot_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.getLogger('pytorch_lightning').setLevel(logging.ERROR)\n",
    "os.environ['NIXTLA_ID_AS_COL'] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('https://datasets-nixtla.s3.amazonaws.com/m4-hourly.parquet')\n",
    "df['exog_0'] = np.random.rand(df.shape[0])\n",
    "static = df.groupby('unique_id').head(1).copy()\n",
    "static['stat_0'] = static['unique_id'].astype('category').cat.codes\n",
    "static = static[['unique_id', 'stat_0']]\n",
    "valid = df.groupby('unique_id').tail(24)\n",
    "train = df.drop(valid.index)\n",
    "# save for loading in spark\n",
    "s3_prefix = 's3://nixtla-tmp/distributed'\n",
    "train.to_parquet(f'{s3_prefix}/train.parquet', index=False)\n",
    "valid.to_parquet(f'{s3_prefix}/valid.parquet', index=False)\n",
    "static.to_parquet(f'{s3_prefix}/static.parquet', index=False)\n",
    "# load in spark\n",
    "spark_train = spark.read.parquet(f'{s3_prefix}/train.parquet')\n",
    "spark_valid = spark.read.parquet(f'{s3_prefix}/valid.parquet')\n",
    "spark_static = spark.read.parquet(f'{s3_prefix}/static.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration required for distributed training\n",
    "dist_cfg = DistributedConfig(\n",
    "    partitions_path=f'{s3_prefix}/partitions',  # path where the partitions will be saved\n",
    "    num_nodes=2,  # number of nodes to use during training (machines)\n",
    "    devices=1,   # number of GPUs in each machine\n",
    ")\n",
    "\n",
    "# pytorch lightning configuration\n",
    "# the executors don't have permission to write on the filesystem, so we disable saving artifacts\n",
    "distributed_kwargs = dict(\n",
    "    accelerator='gpu',\n",
    "    enable_progress_bar=False,\n",
    "    logger=False,\n",
    "    enable_checkpointing=False,\n",
    ")\n",
    "\n",
    "# exogenous features\n",
    "exogs = {\n",
    "    'futr_exog_list': ['exog_0'],\n",
    "    'stat_exog_list': ['stat_0'],\n",
    "}\n",
    "\n",
    "# for the AutoNHITS\n",
    "def config(trial):\n",
    "    return dict(\n",
    "        input_size=48,\n",
    "        max_steps=2_000,\n",
    "        learning_rate=trial.suggest_float('learning_rate', 1e-4, 1e-1, log=True),\n",
    "        **exogs,\n",
    "        **distributed_kwargs\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nf = NeuralForecast(\n",
    "    models=[\n",
    "        NHITS(h=24, input_size=48, max_steps=2_000, **exogs, **distributed_kwargs),\n",
    "        AutoNHITS(h=24, config=config, backend='optuna', num_samples=2, alias='tuned_nhits'),\n",
    "        LSTM(h=24, input_size=48, max_steps=2_000, **exogs, **distributed_kwargs),\n",
    "    ],\n",
    "    freq=1,\n",
    ")\n",
    "nf.fit(spark_train, static_df=spark_static, distributed_config=dist_cfg, val_size=24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we're done training the model in a distributed way we can predict using the stored dataset. If we have future exogenous features we can provide a spark dataframe as `futr_df`. Note that if you want to load the stored dataset you need to provide the spark session through the `engine` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_ds_preds = nf.predict(futr_df=spark_valid.drop(\"y\"), engine=spark).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also provide a spark dataframe as `df` as well as `static_df` and `futr_df` (if applicable) to compute predictions on different data or after loading a saved model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df_preds = nf.predict(df=spark_train, static_df=spark_static, futr_df=spark_valid.drop(\"y\")).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Either of the above methods will yield the same results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.testing.assert_frame_equal(\n",
    "    saved_ds_preds.sort_values(['unique_id', 'ds']).reset_index(drop=True),\n",
    "    new_df_preds.sort_values(['unique_id', 'ds']).reset_index(drop=True),\n",
    "    atol=1e-3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving for inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now persist the trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = f'{s3_prefix}/model-artifacts'\n",
    "nf.save(save_path, save_dataset=False, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And load them back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nf2 = NeuralForecast.load(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use this object to compute forecasts. We can provide either local dataframes (pandas, polars) as well as spark dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = nf.predict(df=train, static_df=static, futr_df=valid.drop(columns='y'))\n",
    "preds2 = nf2.predict(df=train, static_df=static, futr_df=valid.drop(columns='y'))[preds.columns]\n",
    "pd.testing.assert_frame_equal(saved_ds_preds, preds)\n",
    "pd.testing.assert_frame_equal(preds, preds2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    evaluate(\n",
    "        preds.merge(valid.drop(columns='exog_0'), on=['unique_id', 'ds']),\n",
    "        metrics=[mae, rmse, smape],\n",
    "    )\n",
    "    .drop(columns='unique_id')\n",
    "    .groupby('metric')\n",
    "    .mean()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting a sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_series(train, preds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
