{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conformal Prediction\n",
    "> Tutorial on how to train neuralforecast models and obtain prediction intervals using the conformal prediction methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conformal prediction intervals use cross-validation on a point forecaster model to generate the intervals. This means that no prior probabilities are needed, and the output is well-calibrated. No additional training is needed, and the model is treated as a black box. The approach is compatible with any model.\n",
    "\n",
    "In this notebook, we demonstrate how to obtain intervals using the conformal prediction methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from neuralforecast import NeuralForecast\n",
    "from neuralforecast.models import NHITS\n",
    "from utilsforecast.losses import mae, rmse, smape\n",
    "from neuralforecast.utils import AirPassengersPanel, AirPassengersStatic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.getLogger('pytorch_lightning').setLevel(logging.ERROR)\n",
    "os.environ['NIXTLA_ID_AS_COL'] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "We simply use AirPassengers dataset for the demostration of conformal predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AirPassengersPanel_train = AirPassengersPanel[AirPassengersPanel['ds'] < AirPassengersPanel['ds'].values[-12]].reset_index(drop=True)\n",
    "AirPassengersPanel_test = AirPassengersPanel[AirPassengersPanel['ds'] >= AirPassengersPanel['ds'].values[-12]].reset_index(drop=True)\n",
    "AirPassengersPanel_test['y'] = np.nan\n",
    "AirPassengersPanel_test['y_[lag12]'] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also create this directory structure with a spark dataframe using the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "spark.conf.set(\"spark.sql.parquet.outputTimestampType\", \"TIMESTAMP_MICROS\")\n",
    "(\n",
    "  spark_df\n",
    "  .repartition(id_col)\n",
    "  .sortWithinPartitions(id_col, time_col)\n",
    "  .write\n",
    "  .partitionBy(id_col)\n",
    "  .parquet(out_dir)\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DataLoader class still expects the static data to be passed in as a single DataFrame with one row per timeseries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "static = AirPassengersStatic.rename(columns={'unique_id': 'id_col'})\n",
    "static"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training\n",
    "\n",
    "We now train a NHITS model on the above dataset. \n",
    "It is worth noting that NeuralForecast currently does not support scaling when using this DataLoader. If you want to scale the timeseries this should be done before passing it in to the `fit` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon = 12\n",
    "stacks = 3\n",
    "models = [NHITS(input_size=5 * horizon,\n",
    "                h=horizon,\n",
    "                futr_exog_list=['trend', 'y_[lag12]'],\n",
    "                stat_exog_list=['airline1', 'airline2'],\n",
    "                max_steps=100,\n",
    "                stack_types = stacks*['identity'],\n",
    "                n_blocks = stacks*[1],\n",
    "                mlp_units = [[256,256] for _ in range(stacks)],\n",
    "                n_pool_kernel_size = stacks*[1])]\n",
    "nf = NeuralForecast(models=models, freq='ME')\n",
    "nf.fit(df=files_list, static_df=static, id_col='id_col')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecasting\n",
    "\n",
    "When working with large datasets, we need to provide a single DataFrame containing the input timesteps of all the timeseries for which wish to generate predictions. If we have future exogenous features, we should also include the future values of these features in the separate `futr_df` DataFrame. \n",
    "\n",
    "For the below prediction we are assuming we only want to predict the next 12 timesteps for Airline2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_df = valid[valid['id_col'] == 'Airline2']\n",
    "# we set input_size=60 and horizon=12 when fitting the model\n",
    "pred_df = valid_df[:60]\n",
    "futr_df = valid_df[60:72]\n",
    "futr_df = futr_df.drop([\"y\"], axis=1)\n",
    "\n",
    "predictions = nf.predict(df=pred_df, futr_df=futr_df, static_df=static)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "fig, ax = plt.subplots(1, 1, figsize = (20, 7))\n",
    "# only select those from NHITS to show\n",
    "preds = preds.reset_index(drop=True)\n",
    "dropped = [x for x in preds.columns if 'StemGNN' in x or 'RNN' in x]\n",
    "preds = preds.drop(columns=dropped)\n",
    "\n",
    "plot_df = pd.concat([AirPassengersPanel_train, preds]).set_index('ds')\n",
    "\n",
    "plot_df[plot_df['unique_id']=='Airline1'].drop(['unique_id','trend','y_[lag12]'], axis=1).plot(ax=ax, linewidth=2)\n",
    "\n",
    "ax.set_title('AirPassengers Forecast', fontsize=22)\n",
    "ax.set_ylabel('Monthly Passengers', fontsize=20)\n",
    "ax.set_xlabel('Timestamp [t]', fontsize=20)\n",
    "ax.legend(prop={'size': 15})\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caveat\n",
    "\n",
    "One caveat to note is that we do not support the conformalize quantiled prediction outputs computed by loss functions such as\n",
    " * [MQLoss](https://nixtlaverse.nixtla.io/neuralforecast/losses.pytorch.html#multi-quantile-loss-mqloss)\n",
    " * [DistributionLoss](https://nixtlaverse.nixtla.io/neuralforecast/losses.pytorch.html#distributionloss)\n",
    " * [PMM](https://nixtlaverse.nixtla.io/neuralforecast/losses.pytorch.html#poisson-mixture-mesh-pmm)\n",
    " * [GMM](https://nixtlaverse.nixtla.io/neuralforecast/losses.pytorch.html#gaussian-mixture-mesh-gmm)\n",
    " * [NBMM](https://nixtlaverse.nixtla.io/neuralforecast/losses.pytorch.html#negative-binomial-mixture-mesh-nbmm)\n",
    " * [HuberQLoss](https://nixtlaverse.nixtla.io/neuralforecast/losses.pytorch.html#huberized-quantile-loss)\n",
    " * [HuberMQLoss](https://nixtlaverse.nixtla.io/neuralforecast/losses.pytorch.html#huberized-mqloss)\n",
    " * [QuantileLoss](https://nixtlaverse.nixtla.io/neuralforecast/losses.pytorch.html#quantile-loss)\n",
    " * [IQLoss](https://nixtlaverse.nixtla.io/neuralforecast/losses.pytorch.html#implicit-quantile-loss-iqloss)\n",
    " * [sCRPS](https://nixtlaverse.nixtla.io/neuralforecast/losses.pytorch.html#scaled-continuous-ranked-probability-score-scrps)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
