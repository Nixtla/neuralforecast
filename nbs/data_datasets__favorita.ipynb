{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp data.datasets.favorita"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Favorita retail dataset\n",
    "\n",
    "## Dataset Description\n",
    "The 2018 Kaggle competition was organized by Corporaci√≥n Favorita, a major Ecuatorian grocery retailer. The Favorita dataset is comprised of item sales history and promotions information, with additional information of items and stores,regional and national holidays, among other. \n",
    "\n",
    "The competition task consisted on forecasting sixteen days for the log-sales of particular item store combinations, for 210,654 series. \n",
    "The original dataset is available in the [Kaggle Competition url](https://www.kaggle.com/c/favorita-grocery-sales-forecasting/).\n",
    "\n",
    "During the model's optimization we consider a balanced dataset of items and stores, for 217,944 bottom level series (4,036 items * 54 stores) but we evaluate on the original 210,645 test series. We consider a geographical hierarchical structure of 4 levels corresponding to stores, cities, states, and national level for a total of 371,312 time series. The dataset is at the daily level and starts from 2013-01-01 and ends by 2017-08-15 that comprehend 1688 days, we keep 34 days (1654 to 1988 days) as hold-out test and 34 days (1620 to 1654 days) as validation.\n",
    "\n",
    "Table of Contents\n",
    "1.   [Auxiliary Functions](#cell-1)\n",
    "2.   [Favorita Data class](#cell-2)\n",
    "3.   [Favorita Validation](#cell-3)\n",
    "4.   [HierarchicalDataset Validation](#cell-4)\n",
    "\n",
    "Summary of Favorita's hierarchical structure\n",
    "\n",
    "| Geographical Division | Number of nodes per division  | Number of series per division |    Total    |\n",
    "|          ---          |               ---             |              ---              |     ---     |\n",
    "|  Ecuador              |              1                |             4,036             |     4,036   |\n",
    "|  States               |             16                |            64,576             |    64,576   |\n",
    "|  Cities               |             22                |            88,792             |    88,792   |\n",
    "|  Stores               |             54                |           217,944             |   217,944   |\n",
    "|  Total                |             93                |           371,312             |   371,312   |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install py7zr\n",
    "# !pip install matplotlib\n",
    "# !pip install sklearn\n",
    "# !pip install pydantic\n",
    "# !pip install shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import shutil\n",
    "from py7zr import unpack_7zarchive\n",
    "# shutil.register_unpack_format('7zip', ['.7z'], unpack_7zarchive)\n",
    "\n",
    "# import mkl\n",
    "# mkl.set_num_threads(56)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import time\n",
    "import tqdm\n",
    "import timeit\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "\n",
    "import sklearn.preprocessing as preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import logging\n",
    "import requests\n",
    "import subprocess\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple, Union\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from neuralforecast.data.datasets.utils import (\n",
    "    download_file, Info, TimeSeriesDataclass\n",
    ")\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "import pylab as plt\n",
    "from pylab import rcParams\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "\n",
    "from matplotlib import rcParams\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "\n",
    "FONTSIZE = 22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"cell-1\"></a>\n",
    "## 1. Auxiliary Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class CodeTimer:\n",
    "    def __init__(self, name=None, verbose=True):\n",
    "        self.name = \" '\"  + name + \"'\" if name else ''\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.start = timeit.default_timer()\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        self.took = (timeit.default_timer() - self.start)\n",
    "        if self.verbose:\n",
    "            print('Code block' + self.name + \\\n",
    "                  ' took:\\t{0:.5f}'.format(self.took) + ' seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export        \n",
    "def check_nans(df):\n",
    "    \"\"\" For data wrangling logs \"\"\"\n",
    "    n_rows = len(df)\n",
    "    \n",
    "    check_df = {'col': [], 'dtype': [], 'nan_prc': []}\n",
    "    for col in df.columns:\n",
    "        check_df['col'].append(col)\n",
    "        check_df['dtype'].append(df[col].dtype)\n",
    "        check_df['nan_prc'].append(df[col].isna().sum()/n_rows)\n",
    "    \n",
    "    check_df = pd.DataFrame(check_df)\n",
    "    print(\"\\n\")\n",
    "    print(f\"dataframe n_rows {n_rows}\")\n",
    "    print(check_df)\n",
    "    print(\"\\n\")\n",
    "    \n",
    "def check_items(items):\n",
    "    unique_items = items.unique()\n",
    "    print('items.min()', items.min())\n",
    "    print('items.max()', items.max())\n",
    "    \n",
    "def one_hot_encoding(df, index_col):    \n",
    "    encoder = OneHotEncoder()\n",
    "    columns = list(df.columns)\n",
    "    columns.remove(index_col)\n",
    "    one_hot_concat_df = pd.DataFrame(df[index_col].values, columns=[index_col])\n",
    "    for col in columns:\n",
    "        dummy_columns = [f'{col}_[{x}]' for x in list(df[col].unique())]\n",
    "        dummy_values  = encoder.fit_transform(df[col].values.reshape(-1,1)).toarray()\n",
    "        one_hot_df    = pd.DataFrame(dummy_values, columns=dummy_columns)        \n",
    "        one_hot_concat_df = pd.concat([one_hot_concat_df, one_hot_df], axis=1)\n",
    "    return one_hot_concat_df\n",
    "\n",
    "def nonzero_indexes_by_row(M):\n",
    "    return [np.nonzero(M[row,:])[0] for row in range(len(M))]\n",
    "\n",
    "def numpy_balance(*arrs):\n",
    "    \"\"\"\n",
    "    Fast NumPy implementation of balance function.\n",
    "    The function creates all the interactions between\n",
    "    the NumPy arrays provided.\n",
    "        Parameters\n",
    "        ----------\n",
    "        arrs: NumPy arrays\n",
    "        Returns\n",
    "        -------\n",
    "        out: NumPy array\n",
    "    \"\"\"\n",
    "    N = len(arrs)\n",
    "    out =  np.transpose(np.meshgrid(*arrs, indexing='ij'),\n",
    "                        np.roll(np.arange(N + 1), -1)).reshape(-1, N)\n",
    "    return out\n",
    "\n",
    "def numpy_ffill(arr):\n",
    "    \"\"\"\n",
    "    Fast NumPy implementation of forwardfill function.\n",
    "    The function fills missing/nan values with the \n",
    "    following value in the array.\n",
    "        Parameters\n",
    "        ----------\n",
    "        arr: NumPy array    \n",
    "        Returns\n",
    "        -------\n",
    "        arr: NumPy array\n",
    "    \"\"\"\n",
    "    mask = np.isnan(arr)\n",
    "    idx = np.where(~mask, np.arange(mask.shape[1]), 0)\n",
    "    np.maximum.accumulate(idx, axis=1, out=idx)\n",
    "    out = arr[np.arange(idx.shape[0])[:,None], idx]\n",
    "    return out\n",
    "\n",
    "def numpy_bfill(arr):\n",
    "    \"\"\"\n",
    "    Fast NumPy implementation of backfill function.\n",
    "    The function fills missing/nan values with the \n",
    "    previous value in the array.\n",
    "        Parameters\n",
    "        ----------\n",
    "        arr: NumPy array    \n",
    "        Returns\n",
    "        -------\n",
    "        arr: NumPy array\n",
    "    \"\"\"    \n",
    "    mask = np.isnan(arr)\n",
    "    idx = np.where(~mask, np.arange(mask.shape[1]), mask.shape[1] - 1)\n",
    "    idx = np.minimum.accumulate(idx[:, ::-1], axis=1)[:, ::-1]\n",
    "    out = arr[np.arange(idx.shape[0])[:,None], idx]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def distance_to_holiday(holiday_dates, dates):\n",
    "    # Get holidays around dates\n",
    "    dates = pd.DatetimeIndex(dates)\n",
    "    dates_np = np.array(dates).astype('datetime64[D]')\n",
    "    holiday_dates_np = np.array(pd.DatetimeIndex(holiday_dates)).astype('datetime64[D]')\n",
    "\n",
    "    # Compute day distance to holiday\n",
    "    distance = np.expand_dims(dates_np, axis=1) - np.expand_dims(holiday_dates_np, axis=0)\n",
    "    distance = np.abs(distance)\n",
    "    distance = np.min(distance, axis=1)\n",
    "    \n",
    "    # Convert to float\n",
    "    distance = distance.astype(float)\n",
    "    distance = distance * (distance>0)\n",
    "    \n",
    "    # Fix start and end of date range\n",
    "    # TODO: Think better way of fixing absence of holiday\n",
    "    # It seems that the holidays dataframe has missing holidays\n",
    "    distance[distance>183] = 365 - distance[distance>183]\n",
    "    distance = np.abs(distance)\n",
    "    distance[distance>183] = 365 - distance[distance>183]\n",
    "    distance = np.abs(distance)\n",
    "    distance[distance>183] = 365 - distance[distance>183]\n",
    "    distance = np.abs(distance)\n",
    "    distance[distance>183] = 365 - distance[distance>183]    \n",
    "    \n",
    "    # Scale\n",
    "    distance = (distance/183) - 0.5\n",
    "    # distance = (distance == 0)\n",
    "    return distance\n",
    "\n",
    "def make_holidays_distance_df(holidays_df, dates):\n",
    "    #Make dataframe of distance in days to holidays\n",
    "    #for holiday dates and date range\n",
    "    distance_dict = {'date': dates}\n",
    "    for holiday in holidays_df.description.unique():\n",
    "        holiday_dates = holidays_df[holidays_df.description==holiday]['date']\n",
    "        holiday_dates = holiday_dates.tolist()\n",
    "        \n",
    "        holiday_str = f'dist2_[{holiday}]'\n",
    "        distance_dict[holiday_str] = distance_to_holiday(holiday_dates, dates)\n",
    "\n",
    "    holidays_distance_df = pd.DataFrame(distance_dict)\n",
    "    return holidays_distance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"cell-2\"></a>\n",
    "## 2. Favorita Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@dataclass \n",
    "class Favorita(TimeSeriesDataclass):\n",
    "    # original data available from Kaggle directly\n",
    "    # pip install kaggle --upgrade\n",
    "    # kaggle competitions download -c favorita-grocery-sales-forecasting\n",
    "    source_url = 'https://www.dropbox.com/s/fe4y1hnphb4ykpy/favorita-grocery-sales-forecasting.zip?dl=1'\n",
    "    H = 34\n",
    "    \n",
    "    @staticmethod\n",
    "    def unzip(path):\n",
    "        # Unzip Load, Price, Solar and Wind data\n",
    "        # shutil.register_unpack_format('7zip', ['.7z'], unpack_7zarchive)\n",
    "        files = ['holidays_events.csv.7z', 'items.csv.7z', 'oil.csv.7z', \n",
    "                 'sample_submission.csv.7z', 'stores.csv.7z', \n",
    "                 'test.csv.7z', 'train.csv.7z', 'transactions.csv.7z']\n",
    "        for file in files:\n",
    "            filepath = f'{path}/{file}'\n",
    "            #Archive(filepath).extractall(path)\n",
    "            shutil.unpack_archive(filepath, path)\n",
    "            logger.info(f'Successfully decompressed {filepath}')\n",
    "    \n",
    "    @staticmethod            \n",
    "    def download_raw(directory, source_url, decompress) -> None:\n",
    "        if isinstance(directory, str):\n",
    "            directory = Path(directory)\n",
    "        directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        filename = source_url.split('/')[-1]\n",
    "        filepath = Path(f'{directory}/{filename}')\n",
    "\n",
    "        # Streaming, so we can iterate over the response.\n",
    "        headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "        r = requests.get(source_url, stream=True, headers=headers)\n",
    "        # Total size in bytes.\n",
    "        total_size = int(r.headers.get('content-length', 0))\n",
    "        block_size = 1024 #1 Kibibyte\n",
    "\n",
    "        t = tqdm.tqdm(total=total_size, unit='iB', unit_scale=True)\n",
    "        with open(filepath, 'wb') as f:\n",
    "            for data in r.iter_content(block_size):\n",
    "                t.update(len(data))\n",
    "                f.write(data)\n",
    "                f.flush()\n",
    "        t.close()\n",
    "\n",
    "        if total_size != 0 and t.n != total_size:\n",
    "            logger.error('ERROR, something went wrong downloading data')\n",
    "\n",
    "        size = filepath.stat().st_size\n",
    "        logger.info(f'Successfully downloaded {filename}, {size}, bytes.')\n",
    "\n",
    "        if decompress:\n",
    "            if '.zip' in filepath.suffix:\n",
    "                logger.info('Decompressing zip file...')\n",
    "                with zipfile.ZipFile(filepath, 'r') as zip_ref:\n",
    "                    zip_ref.extractall(directory)\n",
    "            else:\n",
    "                from patoolib import extract_archive\n",
    "                extract_archive(filepath, outdir=directory)\n",
    "            logger.info(f'Successfully decompressed {filepath}')\n",
    "        \n",
    "    @staticmethod\n",
    "    def download(directory: str) -> None:\n",
    "        \"\"\"Downloads Favorita Competition Dataset.\"\"\"\n",
    "        if not os.path.exists(directory):\n",
    "            Favorita.download_raw(directory=directory, \n",
    "                                  source_url=Favorita.source_url,\n",
    "                                  decompress=True)\n",
    "        if not os.path.exists(f'{directory}/train.csv'):\n",
    "            Favorita.unzip(directory)\n",
    "            \n",
    "    @staticmethod\n",
    "    def read_raw(directory):\n",
    "        # Download Favorita Kaggle competition and unzip\n",
    "        Favorita.download(directory)\n",
    "\n",
    "        # We avoid memory-intensive task of infering dtypes\n",
    "        dtypes_dict = dict(id='int32',\n",
    "                           date='str',\n",
    "                           item_nbr='int32',\n",
    "                           store_nbr='int8',     # there are only 54 stores\n",
    "                           unit_sales='float64', # values beyond are f32 outliers\n",
    "                           onpromotion='float64')\n",
    "\n",
    "        # We read once from csv then from feather (much faster)\n",
    "        if not os.path.exists(f'{directory}/train.feather'):\n",
    "            train_df = pd.read_csv(f'{directory}/train.csv',\n",
    "                                  dtype=dtypes_dict,\n",
    "                                  parse_dates=['date'])\n",
    "            del train_df['id']\n",
    "            train_df.reset_index(drop=True, inplace=True)\n",
    "            train_df.to_feather(f'{directory}/train.feather')\n",
    "            print(\"saved train.csv to train.feather for fast access\")\n",
    "\n",
    "        # Test is avoided because y_true is unavailable\n",
    "        items        = pd.read_csv(f'{directory}/items.csv')\n",
    "        store_info   = pd.read_csv(f'{directory}/stores.csv')        \n",
    "        temporal     = pd.read_feather(f'{directory}/train.feather')\n",
    "        test         = pd.read_csv(f'{directory}/test.csv', parse_dates=['date'])\n",
    "        oil          = pd.read_csv(f'{directory}/oil.csv', parse_dates=['date'])\n",
    "        holidays     = pd.read_csv(f'{directory}/holidays_events.csv', parse_dates=['date'])\n",
    "        transactions = pd.read_csv(f'{directory}/transactions.csv', parse_dates=['date'])\n",
    "        \n",
    "        temporal['open'] = 1\n",
    "        temporal['open'] = temporal['open'].astype('float32')      \n",
    "        \n",
    "        return temporal, oil, items, store_info, holidays, transactions, test\n",
    "    \n",
    "    @staticmethod\n",
    "    def preprocess_data(directory: str, sample_size: int=100, verbose=False):\n",
    "        \n",
    "        with CodeTimer('Read data             ', verbose):\n",
    "            # Read raw data and create ids for later use\n",
    "            temporal, oil, items, store_info, holidays, transactions, test\\\n",
    "                           = Favorita.read_raw(directory=directory)\n",
    "            \n",
    "            # Hierarchically Regularized Deep Forecasting, \n",
    "            # https://arxiv.org/pdf/2106.07630.pdf\n",
    "            # reported 1687 vs 1688 days (missing one time stamp)\n",
    "            # date_range = pd.date_range(start_date, end_date, freq='D')\n",
    "            # print('len(date_range)', len(date_range))\n",
    "\n",
    "            temporal_dates  = temporal['date'].unique() # 1684 days\n",
    "            start_date      = min(temporal_dates)\n",
    "            end_date        = max(temporal_dates)\n",
    "            \n",
    "            # Extra H observations\n",
    "            # for final H predictions (outside train)\n",
    "            end_date = pd.Timestamp(end_date)\n",
    "            end_date = np.datetime64(end_date + timedelta(days=34))\n",
    "            #end_date = np.datetime64('2017-08-31')  # Kaggle's test last date\n",
    "            \n",
    "            catalog_items   = set(items['item_nbr'].unique())\n",
    "            catalog_stores  = set(store_info['store_nbr'].unique())\n",
    "            catalog_dates   = pd.date_range(start=start_date, \n",
    "                                            end=end_date, freq='D')\n",
    "            catalog_dates   = set(catalog_dates.values.astype('datetime64[ns]'))\n",
    "\n",
    "            temporal_dates  = set(temporal_dates)\n",
    "            temporal_items  = set(temporal['item_nbr'].unique())\n",
    "            temporal_stores = set(temporal['store_nbr'].unique())\n",
    "\n",
    "            DATES  = list(catalog_dates)\n",
    "            ITEMS  = list(catalog_items.intersection(temporal_items))\n",
    "            STORES = list(catalog_stores.intersection(temporal_stores))\n",
    "\n",
    "            ITEMS2 = set(test.item_nbr).intersection(set(ITEMS))\n",
    "        \n",
    "        with CodeTimer('Filter data           ', verbose):\n",
    "            # Filter data for efficient experiments\n",
    "            ITEMS = ITEMS[:sample_size]\n",
    "            \n",
    "            # Sort indexes\n",
    "            ITEMS.sort()\n",
    "            STORES.sort()\n",
    "            DATES.sort()\n",
    "\n",
    "            oil          = oil[(oil['date'] >= start_date) & (oil['date'] < end_date)]\n",
    "            items        = items[items.item_nbr.isin(ITEMS)]\n",
    "            store_info   = store_info[store_info.store_nbr.isin(STORES)]\n",
    "            holidays     = holidays[(holidays['date'] >= start_date)\\\n",
    "                                    & (holidays['date'] < end_date)]\n",
    "            transactions = transactions[(transactions['date'] >= start_date)\\\n",
    "                                        & (transactions['date'] < end_date)]\n",
    "            transactions = transactions[transactions.store_nbr.isin(STORES)]\n",
    "\n",
    "            temporal     = temporal[temporal.item_nbr.isin(ITEMS)]\n",
    "            temporal     = temporal[temporal.store_nbr.isin(STORES)]\n",
    "            \n",
    "        with CodeTimer('Sort/Balance data     ', verbose):\n",
    "            # new sorted by hierarchy store_nbr for R benchmarks\n",
    "            store_info = store_info.sort_values(by=['state', 'city', 'store_nbr'])\n",
    "            store_info['new_store_nbr'] = np.arange(len(store_info))\n",
    "\n",
    "            # Share the new store id with temporal and transactions data\n",
    "            new_store_nbrs = store_info[['store_nbr', 'new_store_nbr']]\n",
    "            transactions   = transactions.merge(new_store_nbrs, \n",
    "                                                on=['store_nbr'], how='left')\n",
    "            temporal       = temporal.merge(new_store_nbrs, \n",
    "                                            on=['store_nbr'], how='left')\n",
    "\n",
    "            # Overwrite the store_nbr ids\n",
    "            del temporal['store_nbr']\n",
    "            del transactions['store_nbr']\n",
    "            del store_info['store_nbr']\n",
    "            \n",
    "            temporal['store_nbr']     = temporal['new_store_nbr']\n",
    "            transactions['store_nbr'] = transactions['new_store_nbr']\n",
    "            store_info['store_nbr']   = store_info['new_store_nbr']\n",
    "            del temporal['new_store_nbr']\n",
    "            del transactions['new_store_nbr']\n",
    "            del store_info['new_store_nbr']\n",
    "\n",
    "            # Final Sort\n",
    "            temporal     = temporal.sort_values(by=['item_nbr','store_nbr','date'])\n",
    "            oil          = oil.sort_values(by=['date'])\n",
    "            items        = items.sort_values(by=['item_nbr'])\n",
    "            store_info   = store_info.sort_values(by=['store_nbr'])\n",
    "            transactions = transactions.sort_values(by=['store_nbr','date'])\n",
    "\n",
    "            n_items  = len(ITEMS)\n",
    "            n_stores = len(STORES)\n",
    "            n_dates  = len(DATES)\n",
    "            \n",
    "            # Create balanced item x store interaction\n",
    "            balanced_prod = numpy_balance(ITEMS, STORES)\n",
    "            item_store_df = pd.DataFrame(balanced_prod, columns=['item_nbr', 'store_nbr'])\n",
    "            traj_nbrs     = np.arange(len(item_store_df))\n",
    "            item_store_df['traj_nbr'] = np.arange(len(item_store_df))\n",
    "\n",
    "            # Create dummy variable for original vs series introduced by\n",
    "            # balance procedure and the main traj_nbr index\n",
    "            idxs = temporal[['item_nbr', 'store_nbr']].values\n",
    "            unique_idxs = np.unique(idxs, axis = 0)\n",
    "\n",
    "            original_series_df = pd.DataFrame(unique_idxs, \n",
    "                                              columns=['item_nbr', 'store_nbr'])\n",
    "            original_series_df['is_original'] = 1\n",
    "            item_store_df = item_store_df.merge(original_series_df, \n",
    "                                                on=['item_nbr', 'store_nbr'], how='left')\n",
    "\n",
    "            item_store_df['is_original'] = item_store_df['is_original'].fillna(0)\n",
    "            \n",
    "        with CodeTimer('Create H.  constraints', verbose):\n",
    "            # Regional Static Variables\n",
    "            store_info['country'] = 'Ecuador'\n",
    "            H_df = store_info[['store_nbr', 'country', 'state', 'city']]\n",
    "\n",
    "            # Geographic Hierarchical constraints matrix \n",
    "            # (on stores level, items x stores * items x stores does not scale)\n",
    "            # Hsum : 54 stores --> 39 (country+states+cities)\n",
    "            Hencoded = one_hot_encoding(df=H_df, index_col='store_nbr')\n",
    "            Hsum = Hencoded.values[:, 1:] # Eliminate stores index\n",
    "            H = np.concatenate((Hsum.T, np.eye(len(Hsum))), axis=0)\n",
    "            \n",
    "        #---------------------------- Temporal Bottom Features ------------------------------#\n",
    "        # Creation of temporal bottom level (item x store x date) data based on:\n",
    "        # 1. balance item x store sales \n",
    "        # 2. ffill and zfill data\n",
    "        \n",
    "        with CodeTimer('Create temporal_bottom', verbose):\n",
    "            #-------------------- with CodeTimer('Temporal Balance') --------------------#\n",
    "            # Fast numpy balance\n",
    "            traj_nbrs     = item_store_df['traj_nbr'].values\n",
    "            balanced_prod = numpy_balance(traj_nbrs, DATES)\n",
    "            balanced_df   = pd.DataFrame(balanced_prod, columns=['traj_nbr', 'date'])\n",
    "            balanced_df['date'] = balanced_df['date'].astype(temporal['date'].dtype)\n",
    "\n",
    "            zfill_cols  = ['unit_sales'] #, 'open'] # OPEN VARIABLE FROM TFT-Google IS BAAAD\n",
    "            ffill_cols  = ['onpromotion']\n",
    "            filter_cols = ['item_nbr', 'store_nbr', 'date']\n",
    "            filter_cols = filter_cols + zfill_cols + ffill_cols\n",
    "            temporal_df = temporal.filter(items=filter_cols)\n",
    "\n",
    "            #-------------------- with CodeTimer('Temporal Merge'): --------------------#\n",
    "            # Two stage merge with balanced data\n",
    "            item_store_df = item_store_df[['traj_nbr', 'item_nbr', 'store_nbr', 'is_original']]\n",
    "            item_store_df.set_index(['traj_nbr'], inplace=True)\n",
    "            balanced_df.set_index(['traj_nbr'], inplace=True)\n",
    "            balanced_df = balanced_df.merge(item_store_df, how='left', \n",
    "                                            left_on=['traj_nbr'],\n",
    "                                            right_index=True).reset_index()\n",
    "            #check_nans(balanced_df)\n",
    "            item_store_df = item_store_df.reset_index()        \n",
    "\n",
    "            temporal_df.set_index(['item_nbr', 'store_nbr', 'date'], inplace=True)\n",
    "            balanced_df.set_index(['item_nbr', 'store_nbr', 'date'], inplace=True)\n",
    "            balanced_df = balanced_df.merge(temporal_df, how='left', \n",
    "                                            left_on=['item_nbr', 'store_nbr', 'date'],\n",
    "                                            right_index=True).reset_index()\n",
    "            #check_nans(balanced_df)\n",
    "            del temporal_df, balanced_prod\n",
    "            gc.collect()\n",
    "\n",
    "            #-------------------- with CodeTimer('ZFill Data'): --------------------#\n",
    "            for col in zfill_cols:\n",
    "                balanced_df[col] = balanced_df[col].fillna(0)\n",
    "            #check_nans(balanced_df)\n",
    "\n",
    "            #-------------------- with CodeTimer('FFill Data'): --------------------#\n",
    "            for col in ffill_cols:\n",
    "                # Fast numpy vectorized ffill, requires balanced dataframe\n",
    "                col_values = balanced_df[col].astype('float32').values\n",
    "                col_values = col_values.reshape(n_items * n_stores, n_dates)\n",
    "                col_values = numpy_ffill(col_values)\n",
    "                col_values = numpy_bfill(col_values)\n",
    "                balanced_df[col] = col_values.flatten()\n",
    "                balanced_df[col] = balanced_df[col].fillna(0)\n",
    "            \n",
    "            #---------------- with CodeTimer('items_prob_bottom'): -----------------#\n",
    "            # TODO: 'items_prob_bottom', for weighted loss -> Kaggle submission\n",
    "            # TODO:  consider to keep track of bfill/ffill created observations\n",
    "            balanced_df['sample_mask'] = 1\n",
    "                \n",
    "            temporal_bottom = balanced_df\n",
    "        \n",
    "        #----------------------------- Static Bottom Features -------------------------------#\n",
    "        # Creation of static item store level data based on:\n",
    "        # 1. Precomputed level/spread will model to \"level\" predictions\n",
    "        # 2. Dummy variables from the geographic hierarchy\n",
    "        \n",
    "        with CodeTimer('Create static_bottom  ', verbose):\n",
    "\n",
    "            #assert static_variant in ['partial', 'statistics']\n",
    "            \n",
    "            #if static_variant=='partial':\n",
    "            static_bottom_columns = Hencoded.columns\n",
    "            static_bottom = np.expand_dims(Hencoded, axis=0)\n",
    "            static_bottom = np.repeat(static_bottom, repeats=len(ITEMS), axis=0)\n",
    "            static_bottom = static_bottom.reshape(-1, static_bottom.shape[-1])\n",
    "            static_bottom = pd.DataFrame(static_bottom, columns=static_bottom_columns)\n",
    "                            \n",
    "            #if static_variant=='statistics':\n",
    "            Y_bottom    = temporal_bottom['unit_sales'].values\n",
    "            Y_bottom    = Y_bottom.reshape((n_items*n_stores,n_dates))\n",
    "            Y_available = Y_bottom[:,-34-34-60:-34-34] # skip test/validation\n",
    "\n",
    "            # [n_regions*n_purpose, n_time] --> [n_regions*n_purpose]\n",
    "            level  = np.median(Y_available, axis=1)\n",
    "            spread = np.median(np.abs(Y_available - level[:,None]), axis=1)\n",
    "            \n",
    "            static_bottom['level'] = level\n",
    "            static_bottom['spread'] = spread\n",
    "            \n",
    "            #static_bottom = np.concatenate([level[:,None], spread[:,None]], axis=1)\n",
    "            #static_bottom = pd.DataFrame.from_records(static_bottom,\n",
    "            #                                          columns=['level', 'spread'])\n",
    "\n",
    "            static_items_bottom  = np.repeat(np.array(ITEMS), len(STORES))\n",
    "            static_stores_bottom = np.tile(np.array(STORES), len(ITEMS))\n",
    "            static_bottom['item_nbr']  = static_items_bottom\n",
    "            static_bottom['store_nbr'] = static_stores_bottom\n",
    "\n",
    "            static_bottom = static_bottom.merge(item_store_df, \n",
    "                                                on=['item_nbr', 'store_nbr'], how='left')\n",
    "            \n",
    "            level_rep = np.repeat(level, n_dates)\n",
    "            temporal_bottom['is_original'] = temporal_bottom['is_original'] * level_rep\n",
    "            \n",
    "        #------------------------------- Static Agg Features --------------------------------#\n",
    "        # Creation of static item national level data based on:\n",
    "        # 1. Item characteristics, groupings dummies\n",
    "            \n",
    "        with CodeTimer('Create static_agg     ', verbose):\n",
    "            static_agg = one_hot_encoding(df=items, index_col='item_nbr')\n",
    "\n",
    "            # Add weight for loss perishable 1.25 and normal 1.0\n",
    "            # https://www.kaggle.com/c/favorita-grocery-sales-forecasting/overview/evaluation\n",
    "            static_agg['prob']    = np.ones(len(static_agg)) +\\\n",
    "                                    0.25 * static_agg[\"perishable_[1]\"]\n",
    "            static_bottom['prob'] = np.repeat(static_agg['prob'].values, len(STORES))\n",
    "\n",
    "            # Transform into category for LGBM\n",
    "            static_cat_agg = items\n",
    "\n",
    "            static_cat_agg = static_cat_agg.set_index(['item_nbr'])\n",
    "            for feature in static_cat_agg.columns:\n",
    "                static_cat_agg[feature] = pd.Series(static_cat_agg[feature], dtype=\"category\")\n",
    "\n",
    "            static_cat_agg.reset_index(inplace=True)            \n",
    "                \n",
    "        #------------------------------ Temporal Agg Features -------------------------------#\n",
    "        # Creation of temporal agg level (item x store x date) data based on:\n",
    "        # 1. balance item x store sales\n",
    "        # 2. ffill and zfill data\n",
    "        # Curate aggregated data from oil, calendar and transactions\n",
    "        \n",
    "        with CodeTimer('Create temporal_agg   ', verbose):\n",
    "            normalizer  = preprocessing.StandardScaler()\n",
    "\n",
    "            #-------------------- with CodeTimer('1. Temporal'): --------------------#    \n",
    "            # National sales per item\n",
    "            balanced_prod = numpy_balance(ITEMS, DATES)\n",
    "            balanced_df = pd.DataFrame(balanced_prod, columns=['item_nbr', 'date'])\n",
    "            balanced_df['item_nbr'] = balanced_df['item_nbr'].astype(ITEMS[0].dtype)\n",
    "            balanced_df['date'] = balanced_df['date'].astype(DATES[0].dtype)\n",
    "\n",
    "            # collapse store dimension -> national\n",
    "            unit_sales  = temporal_bottom[['unit_sales']].values\n",
    "            unit_sales  = unit_sales.reshape(len(ITEMS), len(STORES), len(DATES), 1)\n",
    "            unit_sales  = np.sum(unit_sales, axis=1)\n",
    "            balanced_df['unit_sales'] = unit_sales.reshape(-1, 1)\n",
    "\n",
    "            temporal_agg = balanced_df\n",
    "\n",
    "            #-------------------- with CodeTimer('2. Oil'): --------------------#\n",
    "            balanced_df = pd.DataFrame({'date': DATES})\n",
    "            balanced_df = balanced_df.merge(oil, on='date', how='left')\n",
    "            #check_nans(balanced_df)\n",
    "\n",
    "            balanced_df['dcoilwtico'] = balanced_df['dcoilwtico'].fillna(method='ffill')\n",
    "            balanced_df['dcoilwtico'] = balanced_df['dcoilwtico'].fillna(method='bfill')\n",
    "\n",
    "            #check_nans(balanced_df)\n",
    "            oil_agg = balanced_df\n",
    "            oil_agg['dcoilwtico'] = normalizer.fit_transform(oil_agg['dcoilwtico'].values[:,None])\n",
    "\n",
    "            #-------------------- with CodeTimer('3. Holidays'): --------------------#\n",
    "            # Calendar Variables\n",
    "            calendar = pd.DataFrame({'date': DATES})\n",
    "            calendar['day_of_week']  = calendar['date'].dt.dayofweek\n",
    "            calendar['day_of_month'] = calendar['date'].dt.day\n",
    "            calendar['month']        = calendar['date'].dt.month\n",
    "\n",
    "            calendar['day_of_week']  = calendar['day_of_week'].astype('float64')\n",
    "            calendar['day_of_month'] = calendar['day_of_month'].astype('float64')\n",
    "            calendar['month']        = calendar['month'].astype('float64')\n",
    "\n",
    "            calendar['day_of_week']  = normalizer.fit_transform(calendar['day_of_week'].values[:,None])\n",
    "            calendar['day_of_month'] = normalizer.fit_transform(calendar['day_of_month'].values[:,None])\n",
    "            calendar['month'] = normalizer.fit_transform(calendar['month'].values[:,None])\n",
    "            \n",
    "            # Add weekly seasonality to bottom\n",
    "            #temporal_bottom = calendar[['date','day_of_week']].merge(temporal_bottom,\n",
    "            #                                                         on=['date'], how='left')\n",
    "\n",
    "            # Holiday variables\n",
    "            hdays = holidays[holidays['transferred']==False].copy()\n",
    "            hdays.rename(columns={'type': 'holiday_type'}, inplace=True)\n",
    "\n",
    "            national_hdays = hdays[hdays['locale']=='National']    \n",
    "            national_hdays = national_hdays[national_hdays.holiday_type.isin(['Holiday', 'Transfer'])]\n",
    "            national_hdays = make_holidays_distance_df(national_hdays, DATES)\n",
    "\n",
    "            calendar_agg   = calendar.merge(national_hdays, on=['date'], how='left')\n",
    "\n",
    "            #-------------------- with CodeTimer('4. Transactions'): --------------------#\n",
    "            # Fast numpy balance\n",
    "            balanced_prod       = numpy_balance(STORES, DATES)\n",
    "            balanced_df         = pd.DataFrame(balanced_prod, columns=['store_nbr', 'date'])\n",
    "            balanced_df['date'] = balanced_df['date'].astype('datetime64[ns]')\n",
    "\n",
    "            # 'Transactions Merge'\n",
    "            # Merge with balanced data\n",
    "            transactions.set_index(['store_nbr', 'date'], inplace=True)\n",
    "            balanced_df.set_index(['store_nbr', 'date'], inplace=True)\n",
    "            transactions = balanced_df.merge(transactions, how='left',\n",
    "                                             left_on=['store_nbr', 'date'],\n",
    "                                             right_index=True).reset_index()\n",
    "            #check_nans(transactions)\n",
    "\n",
    "            transactions = transactions.sort_values(by=['store_nbr', 'date'])\n",
    "            trans_values = transactions.transactions.values\n",
    "\n",
    "            trans_values = trans_values.reshape(n_stores, n_dates)\n",
    "            trans_values = numpy_ffill(trans_values)\n",
    "            trans_values = np.nan_to_num(trans_values)\n",
    "            trans_values = trans_values.T\n",
    "            trans_values = trans_values > 0 \n",
    "            trans_columns = [f'transactions_store_[{x}]' for x in STORES]\n",
    "\n",
    "            transactions_agg = pd.DataFrame(trans_values, columns=trans_columns)\n",
    "            transactions_agg['date'] = DATES\n",
    "\n",
    "            del balanced_prod, balanced_df\n",
    "            gc.collect()\n",
    "\n",
    "            #-------------------- with CodeTimer('5. temporal_agg'): --------------------#\n",
    "            #print(\"1. temporal_agg.shape\", temporal_agg.shape)\n",
    "            #print(\"2. oil_agg.shape\", oil_agg.shape)\n",
    "            #print(\"3. calendar_agg.shape\", calendar_agg.shape)\n",
    "            #print(\"4. transactions_agg.shape\", transactions_agg.shape)\n",
    "            #print(\"\\n\\n\")\n",
    "            #print(\"1. temporal_agg.dtypes \\n\", temporal_agg.dtypes, \"\\n\")\n",
    "            #print(\"2. oil_agg.dtypes \\n\", oil_agg.dtypes, \"\\n\")\n",
    "            #print(\"3. calendar_agg.dtypes \\n\", calendar_agg.dtypes, \"\\n\")\n",
    "            #print(\"4. transactions_agg.dtypes \\n\", transactions_agg.dtypes, \"\\n\")\n",
    "\n",
    "            temporal_agg.set_index(['date'], inplace=True)\n",
    "            oil_agg.set_index(['date'], inplace=True)\n",
    "            calendar_agg.set_index(['date'], inplace=True)\n",
    "            transactions_agg.set_index(['date'], inplace=True)\n",
    "\n",
    "            # Compile national aggregated data\n",
    "            temporal_agg = temporal_agg.merge(oil_agg, how='left', left_on=['date'],\n",
    "                                        right_index=True)#.reset_index()\n",
    "            temporal_agg = temporal_agg.merge(calendar_agg, how='left', left_on=['date'],\n",
    "                                        right_index=True)#.reset_index()\n",
    "            temporal_agg = temporal_agg.merge(transactions_agg, how='left', left_on=['date'],\n",
    "                                        right_index=True)#.reset_index()\n",
    "            temporal_agg = temporal_agg.reset_index()\n",
    "            \n",
    "            calendar_agg2 = calendar_agg[['day_of_week', 'day_of_month', \n",
    "                                          'dist2_[Primer dia del ano]', \n",
    "                                          'dist2_[Navidad]']]\n",
    "            temporal_bottom = temporal_bottom.merge(calendar_agg2, how='left', left_on=['date'],\n",
    "                                        right_index=True)\n",
    "            temporal_bottom = temporal_bottom.reset_index()\n",
    "\n",
    "        # Checking dtypes correctness\n",
    "        #if verbose:\n",
    "            #print('1. static_agg.dtypes \\n', static_agg.dtypes)\n",
    "            #print('2. temporal_agg.dtypes \\n', temporal_agg.dtypes)\n",
    "            #print('3. static_bottom.dtypes \\n', static_bottom.dtypes)\n",
    "            #print('4. temporal_bottom.dtypes \\n', temporal_bottom.dtypes)\n",
    "        \n",
    "        # Save feathers for fast access\n",
    "        H_df.to_csv(f'{directory}/H_df.csv', index=False)\n",
    "        static_agg.to_feather(f'{directory}/{str(sample_size)}_static_agg.feather')\n",
    "        temporal_agg.to_feather(f'{directory}/{str(sample_size)}_temporal_agg.feather')\n",
    "        static_bottom.to_feather(f'{directory}/{str(sample_size)}_static_bottom.feather')\n",
    "        temporal_bottom.to_feather(f'{directory}/{str(sample_size)}_temporal_bottom.feather')\n",
    "        np.save(f'{directory}/{sample_size}_hier_constraints.npy', H)\n",
    "        #item_store_df.to_csv(f'{DATA_FOLDER}/{SAMPLE_SIZE}_item_store.csv')\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_process(directory=str, sample_size=int, verbose=False) -> dict:\n",
    "        \n",
    "        with CodeTimer('Reading data           ', verbose):\n",
    "            # Data is assumed to be ordered\n",
    "            static_agg      = pd.read_feather(f'{directory}/{sample_size}_static_agg.feather')\n",
    "            temporal_agg    = pd.read_feather(f'{directory}/{sample_size}_temporal_agg.feather')\n",
    "\n",
    "            static_bottom   = pd.read_feather(f'{directory}/{sample_size}_static_bottom.feather')\n",
    "            temporal_bottom = pd.read_feather(f'{directory}/{sample_size}_temporal_bottom.feather')\n",
    "\n",
    "            # Extract datasets dimensions for later use\n",
    "            dates      = pd.to_datetime(temporal_agg.date.unique())\n",
    "            unique_ids = static_bottom.traj_nbr.values\n",
    "            item_ids   = static_bottom.item_nbr.values\n",
    "            store_ids  = static_bottom.store_nbr.values\n",
    "            n_time     = len(dates)                            # 1688\n",
    "            n_items    = len(static_bottom.item_nbr.unique())  # sample_size=100\n",
    "            n_stores   = len(static_bottom.store_nbr.unique()) # 54\n",
    "            \n",
    "        #-------------------------------------- S/X/Y_agg    --------------------------------------#        \n",
    "        with CodeTimer('Process temporal_agg   ', verbose):\n",
    "            # Drop observation indexes and obtain column indexes\n",
    "            temporal_agg.drop(labels=['item_nbr', 'date'], axis=1, inplace=True)\n",
    "            xcols_agg = temporal_agg.columns\n",
    "        \n",
    "            X_agg  = temporal_agg.values            \n",
    "            Y_agg  = temporal_agg['unit_sales'].values\n",
    "\n",
    "            X_agg  = X_agg.reshape((n_items,n_time,temporal_agg.shape[1]))\n",
    "            Y_agg  = Y_agg.reshape((n_items,n_time,1))\n",
    "                        \n",
    "            xcols_hist_agg = list(temporal_agg.columns)\n",
    "                        \n",
    "            xcols_futr_agg = ['dcoilwtico',\n",
    "                              'day_of_week',\n",
    "                              'day_of_month',\n",
    "                              'month',\n",
    "                              'dist2_[Primer dia del ano]',\n",
    "                              'dist2_[Carnaval]',\n",
    "                              'dist2_[Viernes Santo]',\n",
    "                              'dist2_[Dia del Trabajo]',\n",
    "                              'dist2_[Batalla de Pichincha]',\n",
    "                              'dist2_[Primer Grito de Independencia]',\n",
    "                              'dist2_[Traslado Independencia de Guayaquil]',\n",
    "                              'dist2_[Dia de Difuntos]',\n",
    "                              'dist2_[Independencia de Cuenca]',\n",
    "                              'dist2_[Navidad]',\n",
    "                              'dist2_[Independencia de Guayaquil]',\n",
    "                              'dist2_[Traslado Batalla de Pichincha]',\n",
    "                              'dist2_[Traslado Primer Grito de Independencia]',\n",
    "                              'dist2_[Traslado Primer dia del ano]']\n",
    "\n",
    "        with CodeTimer('Process static_agg     ', verbose):\n",
    "            items_prob_agg = static_agg.prob.values\n",
    "            S_agg = static_agg.drop(labels=['item_nbr', 'prob'], axis=1)\n",
    "            scols_agg = S_agg.columns\n",
    "            S_agg = S_agg.values\n",
    "        \n",
    "        del temporal_agg, static_agg\n",
    "        gc.collect()\n",
    "        \n",
    "        #-------------------------------------- S/X/Y --------------------------------------#\n",
    "        with CodeTimer('Process temporal_bottom', verbose):\n",
    "            stores    = static_bottom.store_nbr.unique()\n",
    "            items     = static_bottom.item_nbr.unique()\n",
    "            n_stores  = len(stores)\n",
    "\n",
    "            temporal_bottom.drop(labels=['item_nbr', 'store_nbr', 'traj_nbr', 'date'], \n",
    "                                 axis=1, inplace=True)\n",
    "            n_features = temporal_bottom.shape[1]\n",
    "            xcols      = temporal_bottom.columns\n",
    "            X_stores   = temporal_bottom.values\n",
    "            Y_stores   = temporal_bottom['unit_sales'].values\n",
    "            \n",
    "            # temporal_bottom assumes to be balanced ie len(Y) = n_items*n_stores*n_time\n",
    "            # temporal_bottom assumes to be sorted by n_items, n_zip1, n_zip2, n_time\n",
    "            X  = X_stores.reshape((n_items,n_stores,n_time,n_features))\n",
    "            Y  = Y_stores.reshape((n_items,n_stores,n_time))\n",
    "            \n",
    "            #Reshape to match with the PMM model's inputs\n",
    "            #n_items, n_stores, n_time, n_features -> n_items, n_time, n_stores, n_features\n",
    "            X = np.transpose(X, (0, 2, 1, 3))\n",
    "            Y = np.transpose(Y, (0, 2, 1))\n",
    "            \n",
    "            xcols_hist = ['unit_sales'] # 'open' seems a bad variable\n",
    "            xcols_futr = ['onpromotion', 'is_original']\n",
    "            xcols_futr = xcols_futr + ['day_of_week', 'day_of_month', \n",
    "                                       'dist2_[Primer dia del ano]', \n",
    "                                       'dist2_[Navidad]']\n",
    "        \n",
    "        with CodeTimer('Process static_bottom  ', verbose):\n",
    "            items_bottom      = static_bottom.item_nbr.values\n",
    "            stores_bottom     = static_bottom.store_nbr.values\n",
    "            items_prob_bottom = static_bottom.prob.values\n",
    "            \n",
    "            static_bottom.drop(labels=['item_nbr', 'store_nbr', \n",
    "                                       'traj_nbr', 'is_original', 'prob'], \n",
    "                               axis=1, inplace=True)\n",
    "            scols = static_bottom.columns\n",
    "            \n",
    "            S = static_bottom.values\n",
    "            S = S.reshape(n_items,n_stores, static_bottom.shape[1])\n",
    "        \n",
    "        with CodeTimer('Hier constraints       ', verbose):\n",
    "            # Read hierarchical constraints dataframe\n",
    "            # Create hierarchical aggregation numpy\n",
    "            H_df = pd.read_csv(f'{directory}/H_df.csv')\n",
    "            Hencoded = one_hot_encoding(df=H_df, index_col='store_nbr')\n",
    "            Hsum = Hencoded.values[:, 1:].T # Eliminate stores index\n",
    "            H = np.concatenate((Hsum, np.eye(len(stores))), axis=0)\n",
    "            hier_linked_idxs = nonzero_indexes_by_row(H.T)\n",
    "            \n",
    "            # Aggregate the store dimension into cities, states, national\n",
    "            Y_hier = np.einsum('hij,jk->hik', Y, H.T)\n",
    "            hier_labels = list(Hencoded.columns[1:]) + list(H_df.store_nbr)\n",
    "            hier_labels = np.array(hier_labels)\n",
    "            hier_idxs = [range(93), \n",
    "                         range(0, 1), range(1, 17), \n",
    "                         range(17, 39), range(39, 93)]\n",
    "            \n",
    "            # Flat Bottom values for evaluation\n",
    "            # n_items, n_time, n_group,n_features --> n_items*n_group, n_time, n_features\n",
    "            X_flat  = temporal_bottom.values\n",
    "            Y_flat  = temporal_bottom['unit_sales'].values\n",
    "            \n",
    "            n_features = X_flat.shape[1]\n",
    "            X_flat = X_flat.reshape(n_items*n_stores, n_time, n_features)\n",
    "            Y_flat = Y_flat.reshape(-1,n_time)\n",
    "            \n",
    "        del temporal_bottom, static_bottom\n",
    "        gc.collect()            \n",
    "        \n",
    "        #------------------------------------- Logs and Output ------------------------------------#\n",
    "        \n",
    "        with CodeTimer('Final processing       ', verbose):\n",
    "            # NaiveSeasonal compatibility X_futr indexing\n",
    "            # Consider T0=0, L=8, H=3 and Y=[0,1,2,3,4,5,6,7|,8,9,10,11]\n",
    "            # T0=0 --> [0,1,2,3,4,5,6,7|,8,9,10]\n",
    "            # T0=1 --> [1,2,3,4,5,6,7,8|9,10,11]\n",
    "            # T0=1 uses T0+1 X_futr=[2,3,4,5,6,7,8,9|10,11,12] ???\n",
    "            # Seasonal Naive12 switch to [-1] for Naive1\n",
    "            X = np.concatenate((X, X[:,[-1],:]), axis=1)\n",
    "            X_agg = np.concatenate((X_agg, X_agg[:,[-1],:]), axis=1)\n",
    "            \n",
    "            # Reshape NumPy arrays for Pytorch inputs\n",
    "            # [0,1,2,3][G,T,N,C] -> [0,2,3,1][G,N,C,T]\n",
    "            S_agg = np.float32(S_agg)\n",
    "            Y_agg = np.float32(np.transpose(Y_agg, (0, 2, 1)))\n",
    "            X_agg = np.float32(np.transpose(X_agg, (0, 2, 1)))\n",
    "            \n",
    "            S = np.float32(S)\n",
    "            Y = np.float32(np.transpose(Y, (0, 2, 1)))\n",
    "            X = np.float32(np.transpose(X, (0, 2, 3, 1)))\n",
    "            \n",
    "            Y_hier = np.float32(np.transpose(Y_hier, (0, 2, 1)))\n",
    "            \n",
    "            # Skip NAs from Y data [G,N,T]            \n",
    "            Y = Y[:,:,:-34]\n",
    "            Y_agg = Y_agg[:,:,:-34]\n",
    "            Y_hier = Y_hier[:,:,:-34]\n",
    "        \n",
    "        print('\\n')\n",
    "        print('TODO: add weights as prob for loss?')\n",
    "        print('TODO: do bfill onpromotion rather than ffill (future)')\n",
    "        \n",
    "        # Assert that the variables are contained in the column indexes\n",
    "        assert all(c in list(xcols_agg) for c in xcols_hist_agg)\n",
    "        assert all(c in list(xcols_agg) for c in xcols_futr_agg)\n",
    "        assert all(c in list(xcols) for c in xcols_hist)\n",
    "        assert all(c in list(xcols) for c in xcols_futr)        \n",
    "        \n",
    "        data = {# Bottom data\n",
    "                'S': S, 'X': X, 'Y': Y,\n",
    "                'scols': scols,\n",
    "                'xcols': xcols,\n",
    "                'xcols_hist': xcols_hist,\n",
    "                'xcols_futr': xcols_futr,\n",
    "                'xcols_sample_mask': 'sample_mask', # TODO: 'items_prob_bottom'\n",
    "                'xcols_available_mask': 'is_original', # availability store x item level\n",
    "                # Aggregate data\n",
    "                'S_agg': S_agg, 'X_agg': X_agg, 'Y_agg': Y_agg,\n",
    "                'scols_agg': scols_agg,\n",
    "                'xcols_agg': xcols_agg,\n",
    "                'xcols_hist_agg': xcols_hist_agg,\n",
    "                'xcols_futr_agg': xcols_futr_agg,\n",
    "                'items_prob_agg': items_prob_agg,\n",
    "                # Hierarchical data for evaluation\n",
    "                'Y_flat': Y_flat,\n",
    "                'X_flat': X_flat,\n",
    "                'Y_hier': Y_hier,\n",
    "                'hier_idxs': hier_idxs,\n",
    "                'hier_labels': hier_labels,\n",
    "                'hier_linked_idxs': hier_linked_idxs,\n",
    "                # Shared data\n",
    "                'H': H,\n",
    "                'dates': dates,\n",
    "                'unique_ids': unique_ids,\n",
    "                'items_ids': item_ids,\n",
    "                'store_ids': store_ids}   \n",
    "        \n",
    "        if verbose:\n",
    "            print('\\n')\n",
    "            print(f'Total days {len(dates)}, train {n_time-34-34} validation {34}, test {34}')\n",
    "            print(f'Whole dates: \\t\\t [{min(dates)}, {max(dates)}]')\n",
    "            print(f'Validation dates: \\t '+\\\n",
    "                  f'[{min(dates[n_time-34-34:n_time-34])}, {max(dates[n_time-34-34:n_time-34])}]')\n",
    "            print(f'Test dates: \\t\\t [{min(dates[n_time-34:])}, {max(dates[n_time-34:])}]')\n",
    "            print('\\n')\n",
    "            print(' '*35 +'BOTTOM')\n",
    "            print('S.shape (n_items,n_stores,n_features):        \\t' + str(data['S'].shape))\n",
    "            print('X.shape (n_items,n_stores,n_features,n_time): \\t' + str(data['X'].shape))\n",
    "            print('Y.shape (n_items,n_stores,n_time):            \\t' + str(data['Y'].shape))\n",
    "            #print(f\"scols ({len(scols)}) {data['scols']}\")\n",
    "            #print(f\"xcols ({len(xcols)}) {data['xcols']}\")\n",
    "            #print(f\"xcols_hist ({len(xcols_hist)}) {data['xcols_hist']}\")\n",
    "            #print(f\"xcols_futr ({len(xcols_futr)}) {data['xcols_futr']}\")\n",
    "            print(' '*35 +'AGGREGATE')\n",
    "            print('S_agg.shape (n_items,n_features):         \\t' + str(data['S_agg'].shape))\n",
    "            print('X_agg.shape (n_items,n_features,n_time):  \\t' + str(data['X_agg'].shape))\n",
    "            print('Y_agg.shape (n_items,n_features,n_time):  \\t' + str(data['Y_agg'].shape))\n",
    "            #print(f\"scols_agg ({len(scols_agg)}) {data['scols_agg'][:4]}\")\n",
    "            #print(f\"xcols_agg ({len(xcols_agg)}) {data['xcols_agg']}\")\n",
    "            #print(f\"xcols_hist_agg ({len(xcols_hist_agg)}) {data['xcols_hist_agg']}\")\n",
    "            #print(f\"xcols_futr_agg ({len(xcols_futr_agg)}) {data['xcols_futr_agg']}\")\n",
    "            print(' '*35 +'HIERARCHICAL')\n",
    "            print('Y_hier.shape (n_items,n_stores,n_time):  \\t' + str(data['Y_hier'].shape))\n",
    "            #print('hier_idxs', hier_idxs)\n",
    "            print('overall.shape'+5*'\\t',  data['Y_hier'][:,hier_idxs[0],:].shape)\n",
    "            print('country.shape'+5*'\\t',  data['Y_hier'][:,hier_idxs[1],:].shape)\n",
    "            print('states.shape '+5*'\\t',  data['Y_hier'][:,hier_idxs[2],:].shape)\n",
    "            print('cities.shape '+5*'\\t',  data['Y_hier'][:,hier_idxs[3],:].shape)\n",
    "            print('stores.shape '+5*'\\t',  data['Y_hier'][:,hier_idxs[4],:].shape)\n",
    "            \n",
    "        return data\n",
    "\n",
    "# directory = './data/hierarchical/favorita'\n",
    "# Favorita.preprocess_data(directory=directory, sample_size=100, verbose=True)\n",
    "# data = Favorita.load_process(directory=directory, sample_size=100, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"cell-3\"></a>\n",
    "## 3. Favorita Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Load and Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from neuralforecast.data.datasets.favorita import Favorita\n",
    "\n",
    "# directory = './data/hierarchical/favorita'\n",
    "# Favorita.preprocess_data(directory=directory, sample_size=100, verbose=False)\n",
    "# data = Favorita.load_process(directory=directory, sample_size=100, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Validate data with plots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking Hierarchical aggregation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['hier_labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(num=1, figsize=(6, 15), dpi=80, facecolor='w')\n",
    "# plt.spy(data['H'])\n",
    "# plt.show()\n",
    "# plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking pre-estimated levels seasons S_bottom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dates = data['dates']\n",
    "# X = data['X']\n",
    "# xcols = data['xcols']\n",
    "# item_idx = 10\n",
    "# store_idx = 31\n",
    "\n",
    "# # Target variable (unit sales)\n",
    "# unitsales_idx = xcols.get_loc('unit_sales')\n",
    "\n",
    "# plt.figure(figsize=(12, 1.8))\n",
    "# plt.plot(dates, X[item_idx, store_idx, unitsales_idx, :-1])\n",
    "# plt.ylabel('Unit Sales')\n",
    "# plt.xlabel('Date')\n",
    "# plt.show()\n",
    "# plt.close()\n",
    "\n",
    "# # On promotion\n",
    "# onpromotion_idx = xcols.get_loc('onpromotion')\n",
    "\n",
    "# plt.figure(figsize=(12, 1.8))\n",
    "# plt.plot(dates, X[item_idx, store_idx, onpromotion_idx, :-1])\n",
    "# plt.ylabel('On Promotion \\n [Indicator]')\n",
    "# plt.xlabel('Date')\n",
    "# plt.show()\n",
    "# plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking S/Y/X_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_agg = data['X_agg']\n",
    "# xcols_agg = data['xcols_agg']\n",
    "# dates = data['dates']\n",
    "\n",
    "# # Plot oil temporal exogenous\n",
    "# item_idx =0\n",
    "# oil_idx = xcols_agg.get_loc('dcoilwtico')\n",
    "\n",
    "# plt.figure(figsize=(12, 1.8))\n",
    "# plt.plot(dates, X_agg[item_idx, oil_idx, :-1])\n",
    "# plt.ylabel('Oil Price Index')\n",
    "# plt.xlabel('Date')\n",
    "# plt.show()\n",
    "# plt.close()\n",
    "\n",
    "# # Plot calendar variables\n",
    "# day_of_week_idx = xcols_agg.get_loc('day_of_week')\n",
    "# day_of_month_idx = xcols_agg.get_loc('day_of_month')\n",
    "# month_idx = xcols_agg.get_loc('month')\n",
    "\n",
    "# plt.figure(figsize=(12, 1.8))\n",
    "# plt.plot(dates[-400:], X_agg[item_idx, day_of_week_idx, -401:-1], label='day_of_week')\n",
    "# plt.plot(dates[-400:], X_agg[item_idx, day_of_month_idx, -401:-1], label='day_of_month')\n",
    "# plt.plot(dates[-400:], X_agg[item_idx, month_idx, -401:-1], label='month')\n",
    "# plt.legend(bbox_to_anchor=(1, 1), loc='upper left', ncol=1)\n",
    "# plt.ylabel('Calendar Variables')\n",
    "# plt.xlabel('Date')\n",
    "# plt.show()\n",
    "# plt.close()\n",
    "\n",
    "# # Plot holidays\n",
    "# holidays = ['dist2_[Primer dia del ano]', 'dist2_[Carnaval]', 'dist2_[Viernes Santo]', \n",
    "#             'dist2_[Dia del Trabajo]', 'dist2_[Batalla de Pichincha]', \n",
    "#             'dist2_[Primer Grito de Independencia]', \n",
    "#             'dist2_[Traslado Independencia de Guayaquil]', 'dist2_[Dia de Difuntos]', \n",
    "#             'dist2_[Independencia de Cuenca]', 'dist2_[Navidad]', \n",
    "#             'dist2_[Independencia de Guayaquil]', 'dist2_[Traslado Batalla de Pichincha]', \n",
    "#             'dist2_[Traslado Primer Grito de Independencia]', 'dist2_[Traslado Primer dia del ano]',]\n",
    "\n",
    "# plt.figure(figsize=(12, 1.8))\n",
    "# for holiday in holidays:\n",
    "#     holiday_idx = xcols_agg.get_loc(holiday)\n",
    "#     plt.plot(dates[-400:], X_agg[item_idx, holiday_idx, -401:-1])\n",
    "# plt.ylabel('Holiday Distance \\n [in days]')\n",
    "# plt.xlabel('Date')\n",
    "# plt.show()\n",
    "# plt.close()\n",
    "\n",
    "# # Plot total store transactions\n",
    "# storeA = 'transactions_store_[41]'\n",
    "# storeB = 'transactions_store_[2]'\n",
    "# storeA_idx = xcols_agg.get_loc(storeA)\n",
    "# storeB_idx = xcols_agg.get_loc(storeB)\n",
    "\n",
    "# plt.figure(figsize=(12, 1.8))\n",
    "# plt.plot(X_agg[item_idx, storeA_idx, :-1], label=storeA)\n",
    "# plt.plot(X_agg[item_idx, storeB_idx, :-1], label=storeB)\n",
    "# plt.ylabel('Open/Closed store \\n [Indicator]')\n",
    "# plt.xlabel('Date')\n",
    "# plt.legend(bbox_to_anchor=(1, 1), loc='upper left', ncol=1)\n",
    "# plt.show()\n",
    "# plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking Hierarchically linked data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_hierarchically_linked_data(data, item_idx, store_idx, \n",
    "#                                     n_days=30*5, plot_file=None):\n",
    "    \n",
    "#     # This plots validate the S/X/Y_bottom\n",
    "#     # We check for the sinchronization of features with Y_bottom\n",
    "#     # We do the same for Y_agg, to check correct linkage\n",
    "#     assert store_idx < 94 and store_idx>=0\n",
    "    \n",
    "#     # Hierarchically linked target data and labels\n",
    "#     # [Total, State, City] [0, 1, 2]\n",
    "#     linked_idxs = data['hier_linked_idxs'][store_idx]\n",
    "    \n",
    "#     # Base series and its label idx \\in {0,...,304}\n",
    "#     # n_agg=1+16+22=39\n",
    "#     Y_hier  = data['Y_hier'][item_idx, :,:]\n",
    "#     Y_base  = data['Y_hier'][item_idx, store_idx+39,:]\n",
    "#     Y_base2 = data['Y'][item_idx,store_idx,:]\n",
    "    \n",
    "#     hier_labels = data['hier_labels']\n",
    "#     label       = data['items_ids'][item_idx]\n",
    "        \n",
    "#     x_plot = data['dates'][:] #-12] # Skip future dates\n",
    "    \n",
    "#     fig, axs = plt.subplots(len(linked_idxs), 1, figsize=(12, 9)) #9\n",
    "#     for idx, linked_idx in enumerate(linked_idxs):\n",
    "#         axs[idx].plot(x_plot[-n_days:], Y_hier[linked_idx,-n_days:],\n",
    "#                       color='#628793', linewidth=1.5)\n",
    "#         plot_label = hier_labels[linked_idx]\n",
    "#         plot_label = plot_label.replace('[', '')\n",
    "#         plot_label = plot_label.replace(']', '')\n",
    "#         plot_label = plot_label.replace('_', ': ').title()\n",
    "#         axs[idx].set_ylabel(f'{plot_label}  \\n [unit demand]', fontsize=13)\n",
    "#         axs[idx].tick_params(labelsize=13)\n",
    "        \n",
    "#         if idx < len(linked_idxs)-1:\n",
    "#             axs[idx].set_xticks([])\n",
    "    \n",
    "#     # Checking that Y_bottom and Y_hier match\n",
    "#     axs[3].plot(x_plot[-n_days:], Y_base[-n_days:],\n",
    "#                 color = 'blue', linewidth=1.5, label='base')\n",
    "#     axs[3].plot(x_plot[-n_days:], Y_base2[-n_days:],\n",
    "#                 color = 'black', linewidth=1.5, label='base2')\n",
    "    \n",
    "#     axs[3].set_xlabel('Date', fontsize=13)\n",
    "#     axs[3].set_ylabel(f'{plot_label} \\n [unit demand]', fontsize=13)\n",
    "#     axs[3].tick_params(labelsize=13)\n",
    "    \n",
    "#     plt.suptitle(f'Hierarchically Linked Series for Item {label}', fontsize=16)\n",
    "#     fig.tight_layout()\n",
    "#     fig.subplots_adjust(top=0.95)\n",
    "#     plt.legend(bbox_to_anchor=(1, 1), loc='upper left', ncol=1)\n",
    "#     if plot_file is not None:\n",
    "#         plt.savefig(plot_file, bbox_inches='tight')\n",
    "#     plt.show()\n",
    "#     plt.close()\n",
    "    \n",
    "# item_idx  = 0\n",
    "# store_idx = 31\n",
    "# plot_hierarchically_linked_data(data=data, \n",
    "#                                 item_idx=item_idx, store_idx=store_idx, \n",
    "#                                 n_days=30*6, plot_file=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking Geographically linked data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['xcols_futr_agg']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"cell-4\"></a>\n",
    "## 4. HierTimeseriesDataset Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "# from neuralforecast.data.datasets.favorita import Favorita\n",
    "# from neuralforecast.data.hiertsdataset import HierTimeseriesDataset\n",
    "\n",
    "# directory = './data/hierarchical/favorita'\n",
    "# Favorita.preprocess_data(directory=directory, sample_size=100, verbose=False)\n",
    "# data = Favorita.load_process(directory=directory, sample_size=100, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T = 1688-34-34\n",
    "# H = 34\n",
    "\n",
    "# train_dataset = HierTimeseriesDataset(# Bottom data\n",
    "#                                       X=data['X'], \n",
    "#                                       S=data['S'], \n",
    "#                                       Y=data['Y'],\n",
    "#                                       xcols=data['xcols'],\n",
    "#                                       xcols_hist=data['xcols_hist'],\n",
    "#                                       xcols_futr=data['xcols_futr'],\n",
    "#                                       xcols_sample_mask=data['xcols_sample_mask'],\n",
    "#                                       xcols_available_mask=data['xcols_available_mask'],\n",
    "#                                       dates = data['dates'],\n",
    "#                                       # Aggregated data\n",
    "#                                       X_agg=data['X_agg'], \n",
    "#                                       S_agg=data['S_agg'], \n",
    "#                                       Y_agg=data['Y_agg'],\n",
    "#                                       xcols_agg=data['xcols_agg'],\n",
    "#                                       xcols_hist_agg=data['xcols_hist_agg'],\n",
    "#                                       xcols_futr_agg=data['xcols_futr_agg'],\n",
    "#                                       # Generator parameters\n",
    "#                                       T0=H,T=T,H=H)\n",
    "\n",
    "# valid_dataset = HierTimeseriesDataset(# Bottom data\n",
    "#                                       X=data['X'], \n",
    "#                                       S=data['S'], \n",
    "#                                       Y=data['Y'],\n",
    "#                                       xcols=data['xcols'],\n",
    "#                                       xcols_hist=data['xcols_hist'],\n",
    "#                                       xcols_futr=data['xcols_futr'],\n",
    "#                                       xcols_sample_mask=data['xcols_sample_mask'],\n",
    "#                                       xcols_available_mask=data['xcols_available_mask'], \n",
    "#                                       dates = data['dates'],\n",
    "#                                       # Aggregated data\n",
    "#                                       X_agg=data['X_agg'], \n",
    "#                                       S_agg=data['S_agg'], \n",
    "#                                       Y_agg=data['Y_agg'],\n",
    "#                                       xcols_agg=data['xcols_agg'],\n",
    "#                                       xcols_hist_agg=data['xcols_hist_agg'],\n",
    "#                                       xcols_futr_agg=data['xcols_futr_agg'],\n",
    "#                                       # Generator parameters\n",
    "#                                       T0=2*H,T=T,H=H,\n",
    "#                                       lastwindow_mask=True)\n",
    "\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=0)\n",
    "# valid_dataloader = DataLoader(valid_dataset, batch_size=4, shuffle=False, num_workers=0)\n",
    "\n",
    "# train_batch = next(iter(train_dataloader))\n",
    "# valid_batch = next(iter(valid_dataloader))\n",
    "\n",
    "# for key in train_batch.keys():\n",
    "#     print(f'{key}.shape', train_batch[key].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Examinate HierTimeSeriesDataset aggregate batch\n",
    "1. Examinate the hierarchically linked Y_agg series.\n",
    "2. Examinate the alignment between Y_agg and X_agg_futr.\n",
    "3. Validate the S_agg dummies generated from H constraints matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['xcols_futr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Parse batch and filter for first entry\n",
    "# batch = valid_batch\n",
    "# item_idx = 0\n",
    "# S_agg = batch['S_agg'][item_idx]\n",
    "# Y_agg = batch['Y_agg'][item_idx]\n",
    "# X_agg = batch['X_agg'][item_idx]\n",
    "# F_agg = batch['F_agg'][item_idx]\n",
    "\n",
    "# #------------------ Hierarchical Y_agg ----------------#\n",
    "# plt.plot(Y_agg[0,:], label='total transactions')\n",
    "# plt.legend(bbox_to_anchor=(1, 1), loc='upper left', ncol=1)\n",
    "# plt.title('Y_agg')\n",
    "# plt.show()\n",
    "# plt.close()\n",
    "\n",
    "# #---------------- X_agg_futr ----------------#\n",
    "# day_of_week_idx  = xcols_agg.get_loc('day_of_week')\n",
    "# day_of_month_idx = xcols_agg.get_loc('day_of_month')\n",
    "# month_idx = xcols_agg.get_loc('month')\n",
    "\n",
    "# plt.plot(F_agg[day_of_week_idx,-365:], label='day_of_week')\n",
    "# plt.plot(F_agg[day_of_month_idx,-365:], label='day_of_month')\n",
    "# plt.plot(F_agg[month_idx,-365:], label='month')\n",
    "# plt.legend(bbox_to_anchor=(1, 1), loc='upper left', ncol=1)\n",
    "# plt.title('F_agg')\n",
    "# plt.show()\n",
    "# plt.close()\n",
    "\n",
    "# # #---------------- S_agg ----------------#\n",
    "# plt.figure(num=1, figsize=(5, 5), dpi=80, facecolor='w')\n",
    "# plt.spy(np.repeat(S_agg[None,:], 6, axis=0))\n",
    "# plt.title('S_agg')\n",
    "# plt.show()\n",
    "# plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Examinate HierTimeSeriesDataset bottom batch\n",
    "1. Examinate the alignment between Y_bottom, S_bottom and X_bottom_futr.\n",
    "2. Validate batch Seq2Seq structure between X_bottom vs Y_bottom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Parse batch and filter for first entry\n",
    "# # batch = train_batch\n",
    "# # dates = train_dataset.dates\n",
    "# item_idx = 0\n",
    "# batch = valid_batch\n",
    "# dates = valid_dataset.dates\n",
    "# S_bottom = batch['S'][item_idx]\n",
    "# Y_bottom = batch['Y'][item_idx]\n",
    "# X_bottom = batch['X'][item_idx]\n",
    "# F_bottom = batch['F'][item_idx]\n",
    "# sample_mask = batch['sample_mask'][item_idx]\n",
    "\n",
    "# # #---------------- Y/S/X_bottom alignment ----------------#\n",
    "\n",
    "# n_days = 60+34 #60+34 #3*365\n",
    "# fig, axs = plt.subplots(9,6, figsize=(12, 9))\n",
    "# fig.tight_layout()\n",
    "\n",
    "# for idx, store in enumerate(np.arange(1,55)):\n",
    "#     row_idx = idx // 6\n",
    "#     col_idx = idx % 6\n",
    "    \n",
    "#     axs[row_idx][col_idx].plot(dates[1:n_days+1],\n",
    "#                                Y_bottom[idx,:n_days], label='y')\n",
    "#     axs[row_idx][col_idx].set_xticks([])\n",
    "#     axs[row_idx][col_idx].axhline(y=S_bottom[idx,-2]*1.1,\n",
    "#                 label='level', color='black')\n",
    "#     axs[row_idx][col_idx].plot(dates[1:n_days+1],\n",
    "#                                F_bottom[idx,1,:n_days], label='is_original')\n",
    "\n",
    "# plt.suptitle('Y/S/X/F_bottom alignment ', y=1.02)\n",
    "# plt.legend(bbox_to_anchor=(1, 1), loc='upper left', ncol=1)\n",
    "# plt.show()\n",
    "# plt.close()\n",
    "\n",
    "# #---------------- Y/X_bottom Seq2Seq structure ----------------#\n",
    "# idx   = 32\n",
    "# start = 1688-34-34-1-300\n",
    "# end   = 1688-34-34-1\n",
    "# H = 34\n",
    "# print('\\n')\n",
    "# plt.figure(figsize=(12, 1.8))\n",
    "\n",
    "# print('Y_bottom.shape', Y_bottom.shape)\n",
    "# print('Y_bottom[idx,start:end].shape', Y_bottom[idx,start:end].shape)\n",
    "# print('dates[start:end].shape', dates[start:end].shape)\n",
    "\n",
    "# plt.plot(dates[start:end],     Y_bottom[idx,start:end]*1.1, label='y')\n",
    "# plt.plot(dates[start-1:end-1], X_bottom[idx,0,start:end], label='y_[lag1]')\n",
    "# plt.plot(dates[start:end+H],   F_bottom[idx,0,start:end+H]*Y_bottom[idx,-1]*2, \n",
    "#          alpha=0.5, label='promotion')\n",
    "# plt.plot(dates[start:end+H],   F_bottom[idx,2,start:end+H]*Y_bottom[idx,-1]*0.5, \n",
    "#          alpha=0.5, label='week_day')\n",
    "# plt.plot(dates[start:end],     sample_mask[idx,start:end]*Y_bottom[idx,-1], label='sample_mask')\n",
    "# plt.ylabel('Tourist Visits\\n [In millions]')\n",
    "# plt.title('Y/X_bottom (Seq2Seq)')\n",
    "# plt.legend(bbox_to_anchor=(1, 1), loc='upper left', ncol=1)\n",
    "# plt.show()\n",
    "# plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuralforecast",
   "language": "python",
   "name": "neuralforecast"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
