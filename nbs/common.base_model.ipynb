{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5c6594-e5e8-4966-8cb8-a3e6a9ed7d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp common._base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce0c950-2e03-4be1-95d4-a02409d8dba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7c2ba5-19ee-421e-9252-7224b03f5201",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import inspect\n",
    "import random\n",
    "import warnings\n",
    "from contextlib import contextmanager\n",
    "from copy import deepcopy\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Union, Any, Optional, Tuple\n",
    "\n",
    "import fsspec\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "import neuralforecast.losses.pytorch as losses\n",
    "\n",
    "from neuralforecast.losses.pytorch import BasePointLoss, DistributionLoss\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from neuralforecast.tsdataset import (\n",
    "    TimeSeriesDataModule,\n",
    "    BaseTimeSeriesDataset,\n",
    "    _DistributedTimeSeriesDataModule,\n",
    ")\n",
    "from neuralforecast.common._scalers import TemporalNorm\n",
    "from neuralforecast.utils import get_indexer_raise_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d4c4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class DistributedConfig:\n",
    "    partitions_path: str\n",
    "    num_nodes: int\n",
    "    devices: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5197e340-11f1-4c8c-96d1-ed396ac2b710",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "@contextmanager\n",
    "def _disable_torch_init():\n",
    "    \"\"\"Context manager used to disable pytorch's weight initialization.\n",
    "\n",
    "    This is especially useful when loading saved models, since when initializing\n",
    "    a model the weights are also initialized following some method\n",
    "    (e.g. kaiming uniform), and that time is wasted since we'll override them with\n",
    "    the saved weights.\"\"\"\n",
    "    def noop(*args, **kwargs):\n",
    "        return\n",
    "        \n",
    "    kaiming_uniform = nn.init.kaiming_uniform_\n",
    "    kaiming_normal = nn.init.kaiming_normal_\n",
    "    xavier_uniform = nn.init.xavier_uniform_\n",
    "    xavier_normal = nn.init.xavier_normal_\n",
    "    \n",
    "    nn.init.kaiming_uniform_ = noop\n",
    "    nn.init.kaiming_normal_ = noop\n",
    "    nn.init.xavier_uniform_ = noop\n",
    "    nn.init.xavier_normal_ = noop\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        nn.init.kaiming_uniform_ = kaiming_uniform\n",
    "        nn.init.kaiming_normal_ = kaiming_normal\n",
    "        nn.init.xavier_uniform_ = xavier_uniform\n",
    "        nn.init.xavier_normal_ = xavier_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58c0e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def tensor_to_numpy(tensor: torch.Tensor) -> np.ndarray:\n",
    "    \"\"\"Convert a tensor to numpy\"\"\"\n",
    "    if tensor.dtype == torch.bfloat16:\n",
    "        return tensor.float().numpy()\n",
    "    \n",
    "    return tensor.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c40a64-8381-46a2-8cbb-70ec70ed7914",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BaseModel(pl.LightningModule):\n",
    "    EXOGENOUS_FUTR = True   # If the model can handle future exogenous variables\n",
    "    EXOGENOUS_HIST = True   # If the model can handle historical exogenous variables\n",
    "    EXOGENOUS_STAT = True   # If the model can handle static exogenous variables\n",
    "    MULTIVARIATE = False    # If the model produces multivariate forecasts (True) or univariate (False)\n",
    "    RECURRENT = False       # If the model produces forecasts recursively (True) or direct (False)\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        h: int,\n",
    "        input_size: int,\n",
    "        loss: Union[BasePointLoss, DistributionLoss, nn.Module],\n",
    "        valid_loss: Union[BasePointLoss, DistributionLoss, nn.Module],\n",
    "        learning_rate: float,\n",
    "        max_steps: int,\n",
    "        val_check_steps: int,\n",
    "        batch_size: int,\n",
    "        valid_batch_size: Union[int, None],\n",
    "        windows_batch_size: int,\n",
    "        inference_windows_batch_size: Union[int, None],\n",
    "        start_padding_enabled: bool,\n",
    "        n_series: Union[int, None] = None,\n",
    "        n_samples: Union[int, None] = 100,\n",
    "        h_train: int = 1,\n",
    "        inference_input_size: Union[int, None] = None,\n",
    "        step_size: int = 1,\n",
    "        num_lr_decays: int = 0,\n",
    "        early_stop_patience_steps: int = -1,\n",
    "        scaler_type: str = 'identity',\n",
    "        futr_exog_list: Union[List, None] = None,\n",
    "        hist_exog_list: Union[List, None] = None,\n",
    "        stat_exog_list: Union[List, None] = None,\n",
    "        exclude_insample_y: Union[bool, None] = False,\n",
    "        drop_last_loader: Union[bool, None] = False,\n",
    "        random_seed: Union[int, None] = 1,\n",
    "        alias: Union[str, None] = None,\n",
    "        optimizer: Union[torch.optim.Optimizer, None] = None,\n",
    "        optimizer_kwargs: Union[Dict, None] = None,\n",
    "        lr_scheduler: Union[torch.optim.lr_scheduler.LRScheduler, None] = None,\n",
    "        lr_scheduler_kwargs: Union[Dict, None] = None,\n",
    "        dataloader_kwargs=None,\n",
    "        **trainer_kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Multivarariate checks\n",
    "        if self.MULTIVARIATE and n_series is None:\n",
    "            raise Exception(f'{type(self).__name__} is a multivariate model. Please set n_series to the number of unique time series in your dataset.')\n",
    "        if not self.MULTIVARIATE:\n",
    "            n_series = 1\n",
    "        self.n_series = n_series          \n",
    "\n",
    "        # Protections for previous recurrent models\n",
    "        if input_size < 1:\n",
    "            input_size = 3 * h\n",
    "            warnings.warn(\n",
    "                f'Input size too small. Automatically setting input size to 3 * horizon = {input_size}'\n",
    "            )\n",
    "\n",
    "        if inference_input_size is None:\n",
    "            inference_input_size = input_size            \n",
    "        elif inference_input_size is not None and inference_input_size < 1:\n",
    "            inference_input_size = input_size\n",
    "            warnings.warn(\n",
    "                f'Inference input size too small. Automatically setting inference input size to input_size = {input_size}'\n",
    "            )\n",
    "\n",
    "        # For recurrent models we need one additional input as we need to shift insample_y to use it as input\n",
    "        if self.RECURRENT:\n",
    "            input_size += 1\n",
    "            inference_input_size += 1\n",
    "\n",
    "        # Attributes needed for recurrent models\n",
    "        self.horizon_backup = h\n",
    "        self.input_size_backup = input_size\n",
    "        self.n_samples = n_samples\n",
    "        if self.RECURRENT:\n",
    "            if (\n",
    "                hasattr(loss, \"horizon_weight\")\n",
    "                and loss.horizon_weight is not None\n",
    "                and h_train != h\n",
    "            ):\n",
    "                warnings.warn(\n",
    "                    f\"Setting h_train={h} to match the horizon_weight length.\"\n",
    "                )\n",
    "                h_train = h\n",
    "            self.h_train = h_train\n",
    "            self.inference_input_size = inference_input_size\n",
    "            self.rnn_state = None\n",
    "            self.maintain_state = False\n",
    "\n",
    "        with warnings.catch_warnings(record=False):\n",
    "            warnings.filterwarnings('ignore')\n",
    "            # the following line issues a warning about the loss attribute being saved\n",
    "            # but we do want to save it\n",
    "            self.save_hyperparameters() # Allows instantiation from a checkpoint from class\n",
    "        self.random_seed = random_seed\n",
    "        pl.seed_everything(self.random_seed, workers=True)\n",
    "\n",
    "        # Loss\n",
    "        self.loss = loss\n",
    "        if valid_loss is None:\n",
    "            self.valid_loss = loss\n",
    "        else:\n",
    "            self.valid_loss = valid_loss\n",
    "        self.train_trajectories: List = []\n",
    "        self.valid_trajectories: List = []\n",
    "\n",
    "        # Optimization\n",
    "        if optimizer is not None and not issubclass(optimizer, torch.optim.Optimizer):\n",
    "            raise TypeError(\"optimizer is not a valid subclass of torch.optim.Optimizer\")\n",
    "        self.optimizer = optimizer\n",
    "        self.optimizer_kwargs = optimizer_kwargs if optimizer_kwargs is not None else {}\n",
    "\n",
    "        # lr scheduler\n",
    "        if lr_scheduler is not None and not issubclass(lr_scheduler, torch.optim.lr_scheduler.LRScheduler):\n",
    "            raise TypeError(\"lr_scheduler is not a valid subclass of torch.optim.lr_scheduler.LRScheduler\")\n",
    "        self.lr_scheduler = lr_scheduler\n",
    "        self.lr_scheduler_kwargs = lr_scheduler_kwargs if lr_scheduler_kwargs is not None else {}\n",
    "\n",
    "        # Variables\n",
    "        self.futr_exog_list = list(futr_exog_list) if futr_exog_list is not None else []\n",
    "        self.hist_exog_list = list(hist_exog_list) if hist_exog_list is not None else []\n",
    "        self.stat_exog_list = list(stat_exog_list) if stat_exog_list is not None else []\n",
    "\n",
    "        # Set data sizes\n",
    "        self.futr_exog_size = len(self.futr_exog_list)\n",
    "        self.hist_exog_size = len(self.hist_exog_list)\n",
    "        self.stat_exog_size = len(self.stat_exog_list)   \n",
    "\n",
    "        # Check if model supports exogenous, otherwise raise Exception\n",
    "        if not self.EXOGENOUS_FUTR and self.futr_exog_size > 0:\n",
    "            raise Exception(f'{type(self).__name__} does not support future exogenous variables.')\n",
    "        if not self.EXOGENOUS_HIST and self.hist_exog_size > 0:\n",
    "            raise Exception(f'{type(self).__name__} does not support historical exogenous variables.')\n",
    "        if not self.EXOGENOUS_STAT and self.stat_exog_size > 0:\n",
    "            raise Exception(f'{type(self).__name__} does not support static exogenous variables.')\n",
    "\n",
    "        # Protections for loss functions\n",
    "        if isinstance(self.loss, (losses.IQLoss, losses.HuberIQLoss)):\n",
    "            loss_type = type(self.loss)\n",
    "            if not isinstance(self.valid_loss, loss_type):\n",
    "                raise Exception(f'Please set valid_loss={type(self.loss).__name__}() when training with {type(self.loss).__name__}')\n",
    "        if isinstance(self.loss, (losses.MQLoss, losses.HuberMQLoss)):\n",
    "            if not isinstance(self.valid_loss, (losses.MQLoss, losses.HuberMQLoss)):\n",
    "                raise Exception(f'Please set valid_loss to MQLoss() or HuberMQLoss() when training with {type(self.loss).__name__}')\n",
    "        if isinstance(self.valid_loss, (losses.IQLoss, losses.HuberIQLoss)):\n",
    "            valid_loss_type = type(self.valid_loss)\n",
    "            if not isinstance(self.loss, valid_loss_type):\n",
    "                raise Exception(f'Please set loss={type(self.valid_loss).__name__}() when validating with {type(self.valid_loss).__name__}')        \n",
    "\n",
    "        # Deny impossible loss / valid_loss combinations\n",
    "        if isinstance(self.loss, losses.BasePointLoss) and self.valid_loss.is_distribution_output:\n",
    "            raise Exception(f'Validation with distribution loss {type(self.valid_loss).__name__} is not possible when using loss={type(self.loss).__name__}. Please use a point valid_loss (MAE, MSE, ...)')\n",
    "        elif self.valid_loss.is_distribution_output and self.valid_loss is not loss:\n",
    "            # Maybe we should raise a Warning or an Exception here, but meh for now.\n",
    "            self.valid_loss = loss\n",
    "        \n",
    "        if isinstance(self.loss, (losses.relMSE, losses.Accuracy, losses.sCRPS)):\n",
    "            raise Exception(f\"{type(self.loss).__name__} cannot be used for training. Please use another loss function (MAE, MSE, ...)\")\n",
    "        \n",
    "        if isinstance(self.valid_loss, (losses.relMSE)):\n",
    "            raise Exception(f\"{type(self.valid_loss).__name__} cannot be used for validation. Please use another valid_loss (MAE, MSE, ...)\")\n",
    "\n",
    "        ## Trainer arguments ##\n",
    "        # Max steps, validation steps and check_val_every_n_epoch\n",
    "        trainer_kwargs = {**trainer_kwargs, 'max_steps': max_steps}\n",
    "\n",
    "        if 'max_epochs' in trainer_kwargs.keys():\n",
    "            raise Exception('max_epochs is deprecated, use max_steps instead.')\n",
    "\n",
    "        # Callbacks\n",
    "        if early_stop_patience_steps > 0:\n",
    "            if 'callbacks' not in trainer_kwargs:\n",
    "                trainer_kwargs['callbacks'] = []\n",
    "            trainer_kwargs['callbacks'].append(\n",
    "                EarlyStopping(\n",
    "                    monitor='ptl/val_loss', patience=early_stop_patience_steps\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # Add GPU accelerator if available\n",
    "        if trainer_kwargs.get('accelerator', None) is None:\n",
    "            if torch.cuda.is_available():\n",
    "                trainer_kwargs['accelerator'] = \"gpu\"\n",
    "        if trainer_kwargs.get('devices', None) is None:\n",
    "            if torch.cuda.is_available():\n",
    "                trainer_kwargs['devices'] = -1\n",
    "\n",
    "        # Avoid saturating local memory, disabled fit model checkpoints\n",
    "        if trainer_kwargs.get('enable_checkpointing', None) is None:\n",
    "            trainer_kwargs['enable_checkpointing'] = False\n",
    "\n",
    "        # Set other attributes\n",
    "        self.trainer_kwargs = trainer_kwargs\n",
    "        self.h = h\n",
    "        self.input_size = input_size\n",
    "        self.windows_batch_size = windows_batch_size\n",
    "        self.start_padding_enabled = start_padding_enabled\n",
    "\n",
    "        # Padder to complete train windows, \n",
    "        # example y=[1,2,3,4,5] h=3 -> last y_output = [5,0,0]\n",
    "        if start_padding_enabled:\n",
    "            self.padder_train = nn.ConstantPad1d(padding=(self.input_size-1, self.h), value=0.0)\n",
    "        else:\n",
    "            self.padder_train = nn.ConstantPad1d(padding=(0, self.h), value=0.0)\n",
    "\n",
    "        # Batch sizes\n",
    "        if self.MULTIVARIATE and n_series is not None:\n",
    "            self.batch_size = max(batch_size, n_series)\n",
    "            if valid_batch_size is not None:\n",
    "                valid_batch_size = max(valid_batch_size, n_series)\n",
    "        else:\n",
    "            self.batch_size = batch_size\n",
    "        \n",
    "        if valid_batch_size is None:\n",
    "            self.valid_batch_size = self.batch_size\n",
    "        else:\n",
    "            self.valid_batch_size = valid_batch_size\n",
    "\n",
    "        if inference_windows_batch_size is None:\n",
    "            self.inference_windows_batch_size = windows_batch_size\n",
    "        else:\n",
    "            self.inference_windows_batch_size = inference_windows_batch_size\n",
    "\n",
    "        # Optimization \n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_steps = max_steps\n",
    "        self.num_lr_decays = num_lr_decays\n",
    "        self.lr_decay_steps = (\n",
    "            max(max_steps // self.num_lr_decays, 1) if self.num_lr_decays > 0 else 10e7\n",
    "        )\n",
    "        self.early_stop_patience_steps = early_stop_patience_steps\n",
    "        self.val_check_steps = val_check_steps\n",
    "        self.windows_batch_size = windows_batch_size\n",
    "        self.step_size = step_size\n",
    "        \n",
    "        # If the model does not support exogenous, it can't support exclude_insample_y\n",
    "        if exclude_insample_y and not (self.EXOGENOUS_FUTR or self.EXOGENOUS_HIST or self.EXOGENOUS_STAT):\n",
    "            raise Exception(f'{type(self).__name__} does not support `exclude_insample_y=True`. Please set `exclude_insample_y=False`')\n",
    "\n",
    "        self.exclude_insample_y = exclude_insample_y\n",
    "\n",
    "        # Scaler\n",
    "        self.scaler = TemporalNorm(\n",
    "            scaler_type=scaler_type,\n",
    "            dim=1,  # Time dimension is 1.\n",
    "            num_features= 1 + len(self.hist_exog_list) + len(self.futr_exog_list)\n",
    "        )\n",
    "\n",
    "        # Fit arguments\n",
    "        self.val_size = 0\n",
    "        self.test_size = 0\n",
    "\n",
    "        # Model state\n",
    "        self.decompose_forecast = False\n",
    "\n",
    "        # DataModule arguments\n",
    "        self.dataloader_kwargs = dataloader_kwargs\n",
    "        self.drop_last_loader = drop_last_loader\n",
    "        # used by on_validation_epoch_end hook\n",
    "        self.validation_step_outputs: List = []\n",
    "        self.alias = alias\n",
    "\n",
    "    def __repr__(self):\n",
    "        return type(self).__name__ if self.alias is None else self.alias\n",
    "\n",
    "    def _check_exog(self, dataset):\n",
    "        temporal_cols = set(dataset.temporal_cols.tolist())\n",
    "        static_cols = set(dataset.static_cols.tolist() if dataset.static_cols is not None else [])\n",
    "\n",
    "        missing_hist = set(self.hist_exog_list) - temporal_cols\n",
    "        missing_futr = set(self.futr_exog_list) - temporal_cols\n",
    "        missing_stat = set(self.stat_exog_list) - static_cols\n",
    "        if missing_hist:\n",
    "            raise Exception(f'{missing_hist} historical exogenous variables not found in input dataset')\n",
    "        if missing_futr:\n",
    "            raise Exception(f'{missing_futr} future exogenous variables not found in input dataset')\n",
    "        if missing_stat:\n",
    "            raise Exception(f'{missing_stat} static exogenous variables not found in input dataset')\n",
    "\n",
    "    def _restart_seed(self, random_seed):\n",
    "        if random_seed is None:\n",
    "            random_seed = self.random_seed\n",
    "        torch.manual_seed(random_seed)\n",
    "\n",
    "    def _get_temporal_exogenous_cols(self, temporal_cols):\n",
    "        return list(\n",
    "            set(temporal_cols.tolist()) & set(self.hist_exog_list + self.futr_exog_list)\n",
    "        )\n",
    "    \n",
    "    def _set_quantiles(self, quantiles=None):\n",
    "        if quantiles is None and isinstance(self.loss, (losses.IQLoss, losses.HuberIQLoss)):\n",
    "            self.loss.update_quantile(q=[0.5])\n",
    "        elif hasattr(self.loss, 'update_quantile') and callable(self.loss.update_quantile):\n",
    "            self.loss.update_quantile(q=quantiles)\n",
    "\n",
    "    def _fit_distributed(\n",
    "        self,\n",
    "        distributed_config,\n",
    "        datamodule,\n",
    "        val_size,\n",
    "        test_size,\n",
    "    ):\n",
    "        assert distributed_config is not None\n",
    "        from pyspark.ml.torch.distributor import TorchDistributor\n",
    "\n",
    "        def train_fn(\n",
    "            model_cls,\n",
    "            model_params,\n",
    "            datamodule,\n",
    "            trainer_kwargs,\n",
    "            num_tasks,\n",
    "            num_proc_per_task,\n",
    "            val_size,\n",
    "            test_size,\n",
    "        ):\n",
    "            import pytorch_lightning as pl\n",
    "\n",
    "            # we instantiate here to avoid pickling large tensors (weights)\n",
    "            model = model_cls(**model_params)\n",
    "            model.val_size = val_size\n",
    "            model.test_size = test_size\n",
    "            for arg in ('devices', 'num_nodes'):\n",
    "                trainer_kwargs.pop(arg, None)\n",
    "            trainer = pl.Trainer(\n",
    "                strategy=\"ddp\",\n",
    "                use_distributed_sampler=False,  # to ensure our dataloaders are used as-is\n",
    "                num_nodes=num_tasks,\n",
    "                devices=num_proc_per_task,\n",
    "                **trainer_kwargs,\n",
    "            )\n",
    "            trainer.fit(model=model, datamodule=datamodule)\n",
    "            model.metrics = trainer.callback_metrics\n",
    "            model.__dict__.pop('_trainer', None)\n",
    "            return model\n",
    "\n",
    "        def is_gpu_accelerator(accelerator):\n",
    "            from pytorch_lightning.accelerators.cuda import CUDAAccelerator\n",
    "\n",
    "            return (\n",
    "                accelerator == \"gpu\"\n",
    "                or isinstance(accelerator, CUDAAccelerator)\n",
    "                or (accelerator == \"auto\" and CUDAAccelerator.is_available())\n",
    "            )\n",
    "\n",
    "        local_mode = distributed_config.num_nodes == 1\n",
    "        if local_mode:\n",
    "            num_tasks = 1\n",
    "            num_proc_per_task = distributed_config.devices\n",
    "        else:\n",
    "            num_tasks = distributed_config.num_nodes * distributed_config.devices\n",
    "            num_proc_per_task = 1  # number of GPUs per task\n",
    "        num_proc = num_tasks * num_proc_per_task\n",
    "        use_gpu = is_gpu_accelerator(self.trainer_kwargs[\"accelerator\"])\n",
    "        model = TorchDistributor(\n",
    "            num_processes=num_proc,\n",
    "            local_mode=local_mode,\n",
    "            use_gpu=use_gpu,\n",
    "        ).run(\n",
    "            train_fn,\n",
    "            model_cls=type(self),\n",
    "            model_params=self.hparams,\n",
    "            datamodule=datamodule,\n",
    "            trainer_kwargs=self.trainer_kwargs,\n",
    "            num_tasks=num_tasks,\n",
    "            num_proc_per_task=num_proc_per_task,\n",
    "            val_size=val_size,\n",
    "            test_size=test_size,\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    def _fit(\n",
    "        self,\n",
    "        dataset,\n",
    "        batch_size,\n",
    "        valid_batch_size=1024,\n",
    "        val_size=0,\n",
    "        test_size=0,\n",
    "        random_seed=None,\n",
    "        shuffle_train=True,\n",
    "        distributed_config=None,\n",
    "    ):\n",
    "        self._check_exog(dataset)\n",
    "        self._restart_seed(random_seed)\n",
    "\n",
    "        self.val_size = val_size\n",
    "        self.test_size = test_size\n",
    "        is_local = isinstance(dataset, BaseTimeSeriesDataset)\n",
    "        if is_local:\n",
    "            datamodule_constructor = TimeSeriesDataModule\n",
    "        else:\n",
    "            datamodule_constructor = _DistributedTimeSeriesDataModule\n",
    "        \n",
    "        dataloader_kwargs = self.dataloader_kwargs if self.dataloader_kwargs is not None else {}\n",
    "        datamodule = datamodule_constructor(\n",
    "            dataset=dataset, \n",
    "            batch_size=batch_size,\n",
    "            valid_batch_size=valid_batch_size,\n",
    "            drop_last=self.drop_last_loader,\n",
    "            shuffle_train=shuffle_train,\n",
    "            **dataloader_kwargs\n",
    "        )\n",
    "\n",
    "        if self.val_check_steps > self.max_steps:\n",
    "            warnings.warn(\n",
    "                'val_check_steps is greater than max_steps, '\n",
    "                'setting val_check_steps to max_steps.'\n",
    "            )\n",
    "        val_check_interval = min(self.val_check_steps, self.max_steps)\n",
    "        self.trainer_kwargs['val_check_interval'] = int(val_check_interval)\n",
    "        self.trainer_kwargs['check_val_every_n_epoch'] = None\n",
    "\n",
    "        if is_local:\n",
    "            model = self\n",
    "            trainer = pl.Trainer(**model.trainer_kwargs)\n",
    "            trainer.fit(model, datamodule=datamodule)\n",
    "            model.metrics = trainer.callback_metrics\n",
    "            model.__dict__.pop('_trainer', None)\n",
    "        else:\n",
    "            model = self._fit_distributed(\n",
    "                distributed_config,\n",
    "                datamodule,\n",
    "                val_size,\n",
    "                test_size,\n",
    "            )\n",
    "        return model\n",
    "\n",
    "    def _extract_target_features(self, dataset_batch: Dict) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Extract target, historical exogenous, future exogenous, and static exogenous features for SHAP explanation.\n",
    "        \"\"\"\n",
    "        # TimeSeriesDataset returns temporal data as [batch_size, features, time_steps]  \n",
    "        temporal_data = dataset_batch['temporal']  # Shape: [batch_size, features, time_steps]\n",
    "        temporal_cols = dataset_batch['temporal_cols']  # Column names\n",
    "        \n",
    "        # Check what exogenous features the model supports\n",
    "        has_hist_exog = (hasattr(self, 'EXOGENOUS_HIST') and self.EXOGENOUS_HIST and \n",
    "                        hasattr(self, 'hist_exog_list') and self.hist_exog_list)\n",
    "        has_futr_exog = (hasattr(self, 'EXOGENOUS_FUTR') and self.EXOGENOUS_FUTR and \n",
    "                        hasattr(self, 'futr_exog_list') and self.futr_exog_list)\n",
    "        has_stat_exog = (hasattr(self, 'EXOGENOUS_STAT') and self.EXOGENOUS_STAT and \n",
    "                        hasattr(self, 'stat_exog_list') and self.stat_exog_list)\n",
    "        \n",
    "        if self.MULTIVARIATE:\n",
    "            # For multivariate models, extract n_series worth of data\n",
    "            n_series = getattr(self, 'n_series', 1)\n",
    "            \n",
    "            # Take the first n_series from temporal_data (assumes target series come first)\n",
    "            if temporal_data.shape[1] >= n_series:\n",
    "                series_data = temporal_data[:, :n_series, :]  # [batch_size, n_series, time_steps]\n",
    "            else:\n",
    "                # If we don't have enough series, pad or repeat\n",
    "                series_data = temporal_data[:, :1, :].repeat(1, n_series, 1)\n",
    "                print(f\"Warning: Dataset has {temporal_data.shape[1]} series but model expects {n_series}. Using repeated data.\")\n",
    "            \n",
    "            # Take the last input_size time steps for all series\n",
    "            if series_data.shape[2] >= self.input_size:\n",
    "                target_features = series_data[:, :, -self.input_size:]  # [batch_size, n_series, input_size]\n",
    "            else:\n",
    "                # Pad with zeros if not enough history\n",
    "                pad_size = self.input_size - series_data.shape[2] \n",
    "                target_features = F.pad(series_data, (pad_size, 0), value=0.0)\n",
    "            \n",
    "            # Flatten to [batch_size, n_series * input_size] for SHAP\n",
    "            batch_size, n_series_actual, input_size = target_features.shape\n",
    "            target_features = target_features.reshape(batch_size, n_series_actual * input_size)\n",
    "            \n",
    "            # Add multivariate historical exogenous support\n",
    "            if has_hist_exog:\n",
    "                hist_exog_features = []\n",
    "                \n",
    "                # Find indices of historical exogenous columns\n",
    "                for exog_col in self.hist_exog_list:\n",
    "                    if exog_col in temporal_cols:\n",
    "                        exog_idx = temporal_cols.get_loc(exog_col)\n",
    "                        exog_data = temporal_data[:, exog_idx, :]  # [batch_size, time_steps]\n",
    "                        \n",
    "                        # Take the last input_size time steps for this exogenous feature\n",
    "                        if exog_data.shape[1] >= self.input_size:\n",
    "                            exog_features = exog_data[:, -self.input_size:]  # [batch_size, input_size]\n",
    "                        else:\n",
    "                            # Pad with zeros if not enough history\n",
    "                            pad_size = self.input_size - exog_data.shape[1]\n",
    "                            exog_features = F.pad(exog_data, (pad_size, 0), value=0.0)\n",
    "                        \n",
    "                        hist_exog_features.append(exog_features)\n",
    "                    else:\n",
    "                        print(f\"Warning: Historical exogenous feature '{exog_col}' not found in dataset columns.\")\n",
    "                        # Create zero features as placeholder\n",
    "                        zero_features = torch.zeros(target_features.shape[0], self.input_size)\n",
    "                        hist_exog_features.append(zero_features)\n",
    "                \n",
    "                # Concatenate historical exogenous features\n",
    "                if hist_exog_features:\n",
    "                    all_hist_exog = torch.stack(hist_exog_features, dim=1)  # [batch_size, n_hist_exog, input_size]\n",
    "                    all_hist_exog = all_hist_exog.reshape(target_features.shape[0], -1)  # [batch_size, n_hist_exog * input_size]\n",
    "                    target_features = torch.cat([target_features, all_hist_exog], dim=1)  # [batch_size, n_series * input_size + n_hist_exog * input_size]\n",
    "            \n",
    "        else:\n",
    "            # For univariate models, extract target series and exogenous features\n",
    "            y_idx = dataset_batch['y_idx']  # Index of target variable in features\n",
    "            target_data = temporal_data[:, y_idx, :]  # Shape: [batch_size, time_steps]\n",
    "            \n",
    "            # Take the last input_size time steps for target\n",
    "            if target_data.shape[1] >= self.input_size:\n",
    "                target_features = target_data[:, -self.input_size:]  # Shape: [batch_size, input_size]\n",
    "            else:\n",
    "                # Pad with zeros if not enough history\n",
    "                pad_size = self.input_size - target_data.shape[1]\n",
    "                target_features = F.pad(target_data, (pad_size, 0), value=0.0)\n",
    "            \n",
    "            # Extract historical exogenous features if model supports them\n",
    "            if has_hist_exog:\n",
    "                hist_exog_features = []\n",
    "                \n",
    "                # Find indices of historical exogenous columns\n",
    "                for exog_col in self.hist_exog_list:\n",
    "                    if exog_col in temporal_cols:\n",
    "                        exog_idx = temporal_cols.get_loc(exog_col)\n",
    "                        exog_data = temporal_data[:, exog_idx, :]  # [batch_size, time_steps]\n",
    "                        \n",
    "                        # Take the last input_size time steps for this exogenous feature\n",
    "                        if exog_data.shape[1] >= self.input_size:\n",
    "                            exog_features = exog_data[:, -self.input_size:]  # [batch_size, input_size]\n",
    "                        else:\n",
    "                            # Pad with zeros if not enough history\n",
    "                            pad_size = self.input_size - exog_data.shape[1]\n",
    "                            exog_features = F.pad(exog_data, (pad_size, 0), value=0.0)\n",
    "                        \n",
    "                        hist_exog_features.append(exog_features)\n",
    "                    else:\n",
    "                        print(f\"Warning: Historical exogenous feature '{exog_col}' not found in dataset columns.\")\n",
    "                        # Create zero features as placeholder\n",
    "                        zero_features = torch.zeros(target_features.shape[0], self.input_size)\n",
    "                        hist_exog_features.append(zero_features)\n",
    "                \n",
    "                # Concatenate target and historical exogenous features\n",
    "                if hist_exog_features:\n",
    "                    all_hist_exog = torch.stack(hist_exog_features, dim=1)  # [batch_size, n_hist_exog, input_size]\n",
    "                    all_hist_exog = all_hist_exog.reshape(target_features.shape[0], -1)  # [batch_size, n_hist_exog * input_size]\n",
    "                    target_features = torch.cat([target_features, all_hist_exog], dim=1)  # [batch_size, input_size + n_hist_exog * input_size]\n",
    "        \n",
    "        # Extract future exogenous features if model supports them (both univariate and multivariate)\n",
    "        if has_futr_exog:\n",
    "            futr_exog_features = []\n",
    "            horizon = getattr(self, 'h', 1)  # Forecast horizon\n",
    "            \n",
    "            # For NHITS, future exogenous needs input_size + horizon time steps\n",
    "            # For other models, just horizon time steps\n",
    "            if self.__class__.__name__ == 'NHITS':\n",
    "                futr_exog_time_steps = self.input_size + horizon\n",
    "            else:\n",
    "                futr_exog_time_steps = horizon\n",
    "            \n",
    "            # Find indices of future exogenous columns\n",
    "            for exog_col in self.futr_exog_list:\n",
    "                if exog_col in temporal_cols:\n",
    "                    exog_idx = temporal_cols.get_loc(exog_col)\n",
    "                    exog_data = temporal_data[:, exog_idx, :]  # [batch_size, time_steps]\n",
    "                    \n",
    "                    # Take the required time steps for future exogenous\n",
    "                    if exog_data.shape[1] >= futr_exog_time_steps:\n",
    "                        futr_exog_data = exog_data[:, -futr_exog_time_steps:]  # [batch_size, required_steps]\n",
    "                    else:\n",
    "                        # Pad with zeros if not enough future data\n",
    "                        pad_size = futr_exog_time_steps - exog_data.shape[1]\n",
    "                        futr_exog_data = F.pad(exog_data, (0, pad_size), value=0.0)[:, -futr_exog_time_steps:]\n",
    "                    \n",
    "                    futr_exog_features.append(futr_exog_data)\n",
    "                else:\n",
    "                    print(f\"Warning: Future exogenous feature '{exog_col}' not found in dataset columns.\")\n",
    "                    # Create zero features as placeholder\n",
    "                    zero_features = torch.zeros(target_features.shape[0], futr_exog_time_steps)\n",
    "                    futr_exog_features.append(zero_features)\n",
    "            \n",
    "            # Concatenate future exogenous features\n",
    "            if futr_exog_features:\n",
    "                all_futr_exog = torch.stack(futr_exog_features, dim=1)  # [batch_size, n_futr_exog, required_steps]\n",
    "                all_futr_exog = all_futr_exog.reshape(target_features.shape[0], -1)  # [batch_size, n_futr_exog * required_steps]\n",
    "                target_features = torch.cat([target_features, all_futr_exog], dim=1)  # [batch_size, prev_features + n_futr_exog * required_steps]\n",
    "        \n",
    "        # Extract static exogenous features if model supports them (both univariate and multivariate)\n",
    "        if has_stat_exog:\n",
    "            # Static exogenous data is in dataset_batch['static']\n",
    "            if 'static' in dataset_batch and dataset_batch['static'] is not None:\n",
    "                static_data = dataset_batch['static']  # Shape: [batch_size, n_static_features]\n",
    "                \n",
    "                if self.MULTIVARIATE:\n",
    "                    # For multivariate models like TSMixerx, static exog is expected as [n_series, n_static_features]\n",
    "                    # We need to repeat static features across series or take first n_series rows\n",
    "                    n_series = getattr(self, 'n_series', 1)\n",
    "                    n_stat_features = static_data.shape[1]\n",
    "                    \n",
    "                    if static_data.shape[0] >= n_series:\n",
    "                        # Take first n_series rows\n",
    "                        static_features = static_data[:n_series, :]  # [n_series, n_static_features]\n",
    "                    else:\n",
    "                        # Repeat first row across series\n",
    "                        static_features = static_data[:1, :].repeat(n_series, 1)  # [n_series, n_static_features]\n",
    "                    \n",
    "                    # Flatten for SHAP: [n_series, n_static_features] -> [n_series * n_static_features]\n",
    "                    static_features = static_features.reshape(-1)  # [n_series * n_static_features]\n",
    "                    target_features = torch.cat([target_features, static_features.unsqueeze(0).repeat(target_features.shape[0], 1)], dim=1)\n",
    "                else:\n",
    "                    # For univariate models like NHITS, static exog is expected as [batch_size, n_static_features]\n",
    "                    static_features = static_data  # [batch_size, n_static_features]\n",
    "                    target_features = torch.cat([target_features, static_features], dim=1)  # [batch_size, prev_features + n_static_features]\n",
    "            else:\n",
    "                print(\"Warning: Model expects static exogenous features but none found in dataset batch.\")\n",
    "                # Create zero static features as placeholder\n",
    "                n_stat_features = len(self.stat_exog_list)\n",
    "                \n",
    "                if self.MULTIVARIATE:\n",
    "                    n_series = getattr(self, 'n_series', 1)\n",
    "                    zero_static = torch.zeros(target_features.shape[0], n_series * n_stat_features)\n",
    "                else:\n",
    "                    zero_static = torch.zeros(target_features.shape[0], n_stat_features)\n",
    "                \n",
    "                target_features = torch.cat([target_features, zero_static], dim=1)\n",
    "        \n",
    "        return target_features.cpu().numpy() if torch.is_tensor(target_features) else target_features\n",
    "\n",
    "    def _get_feature_names(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        Get feature names for SHAP plots (with support for all exogenous types).\n",
    "        \"\"\"\n",
    "        feature_names = []\n",
    "        \n",
    "        if self.MULTIVARIATE:\n",
    "            # For multivariate models, create names for all series and their lags\n",
    "            n_series = getattr(self, 'n_series', 1)\n",
    "            \n",
    "            for series_idx in range(n_series):\n",
    "                for lag_idx in range(self.input_size):\n",
    "                    feature_names.append(f\"series_{series_idx}_lag_{lag_idx + 1}\")\n",
    "            \n",
    "            # Add multivariate historical exogenous feature names\n",
    "            has_hist_exog = (hasattr(self, 'EXOGENOUS_HIST') and self.EXOGENOUS_HIST and \n",
    "                            hasattr(self, 'hist_exog_list') and self.hist_exog_list)\n",
    "            \n",
    "            if has_hist_exog:\n",
    "                for exog_col in self.hist_exog_list:\n",
    "                    for lag_idx in range(self.input_size):\n",
    "                        feature_names.append(f\"{exog_col}_lag_{lag_idx + 1}\")\n",
    "            \n",
    "        else:\n",
    "            # For univariate models, add target lags\n",
    "            for lag_idx in range(self.input_size):\n",
    "                feature_names.append(f\"y_lag_{lag_idx + 1}\")\n",
    "            \n",
    "            # Add historical exogenous feature names if model supports them\n",
    "            has_hist_exog = (hasattr(self, 'EXOGENOUS_HIST') and self.EXOGENOUS_HIST and \n",
    "                            hasattr(self, 'hist_exog_list') and self.hist_exog_list)\n",
    "            \n",
    "            if has_hist_exog:\n",
    "                for exog_col in self.hist_exog_list:\n",
    "                    for lag_idx in range(self.input_size):\n",
    "                        feature_names.append(f\"{exog_col}_lag_{lag_idx + 1}\")\n",
    "        \n",
    "        # Add future exogenous feature names (both univariate and multivariate)\n",
    "        has_futr_exog = (hasattr(self, 'EXOGENOUS_FUTR') and self.EXOGENOUS_FUTR and \n",
    "                        hasattr(self, 'futr_exog_list') and self.futr_exog_list)\n",
    "        \n",
    "        if has_futr_exog:\n",
    "            horizon = getattr(self, 'h', 1)\n",
    "            \n",
    "            # For NHITS, future exogenous spans input_size + horizon time steps\n",
    "            # For other models, just horizon time steps\n",
    "            if self.__class__.__name__ == 'NHITS':\n",
    "                futr_exog_time_steps = self.input_size + horizon\n",
    "                time_step_prefix = \"step\"  # step_1, step_2, etc. (includes both historical and future periods)\n",
    "            else:\n",
    "                futr_exog_time_steps = horizon\n",
    "                time_step_prefix = \"step\"  # step_1, step_2, etc. (forecast period only)\n",
    "            \n",
    "            for exog_col in self.futr_exog_list:\n",
    "                for step_idx in range(futr_exog_time_steps):\n",
    "                    feature_names.append(f\"{exog_col}_{time_step_prefix}_{step_idx + 1}\")\n",
    "        \n",
    "        # Add static exogenous feature names (both univariate and multivariate)\n",
    "        has_stat_exog = (hasattr(self, 'EXOGENOUS_STAT') and self.EXOGENOUS_STAT and \n",
    "                        hasattr(self, 'stat_exog_list') and self.stat_exog_list)\n",
    "        \n",
    "        if has_stat_exog:\n",
    "            if self.MULTIVARIATE:\n",
    "                # For multivariate models, static features are per series\n",
    "                n_series = getattr(self, 'n_series', 1)\n",
    "                for series_idx in range(n_series):\n",
    "                    for stat_col in self.stat_exog_list:\n",
    "                        feature_names.append(f\"{stat_col}_series_{series_idx}_static\")\n",
    "            else:\n",
    "                # For univariate models, static features are global\n",
    "                for stat_col in self.stat_exog_list:\n",
    "                    feature_names.append(f\"{stat_col}_static\")\n",
    "        \n",
    "        return feature_names\n",
    "\n",
    "    def _parse_shap_input(self, X_flat: np.ndarray) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Parse flattened SHAP input back into model's expected format.\n",
    "        \"\"\"\n",
    "        # Convert to tensor\n",
    "        X_tensor = torch.FloatTensor(X_flat)\n",
    "        \n",
    "        # Check what exogenous features the model supports\n",
    "        has_hist_exog = (hasattr(self, 'EXOGENOUS_HIST') and self.EXOGENOUS_HIST and \n",
    "                        hasattr(self, 'hist_exog_list') and self.hist_exog_list)\n",
    "        has_futr_exog = (hasattr(self, 'EXOGENOUS_FUTR') and self.EXOGENOUS_FUTR and \n",
    "                        hasattr(self, 'futr_exog_list') and self.futr_exog_list)\n",
    "        has_stat_exog = (hasattr(self, 'EXOGENOUS_STAT') and self.EXOGENOUS_STAT and \n",
    "                        hasattr(self, 'stat_exog_list') and self.stat_exog_list)\n",
    "        \n",
    "        if self.MULTIVARIATE:\n",
    "            # For multivariate models: handle target + historical exog + future exog + static exog\n",
    "            batch_size = X_tensor.shape[0]\n",
    "            n_series = getattr(self, 'n_series', 1)\n",
    "            \n",
    "            # Calculate sizes\n",
    "            target_size = n_series * self.input_size\n",
    "            hist_exog_size = 0\n",
    "            futr_exog_size = 0\n",
    "            stat_exog_size = 0\n",
    "            \n",
    "            if has_hist_exog:\n",
    "                hist_exog_size = len(self.hist_exog_list) * self.input_size\n",
    "            if has_futr_exog:\n",
    "                horizon = getattr(self, 'h', 1)\n",
    "                # For NHITS, future exogenous spans input_size + horizon time steps\n",
    "                if self.__class__.__name__ == 'NHITS':\n",
    "                    futr_exog_size = len(self.futr_exog_list) * (self.input_size + horizon)\n",
    "                else:\n",
    "                    futr_exog_size = len(self.futr_exog_list) * horizon\n",
    "            if has_stat_exog:\n",
    "                stat_exog_size = len(self.stat_exog_list) * n_series  # For multivariate: n_stat_features * n_series\n",
    "            \n",
    "            # Split features\n",
    "            current_idx = 0\n",
    "            \n",
    "            # Target series data\n",
    "            target_flat = X_tensor[:, current_idx:current_idx + target_size]\n",
    "            current_idx += target_size\n",
    "            \n",
    "            # Reshape target: [batch_size, n_series * input_size] -> [batch_size, input_size, n_series]\n",
    "            insample_y = target_flat.reshape(batch_size, n_series, self.input_size)\n",
    "            insample_y = insample_y.transpose(1, 2)  # [batch_size, input_size, n_series]\n",
    "            \n",
    "            # Handle historical exogenous (TSMixerx format)\n",
    "            if has_hist_exog and hist_exog_size > 0:\n",
    "                hist_exog_flat = X_tensor[:, current_idx:current_idx + hist_exog_size]\n",
    "                current_idx += hist_exog_size\n",
    "                \n",
    "                # TSMixerx expects hist_exog as [batch_size, hist_exog_size, input_size, n_series]\n",
    "                n_hist_exog = len(self.hist_exog_list)\n",
    "                \n",
    "                # Reshape: [batch_size, n_hist_exog * input_size] -> [batch_size, n_hist_exog, input_size]\n",
    "                hist_exog = hist_exog_flat.reshape(batch_size, n_hist_exog, self.input_size)\n",
    "                \n",
    "                # Add n_series dimension: [batch_size, n_hist_exog, input_size] -> [batch_size, n_hist_exog, input_size, n_series]\n",
    "                # For multivariate, hist_exog features are typically repeated across series\n",
    "                hist_exog = hist_exog.unsqueeze(-1).repeat(1, 1, 1, n_series)\n",
    "            else:\n",
    "                hist_exog = None\n",
    "            \n",
    "            # Handle future exogenous for multivariate models (TSMixerx format)\n",
    "            if has_futr_exog and futr_exog_size > 0:\n",
    "                futr_exog_flat = X_tensor[:, current_idx:current_idx + futr_exog_size]\n",
    "                current_idx += futr_exog_size\n",
    "                n_futr_exog = len(self.futr_exog_list)\n",
    "                horizon = getattr(self, 'h', 1)\n",
    "                \n",
    "                if self.__class__.__name__ == 'NHITS':\n",
    "                    # NHITS (though it's univariate) would need input_size + horizon\n",
    "                    futr_exog_time_steps = self.input_size + horizon\n",
    "                else:\n",
    "                    # TSMixerx and other multivariate models expect just horizon\n",
    "                    futr_exog_time_steps = horizon\n",
    "                \n",
    "                # TSMixerx expects futr_exog as [batch_size, futr_exog_size, input_size + horizon, n_series]\n",
    "                # Reshape: [batch_size, n_futr_exog * time_steps] -> [batch_size, n_futr_exog, time_steps]\n",
    "                futr_exog = futr_exog_flat.reshape(batch_size, n_futr_exog, futr_exog_time_steps)\n",
    "                \n",
    "                if self.__class__.__name__ == 'TSMixerx':\n",
    "                    # For TSMixerx, we need to pad with input_size zeros at the beginning\n",
    "                    # since it expects [input_size + horizon] but we only have [horizon] from SHAP\n",
    "                    if futr_exog_time_steps == horizon:  # Only horizon provided\n",
    "                        zeros_pad = torch.zeros(batch_size, n_futr_exog, self.input_size)\n",
    "                        futr_exog = torch.cat([zeros_pad, futr_exog], dim=2)  # [batch_size, n_futr_exog, input_size + horizon]\n",
    "                    \n",
    "                    # Add n_series dimension: [batch_size, n_futr_exog, input_size + horizon] -> [batch_size, n_futr_exog, input_size + horizon, n_series]\n",
    "                    futr_exog = futr_exog.unsqueeze(-1).repeat(1, 1, 1, n_series)\n",
    "                else:\n",
    "                    # For other multivariate models, transpose to [batch_size, time_steps, n_futr_exog]\n",
    "                    futr_exog = futr_exog.transpose(1, 2)\n",
    "            else:\n",
    "                futr_exog = None\n",
    "            \n",
    "            # Handle static exogenous for multivariate models\n",
    "            if has_stat_exog and stat_exog_size > 0:\n",
    "                stat_exog_flat = X_tensor[:, current_idx:current_idx + stat_exog_size]\n",
    "                # For TSMixerx: reshape [batch_size, n_series * n_stat_features] -> [n_series, n_stat_features]\n",
    "                n_stat_features = len(self.stat_exog_list)\n",
    "                stat_exog = stat_exog_flat[0].reshape(n_series, n_stat_features)  # Take first batch sample\n",
    "            else:\n",
    "                stat_exog = None\n",
    "            \n",
    "            # For multivariate models, no insample_mask needed\n",
    "            insample_mask = None\n",
    "            \n",
    "        else:\n",
    "            # For univariate models: handle target + historical exog + future exog + static exog\n",
    "            batch_size = X_tensor.shape[0]\n",
    "            \n",
    "            # Calculate sizes\n",
    "            target_size = self.input_size\n",
    "            hist_exog_size = 0\n",
    "            futr_exog_size = 0\n",
    "            stat_exog_size = 0\n",
    "            \n",
    "            if has_hist_exog:\n",
    "                hist_exog_size = len(self.hist_exog_list) * self.input_size\n",
    "            if has_futr_exog:\n",
    "                horizon = getattr(self, 'h', 1)\n",
    "                # For NHITS, future exogenous spans input_size + horizon time steps\n",
    "                if self.__class__.__name__ == 'NHITS':\n",
    "                    futr_exog_size = len(self.futr_exog_list) * (self.input_size + horizon)\n",
    "                else:\n",
    "                    futr_exog_size = len(self.futr_exog_list) * horizon\n",
    "            if has_stat_exog:\n",
    "                stat_exog_size = len(self.stat_exog_list)\n",
    "            \n",
    "            # Split the flattened input\n",
    "            current_idx = 0\n",
    "            \n",
    "            # Target features\n",
    "            target_flat = X_tensor[:, current_idx:current_idx + target_size]\n",
    "            current_idx += target_size\n",
    "            \n",
    "            # Historical exogenous features\n",
    "            if has_hist_exog and hist_exog_size > 0:\n",
    "                hist_exog_flat = X_tensor[:, current_idx:current_idx + hist_exog_size]\n",
    "                current_idx += hist_exog_size\n",
    "                \n",
    "                # Reshape: [batch_size, n_hist_exog * input_size] -> [batch_size, input_size, n_hist_exog]\n",
    "                n_hist_exog = len(self.hist_exog_list)\n",
    "                hist_exog = hist_exog_flat.reshape(batch_size, n_hist_exog, self.input_size)\n",
    "                hist_exog = hist_exog.transpose(1, 2)\n",
    "            else:\n",
    "                hist_exog = None\n",
    "            \n",
    "            # Future exogenous features\n",
    "            if has_futr_exog and futr_exog_size > 0:\n",
    "                futr_exog_flat = X_tensor[:, current_idx:current_idx + futr_exog_size]\n",
    "                current_idx += futr_exog_size\n",
    "                n_futr_exog = len(self.futr_exog_list)\n",
    "                \n",
    "                # Handle different model requirements for future exogenous\n",
    "                if self.__class__.__name__ == 'NHITS':\n",
    "                    # NHITS expects [batch_size, input_size + horizon, n_futr_exog]\n",
    "                    horizon = getattr(self, 'h', 1)\n",
    "                    futr_exog_time_steps = self.input_size + horizon\n",
    "                    \n",
    "                    # Reshape: [batch_size, n_futr_exog * (input_size + horizon)] -> [batch_size, input_size + horizon, n_futr_exog]\n",
    "                    futr_exog = futr_exog_flat.reshape(batch_size, n_futr_exog, futr_exog_time_steps)\n",
    "                    futr_exog = futr_exog.transpose(1, 2)\n",
    "                else:\n",
    "                    # Other models expect [batch_size, horizon, n_futr_exog]\n",
    "                    horizon = getattr(self, 'h', 1)\n",
    "                    \n",
    "                    futr_exog = futr_exog_flat.reshape(batch_size, n_futr_exog, horizon)\n",
    "                    futr_exog = futr_exog.transpose(1, 2)\n",
    "            else:\n",
    "                futr_exog = None\n",
    "            \n",
    "            # Static exogenous features\n",
    "            if has_stat_exog and stat_exog_size > 0:\n",
    "                stat_exog_flat = X_tensor[:, current_idx:current_idx + stat_exog_size]\n",
    "                # Static exog is used as-is: [batch_size, n_stat_exog]\n",
    "                stat_exog = stat_exog_flat\n",
    "                print(f\"DEBUG: Static exog shape: {stat_exog.shape}\")\n",
    "            else:\n",
    "                stat_exog = None\n",
    "            \n",
    "            # Reshape target: [batch_size, input_size] -> [batch_size, input_size, 1]\n",
    "            insample_y = target_flat.unsqueeze(-1)\n",
    "            \n",
    "            # Create insample_mask based on non-zero values (for models like NHITS)\n",
    "            insample_mask = (target_flat != 0.0).float().unsqueeze(-1)\n",
    "        \n",
    "        # Move to same device as model\n",
    "        device = next(self.parameters()).device\n",
    "        insample_y = insample_y.to(device)\n",
    "        if insample_mask is not None:\n",
    "            insample_mask = insample_mask.to(device)     \n",
    "        if hist_exog is not None:\n",
    "            hist_exog = hist_exog.to(device)\n",
    "        if futr_exog is not None:\n",
    "            futr_exog = futr_exog.to(device)\n",
    "        if stat_exog is not None:\n",
    "            stat_exog = stat_exog.to(device)\n",
    "        \n",
    "        # Return dictionary matching model's forward method expected windows_batch format\n",
    "        windows_batch = {\n",
    "            'insample_y': insample_y,\n",
    "            'hist_exog': hist_exog,\n",
    "            'futr_exog': futr_exog,\n",
    "            'stat_exog': stat_exog   # Now includes actual static exogenous data\n",
    "        }\n",
    "        \n",
    "        # Add insample_mask only for univariate models that might need it\n",
    "        if not self.MULTIVARIATE:\n",
    "            windows_batch['insample_mask'] = insample_mask\n",
    "        \n",
    "        return windows_batch\n",
    "    \n",
    "    def _aggregate_shap_by_feature(self, shap_values: np.ndarray, \n",
    "                              feature_names: List[str], \n",
    "                              aggregate_method: str) -> Tuple[np.ndarray, List[str]]:\n",
    "        \"\"\"\n",
    "        Aggregate SHAP values across lags and steps for each feature.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        shap_values : np.ndarray\n",
    "            Original SHAP values [n_samples, n_features]\n",
    "        feature_names : List[str]\n",
    "            Original feature names with lag/step information\n",
    "        aggregate_method : str\n",
    "            Aggregation method: 'sum', 'mean', 'abs_sum', 'max_abs'\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        Tuple[np.ndarray, List[str]]\n",
    "            Aggregated SHAP values and corresponding feature names\n",
    "        \"\"\"\n",
    "        # Group feature indices by base name (without lag/step/series suffix)\n",
    "        base_features: Dict[str, List[int]] = {}\n",
    "        for i, name in enumerate(feature_names):\n",
    "            if '_lag_' in name:\n",
    "                # Extract base name by removing '_lag_X' suffix (historical features)\n",
    "                base_name = '_'.join(name.split('_')[:-2])\n",
    "            elif '_step_' in name:\n",
    "                # Extract base name by removing '_step_X' suffix (future exogenous features)\n",
    "                base_name = '_'.join(name.split('_')[:-2])\n",
    "            elif '_series_' in name and '_static' in name:\n",
    "                # Extract base name by removing '_series_X_static' suffix (multivariate static features)\n",
    "                parts = name.split('_')\n",
    "                # Find the index of 'series' and remove everything from there\n",
    "                if 'series' in parts:\n",
    "                    series_idx = parts.index('series')\n",
    "                    base_name = '_'.join(parts[:series_idx])\n",
    "                else:\n",
    "                    base_name = name\n",
    "            elif '_static' in name:\n",
    "                # Extract base name by removing '_static' suffix (univariate static features)\n",
    "                base_name = '_'.join(name.split('_')[:-1])\n",
    "            else:\n",
    "                # Feature without lag/step/static suffix (keep as is)\n",
    "                base_name = name\n",
    "            \n",
    "            if base_name not in base_features:\n",
    "                base_features[base_name] = []\n",
    "            base_features[base_name].append(i)\n",
    "        \n",
    "        # Aggregate SHAP values for each base feature\n",
    "        aggregated_shap_list = []\n",
    "        aggregated_names = []\n",
    "        \n",
    "        for base_name, indices in base_features.items():\n",
    "            feature_shap = shap_values[:, indices]  # [n_samples, n_lags_or_steps]\n",
    "            \n",
    "            if aggregate_method == 'sum':\n",
    "                agg_shap = np.sum(feature_shap, axis=1, keepdims=True)\n",
    "            elif aggregate_method == 'mean':\n",
    "                agg_shap = np.mean(feature_shap, axis=1, keepdims=True)\n",
    "            elif aggregate_method == 'abs_sum':\n",
    "                agg_shap = np.sum(np.abs(feature_shap), axis=1, keepdims=True)\n",
    "            elif aggregate_method == 'max_abs':\n",
    "                max_indices = np.argmax(np.abs(feature_shap), axis=1)\n",
    "                agg_shap = feature_shap[np.arange(len(feature_shap)), max_indices].reshape(-1, 1)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown aggregation method: {aggregate_method}\")\n",
    "            \n",
    "            aggregated_shap_list.append(agg_shap)\n",
    "            aggregated_names.append(base_name)\n",
    "        \n",
    "        # Concatenate along feature dimension\n",
    "        aggregated_shap_values = np.concatenate(aggregated_shap_list, axis=1)\n",
    "        \n",
    "        return aggregated_shap_values, aggregated_names\n",
    "\n",
    "    def _create_shap_wrapper(self):\n",
    "        \"\"\"\n",
    "        Create SHAP-compatible prediction wrapper for this model.\n",
    "        \"\"\"\n",
    "        def predict_for_shap(X_flat: np.ndarray) -> np.ndarray:\n",
    "            \"\"\"SHAP prediction function with debugging.\"\"\"\n",
    "            # Set model to eval mode\n",
    "            self.eval()\n",
    "            \n",
    "            # Parse input into model format\n",
    "            windows_batch = self._parse_shap_input(X_flat)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                try:\n",
    "                    # Call model forward method with windows_batch dictionary\n",
    "                    predictions = self.forward(windows_batch)\n",
    "                    \n",
    "                    # Handle different model output formats\n",
    "                    if isinstance(predictions, tuple):\n",
    "                        # Models like NBEATS return (backcast, forecast)\n",
    "                        forecast = predictions[1]\n",
    "                    else:\n",
    "                        forecast = predictions\n",
    "                    \n",
    "                    if self.MULTIVARIATE:\n",
    "                        # Multivariate models return [batch_size, horizon, n_series]\n",
    "                        # Flatten to [batch_size, horizon * n_series] for SHAP\n",
    "                        batch_size, horizon, n_series = forecast.shape\n",
    "                        forecast = forecast.reshape(batch_size, horizon * n_series)\n",
    "                    else:\n",
    "                        # Univariate models: handle different output shapes\n",
    "                        if len(forecast.shape) == 3 and forecast.shape[-1] > 1:\n",
    "                            forecast = forecast.mean(dim=-1)  # Average across output dimensions\n",
    "                        elif len(forecast.shape) == 3:\n",
    "                            forecast = forecast.squeeze(-1)   # Remove single output dimension\n",
    "                    \n",
    "                    # Return numpy array\n",
    "                    return forecast.cpu().numpy()\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    raise e\n",
    "        \n",
    "        return predict_for_shap\n",
    "\n",
    "    def explain_prediction(self, \n",
    "                    input_df: pd.DataFrame,\n",
    "                    X_df: Optional[pd.DataFrame] = None,\n",
    "                    static_df: Optional[pd.DataFrame] = None,\n",
    "                    background_size: int = 100,\n",
    "                    target_samples: Optional[int] = 10,\n",
    "                    explainer_type: str = 'kernel',\n",
    "                    aggregate_lags: Optional[str] = None) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Generate SHAP explanations for model predictions using specific input data with temporal windowing.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            import shap\n",
    "            import pandas as pd\n",
    "            import torch\n",
    "            from neuralforecast.tsdataset import TimeSeriesDataset\n",
    "            from torch.utils.data import DataLoader\n",
    "        except ImportError:\n",
    "            raise ImportError(\"SHAP library is required for explainability. Install with: pip install shap\")\n",
    "        \n",
    "        # Custom collate function to handle pandas Index objects - DEFINE FIRST\n",
    "        def custom_collate_fn(batch):\n",
    "            \"\"\"Custom collate function that handles pandas Index objects\"\"\"\n",
    "            if len(batch) == 0:\n",
    "                return {}\n",
    "            \n",
    "            first_sample = batch[0]\n",
    "            collated = {}\n",
    "            for key in first_sample.keys():\n",
    "                if key in ['temporal_cols', 'static_cols']:\n",
    "                    collated[key] = first_sample[key]\n",
    "                elif key == 'y_idx':\n",
    "                    collated[key] = first_sample[key]\n",
    "                else:\n",
    "                    values = [sample[key] for sample in batch]\n",
    "                    if values[0] is not None:\n",
    "                        collated[key] = torch.stack(values)\n",
    "                    else:\n",
    "                        collated[key] = None\n",
    "            return collated\n",
    "        \n",
    "        # Validate input dataframe has required columns\n",
    "        required_cols = ['unique_id', 'ds', 'y']\n",
    "        missing_cols = [col for col in required_cols if col not in input_df.columns]\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"input_df missing required columns: {missing_cols}\")\n",
    "        \n",
    "        print(\"DEBUG: Input validation passed\")\n",
    "        print(f\"DEBUG: input_df shape: {input_df.shape}\")\n",
    "        print(f\"DEBUG: input_df unique_ids: {input_df['unique_id'].unique()}\")\n",
    "        \n",
    "        # Check if we have sufficient historical data\n",
    "        min_required_periods = self.input_size\n",
    "        series_lengths = input_df.groupby('unique_id').size()\n",
    "        insufficient_series = series_lengths[series_lengths < min_required_periods]\n",
    "        if len(insufficient_series) > 0:\n",
    "            raise ValueError(f\"Some series have insufficient history. Need at least {min_required_periods} periods.\")\n",
    "        \n",
    "        print(\"DEBUG: Historical data validation passed\")\n",
    "        \n",
    "        # Validate exogenous features alignment\n",
    "        has_hist_exog = (hasattr(self, 'EXOGENOUS_HIST') and self.EXOGENOUS_HIST and \n",
    "                        hasattr(self, 'hist_exog_list') and self.hist_exog_list)\n",
    "        has_futr_exog = (hasattr(self, 'EXOGENOUS_FUTR') and self.EXOGENOUS_FUTR and \n",
    "                        hasattr(self, 'futr_exog_list') and self.futr_exog_list)\n",
    "        has_stat_exog = (hasattr(self, 'EXOGENOUS_STAT') and self.EXOGENOUS_STAT and \n",
    "                        hasattr(self, 'stat_exog_list') and self.stat_exog_list)\n",
    "        \n",
    "        print(f\"DEBUG: Exogenous feature flags - hist: {has_hist_exog}, futr: {has_futr_exog}, stat: {has_stat_exog}\")\n",
    "        \n",
    "        # Validate exogenous features\n",
    "        if has_hist_exog:\n",
    "            missing_hist_exog = [col for col in self.hist_exog_list if col not in input_df.columns]\n",
    "            if missing_hist_exog:\n",
    "                raise ValueError(f\"Model expects historical exogenous features but they're missing from input_df: {missing_hist_exog}\")\n",
    "        \n",
    "        if has_futr_exog:\n",
    "            if X_df is None:\n",
    "                raise ValueError(\"Model expects future exogenous features but X_df is None\")\n",
    "            missing_futr_exog = [col for col in self.futr_exog_list if col not in X_df.columns]\n",
    "            if missing_futr_exog:\n",
    "                raise ValueError(f\"Model expects future exogenous features but they're missing from X_df: {missing_futr_exog}\")\n",
    "            if 'unique_id' not in X_df.columns or 'ds' not in X_df.columns:\n",
    "                raise ValueError(\"X_df must contain 'unique_id' and 'ds' columns\")\n",
    "        \n",
    "        if has_stat_exog:\n",
    "            if static_df is None:\n",
    "                raise ValueError(\"Model expects static exogenous features but static_df is None\")\n",
    "            missing_stat_exog = [col for col in self.stat_exog_list if col not in static_df.columns]\n",
    "            if missing_stat_exog:\n",
    "                raise ValueError(f\"Model expects static exogenous features but they're missing from static_df: {missing_stat_exog}\")\n",
    "            if 'unique_id' not in static_df.columns:\n",
    "                raise ValueError(\"static_df must contain 'unique_id' column\")\n",
    "        \n",
    "        try:\n",
    "            # Always use temporal windowing - it's the right approach for time series\n",
    "            print(\"DEBUG: Using temporal windowing approach for all feature types\")\n",
    "            \n",
    "            horizon = getattr(self, 'h', 1)\n",
    "            windows_list = []\n",
    "            \n",
    "            # Group by series to handle each one separately  \n",
    "            for series_id, series_data in input_df.groupby('unique_id'):\n",
    "                series_data = series_data.sort_values('ds').reset_index(drop=True)\n",
    "                series_length = len(series_data)\n",
    "                \n",
    "                print(f\"DEBUG: Creating windows for series {series_id}, length {series_length}\")\n",
    "                \n",
    "            # Group by series to handle each one separately  \n",
    "            for series_id, series_data in input_df.groupby('unique_id'):\n",
    "                series_data = series_data.sort_values('ds').reset_index(drop=True)\n",
    "                series_length = len(series_data)\n",
    "                \n",
    "                print(f\"DEBUG: Creating windows for series {series_id}, length {series_length}\")\n",
    "                \n",
    "                # For background windows: we need different amounts of data based on whether future exog is used\n",
    "                if has_futr_exog and X_df is not None:\n",
    "                    # With future exog: background windows need input_size + horizon to create historical \"future\" exog\n",
    "                    min_background_data = self.input_size + horizon\n",
    "                    print(f\"DEBUG: With future exog - background windows need {min_background_data} periods each\")\n",
    "                else:\n",
    "                    # Without future exog: background windows only need input_size periods\n",
    "                    min_background_data = self.input_size\n",
    "                    print(f\"DEBUG: No future exog - background windows need {min_background_data} periods each\")\n",
    "                \n",
    "                # Calculate how many background windows we can create from historical data\n",
    "                # Leave the last input_size periods for the target window\n",
    "                available_for_background = series_length - self.input_size\n",
    "                max_possible_background = available_for_background - min_background_data + 1\n",
    "                max_background_windows = min(background_size, max_possible_background)\n",
    "                \n",
    "                if max_background_windows < 1:\n",
    "                    print(f\"DEBUG: Series {series_id} has insufficient data for background windows\")\n",
    "                    print(f\"DEBUG: Need {min_background_data} + {self.input_size} = {min_background_data + self.input_size} total periods\")\n",
    "                    print(f\"DEBUG: Have {series_length} periods\")\n",
    "                    continue\n",
    "                \n",
    "                print(f\"DEBUG: Can create {max_background_windows} background windows + 1 target window\")\n",
    "                \n",
    "                # Get future exogenous data for this series if available\n",
    "                series_X_df = None\n",
    "                if X_df is not None and has_futr_exog:\n",
    "                    series_X_df = X_df[X_df['unique_id'] == series_id].sort_values('ds').reset_index(drop=True)\n",
    "                    print(f\"DEBUG: Found {len(series_X_df)} future exog periods for series {series_id}\")\n",
    "                \n",
    "                # Create BACKGROUND windows from historical periods\n",
    "                for window_idx in range(max_background_windows):\n",
    "                    start_idx = window_idx\n",
    "                    hist_end_idx = start_idx + self.input_size\n",
    "                    \n",
    "                    # Get historical data for this background window\n",
    "                    historical_data = series_data.iloc[start_idx:hist_end_idx].copy()\n",
    "                    \n",
    "                    # Start with the historical data\n",
    "                    window_data = historical_data.copy()\n",
    "                    \n",
    "                    # IMPORTANT: If model expects future exog, background windows MUST have it too!\n",
    "                    if series_X_df is not None and has_futr_exog:\n",
    "                        print(f\"DEBUG: Background window {window_idx} - adding historical future exog to match model expectations\")\n",
    "                        \n",
    "                        # For background windows: use subsequent historical periods as \"future\" exog\n",
    "                        future_start_idx = hist_end_idx\n",
    "                        future_end_idx = future_start_idx + horizon\n",
    "                        \n",
    "                        if future_end_idx <= series_length:\n",
    "                            # Use actual historical data as \"future\" exog for this background window\n",
    "                            historical_future_data = series_data.iloc[future_start_idx:future_end_idx].copy()\n",
    "                            \n",
    "                            if self.__class__.__name__ == 'NHITS':\n",
    "                                # NHITS needs input_size + horizon periods for future exog\n",
    "                                # Use last input_size from historical + horizon from future\n",
    "                                hist_futr_start = max(0, hist_end_idx - self.input_size)\n",
    "                                hist_futr_data = series_data.iloc[hist_futr_start:hist_end_idx]\n",
    "                                \n",
    "                                # Combine historical + future periods\n",
    "                                combined_bg_futr = pd.concat([hist_futr_data, historical_future_data], ignore_index=True)\n",
    "                            else:\n",
    "                                # Other models just need horizon periods\n",
    "                                combined_bg_futr = historical_future_data\n",
    "                            \n",
    "                            # Extract only the future exogenous columns\n",
    "                            futr_exog_data = combined_bg_futr[['unique_id', 'ds'] + self.futr_exog_list].copy()\n",
    "                            \n",
    "                            # Set y to 0 for future periods\n",
    "                            futr_exog_data['y'] = 0.0\n",
    "                            \n",
    "                            # Add historical exog columns with 0 values\n",
    "                            if has_hist_exog:\n",
    "                                for hist_col in self.hist_exog_list:\n",
    "                                    futr_exog_data[hist_col] = 0.0\n",
    "                            \n",
    "                            # Combine historical + historical \"future\" exog data\n",
    "                            window_data = pd.concat([historical_data, futr_exog_data], ignore_index=True, sort=False)\n",
    "                            \n",
    "                            print(f\"DEBUG: Background window {window_idx} using {len(futr_exog_data)} periods of historical future exog\")\n",
    "                        else:\n",
    "                            print(f\"DEBUG: Background window {window_idx} - insufficient data for future exog, skipping\")\n",
    "                            continue  # Skip this background window if we can't create proper future exog\n",
    "                    \n",
    "                    # Create unique identifier for each background window\n",
    "                    window_data['unique_id'] = f\"{series_id}_bg_{window_idx}\"\n",
    "                    window_data['window_idx'] = window_idx\n",
    "                    window_data['original_series_id'] = series_id\n",
    "                    window_data['is_target'] = False\n",
    "                    \n",
    "                    windows_list.append(window_data)\n",
    "                    \n",
    "                    print(f\"DEBUG: Created background window {window_idx} for series {series_id}: \"\n",
    "                        f\"dates {window_data['ds'].iloc[0]} to {window_data['ds'].iloc[-1]}, \"\n",
    "                        f\"total periods: {len(window_data)}\")\n",
    "                \n",
    "                # Create TARGET window - the actual prediction scenario\n",
    "                # Use the most recent input_size periods from training data\n",
    "                target_start_idx = series_length - self.input_size\n",
    "                target_historical_data = series_data.iloc[target_start_idx:].copy()\n",
    "                \n",
    "                target_window_data = target_historical_data.copy()\n",
    "                \n",
    "                # Add future exogenous data if provided (this creates the actual prediction scenario)\n",
    "                if series_X_df is not None and has_futr_exog:\n",
    "                    print(\"DEBUG: Target window - adding actual future exog data from X_df\")\n",
    "                    \n",
    "                    # For NHITS: need input_size + horizon future exog periods\n",
    "                    if self.__class__.__name__ == 'NHITS':\n",
    "                        # NHITS needs both historical and future periods for future exog\n",
    "                        # Use last input_size periods from historical + all future exog data\n",
    "                        hist_futr_start = max(0, series_length - self.input_size)\n",
    "                        hist_futr_data = series_data.iloc[hist_futr_start:series_length]\n",
    "                        \n",
    "                        # Use the actual future exog data from X_df\n",
    "                        actual_futr_data = series_X_df.copy()\n",
    "                        \n",
    "                        # Combine them for future exog\n",
    "                        futr_exog_data = pd.concat([hist_futr_data[['unique_id', 'ds'] + self.futr_exog_list], \n",
    "                                                actual_futr_data[['unique_id', 'ds'] + self.futr_exog_list]], \n",
    "                                                ignore_index=True)\n",
    "                    else:\n",
    "                        # Other models just need actual future data\n",
    "                        futr_exog_data = series_X_df[['unique_id', 'ds'] + self.futr_exog_list].copy()\n",
    "                    \n",
    "                    # Set y to 0 for future periods\n",
    "                    futr_exog_data['y'] = 0.0\n",
    "                    \n",
    "                    # Add historical exog columns with 0 values for future periods\n",
    "                    if has_hist_exog:\n",
    "                        for hist_col in self.hist_exog_list:\n",
    "                            futr_exog_data[hist_col] = 0.0\n",
    "                    \n",
    "                    print(f\"DEBUG: Target window using {len(futr_exog_data)} periods of actual future exog\")\n",
    "                    \n",
    "                    # Combine historical + actual future exog data for the target window\n",
    "                    target_window_data = pd.concat([target_historical_data, futr_exog_data], ignore_index=True, sort=False)\n",
    "                \n",
    "                # Create unique identifier for target window\n",
    "                target_window_data['unique_id'] = f\"{series_id}_target\"\n",
    "                target_window_data['window_idx'] = max_background_windows\n",
    "                target_window_data['original_series_id'] = series_id\n",
    "                target_window_data['is_target'] = True\n",
    "                \n",
    "                windows_list.append(target_window_data)\n",
    "                \n",
    "                futr_exog_info = f\"with {len(series_X_df)} future exog periods\" if series_X_df is not None else \"no future exog\"\n",
    "                print(f\"DEBUG: Created target window for series {series_id}: \"\n",
    "                    f\"dates {target_historical_data['ds'].iloc[0]} to {target_historical_data['ds'].iloc[-1]}, \"\n",
    "                    f\"periods: {len(target_window_data)}, {futr_exog_info}\")\n",
    "                \n",
    "                print(f\"DEBUG: Total windows created for {series_id}: {max_background_windows} background + 1 target\")\n",
    "            \n",
    "            if not windows_list:\n",
    "                raise ValueError(\"No valid windows could be created from the provided data\")\n",
    "            \n",
    "            # Combine all windows into a single dataframe\n",
    "            all_windows_df = pd.concat(windows_list, ignore_index=True)\n",
    "            \n",
    "            # Fill any remaining NaN values\n",
    "            all_windows_df = all_windows_df.fillna(0.0)\n",
    "            \n",
    "            print(f\"DEBUG: Created {len(windows_list)} total windows\")\n",
    "            print(f\"DEBUG: Background windows: {sum(1 for w in windows_list if not w['is_target'].iloc[0])}\")\n",
    "            print(f\"DEBUG: Target windows: {sum(1 for w in windows_list if w['is_target'].iloc[0])}\")\n",
    "            \n",
    "            # Prepare static_df for windows if needed\n",
    "            window_static_df = None\n",
    "            if static_df is not None:\n",
    "                static_rows = []\n",
    "                for window_data in windows_list:\n",
    "                    original_series = window_data['original_series_id'].iloc[0]\n",
    "                    window_unique_id = window_data['unique_id'].iloc[0]\n",
    "                    \n",
    "                    original_static = static_df[static_df['unique_id'] == original_series]\n",
    "                    if len(original_static) > 0:\n",
    "                        window_static = original_static.copy()\n",
    "                        window_static['unique_id'] = window_unique_id\n",
    "                        static_rows.append(window_static)\n",
    "                \n",
    "                if static_rows:\n",
    "                    window_static_df = pd.concat(static_rows, ignore_index=True)\n",
    "                    print(f\"DEBUG: Created static data for {len(static_rows)} windows\")\n",
    "            \n",
    "            # Create dataset using the standard method\n",
    "            dataset, indices, dates, ds = TimeSeriesDataset.from_df(\n",
    "                df=all_windows_df.drop(['window_idx', 'original_series_id', 'is_target'], axis=1),\n",
    "                static_df=window_static_df,\n",
    "                id_col='unique_id',\n",
    "                time_col='ds',\n",
    "                target_col='y'\n",
    "            )\n",
    "            \n",
    "            print(f\"DEBUG: TimeSeriesDataset created with {len(dataset)} samples\")\n",
    "            \n",
    "            # Use temporal windowing sampling logic  \n",
    "            dataloader = DataLoader(\n",
    "                dataset, \n",
    "                batch_size=len(dataset), \n",
    "                shuffle=False,\n",
    "                collate_fn=custom_collate_fn\n",
    "            )\n",
    "            batch = next(iter(dataloader))\n",
    "            \n",
    "            all_features = self._extract_target_features(batch)\n",
    "            \n",
    "            # Count background and target windows\n",
    "            background_windows = sum(1 for w in windows_list if not w['is_target'].iloc[0])\n",
    "            target_windows = sum(1 for w in windows_list if w['is_target'].iloc[0])\n",
    "            \n",
    "            background_indices = list(range(background_windows))\n",
    "            target_indices = list(range(background_windows, background_windows + target_windows))\n",
    "            \n",
    "            if len(target_indices) == 0:\n",
    "                raise ValueError(\"No target windows available for explanation.\")\n",
    "            \n",
    "            background_data = all_features[background_indices]\n",
    "            target_data = all_features[target_indices]\n",
    "            \n",
    "            print(f\"DEBUG: Temporal windowing - Background: {background_data.shape}, Target: {target_data.shape}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Failed to create TimeSeriesDataset from provided data: {str(e)}\")\n",
    "        \n",
    "        if len(target_data) == 0:\n",
    "            raise ValueError(\"No target samples available for explanation.\")\n",
    "        \n",
    "        # Create SHAP explainer\n",
    "        prediction_wrapper = self._create_shap_wrapper()\n",
    "        \n",
    "        if explainer_type == 'kernel':\n",
    "            explainer = shap.KernelExplainer(prediction_wrapper, background_data)\n",
    "        else:\n",
    "            raise ValueError(f\"Explainer type '{explainer_type}' not supported yet\")\n",
    "        \n",
    "        # Generate SHAP values\n",
    "        shap_values = explainer.shap_values(target_data)\n",
    "        \n",
    "        # Get original feature names (with individual lags)\n",
    "        original_feature_names = self._get_feature_names()\n",
    "        \n",
    "        # Apply lag aggregation if requested\n",
    "        if aggregate_lags is not None:\n",
    "            shap_values, feature_names = self._aggregate_shap_by_feature(\n",
    "                shap_values, original_feature_names, aggregate_lags\n",
    "            )\n",
    "        else:\n",
    "            feature_names = original_feature_names\n",
    "        \n",
    "        # Prepare results\n",
    "        results = {\n",
    "            'shap_values': shap_values,\n",
    "            'feature_names': feature_names,\n",
    "            'background_data': background_data,\n",
    "            'target_data': target_data,\n",
    "            'base_values': explainer.expected_value,\n",
    "            'model_name': self.__class__.__name__,\n",
    "            'model_alias': getattr(self, 'alias', None),\n",
    "            'is_multivariate': self.MULTIVARIATE,\n",
    "            'aggregate_lags': aggregate_lags,\n",
    "            'original_feature_names': original_feature_names,\n",
    "            'explained_periods': len(target_data),\n",
    "            'background_periods': len(background_data),\n",
    "            'total_series': len(input_df['unique_id'].unique()),\n",
    "            'explanation_type': 'temporal_windows',\n",
    "            'window_info': {\n",
    "                'input_size': self.input_size,\n",
    "                'horizon': getattr(self, 'h', 1),\n",
    "                'background_samples': len(background_data),\n",
    "                'target_samples': len(target_data),\n",
    "                'has_futr_exog': has_futr_exog,\n",
    "                'has_hist_exog': has_hist_exog,\n",
    "                'has_stat_exog': has_stat_exog,\n",
    "                'total_windows_created': len(windows_list)\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def on_fit_start(self):\n",
    "        torch.manual_seed(self.random_seed)\n",
    "        np.random.seed(self.random_seed)\n",
    "        random.seed(self.random_seed)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        if self.optimizer:\n",
    "            optimizer_signature = inspect.signature(self.optimizer)\n",
    "            optimizer_kwargs = deepcopy(self.optimizer_kwargs)\n",
    "            if 'lr' in optimizer_signature.parameters:\n",
    "                if 'lr' in optimizer_kwargs:\n",
    "                    warnings.warn(\"ignoring learning rate passed in optimizer_kwargs, using the model's learning rate\")\n",
    "                optimizer_kwargs['lr'] = self.learning_rate\n",
    "            optimizer = self.optimizer(params=self.parameters(), **optimizer_kwargs)\n",
    "        else:\n",
    "            if self.optimizer_kwargs:\n",
    "                warnings.warn(\n",
    "                    \"ignoring optimizer_kwargs as the optimizer is not specified\"\n",
    "                )            \n",
    "            optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        \n",
    "        lr_scheduler = {'frequency': 1, 'interval': 'step'}\n",
    "        if self.lr_scheduler:\n",
    "            lr_scheduler_signature = inspect.signature(self.lr_scheduler)\n",
    "            lr_scheduler_kwargs = deepcopy(self.lr_scheduler_kwargs)\n",
    "            if 'optimizer' in lr_scheduler_signature.parameters:\n",
    "                if 'optimizer' in lr_scheduler_kwargs:\n",
    "                    warnings.warn(\"ignoring optimizer passed in lr_scheduler_kwargs, using the model's optimizer\")\n",
    "                    del lr_scheduler_kwargs['optimizer']\n",
    "            lr_scheduler['scheduler'] = self.lr_scheduler(optimizer=optimizer, **lr_scheduler_kwargs)\n",
    "        else:\n",
    "            if self.lr_scheduler_kwargs:\n",
    "                warnings.warn(\n",
    "                    \"ignoring lr_scheduler_kwargs as the lr_scheduler is not specified\"\n",
    "                )            \n",
    "            lr_scheduler['scheduler'] = torch.optim.lr_scheduler.StepLR(\n",
    "                optimizer=optimizer, step_size=self.lr_decay_steps, gamma=0.5\n",
    "            )\n",
    "        return {'optimizer': optimizer, 'lr_scheduler': lr_scheduler}\n",
    "\n",
    "    def get_test_size(self):\n",
    "        return self.test_size\n",
    "\n",
    "    def set_test_size(self, test_size):\n",
    "        self.test_size = test_size\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        if self.val_size == 0:\n",
    "            return\n",
    "        losses = torch.stack(self.validation_step_outputs)\n",
    "        avg_loss = losses.mean().detach().item()\n",
    "        self.log(\n",
    "            \"ptl/val_loss\",\n",
    "            avg_loss,\n",
    "            batch_size=losses.size(0),\n",
    "            sync_dist=True,\n",
    "        )\n",
    "        self.valid_trajectories.append((self.global_step, avg_loss))\n",
    "        self.validation_step_outputs.clear() # free memory (compute `avg_loss` per epoch)\n",
    "\n",
    "    def save(self, path):\n",
    "        with fsspec.open(path, 'wb') as f:\n",
    "            torch.save(\n",
    "                {'hyper_parameters': self.hparams, 'state_dict': self.state_dict()},\n",
    "                f,\n",
    "            )\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, path, **kwargs):\n",
    "        if \"weights_only\" in inspect.signature(torch.load).parameters:\n",
    "            kwargs[\"weights_only\"] = False\n",
    "        with fsspec.open(path, 'rb') as f, warnings.catch_warnings():\n",
    "            # ignore possible warnings about weights_only=False\n",
    "            warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "            content = torch.load(f, **kwargs)\n",
    "        with _disable_torch_init():\n",
    "            model = cls(**content['hyper_parameters']) \n",
    "        if \"assign\" in inspect.signature(model.load_state_dict).parameters:\n",
    "            model.load_state_dict(content[\"state_dict\"], strict=True, assign=True)\n",
    "        else:  # pytorch<2.1\n",
    "            model.load_state_dict(content[\"state_dict\"], strict=True)\n",
    "        return model\n",
    "\n",
    "    def _create_windows(self, batch, step):\n",
    "        # Parse common data\n",
    "        window_size = self.input_size + self.h\n",
    "        temporal_cols = batch['temporal_cols']\n",
    "        temporal = batch['temporal']                \n",
    "\n",
    "        if step == 'train':\n",
    "            if self.val_size + self.test_size > 0:\n",
    "                cutoff = -self.val_size - self.test_size\n",
    "                temporal = temporal[:, :, :cutoff]\n",
    "\n",
    "            temporal = self.padder_train(temporal)\n",
    "            \n",
    "            if temporal.shape[-1] < window_size:\n",
    "                raise Exception('Time series is too short for training, consider setting a smaller input size or set start_padding_enabled=True')\n",
    "            \n",
    "            windows = temporal.unfold(dimension=-1, \n",
    "                                      size=window_size, \n",
    "                                      step=self.step_size)\n",
    "\n",
    "            if self.MULTIVARIATE:\n",
    "                # [n_series, C, Ws, L + h] -> [Ws, L + h, C, n_series]\n",
    "                windows = windows.permute(2, 3, 1, 0)\n",
    "            else:\n",
    "                # [n_series, C, Ws, L + h] -> [Ws * n_series, L + h, C, 1]\n",
    "                windows_per_serie = windows.shape[2]\n",
    "                windows = windows.permute(0, 2, 3, 1)\n",
    "                windows = windows.flatten(0, 1)\n",
    "                windows = windows.unsqueeze(-1)\n",
    "\n",
    "            # Sample and Available conditions\n",
    "            available_idx = temporal_cols.get_loc('available_mask')           \n",
    "            available_condition = windows[:, :self.input_size, available_idx]\n",
    "            available_condition = torch.sum(available_condition, axis=(1, -1)) # Sum over time & series dimension\n",
    "            final_condition = (available_condition > 0)\n",
    "            \n",
    "            if self.h > 0:\n",
    "                sample_condition = windows[:, self.input_size:, available_idx]\n",
    "                sample_condition = torch.sum(sample_condition, axis=(1, -1)) # Sum over time & series dimension\n",
    "                final_condition = (sample_condition > 0) & (available_condition > 0)\n",
    "            \n",
    "            windows = windows[final_condition]\n",
    "\n",
    "            # Parse Static data to match windows\n",
    "            static = batch.get(\"static\", None)\n",
    "            static_cols = batch.get(\"static_cols\", None)\n",
    "            \n",
    "            # Repeat static if univariate: [n_series, S] -> [Ws * n_series, S]\n",
    "            if static is not None and not self.MULTIVARIATE:\n",
    "                static = torch.repeat_interleave(static, \n",
    "                                    repeats=windows_per_serie, dim=0)\n",
    "                static = static[final_condition]        \n",
    "\n",
    "            # Protection of empty windows\n",
    "            if final_condition.sum() == 0:\n",
    "                raise Exception('No windows available for training')\n",
    "\n",
    "            return windows, static, static_cols\n",
    "\n",
    "        elif step in ['predict', 'val']:\n",
    "\n",
    "            if step == 'predict':\n",
    "                initial_input = temporal.shape[-1] - self.test_size\n",
    "                if initial_input <= self.input_size: # There is not enough data to predict first timestamp\n",
    "                    temporal = F.pad(temporal, pad=(self.input_size-initial_input, 0), mode=\"constant\", value=0.0)\n",
    "                predict_step_size = self.predict_step_size\n",
    "                cutoff = - self.input_size - self.test_size\n",
    "                temporal = temporal[:, :, cutoff:]\n",
    "\n",
    "            elif step == 'val':\n",
    "                predict_step_size = self.step_size\n",
    "                cutoff = -self.input_size - self.val_size - self.test_size\n",
    "                if self.test_size > 0:\n",
    "                    temporal = batch['temporal'][:, :, cutoff:-self.test_size]\n",
    "                else:\n",
    "                    temporal = batch['temporal'][:, :, cutoff:]\n",
    "                if temporal.shape[-1] < window_size:\n",
    "                    initial_input = temporal.shape[-1] - self.val_size\n",
    "                    temporal = F.pad(temporal, pad=(self.input_size-initial_input, 0), mode=\"constant\", value=0.0)\n",
    "\n",
    "            if (step=='predict') and (self.test_size==0) and (len(self.futr_exog_list)==0):\n",
    "                temporal = F.pad(temporal, pad=(0, self.h), mode=\"constant\", value=0.0)\n",
    "\n",
    "            windows = temporal.unfold(dimension=-1,\n",
    "                                      size=window_size,\n",
    "                                      step=predict_step_size)\n",
    "\n",
    "            static = batch.get('static', None)\n",
    "            static_cols=batch.get('static_cols', None)\n",
    "\n",
    "            if self.MULTIVARIATE:\n",
    "                # [n_series, C, Ws, L + h] -> [Ws, L + h, C, n_series]\n",
    "                windows = windows.permute(2, 3, 1, 0)\n",
    "            else:\n",
    "                # [n_series, C, Ws, L + h] -> [Ws * n_series, L + h, C, 1]\n",
    "                windows_per_serie = windows.shape[2]\n",
    "                windows = windows.permute(0, 2, 3, 1)\n",
    "                windows = windows.flatten(0, 1)\n",
    "                windows = windows.unsqueeze(-1)\n",
    "                if static is not None:\n",
    "                    static = torch.repeat_interleave(static, \n",
    "                                    repeats=windows_per_serie, dim=0)\n",
    "\n",
    "            return windows, static, static_cols\n",
    "        else:\n",
    "            raise ValueError(f'Unknown step {step}') \n",
    "\n",
    "    def _normalization(self, windows, y_idx):\n",
    "        # windows are already filtered by train/validation/test\n",
    "        # from the `create_windows_method` nor leakage risk\n",
    "        temporal = windows['temporal']                  # [Ws, L + h, C, n_series]\n",
    "        temporal_cols = windows['temporal_cols'].copy() # [Ws, L + h, C, n_series]\n",
    "\n",
    "        # To avoid leakage uses only the lags\n",
    "        temporal_data_cols = self._get_temporal_exogenous_cols(temporal_cols=temporal_cols)\n",
    "        temporal_idxs = get_indexer_raise_missing(temporal_cols, temporal_data_cols)\n",
    "        temporal_idxs = np.append(y_idx, temporal_idxs)\n",
    "        temporal_data = temporal[:, :, temporal_idxs] \n",
    "        temporal_mask = temporal[:, :, temporal_cols.get_loc('available_mask')].clone()\n",
    "        if self.h > 0:\n",
    "            temporal_mask[:, -self.h:] = 0.0\n",
    "\n",
    "        # Normalize. self.scaler stores the shift and scale for inverse transform\n",
    "        temporal_mask = temporal_mask.unsqueeze(2) # Add channel dimension for scaler.transform.\n",
    "        temporal_data = self.scaler.transform(x=temporal_data, mask=temporal_mask)\n",
    "\n",
    "        # Replace values in windows dict\n",
    "        temporal[:, :, temporal_idxs] = temporal_data\n",
    "        windows['temporal'] = temporal\n",
    "\n",
    "        return windows\n",
    "\n",
    "    def _inv_normalization(self, y_hat, y_idx):\n",
    "        # Receives window predictions [Ws, h, output, n_series]\n",
    "        # Broadcasts scale if necessary and inverts normalization\n",
    "        add_channel_dim = y_hat.ndim > 3\n",
    "        y_loc, y_scale = self._get_loc_scale(y_idx, add_channel_dim=add_channel_dim)\n",
    "        y_hat = self.scaler.inverse_transform(z=y_hat, x_scale=y_scale, x_shift=y_loc)\n",
    "\n",
    "        return y_hat\n",
    "    \n",
    "    def _sample_windows(self, windows_temporal, static, static_cols, temporal_cols, step, w_idxs=None):\n",
    "        if step == 'train' and self.windows_batch_size is not None:\n",
    "            n_windows = windows_temporal.shape[0]\n",
    "            w_idxs = np.random.choice(n_windows, \n",
    "                                        size=self.windows_batch_size,\n",
    "                                        replace=(n_windows < self.windows_batch_size))\n",
    "        windows_sample = windows_temporal\n",
    "        if w_idxs is not None:\n",
    "            windows_sample = windows_temporal[w_idxs]\n",
    "            \n",
    "            if static is not None and not self.MULTIVARIATE:\n",
    "                static = static[w_idxs]\n",
    "\n",
    "        windows_batch = dict(temporal=windows_sample,\n",
    "                                temporal_cols=temporal_cols,\n",
    "                                static=static,\n",
    "                                static_cols=static_cols)\n",
    "        return windows_batch\n",
    "\n",
    "    def _parse_windows(self, batch, windows):\n",
    "        # windows: [Ws, L + h, C, n_series]\n",
    "\n",
    "        # Filter insample lags from outsample horizon\n",
    "        y_idx = batch['y_idx']\n",
    "        mask_idx = batch['temporal_cols'].get_loc('available_mask')\n",
    "\n",
    "        insample_y = windows['temporal'][:, :self.input_size, y_idx]\n",
    "        insample_mask = windows['temporal'][:, :self.input_size, mask_idx]\n",
    "\n",
    "        # Declare additional information\n",
    "        outsample_y = None\n",
    "        outsample_mask = None\n",
    "        hist_exog = None\n",
    "        futr_exog = None\n",
    "        stat_exog = None\n",
    "\n",
    "        if self.h > 0:\n",
    "            outsample_y = windows['temporal'][:, self.input_size:, y_idx]\n",
    "            outsample_mask = windows['temporal'][:, self.input_size:, mask_idx]\n",
    "\n",
    "        # Recurrent models at t predict t+1, so we shift the input (insample_y) by one\n",
    "        if self.RECURRENT:\n",
    "            insample_y = torch.cat((insample_y, outsample_y[:, :-1]), dim=1)\n",
    "            insample_mask = torch.cat((insample_mask, outsample_mask[:, :-1]), dim=1)\n",
    "            self.maintain_state = False\n",
    "\n",
    "        if len(self.hist_exog_list):\n",
    "            hist_exog_idx = get_indexer_raise_missing(windows['temporal_cols'], self.hist_exog_list)\n",
    "            if self.RECURRENT:\n",
    "                hist_exog = windows['temporal'][:, :, hist_exog_idx]\n",
    "                hist_exog[:, self.input_size:] = 0.0\n",
    "                hist_exog = hist_exog[:, 1:]\n",
    "            else:\n",
    "                hist_exog = windows['temporal'][:, :self.input_size, hist_exog_idx]\n",
    "            if not self.MULTIVARIATE:\n",
    "                hist_exog = hist_exog.squeeze(-1)\n",
    "            else:\n",
    "                hist_exog = hist_exog.swapaxes(1, 2)\n",
    "\n",
    "        if len(self.futr_exog_list):\n",
    "            futr_exog_idx = get_indexer_raise_missing(windows['temporal_cols'], self.futr_exog_list)\n",
    "            futr_exog = windows['temporal'][:, :, futr_exog_idx]\n",
    "            if self.RECURRENT:\n",
    "                futr_exog = futr_exog[:, 1:]\n",
    "            if not self.MULTIVARIATE:\n",
    "                futr_exog = futr_exog.squeeze(-1)\n",
    "            else:\n",
    "                futr_exog = futr_exog.swapaxes(1, 2)                \n",
    "\n",
    "        if len(self.stat_exog_list):\n",
    "            static_idx = get_indexer_raise_missing(windows['static_cols'], self.stat_exog_list)\n",
    "            stat_exog = windows['static'][:, static_idx]\n",
    "\n",
    "        # TODO: think a better way of removing insample_y features\n",
    "        if self.exclude_insample_y:\n",
    "            insample_y = insample_y * 0\n",
    "\n",
    "        return insample_y, insample_mask, outsample_y, outsample_mask, \\\n",
    "               hist_exog, futr_exog, stat_exog     \n",
    "\n",
    "    def _get_loc_scale(self, y_idx, add_channel_dim=False):\n",
    "        # [B, L, C, n_series] -> [B, L, n_series]\n",
    "        y_scale = self.scaler.x_scale[:, :, y_idx]\n",
    "        y_loc = self.scaler.x_shift[:, :, y_idx]\n",
    "        \n",
    "        # [B, L, n_series] -> [B, L, n_series, 1]\n",
    "        if add_channel_dim:\n",
    "            y_scale = y_scale.unsqueeze(-1)\n",
    "            y_loc = y_loc.unsqueeze(-1)\n",
    "\n",
    "        return y_loc, y_scale\n",
    "\n",
    "    def _compute_valid_loss(self, insample_y, outsample_y, output, outsample_mask, y_idx):\n",
    "        if self.loss.is_distribution_output:\n",
    "            y_loc, y_scale = self._get_loc_scale(y_idx)\n",
    "            distr_args = self.loss.scale_decouple(output=output, loc=y_loc, scale=y_scale)\n",
    "            if isinstance(self.valid_loss, (losses.sCRPS, losses.MQLoss, losses.HuberMQLoss)):\n",
    "                _, _, quants  = self.loss.sample(distr_args=distr_args)            \n",
    "                output = quants\n",
    "            elif isinstance(self.valid_loss, losses.BasePointLoss):\n",
    "                distr = self.loss.get_distribution(distr_args=distr_args)\n",
    "                output = distr.mean\n",
    "\n",
    "        # Validation Loss evaluation\n",
    "        if self.valid_loss.is_distribution_output:\n",
    "            valid_loss = self.valid_loss(y=outsample_y, distr_args=distr_args, mask=outsample_mask)\n",
    "        else:\n",
    "            output = self._inv_normalization(y_hat=output, y_idx=y_idx)\n",
    "            valid_loss = self.valid_loss(y=outsample_y, y_hat=output, y_insample=insample_y, mask=outsample_mask)\n",
    "        return valid_loss\n",
    "    \n",
    "    def _validate_step_recurrent_batch(self, insample_y, insample_mask, futr_exog, hist_exog, stat_exog, y_idx):\n",
    "        # Remember state in network and set horizon to 1\n",
    "        self.rnn_state = None\n",
    "        self.maintain_state = True\n",
    "        self.h = 1\n",
    "\n",
    "        # Initialize results array\n",
    "        n_outputs = self.loss.outputsize_multiplier\n",
    "        y_hat = torch.zeros((insample_y.shape[0],\n",
    "                            self.horizon_backup,\n",
    "                            self.n_series * n_outputs),\n",
    "                            device=insample_y.device,\n",
    "                            dtype=insample_y.dtype)\n",
    "\n",
    "        # First step prediction\n",
    "        tau = 0\n",
    "        \n",
    "        # Set exogenous\n",
    "        hist_exog_current = None\n",
    "        if self.hist_exog_size > 0:\n",
    "            hist_exog_current = hist_exog[:, :self.input_size + tau]\n",
    "\n",
    "        futr_exog_current = None\n",
    "        if self.futr_exog_size > 0:\n",
    "            futr_exog_current = futr_exog[:, :self.input_size + tau]\n",
    "\n",
    "        # First forecast step\n",
    "        y_hat[:, tau], insample_y = self._validate_step_recurrent_single(\n",
    "                                                                insample_y=insample_y[:, :self.input_size + tau],\n",
    "                                                                insample_mask=insample_mask[:, :self.input_size + tau],\n",
    "                                                                hist_exog=hist_exog_current,\n",
    "                                                                futr_exog=futr_exog_current,\n",
    "                                                                stat_exog=stat_exog,\n",
    "                                                                y_idx=y_idx,\n",
    "                                                                )\n",
    "\n",
    "        # Horizon prediction recursively\n",
    "        for tau in range(1, self.horizon_backup):\n",
    "            # Set exogenous\n",
    "            if self.hist_exog_size > 0:\n",
    "                hist_exog_current = hist_exog[:, self.input_size + tau - 1].unsqueeze(1)\n",
    "\n",
    "            if self.futr_exog_size > 0:\n",
    "                futr_exog_current = futr_exog[:, self.input_size + tau - 1].unsqueeze(1)\n",
    "            \n",
    "            y_hat[:, tau], insample_y = self._validate_step_recurrent_single(\n",
    "                                                                insample_y=insample_y,\n",
    "                                                                insample_mask=None,\n",
    "                                                                hist_exog=hist_exog_current,\n",
    "                                                                futr_exog=futr_exog_current,\n",
    "                                                                stat_exog=stat_exog,\n",
    "                                                                y_idx = y_idx,\n",
    "                                                                )\n",
    "        \n",
    "        # Reset state and horizon\n",
    "        self.maintain_state = False\n",
    "        self.rnn_state = None\n",
    "        self.h = self.horizon_backup\n",
    "\n",
    "        return y_hat   \n",
    "\n",
    "    def _validate_step_recurrent_single(self, insample_y, insample_mask, hist_exog, futr_exog, stat_exog, y_idx):\n",
    "        # Input sequence\n",
    "        windows_batch = dict(insample_y=insample_y,                 # [Ws, L, n_series]\n",
    "                        insample_mask=insample_mask,                # [Ws, L, n_series]\n",
    "                        futr_exog=futr_exog,                        # univariate: [Ws, L, F]; multivariate: [Ws, F, L, n_series]\n",
    "                        hist_exog=hist_exog,                        # univariate: [Ws, L, X]; multivariate: [Ws, X, L, n_series]\n",
    "                        stat_exog=stat_exog)                        # univariate: [Ws, S]; multivariate: [n_series, S]\n",
    "\n",
    "        # Model Predictions\n",
    "        output_batch_unmapped = self(windows_batch)\n",
    "        output_batch = self.loss.domain_map(output_batch_unmapped)\n",
    "        \n",
    "        # Inverse normalization and sampling\n",
    "        if self.loss.is_distribution_output:\n",
    "            # Sample distribution\n",
    "            y_loc, y_scale = self._get_loc_scale(y_idx)\n",
    "            distr_args = self.loss.scale_decouple(output=output_batch, loc=y_loc, scale=y_scale)\n",
    "            # When validating, the output is the mean of the distribution which is an attribute\n",
    "            distr = self.loss.get_distribution(distr_args=distr_args)\n",
    "\n",
    "            # Scale back to feed back as input\n",
    "            insample_y = self.scaler.scaler(distr.mean, y_loc, y_scale)\n",
    "        else:\n",
    "            # Todo: for now, we assume that in case of a BasePointLoss with ndim==4, the last dimension\n",
    "            # contains a set of predictions for the target (e.g. MQLoss multiple quantiles), for which we use the \n",
    "            # mean as feedback signal for the recurrent predictions. A more precise way is to increase the\n",
    "            # insample input size of the recurrent network by the number of outputs so that each output\n",
    "            # can be fed back to a specific input channel. \n",
    "            if output_batch.ndim == 4:\n",
    "                output_batch = output_batch.mean(dim=-1)\n",
    "\n",
    "            insample_y = output_batch\n",
    "\n",
    "        # Remove horizon dim: [B, 1, N * n_outputs] -> [B, N * n_outputs]\n",
    "        y_hat = output_batch_unmapped.squeeze(1)\n",
    "        return y_hat, insample_y\n",
    "\n",
    "    def _predict_step_recurrent_batch(self, insample_y, insample_mask, futr_exog, hist_exog, stat_exog, y_idx):\n",
    "        # Remember state in network and set horizon to 1\n",
    "        self.rnn_state = None\n",
    "        self.maintain_state = True\n",
    "        self.h = 1\n",
    "\n",
    "        # Initialize results array\n",
    "        n_outputs = len(self.loss.output_names)\n",
    "        y_hat = torch.zeros((insample_y.shape[0],\n",
    "                            self.horizon_backup,\n",
    "                            self.n_series,\n",
    "                            n_outputs),\n",
    "                            device=insample_y.device,\n",
    "                            dtype=insample_y.dtype)\n",
    "\n",
    "        # First step prediction\n",
    "        tau = 0\n",
    "        \n",
    "        # Set exogenous\n",
    "        hist_exog_current = None\n",
    "        if self.hist_exog_size > 0:\n",
    "            hist_exog_current = hist_exog[:, :self.input_size + tau]\n",
    "\n",
    "        futr_exog_current = None\n",
    "        if self.futr_exog_size > 0:\n",
    "            futr_exog_current = futr_exog[:, :self.input_size + tau]\n",
    "\n",
    "        # First forecast step\n",
    "        y_hat[:, tau], insample_y = self._predict_step_recurrent_single(\n",
    "                                                                insample_y=insample_y[:, :self.input_size + tau],\n",
    "                                                                insample_mask=insample_mask[:, :self.input_size + tau],\n",
    "                                                                hist_exog=hist_exog_current,\n",
    "                                                                futr_exog=futr_exog_current,\n",
    "                                                                stat_exog=stat_exog,\n",
    "                                                                y_idx=y_idx,\n",
    "                                                                )\n",
    "\n",
    "        # Horizon prediction recursively\n",
    "        for tau in range(1, self.horizon_backup):\n",
    "            # Set exogenous\n",
    "            if self.hist_exog_size > 0:\n",
    "                hist_exog_current = hist_exog[:, self.input_size + tau - 1].unsqueeze(1)\n",
    "\n",
    "            if self.futr_exog_size > 0:\n",
    "                futr_exog_current = futr_exog[:, self.input_size + tau - 1].unsqueeze(1)\n",
    "            \n",
    "            y_hat[:, tau], insample_y = self._predict_step_recurrent_single(\n",
    "                                                                insample_y=insample_y,\n",
    "                                                                insample_mask=None,\n",
    "                                                                hist_exog=hist_exog_current,\n",
    "                                                                futr_exog=futr_exog_current,\n",
    "                                                                stat_exog=stat_exog,\n",
    "                                                                y_idx = y_idx,\n",
    "                                                                )\n",
    "        \n",
    "        # Reset state and horizon\n",
    "        self.maintain_state = False\n",
    "        self.rnn_state = None\n",
    "        self.h = self.horizon_backup\n",
    "\n",
    "        # Squeeze for univariate case\n",
    "        if not self.MULTIVARIATE:\n",
    "            y_hat = y_hat.squeeze(2)\n",
    "\n",
    "        return y_hat        \n",
    "\n",
    "    def _predict_step_recurrent_single(self, insample_y, insample_mask, hist_exog, futr_exog, stat_exog, y_idx):\n",
    "        # Input sequence\n",
    "        windows_batch = dict(insample_y=insample_y,                 # [Ws, L, n_series]\n",
    "                        insample_mask=insample_mask,                # [Ws, L, n_series]\n",
    "                        futr_exog=futr_exog,                        # univariate: [Ws, L, F]; multivariate: [Ws, F, L, n_series]\n",
    "                        hist_exog=hist_exog,                        # univariate: [Ws, L, X]; multivariate: [Ws, X, L, n_series]\n",
    "                        stat_exog=stat_exog)                        # univariate: [Ws, S]; multivariate: [n_series, S]\n",
    "\n",
    "        # Model Predictions\n",
    "        output_batch_unmapped = self(windows_batch)\n",
    "        output_batch = self.loss.domain_map(output_batch_unmapped)\n",
    "        \n",
    "        # Inverse normalization and sampling\n",
    "        if self.loss.is_distribution_output:\n",
    "            # Sample distribution\n",
    "            y_loc, y_scale = self._get_loc_scale(y_idx)\n",
    "            distr_args = self.loss.scale_decouple(output=output_batch, loc=y_loc, scale=y_scale)\n",
    "            # When predicting, we need to sample to get the quantiles. The mean is an attribute.\n",
    "            _, _, quants = self.loss.sample(distr_args=distr_args, num_samples=self.n_samples)\n",
    "            mean = self.loss.distr_mean\n",
    "\n",
    "            # Scale back to feed back as input\n",
    "            insample_y = self.scaler.scaler(mean, y_loc, y_scale)\n",
    "            \n",
    "            # Save predictions\n",
    "            y_hat = torch.concat((mean.unsqueeze(-1), quants), axis=-1)\n",
    "\n",
    "            if self.loss.return_params:\n",
    "                distr_args = torch.stack(distr_args, dim=-1)\n",
    "                if distr_args.ndim > 4:\n",
    "                    distr_args = distr_args.flatten(-2, -1)\n",
    "                y_hat = torch.concat((y_hat, distr_args), axis=-1)\n",
    "        else:\n",
    "            # Todo: for now, we assume that in case of a BasePointLoss with ndim==4, the last dimension\n",
    "            # contains a set of predictions for the target (e.g. MQLoss multiple quantiles), for which we use the \n",
    "            # mean as feedback signal for the recurrent predictions. A more precise way is to increase the\n",
    "            # insample input size of the recurrent network by the number of outputs so that each output\n",
    "            # can be fed back to a specific input channel. \n",
    "            if output_batch.ndim == 4:\n",
    "                output_batch = output_batch.mean(dim=-1)\n",
    "\n",
    "            insample_y = output_batch\n",
    "            y_hat = self._inv_normalization(y_hat=output_batch, y_idx=y_idx)\n",
    "            y_hat = y_hat.unsqueeze(-1)\n",
    "\n",
    "        # Remove horizon dim: [B, 1, N, n_outputs] -> [B, N, n_outputs]\n",
    "        y_hat = y_hat.squeeze(1)\n",
    "        return y_hat, insample_y\n",
    "\n",
    "    def _predict_step_direct_batch(self, insample_y, insample_mask, hist_exog, futr_exog, stat_exog, y_idx):\n",
    "        windows_batch = dict(insample_y=insample_y,                 # [Ws, L, n_series]\n",
    "                        insample_mask=insample_mask,                # [Ws, L, n_series]\n",
    "                        futr_exog=futr_exog,                        # univariate: [Ws, L, F]; multivariate: [Ws, F, L, n_series]\n",
    "                        hist_exog=hist_exog,                        # univariate: [Ws, L, X]; multivariate: [Ws, X, L, n_series]\n",
    "                        stat_exog=stat_exog)                        # univariate: [Ws, S]; multivariate: [n_series, S]\n",
    "\n",
    "        # Model Predictions\n",
    "        output_batch = self(windows_batch)\n",
    "        output_batch = self.loss.domain_map(output_batch)\n",
    "\n",
    "        # Inverse normalization and sampling\n",
    "        if self.loss.is_distribution_output:\n",
    "            y_loc, y_scale = self._get_loc_scale(y_idx)\n",
    "            distr_args = self.loss.scale_decouple(output=output_batch, loc=y_loc, scale=y_scale)\n",
    "            _, sample_mean, quants = self.loss.sample(distr_args=distr_args)\n",
    "            y_hat = torch.concat((sample_mean, quants), axis=-1)\n",
    "\n",
    "            if self.loss.return_params:\n",
    "                distr_args = torch.stack(distr_args, dim=-1)\n",
    "                if distr_args.ndim > 4:\n",
    "                    distr_args = distr_args.flatten(-2, -1)\n",
    "                y_hat = torch.concat((y_hat, distr_args), axis=-1)                \n",
    "        else:\n",
    "             y_hat = self._inv_normalization(y_hat=output_batch, \n",
    "                                            y_idx=y_idx)\n",
    "\n",
    "        return y_hat\n",
    "            \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # Set horizon to h_train in case of recurrent model to speed up training\n",
    "        if self.RECURRENT:\n",
    "            self.h = self.h_train\n",
    "        \n",
    "        # windows: [Ws, L + h, C, n_series] or [Ws, L + h, C]\n",
    "        y_idx = batch['y_idx']\n",
    "\n",
    "        temporal_cols = batch['temporal_cols']\n",
    "        windows_temporal, static, static_cols = self._create_windows(batch, step='train')\n",
    "        windows = self._sample_windows(windows_temporal, static, static_cols, temporal_cols, step='train')\n",
    "        original_outsample_y = torch.clone(windows['temporal'][:, self.input_size:, y_idx])\n",
    "        windows = self._normalization(windows=windows, y_idx=y_idx)\n",
    "        \n",
    "        # Parse windows\n",
    "        insample_y, insample_mask, outsample_y, outsample_mask, \\\n",
    "               hist_exog, futr_exog, stat_exog = self._parse_windows(batch, windows)\n",
    "\n",
    "        windows_batch = dict(insample_y=insample_y,                 # [Ws, L, n_series]\n",
    "                        insample_mask=insample_mask,                # [Ws, L, n_series]\n",
    "                        futr_exog=futr_exog,                        # univariate: [Ws, L, F]; multivariate: [Ws, F, L, n_series]\n",
    "                        hist_exog=hist_exog,                        # univariate: [Ws, L, X]; multivariate: [Ws, X, L, n_series]\n",
    "                        stat_exog=stat_exog)                        # univariate: [Ws, S]; multivariate: [n_series, S]\n",
    "\n",
    "        # Model Predictions\n",
    "        output = self(windows_batch)\n",
    "        output = self.loss.domain_map(output)\n",
    "        \n",
    "        if self.loss.is_distribution_output:\n",
    "            y_loc, y_scale = self._get_loc_scale(y_idx)\n",
    "            outsample_y = original_outsample_y\n",
    "            distr_args = self.loss.scale_decouple(output=output, loc=y_loc, scale=y_scale)\n",
    "            loss = self.loss(y=outsample_y, distr_args=distr_args, mask=outsample_mask)\n",
    "        else:\n",
    "            loss = self.loss(y=outsample_y, y_hat=output, y_insample=insample_y, mask=outsample_mask)\n",
    "\n",
    "        if torch.isnan(loss):\n",
    "            print('Model Parameters', self.hparams)\n",
    "            print('insample_y', torch.isnan(insample_y).sum())\n",
    "            print('outsample_y', torch.isnan(outsample_y).sum())\n",
    "            raise Exception('Loss is NaN, training stopped.')\n",
    "\n",
    "        train_loss_log = loss.detach().item()\n",
    "        self.log(\n",
    "            'train_loss',\n",
    "            train_loss_log,\n",
    "            batch_size=outsample_y.size(0),\n",
    "            prog_bar=True,\n",
    "            on_epoch=True,\n",
    "        )\n",
    "        self.train_trajectories.append((self.global_step, train_loss_log))\n",
    "\n",
    "        self.h = self.horizon_backup\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        if self.val_size == 0:\n",
    "            return np.nan\n",
    "\n",
    "        temporal_cols = batch['temporal_cols']\n",
    "        windows_temporal, static, static_cols = self._create_windows(batch, step='val')\n",
    "        n_windows = len(windows_temporal)\n",
    "        y_idx = batch['y_idx']\n",
    "\n",
    "        # Number of windows in batch\n",
    "        windows_batch_size = self.inference_windows_batch_size\n",
    "        if windows_batch_size < 0:\n",
    "            windows_batch_size = n_windows\n",
    "        n_batches = int(np.ceil(n_windows / windows_batch_size))\n",
    "\n",
    "        valid_losses = []\n",
    "        batch_sizes = []\n",
    "        for i in range(n_batches):\n",
    "            # Create and normalize windows [Ws, L + h, C, n_series]\n",
    "            w_idxs = np.arange(i*windows_batch_size, \n",
    "                               min((i+1)*windows_batch_size, n_windows))\n",
    "            windows = self._sample_windows(windows_temporal, static, static_cols, temporal_cols, step='val', w_idxs=w_idxs)\n",
    "            original_outsample_y = torch.clone(windows['temporal'][:, self.input_size:, y_idx])\n",
    "\n",
    "            windows = self._normalization(windows=windows, y_idx=y_idx)\n",
    "\n",
    "            # Parse windows\n",
    "            insample_y, insample_mask, _, outsample_mask, \\\n",
    "                hist_exog, futr_exog, stat_exog = self._parse_windows(batch, windows)\n",
    "\n",
    "            if self.RECURRENT:\n",
    "                output_batch = self._validate_step_recurrent_batch(insample_y=insample_y,\n",
    "                                                           insample_mask=insample_mask,\n",
    "                                                           futr_exog=futr_exog,\n",
    "                                                           hist_exog=hist_exog,\n",
    "                                                           stat_exog=stat_exog,\n",
    "                                                           y_idx=y_idx)\n",
    "            else:       \n",
    "                windows_batch = dict(insample_y=insample_y,                 # [Ws, L, n_series]\n",
    "                                insample_mask=insample_mask,                # [Ws, L, n_series]\n",
    "                                futr_exog=futr_exog,                        # univariate: [Ws, L, F]; multivariate: [Ws, F, L, n_series]\n",
    "                                hist_exog=hist_exog,                        # univariate: [Ws, L, X]; multivariate: [Ws, X, L, n_series]\n",
    "                                stat_exog=stat_exog)                        # univariate: [Ws, S]; multivariate: [n_series, S]\n",
    "                \n",
    "                # Model Predictions\n",
    "                output_batch = self(windows_batch)   \n",
    "\n",
    "            output_batch = self.loss.domain_map(output_batch)\n",
    "            valid_loss_batch = self._compute_valid_loss(insample_y=insample_y,\n",
    "                                                        outsample_y=original_outsample_y,\n",
    "                                                output=output_batch, \n",
    "                                                outsample_mask=outsample_mask,\n",
    "                                                y_idx=batch['y_idx'])\n",
    "            valid_losses.append(valid_loss_batch)\n",
    "            batch_sizes.append(len(output_batch))\n",
    "        \n",
    "        valid_loss = torch.stack(valid_losses)\n",
    "        batch_sizes = torch.tensor(batch_sizes, device=valid_loss.device)\n",
    "        batch_size = torch.sum(batch_sizes)\n",
    "        valid_loss = torch.sum(valid_loss * batch_sizes) / batch_size\n",
    "\n",
    "        if torch.isnan(valid_loss):\n",
    "            raise Exception('Loss is NaN, training stopped.')\n",
    "\n",
    "        valid_loss_log = valid_loss.detach()\n",
    "        self.log(\n",
    "            'valid_loss',\n",
    "            valid_loss_log.item(),\n",
    "            batch_size=batch_size,\n",
    "            prog_bar=True,\n",
    "            on_epoch=True,\n",
    "        )\n",
    "        self.validation_step_outputs.append(valid_loss_log)\n",
    "        return valid_loss\n",
    "\n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        if self.RECURRENT:\n",
    "            self.input_size = self.inference_input_size\n",
    "\n",
    "        temporal_cols = batch['temporal_cols']\n",
    "        windows_temporal, static, static_cols = self._create_windows(batch, step='predict')\n",
    "        n_windows = len(windows_temporal)\n",
    "        y_idx = batch['y_idx']\n",
    "\n",
    "        # Number of windows in batch\n",
    "        windows_batch_size = self.inference_windows_batch_size\n",
    "        if windows_batch_size < 0:\n",
    "            windows_batch_size = n_windows\n",
    "        n_batches = int(np.ceil(n_windows / windows_batch_size))\n",
    "        y_hats = []\n",
    "        for i in range(n_batches):\n",
    "            # Create and normalize windows [Ws, L+H, C]\n",
    "            w_idxs = np.arange(i*windows_batch_size, \n",
    "                    min((i+1)*windows_batch_size, n_windows))\n",
    "            windows = self._sample_windows(windows_temporal, static, static_cols, temporal_cols, step='predict', w_idxs=w_idxs)\n",
    "            windows = self._normalization(windows=windows, y_idx=y_idx)\n",
    "\n",
    "            # Parse windows\n",
    "            insample_y, insample_mask, _, _, \\\n",
    "                hist_exog, futr_exog, stat_exog = self._parse_windows(batch, windows)\n",
    "\n",
    "            if self.RECURRENT:                \n",
    "                y_hat = self._predict_step_recurrent_batch(insample_y=insample_y,\n",
    "                                                           insample_mask=insample_mask,\n",
    "                                                           futr_exog=futr_exog,\n",
    "                                                           hist_exog=hist_exog,\n",
    "                                                           stat_exog=stat_exog,\n",
    "                                                           y_idx=y_idx)\n",
    "            else:\n",
    "                y_hat = self._predict_step_direct_batch(insample_y=insample_y,\n",
    "                                                           insample_mask=insample_mask,\n",
    "                                                           futr_exog=futr_exog,\n",
    "                                                           hist_exog=hist_exog,\n",
    "                                                           stat_exog=stat_exog,\n",
    "                                                           y_idx=y_idx)                \n",
    "\n",
    "\n",
    "            y_hats.append(y_hat)\n",
    "        y_hat = torch.cat(y_hats, dim=0)\n",
    "        self.input_size = self.input_size_backup\n",
    "\n",
    "        return y_hat\n",
    "    \n",
    "    def fit(self, dataset, val_size=0, test_size=0, random_seed=None, distributed_config=None):\n",
    "        \"\"\" Fit.\n",
    "\n",
    "        The `fit` method, optimizes the neural network's weights using the\n",
    "        initialization parameters (`learning_rate`, `windows_batch_size`, ...)\n",
    "        and the `loss` function as defined during the initialization. \n",
    "        Within `fit` we use a PyTorch Lightning `Trainer` that\n",
    "        inherits the initialization's `self.trainer_kwargs`, to customize\n",
    "        its inputs, see [PL's trainer arguments](https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.trainer.trainer.Trainer.html?highlight=trainer).\n",
    "\n",
    "        The method is designed to be compatible with SKLearn-like classes\n",
    "        and in particular to be compatible with the StatsForecast library.\n",
    "\n",
    "        By default the `model` is not saving training checkpoints to protect \n",
    "        disk memory, to get them change `enable_checkpointing=True` in `__init__`.\n",
    "\n",
    "        **Parameters:**<br>\n",
    "        `dataset`: NeuralForecast's `TimeSeriesDataset`, see [documentation](https://nixtla.github.io/neuralforecast/tsdataset.html).<br>\n",
    "        `val_size`: int, validation size for temporal cross-validation.<br>\n",
    "        `random_seed`: int=None, random_seed for pytorch initializer and numpy generators, overwrites model.__init__'s.<br>\n",
    "        `test_size`: int, test size for temporal cross-validation.<br>\n",
    "        \"\"\"\n",
    "        return self._fit(\n",
    "            dataset=dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            valid_batch_size=self.valid_batch_size,\n",
    "            val_size=val_size,\n",
    "            test_size=test_size,\n",
    "            random_seed=random_seed,\n",
    "            distributed_config=distributed_config,\n",
    "        )\n",
    "\n",
    "    def predict(self, dataset, test_size=None, step_size=1,\n",
    "                random_seed=None, quantiles=None, **data_module_kwargs):\n",
    "        \"\"\" Predict.\n",
    "\n",
    "        Neural network prediction with PL's `Trainer` execution of `predict_step`.\n",
    "\n",
    "        **Parameters:**<br>\n",
    "        `dataset`: NeuralForecast's `TimeSeriesDataset`, see [documentation](https://nixtla.github.io/neuralforecast/tsdataset.html).<br>\n",
    "        `test_size`: int=None, test size for temporal cross-validation.<br>\n",
    "        `step_size`: int=1, Step size between each window.<br>\n",
    "        `random_seed`: int=None, random_seed for pytorch initializer and numpy generators, overwrites model.__init__'s.<br>\n",
    "        `quantiles`: list of floats, optional (default=None), target quantiles to predict. <br>\n",
    "        `**data_module_kwargs`: PL's TimeSeriesDataModule args, see [documentation](https://pytorch-lightning.readthedocs.io/en/1.6.1/extensions/datamodules.html#using-a-datamodule).\n",
    "        \"\"\"\n",
    "        self._check_exog(dataset)\n",
    "        self._restart_seed(random_seed)\n",
    "        if \"quantile\" in data_module_kwargs:\n",
    "            warnings.warn(\"The 'quantile' argument will be deprecated, use 'quantiles' instead.\")\n",
    "            if quantiles is not None:\n",
    "                raise ValueError(\"You can't specify quantile and quantiles.\")\n",
    "            quantiles = [data_module_kwargs.pop(\"quantile\")]\n",
    "        self._set_quantiles(quantiles)\n",
    "\n",
    "        self.predict_step_size = step_size\n",
    "        self.decompose_forecast = False\n",
    "        datamodule = TimeSeriesDataModule(dataset=dataset,\n",
    "                                          valid_batch_size=self.valid_batch_size,\n",
    "                                          **data_module_kwargs)\n",
    "\n",
    "        # Protect when case of multiple gpu. PL does not support return preds with multiple gpu.\n",
    "        pred_trainer_kwargs = self.trainer_kwargs.copy()\n",
    "        if (pred_trainer_kwargs.get('accelerator', None) == \"gpu\") and (torch.cuda.device_count() > 1):\n",
    "            pred_trainer_kwargs['devices'] = [0]\n",
    "\n",
    "        trainer = pl.Trainer(**pred_trainer_kwargs)\n",
    "        fcsts = trainer.predict(self, datamodule=datamodule)        \n",
    "        fcsts = torch.vstack(fcsts)\n",
    "\n",
    "        if self.MULTIVARIATE:\n",
    "            # [B, h, n_series (, Q)] -> [n_series, B, h (, Q)]\n",
    "            fcsts = fcsts.swapaxes(0, 2)\n",
    "            fcsts = fcsts.swapaxes(1, 2)\n",
    "\n",
    "        fcsts = tensor_to_numpy(fcsts).flatten()\n",
    "        fcsts = fcsts.reshape(-1, len(self.loss.output_names))\n",
    "        return fcsts\n",
    "\n",
    "    def decompose(self, dataset, step_size=1, random_seed=None, quantiles=None, **data_module_kwargs):\n",
    "        \"\"\" Decompose Predictions.\n",
    "\n",
    "        Decompose the predictions through the network's layers.\n",
    "        Available methods are `ESRNN`, `NHITS`, `NBEATS`, and `NBEATSx`.\n",
    "\n",
    "        **Parameters:**<br>\n",
    "        `dataset`: NeuralForecast's `TimeSeriesDataset`, see [documentation here](https://nixtla.github.io/neuralforecast/tsdataset.html).<br>\n",
    "        `step_size`: int=1, step size between each window of temporal data.<br>\n",
    "        `quantiles`: list of floats, optional (default=None), target quantiles to predict. <br>\n",
    "        `**data_module_kwargs`: PL's TimeSeriesDataModule args, see [documentation](https://pytorch-lightning.readthedocs.io/en/1.6.1/extensions/datamodules.html#using-a-datamodule).\n",
    "        \"\"\"\n",
    "        # Restart random seed\n",
    "        if random_seed is None:\n",
    "            random_seed = self.random_seed\n",
    "        torch.manual_seed(random_seed)\n",
    "        self._set_quantiles(quantiles)\n",
    "\n",
    "        self.predict_step_size = step_size\n",
    "        self.decompose_forecast = True\n",
    "        datamodule = TimeSeriesDataModule(dataset=dataset,\n",
    "                                          valid_batch_size=self.valid_batch_size,\n",
    "                                          **data_module_kwargs)\n",
    "        trainer = pl.Trainer(**self.trainer_kwargs)\n",
    "        fcsts = trainer.predict(self, datamodule=datamodule)\n",
    "        self.decompose_forecast = False # Default decomposition back to false\n",
    "        fcsts = torch.vstack(fcsts)\n",
    "        return tensor_to_numpy(fcsts)        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
