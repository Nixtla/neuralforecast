{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5c6594-e5e8-4966-8cb8-a3e6a9ed7d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp common._base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce0c950-2e03-4be1-95d4-a02409d8dba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7c2ba5-19ee-421e-9252-7224b03f5201",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import inspect\n",
    "import random\n",
    "import warnings\n",
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "from neuralforecast.tsdataset import TimeSeriesDataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c40a64-8381-46a2-8cbb-70ec70ed7914",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BaseModel(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        random_seed,\n",
    "        loss,\n",
    "        valid_loss,\n",
    "        optimizer,\n",
    "        optimizer_kwargs,\n",
    "        futr_exog_list,\n",
    "        hist_exog_list,\n",
    "        stat_exog_list,\n",
    "        max_steps,\n",
    "        early_stop_patience_steps,\n",
    "        **trainer_kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters() # Allows instantiation from a checkpoint from class\n",
    "        self.random_seed = random_seed\n",
    "        pl.seed_everything(self.random_seed, workers=True)\n",
    "\n",
    "        # Loss\n",
    "        self.loss = loss\n",
    "        if valid_loss is None:\n",
    "            self.valid_loss = loss\n",
    "        else:\n",
    "            self.valid_loss = valid_loss\n",
    "        self.train_trajectories = []\n",
    "        self.valid_trajectories = []\n",
    "\n",
    "        # Optimization\n",
    "        if optimizer is not None and not issubclass(optimizer, torch.optim.Optimizer):\n",
    "            raise TypeError(\"optimizer is not a valid subclass of torch.optim.Optimizer\")\n",
    "        self.optimizer = optimizer\n",
    "        self.optimizer_kwargs = optimizer_kwargs if optimizer_kwargs else {}\n",
    "\n",
    "        # Variables\n",
    "        self.futr_exog_list = list(futr_exog_list) if futr_exog_list is not None else []\n",
    "        self.hist_exog_list = list(hist_exog_list) if hist_exog_list is not None else []\n",
    "        self.stat_exog_list = list(stat_exog_list) if stat_exog_list is not None else []\n",
    "\n",
    "        ## Trainer arguments ##\n",
    "        # Max steps, validation steps and check_val_every_n_epoch\n",
    "        trainer_kwargs = {**trainer_kwargs, 'max_steps': max_steps}\n",
    "\n",
    "        if 'max_epochs' in trainer_kwargs.keys():\n",
    "            raise Exception('max_epochs is deprecated, use max_steps instead.')\n",
    "\n",
    "        # Callbacks\n",
    "        if early_stop_patience_steps > 0:\n",
    "            if 'callbacks' not in trainer_kwargs:\n",
    "                trainer_kwargs['callbacks'] = []\n",
    "            trainer_kwargs['callbacks'].append(\n",
    "                EarlyStopping(\n",
    "                    monitor='ptl/val_loss', patience=early_stop_patience_steps\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # Add GPU accelerator if available\n",
    "        if trainer_kwargs.get('accelerator', None) is None:\n",
    "            if torch.cuda.is_available():\n",
    "                trainer_kwargs['accelerator'] = \"gpu\"\n",
    "        if trainer_kwargs.get('devices', None) is None:\n",
    "            if torch.cuda.is_available():\n",
    "                trainer_kwargs['devices'] = -1\n",
    "\n",
    "        # Avoid saturating local memory, disabled fit model checkpoints\n",
    "        if trainer_kwargs.get('enable_checkpointing', None) is None:\n",
    "            trainer_kwargs['enable_checkpointing'] = False\n",
    "\n",
    "        self.trainer_kwargs = trainer_kwargs\n",
    "\n",
    "    def __repr__(self):\n",
    "        return type(self).__name__ if self.alias is None else self.alias\n",
    "\n",
    "    def _check_exog(self, dataset):\n",
    "        temporal_cols = set(dataset.temporal_cols.tolist())\n",
    "        static_cols = set(dataset.static_cols.tolist() if dataset.static_cols is not None else [])\n",
    "\n",
    "        missing_hist = set(self.hist_exog_list) - temporal_cols\n",
    "        missing_futr = set(self.futr_exog_list) - temporal_cols\n",
    "        missing_stat = set(self.stat_exog_list) - static_cols\n",
    "        if missing_hist:\n",
    "            raise Exception(f'{missing_hist} historical exogenous variables not found in input dataset')\n",
    "        if missing_futr:\n",
    "            raise Exception(f'{missing_futr} future exogenous variables not found in input dataset')\n",
    "        if missing_stat:\n",
    "            raise Exception(f'{missing_stat} static exogenous variables not found in input dataset')\n",
    "\n",
    "    def _restart_seed(self, random_seed):\n",
    "        if random_seed is None:\n",
    "            random_seed = self.random_seed\n",
    "        torch.manual_seed(random_seed)\n",
    "\n",
    "    def _get_temporal_exogenous_cols(self, temporal_cols):\n",
    "        return list(\n",
    "            set(temporal_cols.tolist()) & set(self.hist_exog_list + self.futr_exog_list)\n",
    "        )\n",
    "\n",
    "    def _fit(\n",
    "        self,\n",
    "        dataset,\n",
    "        batch_size,\n",
    "        valid_batch_size=1024,\n",
    "        val_size=0,\n",
    "        test_size=0,\n",
    "        random_seed=None,\n",
    "        shuffle_train=True,\n",
    "    ):\n",
    "        self._check_exog(dataset)\n",
    "        self._restart_seed(random_seed)\n",
    "\n",
    "        self.val_size = val_size\n",
    "        self.test_size = test_size\n",
    "        datamodule = TimeSeriesDataModule(\n",
    "            dataset=dataset, \n",
    "            batch_size=batch_size,\n",
    "            valid_batch_size=valid_batch_size,\n",
    "            num_workers=self.num_workers_loader,\n",
    "            drop_last=self.drop_last_loader,\n",
    "            shuffle_train=shuffle_train,\n",
    "        )\n",
    "\n",
    "        if self.val_check_steps > self.max_steps:\n",
    "            warnings.warn('val_check_steps is greater than max_steps, \\\n",
    "                    setting val_check_steps to max_steps')\n",
    "        val_check_interval = min(self.val_check_steps, self.max_steps)\n",
    "        self.trainer_kwargs['val_check_interval'] = int(val_check_interval)\n",
    "        self.trainer_kwargs['check_val_every_n_epoch'] = None\n",
    "\n",
    "        trainer = pl.Trainer(**self.trainer_kwargs)\n",
    "        trainer.fit(self, datamodule=datamodule)\n",
    "        return trainer\n",
    "\n",
    "    def on_fit_start(self):\n",
    "        torch.manual_seed(self.random_seed)\n",
    "        np.random.seed(self.random_seed)\n",
    "        random.seed(self.random_seed)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        if self.optimizer:\n",
    "            optimizer_signature = inspect.signature(self.optimizer)\n",
    "            optimizer_kwargs = deepcopy(self.optimizer_kwargs)\n",
    "            if 'lr' in optimizer_signature.parameters:\n",
    "                if 'lr' in optimizer_kwargs:\n",
    "                    warnings.warn(\"ignoring learning rate passed in optimizer_kwargs, using the model's learning rate\")\n",
    "                optimizer_kwargs['lr'] = self.learning_rate\n",
    "            optimizer = self.optimizer(params=self.parameters(), **optimizer_kwargs)\n",
    "        else:\n",
    "            optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        scheduler = {\n",
    "            'scheduler': torch.optim.lr_scheduler.StepLR(\n",
    "                optimizer=optimizer, step_size=self.lr_decay_steps, gamma=0.5\n",
    "            ),\n",
    "            'frequency': 1,\n",
    "            'interval': 'step',\n",
    "        }\n",
    "        return {'optimizer': optimizer, 'lr_scheduler': scheduler}\n",
    "\n",
    "    def get_test_size(self):\n",
    "        return self.test_size\n",
    "\n",
    "    def set_test_size(self, test_size):\n",
    "        self.test_size = test_size\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        if self.val_size == 0:\n",
    "            return\n",
    "        avg_loss = torch.stack(self.validation_step_outputs).mean()\n",
    "        self.log(\"ptl/val_loss\", avg_loss)\n",
    "        self.valid_trajectories.append((self.global_step, float(avg_loss)))\n",
    "        self.validation_step_outputs.clear() # free memory (compute `avg_loss` per epoch)\n",
    "\n",
    "    def save(self, path):\n",
    "        self.trainer.save_checkpoint(path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
