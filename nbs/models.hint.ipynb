{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp models.hint"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Reconciliation methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_bottomup_P(S: np.ndarray):\n",
    "    n_series = len(S)\n",
    "    n_agg = n_series-S.shape[1]\n",
    "    P = np.zeros_like(S)\n",
    "    P[n_agg:,:] = S[n_agg:,:]\n",
    "    P = P.T\n",
    "    return P\n",
    "\n",
    "def get_mintrace_ols_P(S: np.ndarray):\n",
    "    n_hiers, n_bottom = S.shape\n",
    "    n_agg = n_hiers - n_bottom\n",
    "\n",
    "    W = np.eye(n_hiers)\n",
    "\n",
    "    # We compute reconciliation matrix with\n",
    "    # Equation 10 from https://robjhyndman.com/papers/MinT.pdf\n",
    "    A = S[:n_agg,:]\n",
    "    U = np.hstack((np.eye(n_agg), -A)).T\n",
    "    J = np.hstack((np.zeros((n_bottom,n_agg)), np.eye(n_bottom)))\n",
    "    P = J - (J @ W @ U) @ np.linalg.pinv(U.T @ W @ U) @ U.T\n",
    "    return P\n",
    "\n",
    "def get_mintrace_wls_P(S: np.ndarray):\n",
    "    n_hiers, n_bottom = S.shape\n",
    "    n_agg = n_hiers - n_bottom\n",
    "    \n",
    "    W = np.diag(S @ np.ones((n_bottom,)))\n",
    "\n",
    "    # We compute reconciliation matrix with\n",
    "    # Equation 10 from https://robjhyndman.com/papers/MinT.pdf\n",
    "    A = S[:n_agg,:]\n",
    "    U = np.hstack((np.eye(n_agg), -A)).T\n",
    "    J = np.hstack((np.zeros((n_bottom,n_agg)), np.eye(n_bottom)))\n",
    "    P = J - (J @ W @ U) @ np.linalg.pinv(U.T @ W @ U) @ U.T\n",
    "    return P"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. HINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class HINT:\n",
    "    def __init__(self,\n",
    "                 h: int,\n",
    "                 model: nn.Module,\n",
    "                 S: np.ndarray,\n",
    "                 reconciliation: str):\n",
    "        \n",
    "        if model.h != h:\n",
    "            raise Exception(f\"Model h {model.h} does not match HINT h {h}\")\n",
    "        self.h = h\n",
    "        self.model = model\n",
    "        self.S = S\n",
    "        self.reconciliation = reconciliation\n",
    "        self.loss = model.loss\n",
    "\n",
    "        available_reconcitliations = dict(\n",
    "                                BottomUp=get_bottomup_P,\n",
    "                                MinTraceOLS=get_mintrace_ols_P,\n",
    "                                MinTraceWLS=get_mintrace_wls_P   \n",
    "                                )\n",
    "\n",
    "        if reconciliation not in available_reconcitliations:\n",
    "            raise Exception(f\"Reconciliation {reconciliation} not available\")\n",
    "\n",
    "        # Get SP matrix\n",
    "        P = available_reconcitliations[reconciliation](S=S)\n",
    "        self.SP = S @ P\n",
    "\n",
    "        qs = torch.Tensor((np.arange(self.loss.num_samples)/self.loss.num_samples))\n",
    "        self.sample_quantiles = torch.nn.Parameter(qs, requires_grad=False)\n",
    "\n",
    "\n",
    "    def fit(self, dataset, val_size=0, test_size=0, random_seed=None):\n",
    "        \"\"\" HINT.fit\n",
    "\n",
    "        **Parameters:**<br>\n",
    "        `dataset`: NeuralForecast's `TimeSeriesDataset` see details [here](https://nixtla.github.io/neuralforecast/tsdataset.html)<br>\n",
    "        `val_size`: int, size of the validation set, (default 0).<br>\n",
    "        `test_size`: int, size of the test set, (default 0).<br>\n",
    "        `random_seed`: int, random seed for the prediction.<br>\n",
    "\n",
    "        **Returns:**<br>\n",
    "        `y_hat`: numpy predictions of the `NeuralForecast` model.<br>\n",
    "        \"\"\"\n",
    "        self.model.fit(dataset=dataset,\n",
    "                       val_size=val_size,\n",
    "                       test_size=test_size,\n",
    "                       random_seed=random_seed)\n",
    "\n",
    "    def predict(self, dataset, step_size=1, random_seed=None, **data_module_kwargs):\n",
    "        \"\"\" HINT.predict\n",
    "\n",
    "        **Parameters:**<br>\n",
    "        `dataset`: NeuralForecast's `TimeSeriesDataset` see details [here](https://nixtla.github.io/neuralforecast/tsdataset.html)<br>\n",
    "        `step_size`: int, steps between sequential predictions, (default 1).<br>\n",
    "        `random_seed`: int, random seed for the prediction.<br>\n",
    "        `**data_kwarg`: additional parameters for the dataset module.<br>\n",
    "\n",
    "        **Returns:**<br>\n",
    "        `y_hat`: numpy predictions of the `NeuralForecast` model.<br>\n",
    "        \"\"\"\n",
    "        num_samples = self.model.loss.num_samples\n",
    "\n",
    "        # Hack to get samples by simulating quantiles (samples will be ordered)\n",
    "        quantiles_old = self.model.loss.quantiles\n",
    "        names_old = self.model.loss.output_names\n",
    "        self.model.loss.quantiles = self.sample_quantiles\n",
    "        self.model.loss.output_names = ['1']*num_samples\n",
    "        samples = self.model.predict(dataset=dataset, \n",
    "                                       step_size=step_size,\n",
    "                                       random_seed=random_seed,\n",
    "                                       **data_module_kwargs)\n",
    "        self.model.loss.quantiles = quantiles_old\n",
    "        self.model.loss.output_names = names_old\n",
    "\n",
    "        # Hack requires to break quantiles correlations between samples\n",
    "        idxs = np.random.choice(num_samples, size=samples.shape, replace=True)\n",
    "        aux_col_idx = np.arange(len(samples))[:,None] * num_samples\n",
    "        idxs = idxs + aux_col_idx\n",
    "\n",
    "        samples = samples.flatten()[idxs]\n",
    "        samples = samples.reshape(dataset.n_groups, -1, self.h, num_samples)\n",
    "\n",
    "        samples = np.einsum('ij,jwhp->iwhp', self.SP, samples)\n",
    "\n",
    "        forecasts = np.quantile(samples, self.model.loss.quantiles, axis=-1)\n",
    "\n",
    "        forecasts = forecasts.transpose(1,2,3,0)\n",
    "        forecasts = forecasts.reshape(-1, len(self.model.loss.quantiles))\n",
    "\n",
    "        return forecasts\n",
    "\n",
    "    def set_test_size(self, test_size):\n",
    "        self.model.test_size = test_size\n",
    "\n",
    "    def save(self, path):\n",
    "        \"\"\" BaseWindows.save\n",
    "\n",
    "        Save the fitted model to disk.\n",
    "\n",
    "        **Parameters:**<br>\n",
    "        `path`: str, path to save the model.<br>\n",
    "        \"\"\"\n",
    "        self.model.trainer.save_checkpoint(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "def sort_df_hier(Y_df, S):\n",
    "    Y_df.unique_id = Y_df.unique_id.astype('category')\n",
    "    Y_df.unique_id = Y_df.unique_id.cat.set_categories(S.index)\n",
    "    Y_df = Y_df.sort_values(by=['unique_id', 'ds'])\n",
    "\n",
    "    return Y_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from neuralforecast import NeuralForecast\n",
    "from neuralforecast.models import NHITS\n",
    "from neuralforecast.losses.pytorch import MQLoss, DistributionLoss, PMM, GMM, NBMM\n",
    "from neuralforecast.tsdataset import TimeSeriesDataset\n",
    "from neuralforecast.utils import AirPassengers, AirPassengersPanel, AirPassengersStatic\n",
    "\n",
    "from datasetsforecast.hierarchical import HierarchicalData\n",
    "\n",
    "# Load TourismSmall dataset\n",
    "Y_df, S_df, tags = HierarchicalData.load('./data', 'TourismSmall')\n",
    "Y_df['ds'] = pd.to_datetime(Y_df['ds'])\n",
    "\n",
    "Y_df = sort_df_hier(Y_df, S_df)\n",
    "\n",
    "# Base model\n",
    "nhits = NHITS(h=4,\n",
    "             input_size=12,\n",
    "             loss=GMM(n_components=2, level=[80,90]),\n",
    "             max_steps=1000,\n",
    "             early_stop_patience_steps=2,\n",
    "             val_check_steps=100,\n",
    "             scaler_type='robust',\n",
    "             learning_rate=1e-3)\n",
    "\n",
    "# HINT\n",
    "model = HINT(h=4, model=nhits, S=S_df.values, reconciliation='MinTraceOLS')\n",
    "\n",
    "# Fit and Predict\n",
    "fcst = NeuralForecast(models=[model], freq='Q')\n",
    "forecasts = fcst.cross_validation(df=Y_df, val_size=4, n_windows=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "# Plot quantile predictions\n",
    "unique_id = 'total'\n",
    "Y_plot_df = Y_df[Y_df.unique_id==unique_id]\n",
    "plot_df = forecasts[forecasts.unique_id==unique_id]\n",
    "plot_df = Y_plot_df.merge(plot_df, on=['ds', 'unique_id'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "# Plot quantile predictions\n",
    "plt.plot(plot_df['ds'], plot_df['y_x'], c='black', label='True')\n",
    "plt.plot(plot_df['ds'], plot_df['HINT-median'], c='blue', label='median')\n",
    "plt.fill_between(x=plot_df['ds'][-4:],\n",
    "                 y1=plot_df['HINT-lo-90'][-4:].values,\n",
    "                 y2=plot_df['HINT-hi-90'][-4:].values,\n",
    "                 alpha=0.4, label='level 90')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
