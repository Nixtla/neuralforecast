{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp experiments.utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Utils\n",
    "> Set of functions to easily perform experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "ENV_VARS = dict(OMP_NUM_THREADS='2',\n",
    "                OPENBLAS_NUM_THREADS='2',\n",
    "                MKL_NUM_THREADS='3',\n",
    "                VECLIB_MAXIMUM_THREADS='2',\n",
    "                NUMEXPR_NUM_THREADS='3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import os\n",
    "# Limit number of threads in numpy and others to avoid throttling\n",
    "os.environ.update(ENV_VARS)\n",
    "import random\n",
    "import time\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import torch as t\n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
    "\n",
    "from nixtlats.data.scalers import Scaler\n",
    "from nixtlats.data.tsdataset import TimeSeriesDataset, WindowsDataset\n",
    "from nixtlats.data.tsloader import TimeSeriesLoader\n",
    "from nixtlats.models.esrnn.esrnn import ESRNN\n",
    "from nixtlats.models.esrnn.mqesrnn import MQESRNN\n",
    "from nixtlats.models.nbeats.nbeats import NBEATS\n",
    "from nixtlats.models.deepmidas.deepmidas import DeepMIDAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_mask_dfs(Y_df, ds_in_val, ds_in_test):\n",
    "    # train mask\n",
    "    train_mask_df = Y_df.copy()[['unique_id', 'ds']]\n",
    "    train_mask_df.sort_values(by=['unique_id', 'ds'], inplace=True)\n",
    "    train_mask_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    train_mask_df['sample_mask'] = 1\n",
    "    train_mask_df['available_mask'] = 1\n",
    "    \n",
    "    idx_out = train_mask_df.groupby('unique_id').tail(ds_in_val+ds_in_test).index\n",
    "    train_mask_df.loc[idx_out, 'sample_mask'] = 0\n",
    "    \n",
    "    # test mask\n",
    "    test_mask_df = train_mask_df.copy()\n",
    "    test_mask_df['sample_mask'] = 0\n",
    "    idx_test = test_mask_df.groupby('unique_id').tail(ds_in_test).index\n",
    "    test_mask_df.loc[idx_test, 'sample_mask'] = 1\n",
    "    \n",
    "    # validation mask\n",
    "    val_mask_df = train_mask_df.copy()\n",
    "    val_mask_df['sample_mask'] = 1\n",
    "    val_mask_df['sample_mask'] = val_mask_df['sample_mask'] - train_mask_df['sample_mask']\n",
    "    val_mask_df['sample_mask'] = val_mask_df['sample_mask'] - test_mask_df['sample_mask']\n",
    "\n",
    "    assert len(train_mask_df)==len(Y_df), \\\n",
    "        f'The mask_df length {len(train_mask_df)} is not equal to Y_df length {len(Y_df)}'\n",
    "    \n",
    "    return train_mask_df, val_mask_df, test_mask_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_random_mask_dfs(Y_df, ds_in_test, \n",
    "                        n_val_windows, n_ds_val_window,\n",
    "                        n_uids, freq):\n",
    "    \"\"\"\n",
    "    Generates train, test and random validation mask.\n",
    "    Train mask begins by avoiding ds_in_test\n",
    "    \n",
    "    Validation mask: 1) samples n_uids unique ids\n",
    "                     2) creates windows of size n_ds_val_window\n",
    "    Parameters\n",
    "    ----------\n",
    "    ds_in_test: int\n",
    "        Number of ds in test.\n",
    "    n_uids: int\n",
    "        Number of unique ids in validation.\n",
    "    n_val_windows: int\n",
    "        Number of windows for validation.\n",
    "    n_ds_val_window: int\n",
    "        Number of ds in each validation window.\n",
    "    periods: int  \n",
    "        ds_in_test multiplier.\n",
    "    freq: str\n",
    "        string that determines datestamp frequency, used in\n",
    "        random windows creation.\n",
    "    \"\"\"\n",
    "    np.random.seed(1)\n",
    "    #----------------------- Train mask -----------------------#\n",
    "    # Initialize masks\n",
    "    train_mask_df, val_mask_df, test_mask_df = get_mask_dfs(Y_df=Y_df,\n",
    "                                                            ds_in_val=0,\n",
    "                                                            ds_in_test=ds_in_test)\n",
    "    \n",
    "    assert val_mask_df['sample_mask'].sum()==0, 'Muerte'\n",
    "    \n",
    "    #----------------- Random Validation mask -----------------#\n",
    "    # Overwrite validation with random windows\n",
    "    uids = train_mask_df['unique_id'].unique()\n",
    "    val_uids = np.random.choice(uids, n_uids, replace=False)\n",
    "    \n",
    "    # Validation avoids test\n",
    "    idx_test = train_mask_df.groupby('unique_id').tail(ds_in_test).index\n",
    "    available_ds = train_mask_df.loc[~train_mask_df.index.isin(idx_test)]['ds'].unique()\n",
    "    val_init_ds = np.random.choice(available_ds, n_val_windows, replace=False)\n",
    "    \n",
    "    # Creates windows \n",
    "    val_ds = [pd.date_range(init, periods=n_ds_val_window, freq=freq) for init in val_init_ds]\n",
    "    val_ds = np.concatenate(val_ds)\n",
    "\n",
    "    # Cleans random windows from train mask\n",
    "    val_idx = train_mask_df.query('unique_id in @val_uids & ds in @val_ds').index\n",
    "    train_mask_df.loc[val_idx, 'sample_mask'] = 0\n",
    "    val_mask_df.loc[val_idx, 'sample_mask'] = 1\n",
    "    \n",
    "    return train_mask_df, val_mask_df, test_mask_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def scale_data(Y_df, X_df, mask_df, normalizer_y, normalizer_x):\n",
    "    mask = mask_df['available_mask'].values * mask_df['sample_mask'].values\n",
    "    \n",
    "    if normalizer_y is not None:\n",
    "        scaler_y = Scaler(normalizer=normalizer_y)\n",
    "        Y_df['y'] = scaler_y.scale(x=Y_df['y'].values, mask=mask)\n",
    "    else:\n",
    "        scaler_y = None\n",
    "\n",
    "    if normalizer_x is not None:\n",
    "        X_cols = [col for col in X_df.columns if col not in ['unique_id','ds']]\n",
    "        for col in X_cols:\n",
    "            scaler_x = Scaler(normalizer=normalizer_x)\n",
    "            X_df[col] = scaler_x.scale(x=X_df[col].values, mask=mask)\n",
    "\n",
    "    return Y_df, X_df, scaler_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def create_datasets(mc, S_df, Y_df, X_df, f_cols,\n",
    "                    ds_in_test, ds_in_val,\n",
    "                    n_uids, n_val_windows, freq,\n",
    "                    is_val_random):\n",
    "    #------------------------------------- Available and Validation Mask ------------------------------------#\n",
    "    if is_val_random:\n",
    "        train_mask_df, valid_mask_df, test_mask_df = get_random_mask_dfs(Y_df=Y_df, \n",
    "                                                                         ds_in_test=ds_in_test,\n",
    "                                                                         n_uids=n_uids, \n",
    "                                                                         n_val_windows=n_val_windows, \n",
    "                                                                         n_ds_val_window=ds_in_val//n_val_windows,\n",
    "                                                                         freq=freq)\n",
    "    else:\n",
    "        train_mask_df, valid_mask_df, test_mask_df = get_mask_dfs(Y_df=Y_df,\n",
    "                                                                  ds_in_test=ds_in_test, \n",
    "                                                                  ds_in_val=ds_in_val)\n",
    "\n",
    "    #---------------------------------------------- Scale Data ----------------------------------------------#\n",
    "    Y_df, X_df, scaler_y = scale_data(Y_df=Y_df, X_df=X_df, mask_df=train_mask_df,\n",
    "                                      normalizer_y=mc['normalizer_y'], normalizer_x=mc['normalizer_x'])\n",
    "\n",
    "    #----------------------------------------- Declare Dataset and Loaders ----------------------------------#\n",
    "    \n",
    "    if mc['mode'] == 'simple':\n",
    "        train_dataset = WindowsDataset(S_df=S_df, Y_df=Y_df, X_df=X_df,\n",
    "                                       mask_df=train_mask_df, f_cols=f_cols,\n",
    "                                       input_size=int(mc['n_time_in']),\n",
    "                                       output_size=int(mc['n_time_out']),\n",
    "                                       sample_freq=int(mc['idx_to_sample_freq']),\n",
    "                                       verbose=True)\n",
    "        \n",
    "        valid_dataset = WindowsDataset(S_df=S_df, Y_df=Y_df, X_df=X_df,\n",
    "                                       mask_df=valid_mask_df, f_cols=f_cols,\n",
    "                                       input_size=int(mc['n_time_in']),\n",
    "                                       output_size=int(mc['n_time_out']),\n",
    "                                       sample_freq=int(mc['val_idx_to_sample_freq']),\n",
    "                                       verbose=True)\n",
    "        \n",
    "        test_dataset = WindowsDataset(S_df=S_df, Y_df=Y_df, X_df=X_df,\n",
    "                                      mask_df=test_mask_df, f_cols=f_cols,\n",
    "                                      input_size=int(mc['n_time_in']),\n",
    "                                      output_size=int(mc['n_time_out']),\n",
    "                                      sample_freq=int(mc['val_idx_to_sample_freq']),\n",
    "                                      verbose=True)\n",
    "    \n",
    "    if mc['mode'] == 'full':\n",
    "        train_dataset = TimeSeriesDataset(S_df=S_df, Y_df=Y_df, X_df=X_df,\n",
    "                                       mask_df=train_mask_df, f_cols=f_cols,\n",
    "                                       input_size=int(mc['n_time_in']),\n",
    "                                       output_size=int(mc['n_time_out']),\n",
    "                                       verbose=True)\n",
    "        \n",
    "        valid_dataset = TimeSeriesDataset(S_df=S_df, Y_df=Y_df, X_df=X_df,\n",
    "                                       mask_df=valid_mask_df, f_cols=f_cols,\n",
    "                                       input_size=int(mc['n_time_in']),\n",
    "                                       output_size=int(mc['n_time_out']),\n",
    "                                       verbose=True)\n",
    "        \n",
    "        test_dataset = TimeSeriesDataset(S_df=S_df, Y_df=Y_df, X_df=X_df,\n",
    "                                      mask_df=test_mask_df, f_cols=f_cols,\n",
    "                                      input_size=int(mc['n_time_in']),\n",
    "                                      output_size=int(mc['n_time_out']),\n",
    "                                      verbose=True)        \n",
    "    \n",
    "    if ds_in_test == 0:\n",
    "        test_dataset = None\n",
    "\n",
    "    return train_dataset, valid_dataset, test_dataset, scaler_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def instantiate_loaders(mc, train_dataset, val_dataset, test_dataset):\n",
    "    train_loader = TimeSeriesLoader(dataset=train_dataset,\n",
    "                                    batch_size=int(mc['batch_size']),\n",
    "                                    eq_batch_size=mc.get('eq_batch_size') or True,\n",
    "                                    shuffle=True)\n",
    "    if val_dataset is not None:\n",
    "        val_loader = TimeSeriesLoader(dataset=val_dataset,\n",
    "                                      batch_size=1,\n",
    "                                      shuffle=False)\n",
    "        \n",
    "    else:\n",
    "        val_loader = None\n",
    "\n",
    "    if test_dataset is not None:\n",
    "        test_loader = TimeSeriesLoader(dataset=test_dataset,\n",
    "                                       batch_size=1,\n",
    "                                       shuffle=False)\n",
    "    else:\n",
    "        test_loader = None\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def instantiate_nbeats(mc):\n",
    "    mc['n_theta_hidden'] = len(mc['stack_types']) * [ [int(mc['n_hidden']), int(mc['n_hidden'])] ]\n",
    "    model = NBEATS(n_time_in=int(mc['n_time_in']),\n",
    "                   n_time_out=int(mc['n_time_out']),\n",
    "                   n_x=mc['n_x'],\n",
    "                   n_s=mc['n_s'],\n",
    "                   n_s_hidden=int(mc['n_s_hidden']),\n",
    "                   n_x_hidden=int(mc['n_x_hidden']),\n",
    "                   shared_weights=mc['shared_weights'],\n",
    "                   initialization=mc['initialization'],\n",
    "                   activation=mc['activation'],\n",
    "                   stack_types=mc['stack_types'],\n",
    "                   n_blocks=mc['n_blocks'],\n",
    "                   n_layers=mc['n_layers'],\n",
    "                   n_theta_hidden=mc['n_theta_hidden'],\n",
    "                   n_harmonics=int(mc['n_harmonics']),\n",
    "                   n_polynomials=int(mc['n_polynomials']),\n",
    "                   batch_normalization = mc['batch_normalization'],\n",
    "                   dropout_prob_theta=mc['dropout_prob_theta'],\n",
    "                   learning_rate=float(mc['learning_rate']),\n",
    "                   lr_decay=float(mc['lr_decay']),\n",
    "                   lr_decay_step_size=float(mc['lr_decay_step_size']),\n",
    "                   weight_decay=mc['weight_decay'],\n",
    "                   loss_train=mc['loss_train'],\n",
    "                   loss_hypar=float(mc['loss_hypar']),\n",
    "                   loss_valid=mc['loss_valid'],\n",
    "                   frequency=mc['frequency'],\n",
    "                   seasonality=int(mc['seasonality']),\n",
    "                   random_seed=int(mc['random_seed']))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def instantiate_esrnn(mc):    \n",
    "    model = ESRNN(# Architecture parameters\n",
    "                  n_series=mc['n_series'],\n",
    "                  n_x=mc['n_x'],\n",
    "                  n_s=mc['n_s'],\n",
    "                  sample_freq=int(mc['sample_freq']),\n",
    "                  input_size=int(mc['n_time_in']),\n",
    "                  output_size=int(mc['n_time_out']),\n",
    "                  es_component=mc['es_component'],\n",
    "                  cell_type=mc['cell_type'],\n",
    "                  state_hsize=int(mc['state_hsize']),\n",
    "                  dilations=mc['dilations'],\n",
    "                  add_nl_layer=mc['add_nl_layer'],\n",
    "                  # Optimization parameters                \n",
    "                  learning_rate=mc['learning_rate'],\n",
    "                  lr_scheduler_step_size=int(mc['lr_decay_step_size']),\n",
    "                  lr_decay=mc['lr_decay'],\n",
    "                  per_series_lr_multip=mc['per_series_lr_multip'],\n",
    "                  gradient_eps=mc['gradient_eps'],\n",
    "                  gradient_clipping_threshold=mc['gradient_clipping_threshold'],\n",
    "                  rnn_weight_decay=mc['rnn_weight_decay'],\n",
    "                  noise_std=mc['noise_std'],\n",
    "                  level_variability_penalty=mc['level_variability_penalty'],\n",
    "                  testing_percentile=mc['testing_percentile'],\n",
    "                  training_percentile=mc['training_percentile'],\n",
    "                  loss=mc['loss_train'],\n",
    "                  val_loss=mc['loss_valid'],\n",
    "                  seasonality=mc['seasonality']) \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def instantiate_mqesrnn(mc):    \n",
    "    model = MQESRNN(# Architecture parameters\n",
    "                    n_series=mc['n_series'],\n",
    "                    n_x=mc['n_x'],\n",
    "                    n_s=mc['n_s'],\n",
    "                    idx_to_sample_freq=int(mc['idx_to_sample_freq']),\n",
    "                    input_size=int(mc['n_time_in']),\n",
    "                    output_size=int(mc['n_time_out']),\n",
    "                    es_component=mc['es_component'],\n",
    "                    cell_type=mc['cell_type'],\n",
    "                    state_hsize=int(mc['state_hsize']),\n",
    "                    dilations=mc['dilations'],\n",
    "                    add_nl_layer=mc['add_nl_layer'],\n",
    "                    # Optimization parameters                 \n",
    "                    learning_rate=mc['learning_rate'],\n",
    "                    lr_scheduler_step_size=int(mc['lr_decay_step_size']),\n",
    "                    lr_decay=mc['lr_decay'],\n",
    "                    gradient_eps=mc['gradient_eps'],\n",
    "                    gradient_clipping_threshold=mc['gradient_clipping_threshold'],\n",
    "                    rnn_weight_decay=mc['rnn_weight_decay'],\n",
    "                    noise_std=mc['noise_std'],\n",
    "                    testing_percentiles=list(mc['testing_percentiles']),\n",
    "                    training_percentiles=list(mc['training_percentiles']),\n",
    "                    loss=mc['loss_train'],\n",
    "                    val_loss=mc['loss_valid']) \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def instantiate_deepmidas(mc):\n",
    "    mc['n_theta_hidden'] = len(mc['stack_types']) * [ [int(mc['n_hidden']), int(mc['n_hidden'])] ]\n",
    "\n",
    "    model = DeepMIDAS(n_time_in=int(mc['n_time_in']),\n",
    "                      n_time_out=int(mc['n_time_out']),\n",
    "                      n_x=mc['n_x'],\n",
    "                      n_s=mc['n_s'],\n",
    "                      n_s_hidden=int(mc['n_s_hidden']),\n",
    "                      n_x_hidden=int(mc['n_x_hidden']),\n",
    "                      shared_weights = mc['shared_weights'],\n",
    "                      initialization=mc['initialization'],\n",
    "                      activation=mc['activation'],\n",
    "                      stack_types=mc['stack_types'],\n",
    "                      n_blocks=mc['n_blocks'],\n",
    "                      n_layers=mc['n_layers'],\n",
    "                      n_theta_hidden=mc['n_theta_hidden'],\n",
    "                      n_pool_kernel_size=mc['n_pool_kernel_size'],\n",
    "                      n_freq_downsample=mc['n_freq_downsample'],\n",
    "                      batch_normalization = mc['batch_normalization'],\n",
    "                      dropout_prob_theta=mc['dropout_prob_theta'],\n",
    "                      learning_rate=float(mc['learning_rate']),\n",
    "                      lr_decay=float(mc['lr_decay']),\n",
    "                      lr_decay_step_size=float(mc['lr_decay_step_size']),\n",
    "                      weight_decay=mc['weight_decay'],\n",
    "                      loss_train=mc['loss_train'],\n",
    "                      loss_hypar=float(mc['loss_hypar']),\n",
    "                      loss_valid=mc['loss_valid'],\n",
    "                      frequency=mc['frequency'],\n",
    "                      seasonality=int(mc['seasonality']),\n",
    "                      random_seed=int(mc['random_seed']))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def instantiate_model(mc):\n",
    "    MODEL_DICT = {'nbeats': instantiate_nbeats,\n",
    "                  'esrnn': instantiate_esrnn,\n",
    "                  'mqesrnn': instantiate_mqesrnn,\n",
    "                  'deepmidas': instantiate_deepmidas}\n",
    "    return MODEL_DICT[mc['model']](mc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def model_fit_predict(mc, S_df, Y_df, X_df, f_cols,\n",
    "                      ds_in_test, ds_in_val,\n",
    "                      n_uids, n_val_windows, freq,\n",
    "                      is_val_random):\n",
    "    \n",
    "    # Protect inplace modifications\n",
    "    Y_df = Y_df.copy()\n",
    "    if X_df is not None:\n",
    "        X_df = X_df.copy()\n",
    "    if S_df is not None:\n",
    "        S_df = S_df.copy()        \n",
    "\n",
    "    #----------------------------------------------- Datasets -----------------------------------------------#\n",
    "    train_dataset, val_dataset, test_dataset, scaler_y = create_datasets(mc=mc,\n",
    "                                                                         S_df=S_df, Y_df=Y_df, X_df=X_df,\n",
    "                                                                         f_cols=f_cols,\n",
    "                                                                         ds_in_test=ds_in_test,\n",
    "                                                                         ds_in_val=ds_in_val,\n",
    "                                                                         n_uids=n_uids, \n",
    "                                                                         n_val_windows=n_val_windows,\n",
    "                                                                         freq=freq, is_val_random=is_val_random)\n",
    "    mc['n_x'], mc['n_s'] = train_dataset.get_n_variables()\n",
    "\n",
    "    #------------------------------------------- Instantiate & fit -------------------------------------------#\n",
    "    train_loader, val_loader, test_loader = instantiate_loaders(mc=mc,\n",
    "                                                                train_dataset=train_dataset,\n",
    "                                                                val_dataset=val_dataset,\n",
    "                                                                test_dataset=test_dataset)\n",
    "    model = instantiate_model(mc=mc)\n",
    "    callbacks = []\n",
    "    if mc['early_stop_patience']:\n",
    "        early_stopping = pl.callbacks.EarlyStopping(monitor='val_loss', min_delta=1e-4, \n",
    "                                                    patience=mc['early_stop_patience'],\n",
    "                                                    verbose=True, \n",
    "                                                    mode='min') \n",
    "        callbacks=[early_stopping]\n",
    "\n",
    "    gpus = -1 if t.cuda.is_available() else 0\n",
    "    trainer = pl.Trainer(max_epochs=mc['max_epochs'], \n",
    "                         max_steps=mc['max_steps'],\n",
    "                         callbacks=callbacks)\n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "    #------------------------------------------------ Predict ------------------------------------------------#\n",
    "    # Predict test if available\n",
    "    if ds_in_test > 0:\n",
    "        outputs = trainer.predict(model, test_loader)\n",
    "        y_true, y_hat, mask = [t.cat(output).cpu().numpy() for output in zip(*outputs)]\n",
    "        meta_data = test_loader.dataset.meta_data\n",
    "    else:\n",
    "        outputs = trainer.predict(model, val_loader)\n",
    "        y_true, y_hat, mask = [t.cat(output).cpu().numpy()[:, -1] for output in zip(*outputs)]\n",
    "        meta_data = val_loader.dataset.meta_data\n",
    "    \n",
    "    # Scale to original scale\n",
    "    if mc['normalizer_y'] is not None:\n",
    "        y_true_shape = y_true.shape\n",
    "        y_true = scaler_y.inv_scale(x=y_true.flatten())\n",
    "        y_true = np.reshape(y_true, y_true_shape)\n",
    "\n",
    "        y_hat = scaler_y.inv_scale(x=y_hat.flatten())\n",
    "        y_hat = np.reshape(y_hat, y_true_shape)\n",
    "\n",
    "    print(f\"y_true.shape (#n_series, #n_fcds, #lt): {y_true.shape}\")\n",
    "    print(f\"y_hat.shape (#n_series, #n_fcds, #lt): {y_hat.shape}\")\n",
    "    print(\"\\n\")\n",
    "    return y_true, y_hat, mask, meta_data, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def evaluate_model(mc, loss_function, \n",
    "                   S_df, Y_df, X_df, f_cols,\n",
    "                   ds_in_test, ds_in_val,\n",
    "                   n_uids, n_val_windows, freq,\n",
    "                   is_val_random,\n",
    "                   loss_kwargs):\n",
    "    \n",
    "    print(47*'=' + '\\n')\n",
    "    print(pd.Series(mc))\n",
    "    print(47*'=' + '\\n')\n",
    "    \n",
    "    # Some asserts due to work in progress\n",
    "    n_series = Y_df['unique_id'].nunique()\n",
    "    if n_series > 1:\n",
    "        assert mc['normalizer_y'] is None, 'Data scaling not implemented with multiple time series'\n",
    "        assert mc['normalizer_x'] is None, 'Data scaling not implemented with multiple time series'\n",
    "\n",
    "    assert ds_in_test % mc['val_idx_to_sample_freq']==0, 'outsample size should be multiple of val_idx_to_sample_freq'\n",
    "\n",
    "    # Make predictions\n",
    "    start = time.time()\n",
    "    y_true, y_hat, mask, meta_data, model = model_fit_predict(mc=mc,\n",
    "                                                              S_df=S_df, \n",
    "                                                              Y_df=Y_df,\n",
    "                                                              X_df=X_df,\n",
    "                                                              f_cols=f_cols,\n",
    "                                                              ds_in_test=ds_in_test, \n",
    "                                                              ds_in_val=ds_in_val,\n",
    "                                                              n_uids=n_uids,\n",
    "                                                              n_val_windows=n_val_windows,\n",
    "                                                              freq=freq,\n",
    "                                                              is_val_random=is_val_random)\n",
    "    run_time = time.time() - start\n",
    "\n",
    "    # Evaluate predictions\n",
    "    loss = loss_function(y=y_true, y_hat=y_hat, weights=mask, **loss_kwargs)\n",
    "\n",
    "    result =  {'loss': loss,\n",
    "               'mc': mc,\n",
    "               'y_true': y_true,\n",
    "               'y_hat': y_hat,\n",
    "               'run_time': run_time,\n",
    "               'status': STATUS_OK}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def hyperopt_tunning(space, hyperopt_max_evals, loss_function,\n",
    "                     S_df, Y_df, X_df, f_cols,\n",
    "                     ds_in_val,\n",
    "                     n_uids, n_val_windows, freq,\n",
    "                     is_val_random,\n",
    "                     save_trials=False,\n",
    "                     loss_kwargs=None):\n",
    "    trials = Trials()\n",
    "    fmin_objective = partial(evaluate_model, loss_function=loss_function, \n",
    "                             S_df=S_df, Y_df=Y_df, X_df=X_df, f_cols=f_cols,\n",
    "                             ds_in_test=0, ds_in_val=ds_in_val,\n",
    "                             n_uids=n_uids, n_val_windows=n_val_windows, freq=freq,\n",
    "                             is_val_random=is_val_random,\n",
    "                             loss_kwargs=loss_kwargs or {})\n",
    "\n",
    "    fmin(fmin_objective, space=space, algo=tpe.suggest, max_evals=hyperopt_max_evals, trials=trials, verbose=True)\n",
    "\n",
    "    return trials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Utils Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "from nixtlats.losses.numpy import mae, mape, smape, rmse, pinball_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "if t.cuda.is_available(): device = 'cuda'  \n",
    "\n",
    "nbeats_space= {# Architecture parameters\n",
    "               'model':'nbeats',\n",
    "               'mode': 'simple',\n",
    "               'n_time_in': hp.choice('n_time_in', [7*24]),\n",
    "               'n_time_out': hp.choice('n_time_out', [24]),\n",
    "               'n_x_hidden': hp.quniform('n_x_hidden', 1, 10, 1),\n",
    "               'n_s_hidden': hp.choice('n_s_hidden', [0]),\n",
    "               'shared_weights': hp.choice('shared_weights', [False]),\n",
    "               'activation': hp.choice('activation', ['SELU']),\n",
    "               'initialization':  hp.choice('initialization', ['glorot_normal','he_normal']),\n",
    "               'stack_types': hp.choice('stack_types', [2*['identity'],\n",
    "                                                        1*['identity']+1*['exogenous_tcn'],\n",
    "                                                        1*['exogenous_tcn']+1*['identity'] ]),\n",
    "               'n_blocks': hp.choice('n_blocks', [ [1, 1] ]),\n",
    "               'n_layers': hp.choice('n_layers', [ [2, 2] ]),\n",
    "               'n_hidden': hp.choice('n_hidden', [ 256 ]),\n",
    "               'n_harmonics': hp.choice('n_harmonics', [1]),\n",
    "               'n_polynomials': hp.choice('n_polynomials', [2]),\n",
    "               # Regularization and optimization parameters\n",
    "               'batch_normalization': hp.choice('batch_normalization', [False]),\n",
    "               'dropout_prob_theta': hp.uniform('dropout_prob_theta', 0, 0.5),\n",
    "               'dropout_prob_exogenous': hp.uniform('dropout_prob_exogenous', 0, 0.5),\n",
    "               'learning_rate': hp.loguniform('learning_rate', np.log(5e-4), np.log(0.001)),\n",
    "               'lr_decay': hp.uniform('lr_decay', 0.3, 0.5),\n",
    "               'lr_decay_step_size': hp.choice('lr_decay_step_size', [100]), \n",
    "               'weight_decay': hp.loguniform('weight_decay', np.log(5e-5), np.log(5e-3)),\n",
    "               'max_epochs': hp.choice('max_epochs', [10]), #'n_iterations': hp.choice('n_iterations', [10])\n",
    "               'max_steps': hp.choice('max_steps', [None]),\n",
    "               'early_stop_patience': hp.choice('early_stop_patience', [16]),\n",
    "               'eval_freq': hp.choice('eval_freq', [50]),\n",
    "               'n_val_weeks': hp.choice('n_val_weeks', [52*2]),\n",
    "               'loss_train': hp.choice('loss', ['MAE']),\n",
    "               'loss_hypar': hp.choice('loss_hypar', [0.5]),                \n",
    "               'loss_valid': hp.choice('loss_valid', ['MAE']), #[args.val_loss]),\n",
    "               'l1_theta': hp.choice('l1_theta', [0]),\n",
    "               # Data parameters\n",
    "               'len_sample_chunks': hp.choice('len_sample_chunks', [None]),\n",
    "               'normalizer_y': hp.choice('normalizer_y', [None]),\n",
    "               'normalizer_x': hp.choice('normalizer_x', ['median']),\n",
    "               'window_sampling_limit': hp.choice('window_sampling_limit', [100_000]),\n",
    "               'complete_inputs': hp.choice('complete_inputs', [False]),\n",
    "               'complete_sample': hp.choice('complete_sample', [False]),                \n",
    "               'frequency': hp.choice('frequency', ['H']),\n",
    "               'seasonality': hp.choice('seasonality', [24]),      \n",
    "               'idx_to_sample_freq': hp.choice('idx_to_sample_freq', [24]),\n",
    "               'val_idx_to_sample_freq': hp.choice('val_idx_to_sample_freq', [24]),\n",
    "               'batch_size': hp.choice('batch_size', [256]),\n",
    "               'n_series_per_batch': hp.choice('n_series_per_batch', [1]),\n",
    "               'random_seed': hp.quniform('random_seed', 10, 20, 1),\n",
    "               'device': hp.choice('device', [device])}\n",
    "\n",
    "mc = {'model':'nbeats',\n",
    "      # Architecture parameters\n",
    "      'n_time_in': 7*24,\n",
    "      'n_time_out': 24,\n",
    "      'n_x_hidden': 3,\n",
    "      'n_s_hidden': 0,\n",
    "      'shared_weights': False,\n",
    "      'activation': 'SELU',\n",
    "      'initialization': 'he_normal',\n",
    "      'stack_types': ['exogenous_tcn']+1*['identity'],\n",
    "      'n_blocks': [1, 1],\n",
    "      'n_layers': [2, 2],\n",
    "      'n_hidden': 364,\n",
    "      'n_polynomials': 2,\n",
    "      'n_harmonics': 1,\n",
    "      # Regularization and optimization parameters\n",
    "      'max_epochs': 10, #'n_iterations': 100,\n",
    "      'max_steps': None,      \n",
    "      'early_stop_patience': 8,\n",
    "      'batch_normalization': False,\n",
    "      'dropout_prob_theta': 0.2,\n",
    "      'learning_rate': 0.0005, #0.002,\n",
    "      'lr_decay': 0.64,\n",
    "      'lr_decay_step_size': 100,\n",
    "      'weight_decay': 0.00015,\n",
    "      'eval_freq': 50,\n",
    "      'n_val_weeks': 52*2,\n",
    "      'loss_train': 'PINBALL',\n",
    "      'loss_hypar': 0.5, #0.49,\n",
    "      'loss_valid': 'MAE',\n",
    "      'l1_theta': 0,\n",
    "      # Data parameters\n",
    "      'normalizer_y': None,\n",
    "      'normalizer_x': 'median',\n",
    "      'window_sampling_limit': 100_000,\n",
    "      'complete_inputs': False,\n",
    "      'frequency':'H',\n",
    "      'seasonality': 24,\n",
    "      'idx_to_sample_freq': 24,\n",
    "      'val_idx_to_sample_freq': 24,\n",
    "      'batch_size': 256,\n",
    "      'n_series_per_batch': 1,\n",
    "      'random_seed': 10,\n",
    "      'device': 'cpu'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esrnn_space = {'model': hp.choice('model', ['esrnn']),\n",
    "               'mode': 'full',\n",
    "               # Architecture parameters\n",
    "               'n_time_in': hp.choice('n_time_in', [7*24]),\n",
    "               'n_time_out': hp.choice('n_time_out', [24]),\n",
    "               'dilations': hp.choice('dilations', [ [[1, 2]], [[1,2], [7, 14]] ]),\n",
    "               'es_component': hp.choice('es_component', ['multiplicative']),\n",
    "               'cell_type': hp.choice('cell_type', ['LSTM']),\n",
    "               'state_hsize': hp.quniform('state_hsize', 10, 100, 10),\n",
    "               'add_nl_layer': hp.choice('add_nl_layer', [True, False]),\n",
    "               'seasonality': hp.choice('seasonality', [ [24] ]),\n",
    "               # Regularization and optimization parameters\n",
    "               'max_epochs':hp.choice('max_epochs', [10]),\n",
    "               'max_steps':hp.choice('max_steps', [None]),\n",
    "               'early_stop_patience':hp.choice('early_stop_patience', [10]),\n",
    "               'eval_freq': hp.choice('eval_freq', [10]),\n",
    "               'batch_size': hp.choice('batch_size', [32]),\n",
    "               'learning_rate': hp.loguniform('learning_rate', np.log(5e-4), np.log(0.01)),\n",
    "               'lr_decay': hp.quniform('lr_decay', 0.5, 0.8, 0.1),\n",
    "               'lr_decay_step_size': hp.choice('lr_decay_step_size', [100]), \n",
    "               'per_series_lr_multip': hp.choice('per_series_lr_multip', [0.5, 1.0, 1.5, 2.0, 3.0]),\n",
    "               'gradient_eps': hp.choice('gradient_eps', [1e-8]),\n",
    "               'gradient_clipping_threshold': hp.choice('gradient_clipping_threshold', [10, 50]),\n",
    "               'rnn_weight_decay': hp.choice('rnn_weight_decay', [0, 0.0005, 0.005]),\n",
    "               'noise_std': hp.loguniform('noise_std', np.log(0.0001), np.log(0.001)),\n",
    "               'level_variability_penalty': hp.quniform('level_variability_penalty', 0, 100, 10),\n",
    "               'testing_percentile': hp.choice('testing_percentile', [50]),\n",
    "               'training_percentile': hp.choice('training_percentile', [48, 49, 50, 51]),\n",
    "               'random_seed': hp.quniform('random_seed', 1, 1000, 1),\n",
    "               'loss_train': hp.choice('loss_train', ['SMYL']),\n",
    "               'loss_valid': hp.choice('loss_valid', ['MAE']),\n",
    "               # Data parameters\n",
    "               'len_sample_chunks': hp.choice('len_sample_chunks', [7*3*24]),\n",
    "               'window_sampling_limit': hp.choice('window_sampling_limit', [500_000]),\n",
    "               'complete_inputs': hp.choice('complete_inputs', [True]),\n",
    "               'complete_sample': hp.choice('complete_sample', [True]),\n",
    "               'sample_freq': hp.choice('sample_freq', [24]),\n",
    "               'val_sample_freq': hp.choice('val_sample_freq', [24]),\n",
    "               'n_series_per_batch': hp.choice('n_series_per_batch', [1]),\n",
    "               'normalizer_y': hp.choice('normalizer_y', [None]),\n",
    "               'normalizer_x': hp.choice('normalizer_x',  [None])}\n",
    "\n",
    "mc = {'model':'esrnn',\n",
    "      'mode': 'full',\n",
    "      # Architecture parameters\n",
    "      'n_series': 1,\n",
    "      'n_time_in': 7*24,\n",
    "      'n_time_out': 24,\n",
    "      'n_x': 1,\n",
    "      'n_s': 1,\n",
    "      'dilations': [[1,2], [7]],\n",
    "      'es_component': 'multiplicative',\n",
    "      'cell_type': 'LSTM',\n",
    "      'state_hsize': 50,\n",
    "      'add_nl_layer': False,\n",
    "      'seasonality': [24],\n",
    "      # Regularization and optimization parameters\n",
    "      'max_epochs': 10, #'n_iterations': 100,\n",
    "      'max_steps': None,\n",
    "      'early_stop_patience': 10,\n",
    "      'eval_freq': 10,\n",
    "      'batch_size': 32,\n",
    "      'eq_batch_size': False,\n",
    "      'learning_rate': 0.0005,\n",
    "      'lr_decay': 0.8,\n",
    "      'lr_decay_step_size': 100,\n",
    "      'per_series_lr_multip': 1.5,\n",
    "      'gradient_eps': 1e-8, \n",
    "      'gradient_clipping_threshold': 20,\n",
    "      'rnn_weight_decay': 0.0,\n",
    "      'noise_std': 0.0005,\n",
    "      'level_variability_penalty': 10,\n",
    "      'testing_percentile': 50,\n",
    "      'training_percentile': 50,\n",
    "      'random_seed': 1,\n",
    "      'loss_train': 'SMYL',\n",
    "      'loss_valid': 'MAE',\n",
    "      # Data parameters\n",
    "      'len_sample_chunks': 7*4*24,\n",
    "      'window_sampling_limit': 500_000,\n",
    "      'complete_inputs': True,\n",
    "      'sample_freq': 24,\n",
    "      'val_idx_to_sample_freq': 24,\n",
    "      'n_series_per_batch': 1,\n",
    "      'normalizer_y': None,\n",
    "      'normalizer_x': None}\n",
    "\n",
    "model = instantiate_esrnn(mc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "if t.cuda.is_available(): device = 'cuda'  \n",
    "\n",
    "deepmidas_space= {# Architecture parameters\n",
    "               'model':'deepmidas',\n",
    "               'mode': 'simple',\n",
    "               'n_time_in': hp.choice('n_time_in', [7*24]),\n",
    "               'n_time_out': hp.choice('n_time_out', [24]),\n",
    "               'n_x_hidden': hp.quniform('n_x_hidden', 1, 10, 1),\n",
    "               'n_s_hidden': hp.choice('n_s_hidden', [0]),\n",
    "               'shared_weights': hp.choice('shared_weights', [False]),\n",
    "               'activation': hp.choice('activation', ['SELU']),\n",
    "               'initialization':  hp.choice('initialization', ['glorot_normal','he_normal']),\n",
    "               'stack_types': hp.choice('stack_types', [2*['identity']]),\n",
    "               'n_blocks': hp.choice('n_blocks', [ [1, 1] ]),\n",
    "               'n_layers': hp.choice('n_layers', [ [2, 2] ]),\n",
    "               'n_hidden': hp.choice('n_hidden', [ 256 ]),\n",
    "               'n_pool_kernel_size': hp.choice('n_pool_kernel_size', [ [ 4, 1 ] ]),\n",
    "               'n_freq_downsample': hp.choice('n_freq_downsample', [ [ 24, 1 ] ]),\n",
    "               # Regularization and optimization parameters\n",
    "               'batch_normalization': hp.choice('batch_normalization', [False]),\n",
    "               'dropout_prob_theta': hp.uniform('dropout_prob_theta', 0, 0.5),\n",
    "               'dropout_prob_exogenous': hp.uniform('dropout_prob_exogenous', 0, 0.5),\n",
    "               'learning_rate': hp.loguniform('learning_rate', np.log(5e-4), np.log(0.001)),\n",
    "               'lr_decay': hp.uniform('lr_decay', 0.3, 0.5),\n",
    "               'lr_decay_step_size': hp.choice('lr_decay_step_size', [100]), \n",
    "               'weight_decay': hp.loguniform('weight_decay', np.log(5e-5), np.log(5e-3)),\n",
    "               'max_epochs': hp.choice('max_epochs', [10]), #'n_iterations': hp.choice('n_iterations', [10])\n",
    "               'max_steps': hp.choice('max_steps', [None]),\n",
    "               'early_stop_patience': hp.choice('early_stop_patience', [16]),\n",
    "               'eval_freq': hp.choice('eval_freq', [50]),\n",
    "               'n_val_weeks': hp.choice('n_val_weeks', [52*2]),\n",
    "               'loss_train': hp.choice('loss', ['MAE']),\n",
    "               'loss_hypar': hp.choice('loss_hypar', [0.5]),                \n",
    "               'loss_valid': hp.choice('loss_valid', ['MAE']), #[args.val_loss]),\n",
    "               'l1_theta': hp.choice('l1_theta', [0]),\n",
    "               # Data parameters\n",
    "               'len_sample_chunks': hp.choice('len_sample_chunks', [None]),\n",
    "               'normalizer_y': hp.choice('normalizer_y', [None]),\n",
    "               'normalizer_x': hp.choice('normalizer_x', ['median']),\n",
    "               'window_sampling_limit': hp.choice('window_sampling_limit', [100_000]),\n",
    "               'complete_inputs': hp.choice('complete_inputs', [False]),\n",
    "               'complete_sample': hp.choice('complete_sample', [False]),                \n",
    "               'frequency': hp.choice('frequency', ['H']),\n",
    "               'seasonality': hp.choice('seasonality', [24]),      \n",
    "               'idx_to_sample_freq': hp.choice('idx_to_sample_freq', [24]),\n",
    "               'val_idx_to_sample_freq': hp.choice('val_idx_to_sample_freq', [24]),\n",
    "               'batch_size': hp.choice('batch_size', [256]),\n",
    "               'n_series_per_batch': hp.choice('n_series_per_batch', [1]),\n",
    "               'random_seed': hp.quniform('random_seed', 10, 20, 1),\n",
    "               'device': hp.choice('device', [device])}\n",
    "\n",
    "mc = {'model':'deepmidas',\n",
    "      # Architecture parameters\n",
    "      'n_time_in': 7*24,\n",
    "      'n_time_out': 24,\n",
    "      'n_x_hidden': 3,\n",
    "      'n_s_hidden': 0,\n",
    "      'shared_weights': False,\n",
    "      'activation': 'SELU',\n",
    "      'initialization': 'he_normal',\n",
    "      'stack_types': ['identity', 'identity'],\n",
    "      'n_blocks': [1, 1],\n",
    "      'n_layers': [2, 2],\n",
    "      'n_pool_kernel_size': [4, 1],\n",
    "      'n_freq_downsample': [24, 1],\n",
    "      'n_hidden': 364,\n",
    "      # Regularization and optimization parameters\n",
    "      'max_epochs': 10, #'n_iterations': 100,\n",
    "      'max_steps': None,      \n",
    "      'early_stop_patience': 8,\n",
    "      'batch_normalization': False,\n",
    "      'dropout_prob_theta': 0.2,\n",
    "      'learning_rate': 0.0005, #0.002,\n",
    "      'lr_decay': 0.64,\n",
    "      'lr_decay_step_size': 100,\n",
    "      'weight_decay': 0.00015,\n",
    "      'eval_freq': 50,\n",
    "      'n_val_weeks': 52*2,\n",
    "      'loss_train': 'PINBALL',\n",
    "      'loss_hypar': 0.5, #0.49,\n",
    "      'loss_valid': 'MAE',\n",
    "      'l1_theta': 0,\n",
    "      # Data parameters\n",
    "      'normalizer_y': None,\n",
    "      'normalizer_x': 'median',\n",
    "      'window_sampling_limit': 100_000,\n",
    "      'complete_inputs': False,\n",
    "      'frequency':'H',\n",
    "      'seasonality': 24,\n",
    "      'idx_to_sample_freq': 24,\n",
    "      'val_idx_to_sample_freq': 24,\n",
    "      'batch_size': 256,\n",
    "      'n_series_per_batch': 1,\n",
    "      'random_seed': 10,\n",
    "      'device': 'cpu'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from nixtlats.data.datasets.epf import EPF, EPFInfo\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dataset = ['NP']\n",
    "\n",
    "Y_df, X_df, S_df = EPF.load_groups(directory='data', groups=dataset)\n",
    "\n",
    "X_df = X_df[['unique_id', 'ds', 'week_day']]\n",
    "Y_min = Y_df.y.min()\n",
    "#Y_df.y = Y_df.y - Y_min + 20\n",
    "\n",
    "plt.plot(Y_df.y.values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: think about use of big windows during train for extremely long series\n",
    "# backpropagation trough time is slow\n",
    "# result = evaluate_model(loss_function=mae, mc=mc, \n",
    "#                         S_df=S_df, Y_df=Y_df, X_df=X_df, f_cols=[],\n",
    "#                         ds_in_test=0, ds_in_val=728*24,\n",
    "#                         n_uids=None, n_val_windows=None, freq=None,\n",
    "#                         is_val_random=False, loss_kwargs={})\n",
    "# result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(result['y_hat'].flatten())\n",
    "# plt.plot(Y_df['y'][-728*24:].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.074404 seconds\n",
      "INFO:hyperopt.tpe:TPE using 0 trials\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================\n",
      "\n",
      "activation                                SELU\n",
      "batch_normalization                      False\n",
      "batch_size                                 256\n",
      "complete_inputs                          False\n",
      "complete_sample                          False\n",
      "device                                     cpu\n",
      "dropout_prob_exogenous                0.206928\n",
      "dropout_prob_theta                    0.095228\n",
      "early_stop_patience                         16\n",
      "eval_freq                                   50\n",
      "frequency                                    H\n",
      "idx_to_sample_freq                          24\n",
      "initialization                       he_normal\n",
      "l1_theta                                     0\n",
      "learning_rate                          0.00074\n",
      "len_sample_chunks                         None\n",
      "loss_hypar                                 0.5\n",
      "loss_train                                 MAE\n",
      "loss_valid                                 MAE\n",
      "lr_decay                              0.488883\n",
      "lr_decay_step_size                         100\n",
      "max_epochs                                  10\n",
      "max_steps                                 None\n",
      "mode                                    simple\n",
      "model                                deepmidas\n",
      "n_blocks                                (1, 1)\n",
      "n_freq_downsample                      (24, 1)\n",
      "n_hidden                                   256\n",
      "n_layers                                (2, 2)\n",
      "n_pool_kernel_size                      (4, 1)\n",
      "n_s_hidden                                   0\n",
      "n_series_per_batch                           1\n",
      "n_time_in                                  168\n",
      "n_time_out                                  24\n",
      "n_val_weeks                                104\n",
      "n_x_hidden                                 4.0\n",
      "normalizer_x                            median\n",
      "normalizer_y                              None\n",
      "random_seed                               13.0\n",
      "seasonality                                 24\n",
      "shared_weights                           False\n",
      "stack_types               (identity, identity)\n",
      "val_idx_to_sample_freq                      24\n",
      "weight_decay                          0.003854\n",
      "window_sampling_limit                   100000\n",
      "dtype: object\n",
      "===============================================\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Train Validation splits\n",
      "\n",
      "INFO:root:                              ds                    \n",
      "                             min                 max\n",
      "unique_id sample_mask                               \n",
      "NP        0           2016-12-27 2018-12-24 23:00:00\n",
      "          1           2013-01-01 2016-12-26 23:00:00\n",
      "INFO:root:\n",
      "Total data \t\t\t52416 time stamps \n",
      "Available percentage=100.0, \t52416 time stamps \n",
      "Insample  percentage=66.67, \t34944 time stamps \n",
      "Outsample percentage=33.33, \t17472 time stamps \n",
      "\n",
      "/Users/cchallu/NIXTLA/nixtlats/nixtlats/data/tsdataset.py:208: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  X.drop(['unique_id', 'ds'], 1, inplace=True)\n",
      "\n",
      "INFO:root:Train Validation splits\n",
      "\n",
      "INFO:root:                              ds                    \n",
      "                             min                 max\n",
      "unique_id sample_mask                               \n",
      "NP        0           2013-01-01 2016-12-26 23:00:00\n",
      "          1           2016-12-27 2018-12-24 23:00:00\n",
      "INFO:root:\n",
      "Total data \t\t\t52416 time stamps \n",
      "Available percentage=100.0, \t52416 time stamps \n",
      "Insample  percentage=33.33, \t17472 time stamps \n",
      "Outsample percentage=66.67, \t34944 time stamps \n",
      "\n",
      "/Users/cchallu/NIXTLA/nixtlats/nixtlats/data/tsdataset.py:208: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  X.drop(['unique_id', 'ds'], 1, inplace=True)\n",
      "\n",
      "INFO:root:Train Validation splits\n",
      "\n",
      "INFO:root:                              ds                    \n",
      "                             min                 max\n",
      "unique_id sample_mask                               \n",
      "NP        0           2013-01-01 2018-12-24 23:00:00\n",
      "INFO:root:\n",
      "Total data \t\t\t52416 time stamps \n",
      "Available percentage=100.0, \t52416 time stamps \n",
      "Insample  percentage=0.0, \t0 time stamps \n",
      "Outsample percentage=100.0, \t52416 time stamps \n",
      "\n",
      "/Users/cchallu/NIXTLA/nixtlats/nixtlats/data/tsdataset.py:208: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  X.drop(['unique_id', 'ds'], 1, inplace=True)\n",
      "\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "\n",
      "  | Name  | Type       | Params\n",
      "-------------------------------------\n",
      "0 | model | _DeepMIDAS | 376 K \n",
      "-------------------------------------\n",
      "376 K     Trainable params\n",
      "0         Non-trainable params\n",
      "376 K     Total params\n",
      "1.508     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check: 0it [00:00, ?it/s]\n",
      "Validation sanity check:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cchallu/anaconda3/envs/nixtla/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:106: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "\n",
      "/Users/cchallu/anaconda3/envs/nixtla/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:106: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "\n",
      "/Users/cchallu/anaconda3/envs/nixtla/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:323: UserWarning: The number of training samples (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: -1it [00:00, ?it/s]\n",
      "Training:   0%|          | 0/2 [00:00<00:00, 31300.78it/s]\n",
      "Epoch 0:   0%|          | 0/2 [00:00<00:00, 434.96it/s]   \n",
      "Epoch 0:  50%|#####     | 1/2 [00:00<00:00, 25.15it/s, loss=3.44, v_num=25, train_loss_step=3.440]\n",
      "  0%|          | 0/2 [00:01<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cchallu/anaconda3/envs/nixtla/lib/python3.7/site-packages/torch/nn/functional.py:652: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /Users/distiller/project/conda/conda-bld/pytorch_1623459064158/work/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool1d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating: 0it [00:00, ?it/s]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Epoch 0: 100%|##########| 2/2 [00:00<00:00, 27.67it/s, loss=3.44, v_num=25, train_loss_step=3.440, val_loss=6.790]\n",
      "  0%|          | 0/2 [00:01<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 6.787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/2 [00:00<00:00, 17189.77it/s, loss=3.44, v_num=25, train_loss_step=3.440, val_loss=6.790]\n",
      "Epoch 1:   0%|          | 0/2 [00:00<00:00, 168.86it/s, loss=3.44, v_num=25, train_loss_step=3.440, val_loss=6.790]  \n",
      "Epoch 1:  50%|#####     | 1/2 [00:00<00:00, 17.28it/s, loss=3.44, v_num=25, train_loss_step=3.440, val_loss=6.790] \n",
      "Epoch 1:  50%|#####     | 1/2 [00:00<00:00, 16.80it/s, loss=4.69, v_num=25, train_loss_step=5.940, val_loss=6.790, train_loss_epoch=3.440]\n",
      "Validating: 0it [00:00, ?it/s]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Epoch 1: 100%|##########| 2/2 [00:00<00:00, 19.33it/s, loss=4.69, v_num=25, train_loss_step=5.940, val_loss=4.150, train_loss_epoch=3.440]\n",
      "  0%|          | 0/2 [00:01<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 2.640 >= min_delta = 0.0001. New best score: 4.147\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/2 [00:00<00:00, 20360.70it/s, loss=4.69, v_num=25, train_loss_step=5.940, val_loss=4.150, train_loss_epoch=3.440]\n",
      "Epoch 2:   0%|          | 0/2 [00:00<00:00, 359.56it/s, loss=4.69, v_num=25, train_loss_step=5.940, val_loss=4.150, train_loss_epoch=3.440]  \n",
      "Epoch 2:  50%|#####     | 1/2 [00:00<00:00, 27.44it/s, loss=4.29, v_num=25, train_loss_step=3.500, val_loss=4.150, train_loss_epoch=5.940] \n",
      "Validating: 0it [00:00, ?it/s]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Epoch 2: 100%|##########| 2/2 [00:00<00:00, 29.66it/s, loss=4.29, v_num=25, train_loss_step=3.500, val_loss=4.840, train_loss_epoch=5.940]\n",
      "Epoch 2:   0%|          | 0/2 [00:00<00:00, 21290.88it/s, loss=4.29, v_num=25, train_loss_step=3.500, val_loss=4.840, train_loss_epoch=5.940]\n",
      "Epoch 3:   0%|          | 0/2 [00:00<00:00, 365.23it/s, loss=4.29, v_num=25, train_loss_step=3.500, val_loss=4.840, train_loss_epoch=5.940]  \n",
      "Epoch 3:  50%|#####     | 1/2 [00:00<00:00, 26.65it/s, loss=4.21, v_num=25, train_loss_step=3.970, val_loss=4.840, train_loss_epoch=3.500] \n",
      "Validating: 0it [00:00, ?it/s]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Epoch 3: 100%|##########| 2/2 [00:00<00:00, 28.05it/s, loss=4.21, v_num=25, train_loss_step=3.970, val_loss=4.500, train_loss_epoch=3.500]\n",
      "Epoch 3:   0%|          | 0/2 [00:00<00:00, 20460.02it/s, loss=4.21, v_num=25, train_loss_step=3.970, val_loss=4.500, train_loss_epoch=3.500]\n",
      "Epoch 4:   0%|          | 0/2 [00:00<00:00, 314.96it/s, loss=4.21, v_num=25, train_loss_step=3.970, val_loss=4.500, train_loss_epoch=3.500]  \n",
      "Epoch 4:  50%|#####     | 1/2 [00:00<00:00, 22.06it/s, loss=4.11, v_num=25, train_loss_step=3.720, val_loss=4.500, train_loss_epoch=3.970] \n",
      "Validating: 0it [00:00, ?it/s]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Epoch 4: 100%|##########| 2/2 [00:00<00:00, 21.96it/s, loss=4.11, v_num=25, train_loss_step=3.720, val_loss=3.120, train_loss_epoch=3.970]\n",
      "  0%|          | 0/2 [00:01<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 1.032 >= min_delta = 0.0001. New best score: 3.115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4:   0%|          | 0/2 [00:00<00:00, 20068.44it/s, loss=4.11, v_num=25, train_loss_step=3.720, val_loss=3.120, train_loss_epoch=3.970]\n",
      "Epoch 5:   0%|          | 0/2 [00:00<00:00, 249.68it/s, loss=4.11, v_num=25, train_loss_step=3.720, val_loss=3.120, train_loss_epoch=3.970]  \n",
      "Epoch 5:  50%|#####     | 1/2 [00:00<00:00, 18.89it/s, loss=4.11, v_num=25, train_loss_step=3.720, val_loss=3.120, train_loss_epoch=3.970] \n",
      "Epoch 5:  50%|#####     | 1/2 [00:00<00:00, 18.18it/s, loss=3.87, v_num=25, train_loss_step=2.670, val_loss=3.120, train_loss_epoch=3.720]\n",
      "Validating: 0it [00:00, ?it/s]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Epoch 5: 100%|##########| 2/2 [00:00<00:00, 18.66it/s, loss=3.87, v_num=25, train_loss_step=2.670, val_loss=3.230, train_loss_epoch=3.720]\n",
      "Epoch 5:   0%|          | 0/2 [00:00<00:00, 8542.37it/s, loss=3.87, v_num=25, train_loss_step=2.670, val_loss=3.230, train_loss_epoch=3.720]\n",
      "Epoch 6:   0%|          | 0/2 [00:00<00:00, 196.77it/s, loss=3.87, v_num=25, train_loss_step=2.670, val_loss=3.230, train_loss_epoch=3.720] \n",
      "Epoch 6:  50%|#####     | 1/2 [00:00<00:00, 23.30it/s, loss=3.72, v_num=25, train_loss_step=2.820, val_loss=3.230, train_loss_epoch=2.670] \n",
      "Validating: 0it [00:00, ?it/s]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Epoch 6: 100%|##########| 2/2 [00:00<00:00, 25.09it/s, loss=3.72, v_num=25, train_loss_step=2.820, val_loss=3.570, train_loss_epoch=2.670]\n",
      "Epoch 6:   0%|          | 0/2 [00:00<00:00, 21183.35it/s, loss=3.72, v_num=25, train_loss_step=2.820, val_loss=3.570, train_loss_epoch=2.670]\n",
      "Epoch 7:   0%|          | 0/2 [00:00<00:00, 346.15it/s, loss=3.72, v_num=25, train_loss_step=2.820, val_loss=3.570, train_loss_epoch=2.670]  \n",
      "Epoch 7:  50%|#####     | 1/2 [00:00<00:00, 25.79it/s, loss=3.65, v_num=25, train_loss_step=3.110, val_loss=3.570, train_loss_epoch=2.820] \n",
      "Validating: 0it [00:00, ?it/s]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Epoch 7: 100%|##########| 2/2 [00:00<00:00, 27.61it/s, loss=3.65, v_num=25, train_loss_step=3.110, val_loss=3.200, train_loss_epoch=2.820]\n",
      "Epoch 7:   0%|          | 0/2 [00:00<00:00, 21399.51it/s, loss=3.65, v_num=25, train_loss_step=3.110, val_loss=3.200, train_loss_epoch=2.820]\n",
      "Epoch 8:   0%|          | 0/2 [00:00<00:00, 385.65it/s, loss=3.65, v_num=25, train_loss_step=3.110, val_loss=3.200, train_loss_epoch=2.820]  \n",
      "Epoch 8:  50%|#####     | 1/2 [00:00<00:00, 26.70it/s, loss=3.55, v_num=25, train_loss_step=2.790, val_loss=3.200, train_loss_epoch=3.110] \n",
      "Validating: 0it [00:00, ?it/s]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Epoch 8: 100%|##########| 2/2 [00:00<00:00, 27.04it/s, loss=3.55, v_num=25, train_loss_step=2.790, val_loss=2.830, train_loss_epoch=3.110]\n",
      "  0%|          | 0/2 [00:02<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.281 >= min_delta = 0.0001. New best score: 2.834\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8:   0%|          | 0/2 [00:00<00:00, 19152.07it/s, loss=3.55, v_num=25, train_loss_step=2.790, val_loss=2.830, train_loss_epoch=3.110]\n",
      "Epoch 9:   0%|          | 0/2 [00:00<00:00, 286.28it/s, loss=3.55, v_num=25, train_loss_step=2.790, val_loss=2.830, train_loss_epoch=3.110]  \n",
      "Epoch 9:  50%|#####     | 1/2 [00:00<00:00, 20.56it/s, loss=3.44, v_num=25, train_loss_step=2.460, val_loss=2.830, train_loss_epoch=2.790] \n",
      "Epoch 9: 100%|##########| 2/2 [00:00<00:00, 29.17it/s, loss=3.44, v_num=25, train_loss_step=2.460, val_loss=2.830, train_loss_epoch=2.790]\n",
      "Validating: 0it [00:00, ?it/s]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Epoch 9: 100%|##########| 2/2 [00:00<00:00, 20.55it/s, loss=3.44, v_num=25, train_loss_step=2.460, val_loss=3.040, train_loss_epoch=2.790]\n",
      "Epoch 9: 100%|##########| 2/2 [00:00<00:00, 17.28it/s, loss=3.44, v_num=25, train_loss_step=2.460, val_loss=3.040, train_loss_epoch=2.790]\n",
      "  0%|          | 0/2 [00:02<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cchallu/anaconda3/envs/nixtla/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:106: UserWarning: The dataloader, predict dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting: 1it [00:00, ?it/s]\n",
      "Predicting: 100%|##########| 1/1 [00:00<?, ?it/s]\n",
      "y_true.shape (#n_series, #n_fcds, #lt): (728,)\n",
      "y_hat.shape (#n_series, #n_fcds, #lt): (728,)\n",
      " 50%|     | 1/2 [00:02<00:02,  2.58s/trial, best loss: 1.9853875637054443]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.013553 seconds\n",
      "INFO:hyperopt.tpe:TPE using 1/1 trials with best loss 1.985388\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================\n",
      "\n",
      "activation                                SELU\n",
      "batch_normalization                      False\n",
      "batch_size                                 256\n",
      "complete_inputs                          False\n",
      "complete_sample                          False\n",
      "device                                     cpu\n",
      "dropout_prob_exogenous                0.459546\n",
      "dropout_prob_theta                    0.337756\n",
      "early_stop_patience                         16\n",
      "eval_freq                                   50\n",
      "frequency                                    H\n",
      "idx_to_sample_freq                          24\n",
      "initialization                   glorot_normal\n",
      "l1_theta                                     0\n",
      "learning_rate                         0.000546\n",
      "len_sample_chunks                         None\n",
      "loss_hypar                                 0.5\n",
      "loss_train                                 MAE\n",
      "loss_valid                                 MAE\n",
      "lr_decay                              0.412224\n",
      "lr_decay_step_size                         100\n",
      "max_epochs                                  10\n",
      "max_steps                                 None\n",
      "mode                                    simple\n",
      "model                                deepmidas\n",
      "n_blocks                                (1, 1)\n",
      "n_freq_downsample                      (24, 1)\n",
      "n_hidden                                   256\n",
      "n_layers                                (2, 2)\n",
      "n_pool_kernel_size                      (4, 1)\n",
      "n_s_hidden                                   0\n",
      "n_series_per_batch                           1\n",
      "n_time_in                                  168\n",
      "n_time_out                                  24\n",
      "n_val_weeks                                104\n",
      "n_x_hidden                                10.0\n",
      "normalizer_x                            median\n",
      "normalizer_y                              None\n",
      "random_seed                               14.0\n",
      "seasonality                                 24\n",
      "shared_weights                           False\n",
      "stack_types               (identity, identity)\n",
      "val_idx_to_sample_freq                      24\n",
      "weight_decay                          0.000399\n",
      "window_sampling_limit                   100000\n",
      "dtype: object\n",
      "===============================================\n",
      "\n",
      " 50%|     | 1/2 [00:02<00:02,  2.58s/trial, best loss: 1.9853875637054443]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Train Validation splits\n",
      "\n",
      "INFO:root:                              ds                    \n",
      "                             min                 max\n",
      "unique_id sample_mask                               \n",
      "NP        0           2016-12-27 2018-12-24 23:00:00\n",
      "          1           2013-01-01 2016-12-26 23:00:00\n",
      "INFO:root:\n",
      "Total data \t\t\t52416 time stamps \n",
      "Available percentage=100.0, \t52416 time stamps \n",
      "Insample  percentage=66.67, \t34944 time stamps \n",
      "Outsample percentage=33.33, \t17472 time stamps \n",
      "\n",
      "/Users/cchallu/NIXTLA/nixtlats/nixtlats/data/tsdataset.py:208: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  X.drop(['unique_id', 'ds'], 1, inplace=True)\n",
      "\n",
      "INFO:root:Train Validation splits\n",
      "\n",
      "INFO:root:                              ds                    \n",
      "                             min                 max\n",
      "unique_id sample_mask                               \n",
      "NP        0           2013-01-01 2016-12-26 23:00:00\n",
      "          1           2016-12-27 2018-12-24 23:00:00\n",
      "INFO:root:\n",
      "Total data \t\t\t52416 time stamps \n",
      "Available percentage=100.0, \t52416 time stamps \n",
      "Insample  percentage=33.33, \t17472 time stamps \n",
      "Outsample percentage=66.67, \t34944 time stamps \n",
      "\n",
      "/Users/cchallu/NIXTLA/nixtlats/nixtlats/data/tsdataset.py:208: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  X.drop(['unique_id', 'ds'], 1, inplace=True)\n",
      "\n",
      "INFO:root:Train Validation splits\n",
      "\n",
      "INFO:root:                              ds                    \n",
      "                             min                 max\n",
      "unique_id sample_mask                               \n",
      "NP        0           2013-01-01 2018-12-24 23:00:00\n",
      "INFO:root:\n",
      "Total data \t\t\t52416 time stamps \n",
      "Available percentage=100.0, \t52416 time stamps \n",
      "Insample  percentage=0.0, \t0 time stamps \n",
      "Outsample percentage=100.0, \t52416 time stamps \n",
      "\n",
      "/Users/cchallu/NIXTLA/nixtlats/nixtlats/data/tsdataset.py:208: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  X.drop(['unique_id', 'ds'], 1, inplace=True)\n",
      "\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "\n",
      "  | Name  | Type       | Params\n",
      "-------------------------------------\n",
      "0 | model | _DeepMIDAS | 376 K \n",
      "-------------------------------------\n",
      "376 K     Trainable params\n",
      "0         Non-trainable params\n",
      "376 K     Total params\n",
      "1.508     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check: 0it [00:00, ?it/s]\n",
      "Validation sanity check:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      " 50%|     | 1/2 [00:03<00:02,  2.58s/trial, best loss: 1.9853875637054443]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cchallu/anaconda3/envs/nixtla/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:106: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "\n",
      "/Users/cchallu/anaconda3/envs/nixtla/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:106: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "\n",
      "/Users/cchallu/anaconda3/envs/nixtla/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:323: UserWarning: The number of training samples (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: -1it [00:00, ?it/s]\n",
      "Training:   0%|          | 0/2 [00:00<00:00, 18808.54it/s]\n",
      "Epoch 0:   0%|          | 0/2 [00:00<00:00, 419.47it/s]   \n",
      "Epoch 0:  50%|#####     | 1/2 [00:00<00:00, 25.13it/s, loss=4.14, v_num=26, train_loss_step=4.140]\n",
      "Validating: 0it [00:00, ?it/s]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Epoch 0: 100%|##########| 2/2 [00:00<00:00, 24.93it/s, loss=4.14, v_num=26, train_loss_step=4.140, val_loss=5.750]\n",
      " 50%|     | 1/2 [00:03<00:02,  2.58s/trial, best loss: 1.9853875637054443]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 5.752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/2 [00:00<00:00, 13888.42it/s, loss=4.14, v_num=26, train_loss_step=4.140, val_loss=5.750]\n",
      "Epoch 1:   0%|          | 0/2 [00:00<00:00, 316.17it/s, loss=4.14, v_num=26, train_loss_step=4.140, val_loss=5.750]  \n",
      "Epoch 1:  50%|#####     | 1/2 [00:00<00:00, 22.27it/s, loss=4.58, v_num=26, train_loss_step=5.020, val_loss=5.750, train_loss_epoch=4.140]\n",
      "Validating: 0it [00:00, ?it/s]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Epoch 1: 100%|##########| 2/2 [00:00<00:00, 23.07it/s, loss=4.58, v_num=26, train_loss_step=5.020, val_loss=3.090, train_loss_epoch=4.140]\n",
      " 50%|     | 1/2 [00:04<00:02,  2.58s/trial, best loss: 1.9853875637054443]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 2.664 >= min_delta = 0.0001. New best score: 3.088\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/2 [00:00<00:00, 18893.26it/s, loss=4.58, v_num=26, train_loss_step=5.020, val_loss=3.090, train_loss_epoch=4.140]\n",
      "Epoch 2:   0%|          | 0/2 [00:00<00:00, 207.46it/s, loss=4.58, v_num=26, train_loss_step=5.020, val_loss=3.090, train_loss_epoch=4.140]  \n",
      "Epoch 2:  50%|#####     | 1/2 [00:00<00:00, 21.81it/s, loss=4.19, v_num=26, train_loss_step=3.410, val_loss=3.090, train_loss_epoch=5.020] \n",
      "Validating: 0it [00:00, ?it/s]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Epoch 2: 100%|##########| 2/2 [00:00<00:00, 23.29it/s, loss=4.19, v_num=26, train_loss_step=3.410, val_loss=4.460, train_loss_epoch=5.020]\n",
      "Epoch 2:   0%|          | 0/2 [00:00<00:00, 18157.16it/s, loss=4.19, v_num=26, train_loss_step=3.410, val_loss=4.460, train_loss_epoch=5.020]\n",
      "Epoch 3:   0%|          | 0/2 [00:00<00:00, 235.52it/s, loss=4.19, v_num=26, train_loss_step=3.410, val_loss=4.460, train_loss_epoch=5.020]  \n",
      "Epoch 3:  50%|#####     | 1/2 [00:00<00:00, 20.89it/s, loss=4.14, v_num=26, train_loss_step=4.000, val_loss=4.460, train_loss_epoch=3.410] \n",
      "Epoch 3: 100%|##########| 2/2 [00:00<00:00, 29.57it/s, loss=4.14, v_num=26, train_loss_step=4.000, val_loss=4.460, train_loss_epoch=3.410]\n",
      "Validating: 0it [00:00, ?it/s]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Epoch 3: 100%|##########| 2/2 [00:00<00:00, 19.77it/s, loss=4.14, v_num=26, train_loss_step=4.000, val_loss=4.120, train_loss_epoch=3.410]\n",
      "Epoch 3:   0%|          | 0/2 [00:00<00:00, 14315.03it/s, loss=4.14, v_num=26, train_loss_step=4.000, val_loss=4.120, train_loss_epoch=3.410]\n",
      "Epoch 4:   0%|          | 0/2 [00:00<00:00, 150.11it/s, loss=4.14, v_num=26, train_loss_step=4.000, val_loss=4.120, train_loss_epoch=3.410]  \n",
      "Epoch 4:  50%|#####     | 1/2 [00:00<00:00, 18.75it/s, loss=4.07, v_num=26, train_loss_step=3.780, val_loss=4.120, train_loss_epoch=4.000] \n",
      "Epoch 4: 100%|##########| 2/2 [00:00<00:00, 26.40it/s, loss=4.07, v_num=26, train_loss_step=3.780, val_loss=4.120, train_loss_epoch=4.000]\n",
      "Validating: 0it [00:00, ?it/s]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Epoch 4: 100%|##########| 2/2 [00:00<00:00, 17.95it/s, loss=4.07, v_num=26, train_loss_step=3.780, val_loss=2.920, train_loss_epoch=4.000]\n",
      " 50%|     | 1/2 [00:04<00:02,  2.58s/trial, best loss: 1.9853875637054443]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.166 >= min_delta = 0.0001. New best score: 2.922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4:   0%|          | 0/2 [00:00<00:00, 18893.26it/s, loss=4.07, v_num=26, train_loss_step=3.780, val_loss=2.920, train_loss_epoch=4.000]\n",
      "Epoch 5:   0%|          | 0/2 [00:00<00:00, 266.95it/s, loss=4.07, v_num=26, train_loss_step=3.780, val_loss=2.920, train_loss_epoch=4.000]  \n",
      "Epoch 5:  50%|#####     | 1/2 [00:00<00:00, 20.26it/s, loss=3.89, v_num=26, train_loss_step=3.000, val_loss=2.920, train_loss_epoch=3.780] \n",
      "Epoch 5: 100%|##########| 2/2 [00:00<00:00, 28.26it/s, loss=3.89, v_num=26, train_loss_step=3.000, val_loss=2.920, train_loss_epoch=3.780]\n",
      "Validating: 0it [00:00, ?it/s]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Epoch 5: 100%|##########| 2/2 [00:00<00:00, 20.17it/s, loss=3.89, v_num=26, train_loss_step=3.000, val_loss=3.030, train_loss_epoch=3.780]\n",
      "Epoch 5:   0%|          | 0/2 [00:00<00:00, 10330.80it/s, loss=3.89, v_num=26, train_loss_step=3.000, val_loss=3.030, train_loss_epoch=3.780]\n",
      "Epoch 6:   0%|          | 0/2 [00:00<00:00, 270.57it/s, loss=3.89, v_num=26, train_loss_step=3.000, val_loss=3.030, train_loss_epoch=3.780]  \n",
      "Epoch 6:  50%|#####     | 1/2 [00:00<00:00, 14.36it/s, loss=3.78, v_num=26, train_loss_step=3.090, val_loss=3.030, train_loss_epoch=3.000] \n",
      "Epoch 6: 100%|##########| 2/2 [00:00<00:00, 20.55it/s, loss=3.78, v_num=26, train_loss_step=3.090, val_loss=3.030, train_loss_epoch=3.000]\n",
      "Validating: 0it [00:00, ?it/s]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Epoch 6: 100%|##########| 2/2 [00:00<00:00, 15.63it/s, loss=3.78, v_num=26, train_loss_step=3.090, val_loss=3.450, train_loss_epoch=3.000]\n",
      "Epoch 6:   0%|          | 0/2 [00:00<00:00, 19239.93it/s, loss=3.78, v_num=26, train_loss_step=3.090, val_loss=3.450, train_loss_epoch=3.000]\n",
      "Epoch 7:   0%|          | 0/2 [00:00<00:00, 276.00it/s, loss=3.78, v_num=26, train_loss_step=3.090, val_loss=3.450, train_loss_epoch=3.000]  \n",
      "Epoch 7:  50%|#####     | 1/2 [00:00<00:00, 25.94it/s, loss=3.71, v_num=26, train_loss_step=3.230, val_loss=3.450, train_loss_epoch=3.090] \n",
      "Validating: 0it [00:00, ?it/s]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Epoch 7: 100%|##########| 2/2 [00:00<00:00, 27.92it/s, loss=3.71, v_num=26, train_loss_step=3.230, val_loss=3.200, train_loss_epoch=3.090]\n",
      "Epoch 7:   0%|          | 0/2 [00:00<00:00, 20360.70it/s, loss=3.71, v_num=26, train_loss_step=3.230, val_loss=3.200, train_loss_epoch=3.090]\n",
      "Epoch 8:   0%|          | 0/2 [00:00<00:00, 298.31it/s, loss=3.71, v_num=26, train_loss_step=3.230, val_loss=3.200, train_loss_epoch=3.090]  \n",
      "Epoch 8:  50%|#####     | 1/2 [00:00<00:00, 26.30it/s, loss=3.64, v_num=26, train_loss_step=3.080, val_loss=3.200, train_loss_epoch=3.230] \n",
      "Validating: 0it [00:00, ?it/s]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Epoch 8: 100%|##########| 2/2 [00:00<00:00, 26.42it/s, loss=3.64, v_num=26, train_loss_step=3.080, val_loss=2.750, train_loss_epoch=3.230]\n",
      " 50%|     | 1/2 [00:05<00:02,  2.58s/trial, best loss: 1.9853875637054443]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.174 >= min_delta = 0.0001. New best score: 2.749\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8:   0%|          | 0/2 [00:00<00:00, 12018.06it/s, loss=3.64, v_num=26, train_loss_step=3.080, val_loss=2.750, train_loss_epoch=3.230]\n",
      "Epoch 9:   0%|          | 0/2 [00:00<00:00, 304.49it/s, loss=3.64, v_num=26, train_loss_step=3.080, val_loss=2.750, train_loss_epoch=3.230]  \n",
      "Epoch 9:  50%|#####     | 1/2 [00:00<00:00, 22.69it/s, loss=3.55, v_num=26, train_loss_step=2.780, val_loss=2.750, train_loss_epoch=3.080] \n",
      "Validating: 0it [00:00, ?it/s]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Epoch 9: 100%|##########| 2/2 [00:00<00:00, 22.42it/s, loss=3.55, v_num=26, train_loss_step=2.780, val_loss=2.730, train_loss_epoch=3.080]\n",
      " 50%|     | 1/2 [00:05<00:02,  2.58s/trial, best loss: 1.9853875637054443]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.022 >= min_delta = 0.0001. New best score: 2.727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|##########| 2/2 [00:00<00:00, 18.78it/s, loss=3.55, v_num=26, train_loss_step=2.780, val_loss=2.730, train_loss_epoch=3.080]\n",
      " 50%|     | 1/2 [00:05<00:02,  2.58s/trial, best loss: 1.9853875637054443]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cchallu/anaconda3/envs/nixtla/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:106: UserWarning: The dataloader, predict dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting: 1it [00:00, ?it/s]\n",
      "Predicting: 100%|##########| 1/1 [00:00<?, ?it/s]\n",
      "y_true.shape (#n_series, #n_fcds, #lt): (728,)\n",
      "y_hat.shape (#n_series, #n_fcds, #lt): (728,)\n",
      "100%|| 2/2 [00:05<00:00,  2.76s/trial, best loss: 1.9818161725997925]\n"
     ]
    }
   ],
   "source": [
    "trials = hyperopt_tunning(space=deepmidas_space, hyperopt_max_evals=2, loss_function=mae,\n",
    "                          S_df=S_df, Y_df=Y_df, X_df=X_df, f_cols=[],\n",
    "                          ds_in_val=728*24, n_uids=None, n_val_windows=None, freq=None,\n",
    "                          is_val_random=False, loss_kwargs={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'state': 2,\n",
       "  'tid': 0,\n",
       "  'spec': None,\n",
       "  'result': {'loss': 1.9853875637054443,\n",
       "   'mc': {'activation': 'SELU',\n",
       "    'batch_normalization': False,\n",
       "    'batch_size': 256,\n",
       "    'complete_inputs': False,\n",
       "    'complete_sample': False,\n",
       "    'device': 'cpu',\n",
       "    'dropout_prob_exogenous': 0.2069281539939935,\n",
       "    'dropout_prob_theta': 0.09522759273514453,\n",
       "    'early_stop_patience': 16,\n",
       "    'eval_freq': 50,\n",
       "    'frequency': 'H',\n",
       "    'idx_to_sample_freq': 24,\n",
       "    'initialization': 'he_normal',\n",
       "    'l1_theta': 0,\n",
       "    'learning_rate': 0.0007396856533221593,\n",
       "    'len_sample_chunks': None,\n",
       "    'loss_hypar': 0.5,\n",
       "    'loss_train': 'MAE',\n",
       "    'loss_valid': 'MAE',\n",
       "    'lr_decay': 0.488883166981553,\n",
       "    'lr_decay_step_size': 100,\n",
       "    'max_epochs': 10,\n",
       "    'max_steps': None,\n",
       "    'mode': 'simple',\n",
       "    'model': 'deepmidas',\n",
       "    'n_blocks': (1, 1),\n",
       "    'n_freq_downsample': (24, 1),\n",
       "    'n_hidden': 256,\n",
       "    'n_layers': (2, 2),\n",
       "    'n_pool_kernel_size': (4, 1),\n",
       "    'n_s_hidden': 0,\n",
       "    'n_series_per_batch': 1,\n",
       "    'n_time_in': 168,\n",
       "    'n_time_out': 24,\n",
       "    'n_val_weeks': 104,\n",
       "    'n_x_hidden': 4.0,\n",
       "    'normalizer_x': 'median',\n",
       "    'normalizer_y': None,\n",
       "    'random_seed': 13.0,\n",
       "    'seasonality': 24,\n",
       "    'shared_weights': False,\n",
       "    'stack_types': ('identity', 'identity'),\n",
       "    'val_idx_to_sample_freq': 24,\n",
       "    'weight_decay': 0.003853558423761368,\n",
       "    'window_sampling_limit': 100000,\n",
       "    'n_x': 1,\n",
       "    'n_s': 1,\n",
       "    'n_theta_hidden': [[256, 256], [256, 256]]},\n",
       "   'y_true': array([25.73, 29.37, 28.76, 25.95, 26.71, 29.36, 30.93, 28.3 , 30.58,\n",
       "          33.32, 28.86, 30.5 , 29.72, 28.69, 28.  , 25.04, 28.72, 29.79,\n",
       "          29.41, 30.89, 30.99, 29.56, 27.84, 28.19, 27.36, 27.84, 28.13,\n",
       "          28.42, 28.4 , 27.41, 28.66, 28.67, 28.51, 27.96, 29.31, 29.99,\n",
       "          28.97, 29.96, 30.59, 30.8 , 30.62, 31.01, 32.  , 33.14, 32.63,\n",
       "          32.11, 31.77, 31.04, 31.35, 31.96, 31.01, 29.98, 30.36, 28.15,\n",
       "          28.87, 27.66, 29.85, 26.38, 27.82, 30.46, 28.49, 28.67, 27.58,\n",
       "          29.55, 29.93, 29.03, 30.83, 29.08, 30.54, 30.97, 32.54, 30.1 ,\n",
       "          29.72, 31.56, 29.66, 29.13, 29.94, 25.2 , 28.56, 27.86, 25.65,\n",
       "          29.17, 27.05, 27.23, 28.04, 28.94, 29.22, 28.55, 28.05, 28.15,\n",
       "          28.5 , 28.9 , 29.68, 27.87, 28.14, 27.49, 27.38, 28.06, 27.12,\n",
       "          24.5 , 26.57, 27.03, 27.59, 25.54, 22.99, 25.42, 24.72, 27.11,\n",
       "          28.73, 27.15, 29.23, 28.95, 29.76, 29.91, 27.27, 24.44, 25.9 ,\n",
       "          29.06, 28.89, 31.36, 31.24, 31.39, 30.1 , 30.12, 27.03, 27.1 ,\n",
       "          28.  , 27.99, 27.48, 29.84, 30.25, 27.39, 30.99, 31.73, 32.4 ,\n",
       "          28.76, 30.05, 30.05, 30.26, 30.21, 26.95, 26.68, 26.74, 25.06,\n",
       "          25.17, 24.04, 25.8 , 25.6 , 25.65, 25.42, 26.33, 24.14, 24.06,\n",
       "          26.78, 26.11, 25.61, 26.57, 26.97, 26.21, 26.29, 26.2 , 13.97,\n",
       "          19.62, 26.43, 24.68, 24.5 , 24.28, 20.64, 25.43, 24.76, 24.89,\n",
       "          22.06, 23.07, 24.79, 24.81, 24.03, 26.5 , 26.34, 24.42, 23.49,\n",
       "          21.8 , 23.48, 24.23, 23.69, 23.57, 22.99, 24.14, 24.19, 24.33,\n",
       "          25.6 , 26.23, 26.55, 27.41, 26.73, 27.4 , 27.12, 27.44, 26.51,\n",
       "          26.96, 26.51, 26.33, 25.51, 25.02, 25.63, 25.47, 26.65, 27.45,\n",
       "          26.47, 26.65, 26.77, 27.02, 26.89, 27.46, 26.35, 26.75, 26.36,\n",
       "          26.42, 26.7 , 27.04, 25.04, 23.65, 24.86, 25.4 , 26.11, 24.57,\n",
       "          25.86, 25.87, 25.57, 21.97, 26.01, 26.3 , 26.17, 26.85, 25.74,\n",
       "          25.99, 23.77, 25.79, 27.04, 27.77, 27.7 , 28.65, 29.16, 29.65,\n",
       "          30.36, 29.78, 30.32, 30.15, 31.4 , 32.27, 31.37, 31.55, 31.47,\n",
       "          31.56, 29.66, 30.83, 29.27, 29.87, 27.35, 27.67, 28.47, 26.5 ,\n",
       "          28.19, 30.22, 31.05, 31.52, 31.57, 31.55, 31.55, 30.51, 30.65,\n",
       "          30.42, 29.68, 29.51, 29.8 , 29.38, 29.81, 27.96, 27.85, 24.06,\n",
       "          20.41, 24.3 , 17.11, 26.12, 26.91, 26.6 , 28.79, 30.26, 28.56,\n",
       "          27.11, 27.63, 26.63, 26.03, 21.72, 26.06, 25.26, 28.37, 27.81,\n",
       "          27.91, 26.  , 28.97, 29.57, 27.19, 27.1 , 27.64, 20.1 , 20.06,\n",
       "          20.03, 28.5 , 28.95, 23.95, 28.85, 27.1 , 24.96, 28.03, 28.25,\n",
       "          28.3 , 28.57, 26.03, 26.01, 28.29, 29.71, 30.37, 29.74, 29.66,\n",
       "          27.57, 27.58, 27.88, 28.67, 31.55, 29.76, 27.01, 26.05, 29.13,\n",
       "          28.24, 28.01, 28.62, 30.82, 32.18, 30.92, 30.97, 27.9 , 28.24,\n",
       "          29.83, 27.07, 27.94, 24.64, 26.06, 26.79, 27.52, 28.21, 27.96,\n",
       "          26.49, 28.26, 29.03, 29.02, 28.59, 30.43, 29.06, 28.  , 27.3 ,\n",
       "          26.94, 20.08, 25.65, 25.22, 24.51, 28.22, 26.82, 26.68, 25.59,\n",
       "          25.43, 25.02, 27.55, 27.31, 28.07, 29.66, 29.03, 27.75, 28.11,\n",
       "          28.6 , 30.37, 30.05, 29.53, 28.79, 28.82, 25.69, 28.05, 29.31,\n",
       "          30.45, 32.79, 31.64, 30.52, 33.06, 28.08, 25.5 , 28.98, 30.08,\n",
       "          28.01, 27.57, 28.33, 30.14, 27.7 , 29.53, 31.9 , 32.12, 32.78,\n",
       "          34.9 , 33.7 , 32.74, 30.91, 31.17, 29.96, 28.87, 31.02, 31.14,\n",
       "          29.74, 32.22, 32.97, 34.89, 33.77, 35.77, 37.41, 37.34, 38.21,\n",
       "          36.05, 36.93, 36.48, 38.98, 37.72, 37.73, 39.09, 39.81, 38.41,\n",
       "          38.17, 40.4 , 38.6 , 37.29, 35.57, 37.9 , 36.03, 35.68, 36.62,\n",
       "          37.64, 37.75, 37.68, 37.43, 36.79, 37.14, 37.48, 39.6 , 37.88,\n",
       "          41.43, 39.59, 38.68, 39.54, 42.1 , 39.38, 39.26, 40.87, 39.52,\n",
       "          39.56, 39.22, 39.22, 39.13, 38.39, 38.09, 37.42, 38.62, 38.68,\n",
       "          38.01, 37.64, 38.96, 38.5 , 37.96, 40.01, 39.18, 41.55, 39.83,\n",
       "          38.04, 34.93, 32.64, 35.34, 33.97, 34.21, 34.77, 34.35, 35.93,\n",
       "          36.02, 35.91, 33.77, 32.13, 31.09, 36.03, 35.26, 34.15, 28.06,\n",
       "          27.2 , 24.88, 12.42,  4.44,  9.48, 30.11, 19.5 , 20.07, 32.98,\n",
       "          32.97, 24.98, 29.06, 33.81, 31.77, 15.27, 28.05, 36.81, 36.07,\n",
       "          36.34, 39.04, 39.12, 37.69, 39.17, 38.85, 42.26, 41.36, 44.16,\n",
       "          42.2 , 40.66, 43.07, 44.91, 45.17, 46.5 , 47.28, 45.29, 46.73,\n",
       "          45.1 , 46.11, 46.91, 41.1 , 45.56, 44.46, 44.48, 41.49, 42.22,\n",
       "          41.97, 38.05, 37.47, 41.01, 43.27, 43.92, 44.45, 45.29, 42.82,\n",
       "          44.73, 45.48, 46.78, 48.94, 50.24, 50.84, 50.19, 47.56, 48.25,\n",
       "          49.76, 49.68, 50.8 , 51.54, 51.2 , 51.94, 52.4 , 53.18, 51.73,\n",
       "          51.33, 50.51, 51.9 , 52.56, 52.44, 53.29, 53.92, 54.39, 54.33,\n",
       "          51.99, 50.87, 50.53, 53.  , 52.67, 52.72, 53.6 , 53.33, 53.44,\n",
       "          49.6 , 51.51, 53.6 , 51.78, 50.74, 47.48, 49.08, 49.32, 49.43,\n",
       "          49.87, 50.53, 50.59, 48.72, 48.83, 43.64, 48.23, 48.17, 49.92,\n",
       "          48.26, 50.62, 49.63, 49.77, 50.14, 50.15, 55.8 , 52.37, 54.05,\n",
       "          56.58, 55.2 , 53.87, 53.8 , 56.61, 56.84, 54.61, 53.44, 53.65,\n",
       "          53.05, 54.19, 50.99, 49.92, 53.13, 50.19, 49.35, 48.58, 48.91,\n",
       "          44.41, 39.04, 43.01,  3.27, 21.04, 30.61, 39.25, 34.23, 20.99,\n",
       "          30.48, 41.79, 19.92, 42.05, 42.76, 42.51, 44.67, 43.34, 45.61,\n",
       "          45.34, 47.03, 44.08, 41.38, 41.88, 42.4 , 39.36, 19.55,  4.91,\n",
       "          40.59, 41.44, 41.01, 39.89, 41.37, 40.77, 35.08, 34.58, 30.02,\n",
       "          41.19, 44.3 , 42.5 , 44.23, 40.99, 39.09, 39.04, 40.23, 41.93,\n",
       "          42.54, 41.44, 42.1 , 42.99, 43.91, 43.49, 43.8 , 42.69, 39.58,\n",
       "          38.47, 41.95, 43.68, 45.17, 44.97, 43.78, 45.38, 44.16, 43.48,\n",
       "          44.65, 47.6 , 49.43, 47.59, 48.16, 47.88, 49.28, 49.5 , 42.4 ,\n",
       "          42.66, 43.21, 43.23, 42.89, 44.65, 45.06, 46.69, 46.56, 43.81,\n",
       "          44.25, 43.8 , 44.43, 48.58, 49.21, 49.65, 51.36, 46.47, 49.86,\n",
       "          52.49, 48.69, 50.12, 48.12, 49.01, 50.47, 52.32, 48.1 ],\n",
       "         dtype=float32),\n",
       "   'y_hat': array([26.301683 , 26.385994 , 29.719973 , 28.562244 , 24.831812 ,\n",
       "          25.609957 , 29.232218 , 31.214191 , 28.805815 , 30.46017  ,\n",
       "          32.329094 , 28.177994 , 29.08504  , 29.37627  , 28.709423 ,\n",
       "          28.182892 , 25.147552 , 28.106152 , 28.895327 , 28.837278 ,\n",
       "          30.69965  , 30.62263  , 30.005648 , 27.746191 , 27.334051 ,\n",
       "          26.289698 , 27.18077  , 28.588646 , 28.56786  , 28.571556 ,\n",
       "          27.49182  , 27.87133  , 27.56733  , 27.617666 , 27.934923 ,\n",
       "          29.383493 , 30.201838 , 28.995504 , 29.295563 , 29.433994 ,\n",
       "          29.73864  , 30.467342 , 31.073305 , 32.053055 , 32.974697 ,\n",
       "          31.703629 , 31.32528  , 30.625069 , 30.814648 , 31.497751 ,\n",
       "          32.399593 , 30.989655 , 29.39724  , 29.095755 , 27.131489 ,\n",
       "          28.864384 , 27.999271 , 29.95137  , 26.435478 , 27.27806  ,\n",
       "          29.483934 , 27.690773 , 28.622637 , 27.740705 , 29.827885 ,\n",
       "          29.88248  , 28.579159 , 29.749945 , 28.115583 , 30.395227 ,\n",
       "          31.110556 , 32.801716 , 30.533058 , 28.825726 , 30.495033 ,\n",
       "          28.735376 , 29.210878 , 30.19758  , 25.56111  , 28.493664 ,\n",
       "          27.340828 , 24.933159 , 28.350052 , 27.145864 , 27.521551 ,\n",
       "          28.505829 , 28.868546 , 28.749475 , 27.719961 , 27.266512 ,\n",
       "          27.980768 , 28.735254 , 29.419634 , 29.929089 , 27.345434 ,\n",
       "          27.183502 , 26.71283  , 27.50426  , 28.41916  , 27.844297 ,\n",
       "          24.424267 , 25.989256 , 26.223175 , 26.93488  , 25.611753 ,\n",
       "          23.269585 , 25.79559  , 24.88989  , 26.90884  , 27.912222 ,\n",
       "          26.303944 , 29.20809  , 29.207264 , 29.985086 , 30.238104 ,\n",
       "          26.878416 , 23.208168 , 24.946377 , 29.317125 , 29.384333 ,\n",
       "          31.895458 , 31.444206 , 30.964783 , 29.04924  , 29.379765 ,\n",
       "          26.863071 , 27.605824 , 28.313808 , 28.265623 , 26.938747 ,\n",
       "          28.843779 , 29.373808 , 27.495306 , 31.124432 , 32.027832 ,\n",
       "          32.896824 , 28.329786 , 28.980312 , 29.03841  , 30.416489 ,\n",
       "          30.52733  , 27.590614 , 26.70111  , 26.164925 , 24.22654  ,\n",
       "          24.683098 , 24.36169  , 26.132477 , 26.67356  , 25.98287  ,\n",
       "          25.14618  , 25.262638 , 23.482288 , 24.295095 , 27.090199 ,\n",
       "          27.065186 , 26.072939 , 26.156172 , 26.318665 , 25.594088 ,\n",
       "          26.462652 , 26.65578  , 14.43177  , 20.063019 , 26.393261 ,\n",
       "          25.01522  , 24.40084  , 24.874165 , 21.009958 , 26.030495 ,\n",
       "          25.336197 , 25.156244 , 21.368816 , 22.412392 , 24.903347 ,\n",
       "          25.525667 , 24.662008 , 26.73988  , 25.936253 , 23.46792  ,\n",
       "          22.682297 , 22.073843 , 23.85582  , 25.088251 , 24.113113 ,\n",
       "          23.2473   , 22.09893  , 23.255821 , 24.428057 , 24.74726  ,\n",
       "          26.167278 , 26.463844 , 26.143703 , 26.448254 , 25.729246 ,\n",
       "          27.30203  , 27.381132 , 27.740541 , 26.533176 , 26.374382 ,\n",
       "          25.450058 , 25.471699 , 25.536266 , 25.2356   , 25.994406 ,\n",
       "          25.641426 , 26.23631  , 26.5592   , 25.558712 , 26.641691 ,\n",
       "          26.971245 , 27.383245 , 26.865541 , 27.016823 , 25.45581  ,\n",
       "          25.89759  , 26.50282  , 27.069826 , 27.105797 , 27.314259 ,\n",
       "          24.688429 , 22.586237 , 23.912533 , 25.741537 , 26.648317 ,\n",
       "          25.414057 , 25.99693  , 25.55484  , 24.670004 , 21.13306  ,\n",
       "          26.181293 , 26.798141 , 26.894331 , 26.958977 , 25.409683 ,\n",
       "          24.887018 , 22.891273 , 25.907269 , 27.402761 , 28.42978  ,\n",
       "          27.90912  , 28.3029   , 28.13991  , 28.57349  , 30.310823 ,\n",
       "          29.94823  , 30.781677 , 29.881418 , 30.86108  , 31.2426   ,\n",
       "          30.571558 , 31.465317 , 31.49953  , 31.776735 , 29.611559 ,\n",
       "          30.166388 , 28.27309  , 28.98545  , 27.40707  , 27.890717 ,\n",
       "          28.769125 , 26.808105 , 27.832306 , 29.329096 , 30.234116 ,\n",
       "          31.501884 , 31.715801 , 31.798155 , 31.375355 , 29.869392 ,\n",
       "          29.660313 , 29.482527 , 29.580666 , 29.722141 , 30.055449 ,\n",
       "          29.300156 , 29.197662 , 26.811733 , 26.800234 , 23.974346 ,\n",
       "          20.791683 , 24.71351  , 17.59049  , 26.294357 , 27.02798  ,\n",
       "          26.292387 , 28.981304 , 30.539316 , 29.070307 , 26.965282 ,\n",
       "          27.167248 , 25.460083 , 25.493454 , 21.864168 , 26.513618 ,\n",
       "          25.741556 , 28.656208 , 27.431261 , 27.217388 , 25.021454 ,\n",
       "          28.825619 , 29.693722 , 27.783993 , 27.01975  , 26.98048  ,\n",
       "          19.311022 , 19.43975  , 20.17363  , 29.056622 , 29.926529 ,\n",
       "          24.620068 , 28.3019   , 26.077183 , 24.095924 , 28.016808 ,\n",
       "          28.574186 , 28.655743 , 28.575792 , 25.485815 , 24.71288  ,\n",
       "          27.279812 , 29.72959  , 30.636625 , 30.053986 , 29.525229 ,\n",
       "          26.708775 , 26.451082 , 26.84833  , 28.675873 , 31.415545 ,\n",
       "          30.233858 , 27.559717 , 24.914503 , 27.689648 , 27.183338 ,\n",
       "          28.471184 , 28.556562 , 30.825163 , 31.296    , 30.182419 ,\n",
       "          29.915638 , 25.899607 , 27.632896 , 29.61072  , 28.265186 ,\n",
       "          27.90579  , 24.04475  , 24.919489 , 25.958052 , 27.708214 ,\n",
       "          28.22754  , 29.038143 , 26.29481  , 27.360825 , 27.827492 ,\n",
       "          28.743511 , 28.054409 , 30.437695 , 29.37733  , 28.736746 ,\n",
       "          26.195606 , 25.623108 , 19.398846 , 26.226665 , 25.70171  ,\n",
       "          25.042982 , 28.286417 , 26.547482 , 25.800913 , 24.189793 ,\n",
       "          25.272161 , 25.158823 , 27.679653 , 27.524288 , 27.313875 ,\n",
       "          28.55144  , 27.763447 , 27.612911 , 28.070078 , 28.99372  ,\n",
       "          30.112095 , 29.696619 , 28.875378 , 27.678574 , 28.517532 ,\n",
       "          25.893595 , 28.530079 , 29.260492 , 29.900593 , 31.911623 ,\n",
       "          31.016052 , 30.07782  , 33.158264 , 28.906698 , 26.459808 ,\n",
       "          28.007494 , 29.168802 , 27.668262 , 28.18971  , 27.944208 ,\n",
       "          30.42948  , 27.806917 , 28.930664 , 30.744259 , 31.157703 ,\n",
       "          32.60016  , 34.7588   , 34.35242  , 32.69654  , 30.01577  ,\n",
       "          30.297987 , 29.033613 , 28.993385 , 31.61465  , 31.476116 ,\n",
       "          29.680403 , 31.728378 , 31.704103 , 33.779705 , 33.911785 ,\n",
       "          35.378494 , 37.670605 , 37.3441   , 37.483173 , 35.305557 ,\n",
       "          36.20087  , 36.210644 , 39.343304 , 38.335365 , 38.231934 ,\n",
       "          38.62953  , 40.138138 , 37.809296 , 38.20626  , 42.090385 ,\n",
       "          39.504955 , 37.51156  , 34.81806  , 36.764385 , 35.264023 ,\n",
       "          35.97805  , 36.393524 , 37.929634 , 37.48696  , 37.405346 ,\n",
       "          36.09732  , 35.76649  , 36.832882 , 37.49315  , 40.08994  ,\n",
       "          37.509014 , 40.625248 , 38.376503 , 38.21383  , 38.797527 ,\n",
       "          42.0986   , 39.84637  , 39.455933 , 39.481556 , 38.33135  ,\n",
       "          38.69095  , 38.932983 , 38.777412 , 38.83876  , 38.139053 ,\n",
       "          37.250183 , 36.04578  , 37.342575 , 38.575996 , 38.134995 ,\n",
       "          37.820915 , 38.43679  , 37.50335  , 36.799576 , 38.94233  ,\n",
       "          38.67342  , 41.435436 , 40.335148 , 37.582527 , 34.08402  ,\n",
       "          31.642807 , 34.723244 , 33.895508 , 34.709328 , 35.21404  ,\n",
       "          34.385345 , 35.537315 , 34.97259  , 35.10784  , 33.455025 ,\n",
       "          32.244797 , 31.421932 , 35.881176 , 35.24613  , 33.153454 ,\n",
       "          27.177471 , 27.125383 , 24.801754 , 13.403278 ,  5.403449 ,\n",
       "          10.271137 , 30.315653 , 20.792187 , 21.792835 , 33.204628 ,\n",
       "          33.730743 , 25.26076  , 28.361073 , 33.099102 , 31.318775 ,\n",
       "          15.65436  , 28.124752 , 37.65521  , 36.92495  , 35.977478 ,\n",
       "          38.35012  , 37.66408  , 37.02532  , 39.043877 , 39.090843 ,\n",
       "          42.00324  , 40.656452 , 42.96236  , 41.079407 , 40.130787 ,\n",
       "          42.785065 , 44.78034  , 44.90923  , 45.555824 , 45.842354 ,\n",
       "          43.94396  , 45.978127 , 44.796345 , 45.766724 , 46.39083  ,\n",
       "          40.22878  , 43.938698 , 43.094746 , 43.977375 , 41.271507 ,\n",
       "          42.074215 , 41.41849  , 37.181194 , 36.219307 , 39.725616 ,\n",
       "          42.872036 , 43.840057 , 44.320496 , 44.737385 , 41.67417  ,\n",
       "          43.207058 , 44.103256 , 46.235798 , 48.4484   , 49.996555 ,\n",
       "          50.10521  , 48.861275 , 45.846973 , 46.555676 , 48.868977 ,\n",
       "          49.138554 , 50.281033 , 50.78413  , 49.785454 , 50.14541  ,\n",
       "          50.69664  , 52.15864  , 50.98295  , 50.75011  , 49.639004 ,\n",
       "          50.58988  , 50.928753 , 50.93966  , 52.4733   , 53.21396  ,\n",
       "          53.774662 , 53.4328   , 50.57188  , 49.07322  , 48.917213 ,\n",
       "          52.108204 , 52.106125 , 52.334946 , 52.74018  , 52.00271  ,\n",
       "          51.636982 , 47.983776 , 50.587017 , 52.718933 , 51.21696  ,\n",
       "          49.888588 , 46.120922 , 47.484356 , 47.7942   , 48.880203 ,\n",
       "          49.302116 , 50.258904 , 50.018784 , 47.317898 , 47.11763  ,\n",
       "          42.159977 , 47.452797 , 47.484108 , 49.713394 , 47.62786  ,\n",
       "          49.355064 , 47.91384  , 48.01965  , 49.183067 , 49.58959  ,\n",
       "          55.167133 , 52.036987 , 52.62575  , 54.64747  , 53.733673 ,\n",
       "          52.9229   , 53.04863  , 56.027523 , 55.918465 , 53.09096  ,\n",
       "          51.724632 , 52.0307   , 51.984688 , 53.48976  , 50.493385 ,\n",
       "          48.98613  , 51.631573 , 48.58235  , 47.8124   , 47.570644 ,\n",
       "          48.191315 , 44.047165 , 38.456337 , 41.86176  ,  2.1390512,\n",
       "          20.437647 , 30.650309 , 40.841824 , 35.633236 , 22.331326 ,\n",
       "          29.70911  , 41.11425  , 19.705511 , 42.22832  , 42.6336   ,\n",
       "          43.406055 , 44.190136 , 42.488987 , 44.185287 , 43.77185  ,\n",
       "          46.508385 , 43.678104 , 40.931362 , 41.13852  , 41.386288 ,\n",
       "          38.031265 , 18.737638 ,  4.726343 , 40.116776 , 43.45519  ,\n",
       "          41.884987 , 39.625263 , 39.928123 , 38.756313 , 34.28938  ,\n",
       "          34.14754  , 30.015583 , 41.439922 , 44.516182 , 41.963642 ,\n",
       "          43.332035 , 40.085377 , 38.527695 , 38.907673 , 39.619987 ,\n",
       "          40.813942 , 41.29755  , 40.056656 , 41.36163  , 42.43798  ,\n",
       "          43.50664  , 42.946648 , 42.62326  , 41.051243 , 38.055992 ,\n",
       "          37.595978 , 41.416504 , 43.634403 , 44.645374 , 43.879604 ,\n",
       "          42.051582 , 43.754166 , 43.259754 , 42.90935  , 44.22158  ,\n",
       "          46.857925 , 47.848866 , 46.375042 , 46.509132 , 46.442554 ,\n",
       "          48.085342 , 48.76744  , 42.52792  , 40.49239  , 41.38394  ,\n",
       "          41.657063 , 42.664753 , 44.392166 , 44.618347 , 45.844624 ,\n",
       "          45.329895 , 42.199257 , 42.71185  , 42.85293  , 43.94978  ,\n",
       "          48.232002 , 48.335377 , 48.465393 , 49.397606 , 44.71593  ,\n",
       "          48.545677 , 51.13996  , 48.664326 , 49.233215 , 46.254604 ,\n",
       "          47.25535  , 48.790386 , 51.34392  ], dtype=float32),\n",
       "   'run_time': 2.484513998031616,\n",
       "   'status': 'ok'},\n",
       "  'misc': {'tid': 0,\n",
       "   'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'workdir': None,\n",
       "   'idxs': {'activation': [0],\n",
       "    'batch_normalization': [0],\n",
       "    'batch_size': [0],\n",
       "    'complete_inputs': [0],\n",
       "    'complete_sample': [0],\n",
       "    'device': [0],\n",
       "    'dropout_prob_exogenous': [0],\n",
       "    'dropout_prob_theta': [0],\n",
       "    'early_stop_patience': [0],\n",
       "    'eval_freq': [0],\n",
       "    'frequency': [0],\n",
       "    'idx_to_sample_freq': [0],\n",
       "    'initialization': [0],\n",
       "    'l1_theta': [0],\n",
       "    'learning_rate': [0],\n",
       "    'len_sample_chunks': [0],\n",
       "    'loss': [0],\n",
       "    'loss_hypar': [0],\n",
       "    'loss_valid': [0],\n",
       "    'lr_decay': [0],\n",
       "    'lr_decay_step_size': [0],\n",
       "    'max_epochs': [0],\n",
       "    'max_steps': [0],\n",
       "    'n_blocks': [0],\n",
       "    'n_freq_downsample': [0],\n",
       "    'n_hidden': [0],\n",
       "    'n_layers': [0],\n",
       "    'n_pool_kernel_size': [0],\n",
       "    'n_s_hidden': [0],\n",
       "    'n_series_per_batch': [0],\n",
       "    'n_time_in': [0],\n",
       "    'n_time_out': [0],\n",
       "    'n_val_weeks': [0],\n",
       "    'n_x_hidden': [0],\n",
       "    'normalizer_x': [0],\n",
       "    'normalizer_y': [0],\n",
       "    'random_seed': [0],\n",
       "    'seasonality': [0],\n",
       "    'shared_weights': [0],\n",
       "    'stack_types': [0],\n",
       "    'val_idx_to_sample_freq': [0],\n",
       "    'weight_decay': [0],\n",
       "    'window_sampling_limit': [0]},\n",
       "   'vals': {'activation': [0],\n",
       "    'batch_normalization': [0],\n",
       "    'batch_size': [0],\n",
       "    'complete_inputs': [0],\n",
       "    'complete_sample': [0],\n",
       "    'device': [0],\n",
       "    'dropout_prob_exogenous': [0.2069281539939935],\n",
       "    'dropout_prob_theta': [0.09522759273514453],\n",
       "    'early_stop_patience': [0],\n",
       "    'eval_freq': [0],\n",
       "    'frequency': [0],\n",
       "    'idx_to_sample_freq': [0],\n",
       "    'initialization': [1],\n",
       "    'l1_theta': [0],\n",
       "    'learning_rate': [0.0007396856533221593],\n",
       "    'len_sample_chunks': [0],\n",
       "    'loss': [0],\n",
       "    'loss_hypar': [0],\n",
       "    'loss_valid': [0],\n",
       "    'lr_decay': [0.488883166981553],\n",
       "    'lr_decay_step_size': [0],\n",
       "    'max_epochs': [0],\n",
       "    'max_steps': [0],\n",
       "    'n_blocks': [0],\n",
       "    'n_freq_downsample': [0],\n",
       "    'n_hidden': [0],\n",
       "    'n_layers': [0],\n",
       "    'n_pool_kernel_size': [0],\n",
       "    'n_s_hidden': [0],\n",
       "    'n_series_per_batch': [0],\n",
       "    'n_time_in': [0],\n",
       "    'n_time_out': [0],\n",
       "    'n_val_weeks': [0],\n",
       "    'n_x_hidden': [4.0],\n",
       "    'normalizer_x': [0],\n",
       "    'normalizer_y': [0],\n",
       "    'random_seed': [13.0],\n",
       "    'seasonality': [0],\n",
       "    'shared_weights': [0],\n",
       "    'stack_types': [0],\n",
       "    'val_idx_to_sample_freq': [0],\n",
       "    'weight_decay': [0.003853558423761368],\n",
       "    'window_sampling_limit': [0]}},\n",
       "  'exp_key': None,\n",
       "  'owner': None,\n",
       "  'version': 0,\n",
       "  'book_time': datetime.datetime(2021, 11, 9, 21, 50, 14, 608000),\n",
       "  'refresh_time': datetime.datetime(2021, 11, 9, 21, 50, 17, 106000)},\n",
       " {'state': 2,\n",
       "  'tid': 1,\n",
       "  'spec': None,\n",
       "  'result': {'loss': 1.9818161725997925,\n",
       "   'mc': {'activation': 'SELU',\n",
       "    'batch_normalization': False,\n",
       "    'batch_size': 256,\n",
       "    'complete_inputs': False,\n",
       "    'complete_sample': False,\n",
       "    'device': 'cpu',\n",
       "    'dropout_prob_exogenous': 0.45954562927266907,\n",
       "    'dropout_prob_theta': 0.3377557670885139,\n",
       "    'early_stop_patience': 16,\n",
       "    'eval_freq': 50,\n",
       "    'frequency': 'H',\n",
       "    'idx_to_sample_freq': 24,\n",
       "    'initialization': 'glorot_normal',\n",
       "    'l1_theta': 0,\n",
       "    'learning_rate': 0.0005455698037098191,\n",
       "    'len_sample_chunks': None,\n",
       "    'loss_hypar': 0.5,\n",
       "    'loss_train': 'MAE',\n",
       "    'loss_valid': 'MAE',\n",
       "    'lr_decay': 0.4122236461769361,\n",
       "    'lr_decay_step_size': 100,\n",
       "    'max_epochs': 10,\n",
       "    'max_steps': None,\n",
       "    'mode': 'simple',\n",
       "    'model': 'deepmidas',\n",
       "    'n_blocks': (1, 1),\n",
       "    'n_freq_downsample': (24, 1),\n",
       "    'n_hidden': 256,\n",
       "    'n_layers': (2, 2),\n",
       "    'n_pool_kernel_size': (4, 1),\n",
       "    'n_s_hidden': 0,\n",
       "    'n_series_per_batch': 1,\n",
       "    'n_time_in': 168,\n",
       "    'n_time_out': 24,\n",
       "    'n_val_weeks': 104,\n",
       "    'n_x_hidden': 10.0,\n",
       "    'normalizer_x': 'median',\n",
       "    'normalizer_y': None,\n",
       "    'random_seed': 14.0,\n",
       "    'seasonality': 24,\n",
       "    'shared_weights': False,\n",
       "    'stack_types': ('identity', 'identity'),\n",
       "    'val_idx_to_sample_freq': 24,\n",
       "    'weight_decay': 0.0003991392255810992,\n",
       "    'window_sampling_limit': 100000,\n",
       "    'n_x': 1,\n",
       "    'n_s': 1,\n",
       "    'n_theta_hidden': [[256, 256], [256, 256]]},\n",
       "   'y_true': array([25.73, 29.37, 28.76, 25.95, 26.71, 29.36, 30.93, 28.3 , 30.58,\n",
       "          33.32, 28.86, 30.5 , 29.72, 28.69, 28.  , 25.04, 28.72, 29.79,\n",
       "          29.41, 30.89, 30.99, 29.56, 27.84, 28.19, 27.36, 27.84, 28.13,\n",
       "          28.42, 28.4 , 27.41, 28.66, 28.67, 28.51, 27.96, 29.31, 29.99,\n",
       "          28.97, 29.96, 30.59, 30.8 , 30.62, 31.01, 32.  , 33.14, 32.63,\n",
       "          32.11, 31.77, 31.04, 31.35, 31.96, 31.01, 29.98, 30.36, 28.15,\n",
       "          28.87, 27.66, 29.85, 26.38, 27.82, 30.46, 28.49, 28.67, 27.58,\n",
       "          29.55, 29.93, 29.03, 30.83, 29.08, 30.54, 30.97, 32.54, 30.1 ,\n",
       "          29.72, 31.56, 29.66, 29.13, 29.94, 25.2 , 28.56, 27.86, 25.65,\n",
       "          29.17, 27.05, 27.23, 28.04, 28.94, 29.22, 28.55, 28.05, 28.15,\n",
       "          28.5 , 28.9 , 29.68, 27.87, 28.14, 27.49, 27.38, 28.06, 27.12,\n",
       "          24.5 , 26.57, 27.03, 27.59, 25.54, 22.99, 25.42, 24.72, 27.11,\n",
       "          28.73, 27.15, 29.23, 28.95, 29.76, 29.91, 27.27, 24.44, 25.9 ,\n",
       "          29.06, 28.89, 31.36, 31.24, 31.39, 30.1 , 30.12, 27.03, 27.1 ,\n",
       "          28.  , 27.99, 27.48, 29.84, 30.25, 27.39, 30.99, 31.73, 32.4 ,\n",
       "          28.76, 30.05, 30.05, 30.26, 30.21, 26.95, 26.68, 26.74, 25.06,\n",
       "          25.17, 24.04, 25.8 , 25.6 , 25.65, 25.42, 26.33, 24.14, 24.06,\n",
       "          26.78, 26.11, 25.61, 26.57, 26.97, 26.21, 26.29, 26.2 , 13.97,\n",
       "          19.62, 26.43, 24.68, 24.5 , 24.28, 20.64, 25.43, 24.76, 24.89,\n",
       "          22.06, 23.07, 24.79, 24.81, 24.03, 26.5 , 26.34, 24.42, 23.49,\n",
       "          21.8 , 23.48, 24.23, 23.69, 23.57, 22.99, 24.14, 24.19, 24.33,\n",
       "          25.6 , 26.23, 26.55, 27.41, 26.73, 27.4 , 27.12, 27.44, 26.51,\n",
       "          26.96, 26.51, 26.33, 25.51, 25.02, 25.63, 25.47, 26.65, 27.45,\n",
       "          26.47, 26.65, 26.77, 27.02, 26.89, 27.46, 26.35, 26.75, 26.36,\n",
       "          26.42, 26.7 , 27.04, 25.04, 23.65, 24.86, 25.4 , 26.11, 24.57,\n",
       "          25.86, 25.87, 25.57, 21.97, 26.01, 26.3 , 26.17, 26.85, 25.74,\n",
       "          25.99, 23.77, 25.79, 27.04, 27.77, 27.7 , 28.65, 29.16, 29.65,\n",
       "          30.36, 29.78, 30.32, 30.15, 31.4 , 32.27, 31.37, 31.55, 31.47,\n",
       "          31.56, 29.66, 30.83, 29.27, 29.87, 27.35, 27.67, 28.47, 26.5 ,\n",
       "          28.19, 30.22, 31.05, 31.52, 31.57, 31.55, 31.55, 30.51, 30.65,\n",
       "          30.42, 29.68, 29.51, 29.8 , 29.38, 29.81, 27.96, 27.85, 24.06,\n",
       "          20.41, 24.3 , 17.11, 26.12, 26.91, 26.6 , 28.79, 30.26, 28.56,\n",
       "          27.11, 27.63, 26.63, 26.03, 21.72, 26.06, 25.26, 28.37, 27.81,\n",
       "          27.91, 26.  , 28.97, 29.57, 27.19, 27.1 , 27.64, 20.1 , 20.06,\n",
       "          20.03, 28.5 , 28.95, 23.95, 28.85, 27.1 , 24.96, 28.03, 28.25,\n",
       "          28.3 , 28.57, 26.03, 26.01, 28.29, 29.71, 30.37, 29.74, 29.66,\n",
       "          27.57, 27.58, 27.88, 28.67, 31.55, 29.76, 27.01, 26.05, 29.13,\n",
       "          28.24, 28.01, 28.62, 30.82, 32.18, 30.92, 30.97, 27.9 , 28.24,\n",
       "          29.83, 27.07, 27.94, 24.64, 26.06, 26.79, 27.52, 28.21, 27.96,\n",
       "          26.49, 28.26, 29.03, 29.02, 28.59, 30.43, 29.06, 28.  , 27.3 ,\n",
       "          26.94, 20.08, 25.65, 25.22, 24.51, 28.22, 26.82, 26.68, 25.59,\n",
       "          25.43, 25.02, 27.55, 27.31, 28.07, 29.66, 29.03, 27.75, 28.11,\n",
       "          28.6 , 30.37, 30.05, 29.53, 28.79, 28.82, 25.69, 28.05, 29.31,\n",
       "          30.45, 32.79, 31.64, 30.52, 33.06, 28.08, 25.5 , 28.98, 30.08,\n",
       "          28.01, 27.57, 28.33, 30.14, 27.7 , 29.53, 31.9 , 32.12, 32.78,\n",
       "          34.9 , 33.7 , 32.74, 30.91, 31.17, 29.96, 28.87, 31.02, 31.14,\n",
       "          29.74, 32.22, 32.97, 34.89, 33.77, 35.77, 37.41, 37.34, 38.21,\n",
       "          36.05, 36.93, 36.48, 38.98, 37.72, 37.73, 39.09, 39.81, 38.41,\n",
       "          38.17, 40.4 , 38.6 , 37.29, 35.57, 37.9 , 36.03, 35.68, 36.62,\n",
       "          37.64, 37.75, 37.68, 37.43, 36.79, 37.14, 37.48, 39.6 , 37.88,\n",
       "          41.43, 39.59, 38.68, 39.54, 42.1 , 39.38, 39.26, 40.87, 39.52,\n",
       "          39.56, 39.22, 39.22, 39.13, 38.39, 38.09, 37.42, 38.62, 38.68,\n",
       "          38.01, 37.64, 38.96, 38.5 , 37.96, 40.01, 39.18, 41.55, 39.83,\n",
       "          38.04, 34.93, 32.64, 35.34, 33.97, 34.21, 34.77, 34.35, 35.93,\n",
       "          36.02, 35.91, 33.77, 32.13, 31.09, 36.03, 35.26, 34.15, 28.06,\n",
       "          27.2 , 24.88, 12.42,  4.44,  9.48, 30.11, 19.5 , 20.07, 32.98,\n",
       "          32.97, 24.98, 29.06, 33.81, 31.77, 15.27, 28.05, 36.81, 36.07,\n",
       "          36.34, 39.04, 39.12, 37.69, 39.17, 38.85, 42.26, 41.36, 44.16,\n",
       "          42.2 , 40.66, 43.07, 44.91, 45.17, 46.5 , 47.28, 45.29, 46.73,\n",
       "          45.1 , 46.11, 46.91, 41.1 , 45.56, 44.46, 44.48, 41.49, 42.22,\n",
       "          41.97, 38.05, 37.47, 41.01, 43.27, 43.92, 44.45, 45.29, 42.82,\n",
       "          44.73, 45.48, 46.78, 48.94, 50.24, 50.84, 50.19, 47.56, 48.25,\n",
       "          49.76, 49.68, 50.8 , 51.54, 51.2 , 51.94, 52.4 , 53.18, 51.73,\n",
       "          51.33, 50.51, 51.9 , 52.56, 52.44, 53.29, 53.92, 54.39, 54.33,\n",
       "          51.99, 50.87, 50.53, 53.  , 52.67, 52.72, 53.6 , 53.33, 53.44,\n",
       "          49.6 , 51.51, 53.6 , 51.78, 50.74, 47.48, 49.08, 49.32, 49.43,\n",
       "          49.87, 50.53, 50.59, 48.72, 48.83, 43.64, 48.23, 48.17, 49.92,\n",
       "          48.26, 50.62, 49.63, 49.77, 50.14, 50.15, 55.8 , 52.37, 54.05,\n",
       "          56.58, 55.2 , 53.87, 53.8 , 56.61, 56.84, 54.61, 53.44, 53.65,\n",
       "          53.05, 54.19, 50.99, 49.92, 53.13, 50.19, 49.35, 48.58, 48.91,\n",
       "          44.41, 39.04, 43.01,  3.27, 21.04, 30.61, 39.25, 34.23, 20.99,\n",
       "          30.48, 41.79, 19.92, 42.05, 42.76, 42.51, 44.67, 43.34, 45.61,\n",
       "          45.34, 47.03, 44.08, 41.38, 41.88, 42.4 , 39.36, 19.55,  4.91,\n",
       "          40.59, 41.44, 41.01, 39.89, 41.37, 40.77, 35.08, 34.58, 30.02,\n",
       "          41.19, 44.3 , 42.5 , 44.23, 40.99, 39.09, 39.04, 40.23, 41.93,\n",
       "          42.54, 41.44, 42.1 , 42.99, 43.91, 43.49, 43.8 , 42.69, 39.58,\n",
       "          38.47, 41.95, 43.68, 45.17, 44.97, 43.78, 45.38, 44.16, 43.48,\n",
       "          44.65, 47.6 , 49.43, 47.59, 48.16, 47.88, 49.28, 49.5 , 42.4 ,\n",
       "          42.66, 43.21, 43.23, 42.89, 44.65, 45.06, 46.69, 46.56, 43.81,\n",
       "          44.25, 43.8 , 44.43, 48.58, 49.21, 49.65, 51.36, 46.47, 49.86,\n",
       "          52.49, 48.69, 50.12, 48.12, 49.01, 50.47, 52.32, 48.1 ],\n",
       "         dtype=float32),\n",
       "   'y_hat': array([25.615463  , 25.772533  , 29.425692  , 28.47438   , 24.887777  ,\n",
       "          25.857983  , 29.377028  , 31.269928  , 28.655613  , 30.685085  ,\n",
       "          32.748196  , 27.944223  , 29.817093  , 29.841553  , 28.677052  ,\n",
       "          27.774767  , 24.785257  , 27.50675   , 28.356148  , 28.381443  ,\n",
       "          31.080103  , 31.827171  , 30.33777   , 27.662582  , 27.685364  ,\n",
       "          26.176434  , 26.502644  , 28.2672    , 28.665684  , 28.486855  ,\n",
       "          27.343262  , 27.970427  , 27.332565  , 27.429678  , 27.995882  ,\n",
       "          29.55727   , 30.03673   , 28.897133  , 29.226908  , 29.360998  ,\n",
       "          29.837463  , 30.651043  , 31.315989  , 32.098816  , 33.12555   ,\n",
       "          32.14359   , 30.967531  , 30.671713  , 30.951237  , 31.162687  ,\n",
       "          31.681482  , 30.895658  , 28.982357  , 29.128283  , 26.928745  ,\n",
       "          28.439676  , 27.931223  , 29.72227   , 26.147512  , 27.068766  ,\n",
       "          28.99483   , 27.476185  , 28.712013  , 27.930828  , 29.517368  ,\n",
       "          29.77248   , 28.38935   , 29.56017   , 28.214035  , 30.47874   ,\n",
       "          31.307661  , 32.55378   , 30.225008  , 29.187347  , 30.272787  ,\n",
       "          28.491417  , 28.7513    , 30.044733  , 24.998642  , 28.20006   ,\n",
       "          26.93852   , 24.032986  , 28.105694  , 27.135696  , 27.324871  ,\n",
       "          28.177975  , 28.775362  , 28.423258  , 27.296001  , 27.001236  ,\n",
       "          28.098372  , 28.592718  , 28.837357  , 29.570297  , 27.1181    ,\n",
       "          26.717503  , 26.288454  , 27.218908  , 28.081684  , 27.125645  ,\n",
       "          24.391186  , 25.71652   , 25.462467  , 26.6663    , 25.54985   ,\n",
       "          23.242788  , 25.33383   , 24.40275   , 26.416061  , 27.606457  ,\n",
       "          26.39907   , 29.398615  , 29.564798  , 29.915598  , 29.783045  ,\n",
       "          26.81147   , 23.143835  , 24.4727    , 28.627523  , 29.055264  ,\n",
       "          31.474424  , 31.32962   , 30.908522  , 29.04781   , 29.112915  ,\n",
       "          26.756252  , 26.925423  , 27.773224  , 27.597942  , 26.653835  ,\n",
       "          28.542015  , 29.065113  , 27.367723  , 31.26335   , 31.775822  ,\n",
       "          32.21722   , 28.34542   , 28.798025  , 28.687044  , 29.909882  ,\n",
       "          30.23095   , 26.93154   , 26.364632  , 25.824368  , 23.302095  ,\n",
       "          23.852497  , 23.883335  , 26.028713  , 25.608757  , 25.882328  ,\n",
       "          24.842564  , 25.063372  , 23.386873  , 24.031643  , 27.044615  ,\n",
       "          26.272587  , 25.858303  , 26.08051   , 25.473581  , 25.286682  ,\n",
       "          26.493284  , 26.400705  , 13.830498  , 19.123419  , 25.02356   ,\n",
       "          23.14068   , 23.97559   , 24.911821  , 21.076954  , 25.243551  ,\n",
       "          24.915035  , 24.375195  , 21.019253  , 22.262316  , 24.552626  ,\n",
       "          25.280056  , 24.348034  , 26.578117  , 25.709576  , 23.46399   ,\n",
       "          22.46646   , 21.69936   , 23.679642  , 24.428997  , 23.769035  ,\n",
       "          23.109516  , 21.707659  , 23.146105  , 24.345762  , 24.678303  ,\n",
       "          25.795212  , 26.347757  , 26.111753  , 26.46074   , 25.983465  ,\n",
       "          27.613022  , 27.521753  , 27.581478  , 26.50455   , 26.221478  ,\n",
       "          25.138538  , 25.292187  , 25.510372  , 25.322248  , 25.564686  ,\n",
       "          25.359024  , 25.960268  , 26.262594  , 25.568539  , 26.806726  ,\n",
       "          27.199562  , 27.217247  , 26.874743  , 26.80956   , 25.017002  ,\n",
       "          25.703432  , 26.300657  , 26.701258  , 27.119226  , 27.23916   ,\n",
       "          24.222832  , 22.460968  , 23.684414  , 25.238756  , 26.524883  ,\n",
       "          24.998762  , 26.076963  , 25.37207   , 24.252743  , 21.192995  ,\n",
       "          25.994003  , 26.6079    , 26.605019  , 27.138607  , 25.235914  ,\n",
       "          24.661543  , 22.92413   , 25.688995  , 27.240158  , 27.939442  ,\n",
       "          27.826609  , 28.205109  , 28.14485   , 28.95946   , 30.554611  ,\n",
       "          30.033928  , 30.461329  , 30.16699   , 30.697884  , 31.011686  ,\n",
       "          30.412266  , 31.50735   , 31.654314  , 31.489403  , 29.402273  ,\n",
       "          29.989037  , 27.85457   , 28.650335  , 27.170414  , 27.670418  ,\n",
       "          28.106796  , 26.232996  , 27.360516  , 28.997831  , 30.056622  ,\n",
       "          31.732998  , 32.03479   , 31.684967  , 31.462744  , 29.763298  ,\n",
       "          29.198956  , 29.30827   , 29.468847  , 29.535042  , 29.793459  ,\n",
       "          29.105768  , 28.991388  , 26.65831   , 26.621048  , 23.705418  ,\n",
       "          20.286467  , 23.858345  , 16.344296  , 24.705551  , 25.46345   ,\n",
       "          25.881939  , 29.273941  , 31.02686   , 29.21137   , 27.32811   ,\n",
       "          26.783995  , 25.160707  , 24.701153  , 21.539726  , 26.088017  ,\n",
       "          25.139914  , 28.268097  , 27.169378  , 26.860628  , 25.211544  ,\n",
       "          29.225063  , 29.744753  , 27.386017  , 26.972416  , 26.70347   ,\n",
       "          18.431986  , 18.766573  , 19.805206  , 28.37337   , 29.079554  ,\n",
       "          24.39251   , 28.465244  , 25.914698  , 23.896648  , 28.22604   ,\n",
       "          28.51892   , 28.36377   , 28.885677  , 25.402687  , 24.696182  ,\n",
       "          27.233574  , 29.65705   , 30.768478  , 30.2053    , 29.666132  ,\n",
       "          26.954771  , 26.141579  , 26.658857  , 28.489326  , 31.95139   ,\n",
       "          30.525343  , 26.725529  , 25.29722   , 27.892841  , 26.609915  ,\n",
       "          27.834713  , 28.769135  , 31.292873  , 33.158813  , 30.937254  ,\n",
       "          29.07942   , 26.690275  , 28.144571  , 29.559092  , 27.770155  ,\n",
       "          27.599726  , 24.0817    , 24.492277  , 25.431469  , 27.467722  ,\n",
       "          28.380697  , 28.301489  , 26.99813   , 27.680225  , 27.776787  ,\n",
       "          27.784746  , 28.774372  , 31.036139  , 29.19462   , 27.55687   ,\n",
       "          26.62205   , 25.660587  , 18.515192  , 25.594614  , 25.274363  ,\n",
       "          24.61545   , 28.411415  , 26.498716  , 25.652449  , 24.970968  ,\n",
       "          25.614843  , 25.168047  , 27.929813  , 27.290016  , 27.641512  ,\n",
       "          28.782053  , 28.41271   , 27.988934  , 28.413395  , 28.63491   ,\n",
       "          30.326183  , 29.611034  , 28.487547  , 27.97772   , 28.777517  ,\n",
       "          25.427292  , 27.80026   , 28.975279  , 29.495256  , 31.693937  ,\n",
       "          30.919538  , 30.626495  , 33.651752  , 27.624907  , 24.732662  ,\n",
       "          28.384567  , 28.45166   , 26.452375  , 27.75144   , 28.715317  ,\n",
       "          30.259104  , 27.785492  , 28.960524  , 30.636547  , 31.431019  ,\n",
       "          33.068085  , 35.61859   , 34.098755  , 33.240738  , 30.332846  ,\n",
       "          29.286846  , 28.599352  , 28.660349  , 30.850357  , 31.433294  ,\n",
       "          29.550602  , 31.52291   , 31.934425  , 34.044262  , 34.214478  ,\n",
       "          36.69497   , 37.729748  , 37.59171   , 37.99832   , 34.70367   ,\n",
       "          35.43099   , 35.927223  , 38.858074  , 37.53868   , 37.75249   ,\n",
       "          37.99527   , 37.734474  , 37.247997  , 38.883427  , 39.402176  ,\n",
       "          38.689377  , 37.645153  , 35.12951   , 36.325977  , 34.51689   ,\n",
       "          35.475815  , 36.41149   , 37.420925  , 37.414024  , 37.246372  ,\n",
       "          36.17913   , 35.673195  , 36.623577  , 37.284637  , 39.29227   ,\n",
       "          38.06622   , 40.691     , 38.567734  , 37.22363   , 39.353104  ,\n",
       "          42.056175  , 38.95328   , 39.375206  , 40.197685  , 37.983948  ,\n",
       "          38.36313   , 39.033897  , 39.18284   , 38.879753  , 38.053223  ,\n",
       "          37.16256   , 36.011955  , 37.253178  , 38.19347   , 37.82601   ,\n",
       "          37.478264  , 38.60073   , 37.499508  , 36.396015  , 38.878376  ,\n",
       "          38.907715  , 41.405663  , 39.79448   , 37.898087  , 33.71777   ,\n",
       "          30.65203   , 33.622208  , 33.26093   , 33.821213  , 34.31817   ,\n",
       "          33.968056  , 34.927547  , 34.792847  , 34.77319   , 33.644646  ,\n",
       "          32.130726  , 30.478697  , 35.676453  , 34.35791   , 33.124268  ,\n",
       "          27.02919   , 26.546059  , 24.7311    , 11.524433  ,  3.4738617 ,\n",
       "           7.900752  , 28.029453  , 18.341265  , 21.058867  , 34.017967  ,\n",
       "          33.31958   , 25.649763  , 29.315617  , 31.975336  , 30.617785  ,\n",
       "          15.678229  , 28.266642  , 35.84759   , 36.324287  , 36.611404  ,\n",
       "          38.226143  , 37.98168   , 38.153076  , 39.331978  , 38.59403   ,\n",
       "          41.907524  , 40.564735  , 42.84377   , 41.155483  , 40.52741   ,\n",
       "          42.93306   , 44.669643  , 44.82577   , 45.740383  , 46.06469   ,\n",
       "          44.059788  , 46.470108  , 44.882664  , 45.62289   , 46.339214  ,\n",
       "          39.988327  , 44.060844  , 43.05189   , 43.74653   , 41.226955  ,\n",
       "          41.850307  , 41.258705  , 36.993507  , 35.82323   , 39.710934  ,\n",
       "          42.769043  , 44.110508  , 44.54949   , 45.219578  , 41.979866  ,\n",
       "          43.29342   , 44.29626   , 46.508533  , 48.916515  , 50.225536  ,\n",
       "          50.531765  , 49.547935  , 46.1631    , 46.985363  , 49.192337  ,\n",
       "          49.338318  , 50.515896  , 51.101036  , 50.30003   , 50.57305   ,\n",
       "          51.259037  , 52.78511   , 51.56931   , 50.93404   , 49.84329   ,\n",
       "          50.737694  , 50.975643  , 51.233368  , 52.92453   , 53.764935  ,\n",
       "          54.049423  , 53.885555  , 50.90894   , 49.22063   , 48.949577  ,\n",
       "          52.270008  , 52.318394  , 52.242012  , 53.055767  , 52.312305  ,\n",
       "          51.946     , 48.334953  , 50.973793  , 53.294235  , 51.388737  ,\n",
       "          50.136864  , 46.32715   , 47.19116   , 47.65766   , 48.90163   ,\n",
       "          49.689995  , 50.26645   , 50.24049   , 47.976006  , 47.358204  ,\n",
       "          42.32303   , 47.513863  , 47.753704  , 49.537415  , 47.82758   ,\n",
       "          49.84778   , 48.228374  , 48.7172    , 49.7377    , 50.05682   ,\n",
       "          55.48911   , 52.269405  , 53.407883  , 55.39372   , 53.89044   ,\n",
       "          53.443123  , 53.509716  , 55.96145   , 56.21267   , 53.439686  ,\n",
       "          51.653187  , 51.96985   , 52.12493   , 53.592     , 50.29677   ,\n",
       "          49.06039   , 51.722736  , 48.468678  , 47.873287  , 47.980732  ,\n",
       "          48.438362  , 43.76641   , 38.18752   , 41.424408  ,  0.84666634,\n",
       "          18.544727  , 29.109684  , 38.609116  , 34.15808   , 21.240166  ,\n",
       "          28.573755  , 40.565613  , 18.7381    , 43.153564  , 43.019104  ,\n",
       "          42.778736  , 44.971962  , 43.209732  , 43.868237  , 44.907883  ,\n",
       "          46.73137   , 43.75372   , 41.147892  , 41.036884  , 41.08758   ,\n",
       "          37.456573  , 17.743605  ,  3.4229403 , 39.916626  , 40.377197  ,\n",
       "          41.744156  , 40.103596  , 40.249428  , 40.512413  , 35.56589   ,\n",
       "          34.270317  , 29.050854  , 39.85968   , 43.060654  , 41.5996    ,\n",
       "          43.389297  , 41.378944  , 39.226246  , 38.766212  , 39.86093   ,\n",
       "          40.591206  , 41.12049   , 40.286747  , 41.84652   , 42.84527   ,\n",
       "          43.762745  , 43.183216  , 42.91275   , 41.326336  , 38.16428   ,\n",
       "          37.737038  , 41.553543  , 43.13719   , 44.89828   , 44.17803   ,\n",
       "          42.516514  , 44.10824   , 43.75467   , 43.18504   , 44.111523  ,\n",
       "          47.139973  , 48.649235  , 46.52467   , 46.72194   , 47.468327  ,\n",
       "          49.29869   , 48.929783  , 41.989796  , 41.430077  , 41.500683  ,\n",
       "          41.214542  , 42.02893   , 44.41208   , 44.561897  , 46.220974  ,\n",
       "          45.73678   , 42.406     , 42.847534  , 43.138905  , 44.142406  ,\n",
       "          48.122456  , 49.05447   , 48.97362   , 50.07043   , 45.49514   ,\n",
       "          49.0805    , 52.061825  , 48.176144  , 49.257343  , 47.432262  ,\n",
       "          46.92142   , 48.769585  , 51.78891   ], dtype=float32),\n",
       "   'run_time': 2.906038999557495,\n",
       "   'status': 'ok'},\n",
       "  'misc': {'tid': 1,\n",
       "   'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'workdir': None,\n",
       "   'idxs': {'activation': [1],\n",
       "    'batch_normalization': [1],\n",
       "    'batch_size': [1],\n",
       "    'complete_inputs': [1],\n",
       "    'complete_sample': [1],\n",
       "    'device': [1],\n",
       "    'dropout_prob_exogenous': [1],\n",
       "    'dropout_prob_theta': [1],\n",
       "    'early_stop_patience': [1],\n",
       "    'eval_freq': [1],\n",
       "    'frequency': [1],\n",
       "    'idx_to_sample_freq': [1],\n",
       "    'initialization': [1],\n",
       "    'l1_theta': [1],\n",
       "    'learning_rate': [1],\n",
       "    'len_sample_chunks': [1],\n",
       "    'loss': [1],\n",
       "    'loss_hypar': [1],\n",
       "    'loss_valid': [1],\n",
       "    'lr_decay': [1],\n",
       "    'lr_decay_step_size': [1],\n",
       "    'max_epochs': [1],\n",
       "    'max_steps': [1],\n",
       "    'n_blocks': [1],\n",
       "    'n_freq_downsample': [1],\n",
       "    'n_hidden': [1],\n",
       "    'n_layers': [1],\n",
       "    'n_pool_kernel_size': [1],\n",
       "    'n_s_hidden': [1],\n",
       "    'n_series_per_batch': [1],\n",
       "    'n_time_in': [1],\n",
       "    'n_time_out': [1],\n",
       "    'n_val_weeks': [1],\n",
       "    'n_x_hidden': [1],\n",
       "    'normalizer_x': [1],\n",
       "    'normalizer_y': [1],\n",
       "    'random_seed': [1],\n",
       "    'seasonality': [1],\n",
       "    'shared_weights': [1],\n",
       "    'stack_types': [1],\n",
       "    'val_idx_to_sample_freq': [1],\n",
       "    'weight_decay': [1],\n",
       "    'window_sampling_limit': [1]},\n",
       "   'vals': {'activation': [0],\n",
       "    'batch_normalization': [0],\n",
       "    'batch_size': [0],\n",
       "    'complete_inputs': [0],\n",
       "    'complete_sample': [0],\n",
       "    'device': [0],\n",
       "    'dropout_prob_exogenous': [0.45954562927266907],\n",
       "    'dropout_prob_theta': [0.3377557670885139],\n",
       "    'early_stop_patience': [0],\n",
       "    'eval_freq': [0],\n",
       "    'frequency': [0],\n",
       "    'idx_to_sample_freq': [0],\n",
       "    'initialization': [0],\n",
       "    'l1_theta': [0],\n",
       "    'learning_rate': [0.0005455698037098191],\n",
       "    'len_sample_chunks': [0],\n",
       "    'loss': [0],\n",
       "    'loss_hypar': [0],\n",
       "    'loss_valid': [0],\n",
       "    'lr_decay': [0.4122236461769361],\n",
       "    'lr_decay_step_size': [0],\n",
       "    'max_epochs': [0],\n",
       "    'max_steps': [0],\n",
       "    'n_blocks': [0],\n",
       "    'n_freq_downsample': [0],\n",
       "    'n_hidden': [0],\n",
       "    'n_layers': [0],\n",
       "    'n_pool_kernel_size': [0],\n",
       "    'n_s_hidden': [0],\n",
       "    'n_series_per_batch': [0],\n",
       "    'n_time_in': [0],\n",
       "    'n_time_out': [0],\n",
       "    'n_val_weeks': [0],\n",
       "    'n_x_hidden': [10.0],\n",
       "    'normalizer_x': [0],\n",
       "    'normalizer_y': [0],\n",
       "    'random_seed': [14.0],\n",
       "    'seasonality': [0],\n",
       "    'shared_weights': [0],\n",
       "    'stack_types': [0],\n",
       "    'val_idx_to_sample_freq': [0],\n",
       "    'weight_decay': [0.0003991392255810992],\n",
       "    'window_sampling_limit': [0]}},\n",
       "  'exp_key': None,\n",
       "  'owner': None,\n",
       "  'version': 0,\n",
       "  'book_time': datetime.datetime(2021, 11, 9, 21, 50, 17, 126000),\n",
       "  'refresh_time': datetime.datetime(2021, 11, 9, 21, 50, 20, 48000)}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trials.trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('nixtla': conda)",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
