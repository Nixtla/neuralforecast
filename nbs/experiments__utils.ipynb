{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp experiments.utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Utils\n",
    "> This notebook contains a set of functions to easily perform experiments on time series datasets. In this notebook you can see functions for:\n",
    "1. Preparing dataset\n",
    "2. Loadnig models\n",
    "3. Training models\n",
    "4. Model evaluation\n",
    "5. Hyperparameter tunning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">In the next two scetion you can see enviroment variables and imports that are used for set of functions shown in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "ENV_VARS = dict(OMP_NUM_THREADS='2',\n",
    "                OPENBLAS_NUM_THREADS='2',\n",
    "                MKL_NUM_THREADS='3',\n",
    "                VECLIB_MAXIMUM_THREADS='2',\n",
    "                NUMEXPR_NUM_THREADS='3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import os\n",
    "import pickle\n",
    "# Limit number of threads in numpy and others to avoid throttling\n",
    "os.environ.update(ENV_VARS)\n",
    "import time\n",
    "from functools import partial\n",
    "from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import torch as t\n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from neuralforecast.data.scalers import Scaler\n",
    "from neuralforecast.data.tsdataset import TimeSeriesDataset, WindowsDataset, IterateWindowsDataset, BaseDataset\n",
    "from neuralforecast.data.tsloader import TimeSeriesLoader\n",
    "from neuralforecast.models.esrnn.esrnn import ESRNN\n",
    "from neuralforecast.models.rnn.rnn import RNN\n",
    "from neuralforecast.models.esrnn.mqesrnn import MQESRNN\n",
    "from neuralforecast.models.nbeats.nbeats import NBEATS\n",
    "from neuralforecast.models.nhits.nhits import NHITS\n",
    "from neuralforecast.models.transformer.autoformer import Autoformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions that represent basic use of neuralforecast library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_mask_dfs(Y_df: pd.DataFrame,\n",
    "                 ds_in_val: int, ds_in_test: int) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Generates train, test and validation mask.\n",
    "    Train mask begins by avoiding ds_in_test.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    Y_df: pd.DataFrame\n",
    "        Target time series with columns ['unique_id', 'ds', 'y'].\n",
    "    ds_in_val: int\n",
    "        Number of ds in validation.\n",
    "    ds_in_test: int\n",
    "        Number of ds in test.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    train_mask_df: pd.DataFrame\n",
    "        Train mask dataframe.\n",
    "    val_mask_df: pd.DataFrame\n",
    "        Validation mask dataframe.\n",
    "    test_mask_df: pd.DataFrame\n",
    "        Test mask dataframe.\n",
    "    \"\"\"\n",
    "\n",
    "    # train mask\n",
    "    train_mask_df = Y_df.copy()[['unique_id', 'ds']]\n",
    "    train_mask_df.sort_values(by=['unique_id', 'ds'], inplace=True)\n",
    "    train_mask_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    train_mask_df['sample_mask'] = 1\n",
    "    train_mask_df['available_mask'] = 1\n",
    "    \n",
    "    idx_out = train_mask_df.groupby('unique_id').tail(ds_in_val+ds_in_test).index\n",
    "    train_mask_df.loc[idx_out, 'sample_mask'] = 0\n",
    "    \n",
    "    # test mask\n",
    "    test_mask_df = train_mask_df.copy()\n",
    "    test_mask_df['sample_mask'] = 0\n",
    "    idx_test = test_mask_df.groupby('unique_id').tail(ds_in_test).index\n",
    "    test_mask_df.loc[idx_test, 'sample_mask'] = 1\n",
    "    \n",
    "    # validation mask\n",
    "    val_mask_df = train_mask_df.copy()\n",
    "    val_mask_df['sample_mask'] = 1\n",
    "    val_mask_df['sample_mask'] = val_mask_df['sample_mask'] - train_mask_df['sample_mask']\n",
    "    val_mask_df['sample_mask'] = val_mask_df['sample_mask'] - test_mask_df['sample_mask']\n",
    "\n",
    "    assert len(train_mask_df)==len(Y_df), \\\n",
    "        f'The mask_df length {len(train_mask_df)} is not equal to Y_df length {len(Y_df)}'\n",
    "    \n",
    "    return train_mask_df, val_mask_df, test_mask_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_random_mask_dfs(Y_df: pd.DataFrame, ds_in_test: int, \n",
    "                        n_val_windows: int, n_ds_val_window: int,\n",
    "                        n_uids: int, freq: str) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Generates train, test and random validation mask.\n",
    "    Train mask begins by avoiding ds_in_test\n",
    "    \n",
    "    Validation mask: 1) samples n_uids unique ids\n",
    "                     2) creates windows of size n_ds_val_window\n",
    "                     \n",
    "    Parameters\n",
    "    ----------\n",
    "    Y_df: pd.DataFrame\n",
    "        Target time series with columns ['unique_id', 'ds', 'y'].\n",
    "    ds_in_test: int\n",
    "        Number of ds in test.\n",
    "    n_val_windows: int\n",
    "        Number of windows for validation.\n",
    "    n_ds_val_window: int\n",
    "        Number of ds in each validation window.\n",
    "    n_uids: int\n",
    "        Number of unique ids in validation.\n",
    "    freq: str\n",
    "        string that determines datestamp frequency, used in\n",
    "        random windows creation.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    train_mask_df: pd.DataFrame\n",
    "        Train mask dataframe.\n",
    "    val_mask_df: pd.DataFrame\n",
    "        Validation mask dataframe.\n",
    "    test_mask_df: pd.DataFrame\n",
    "        Test mask dataframe.\n",
    "    \"\"\"\n",
    "    np.random.seed(1)\n",
    "    #----------------------- Train mask -----------------------#\n",
    "    # Initialize masks\n",
    "    train_mask_df, val_mask_df, test_mask_df = get_mask_dfs(Y_df=Y_df,\n",
    "                                                            ds_in_val=0,\n",
    "                                                            ds_in_test=ds_in_test)\n",
    "    \n",
    "    assert val_mask_df['sample_mask'].sum()==0, 'Muerte'\n",
    "    \n",
    "    #----------------- Random Validation mask -----------------#\n",
    "    # Overwrite validation with random windows\n",
    "    uids = train_mask_df['unique_id'].unique()\n",
    "    val_uids = np.random.choice(uids, n_uids, replace=False)\n",
    "    \n",
    "    # Validation avoids test\n",
    "    idx_test = train_mask_df.groupby('unique_id').tail(ds_in_test).index\n",
    "    available_ds = train_mask_df.loc[~train_mask_df.index.isin(idx_test)]['ds'].unique()\n",
    "    val_init_ds = np.random.choice(available_ds, n_val_windows, replace=False)\n",
    "    \n",
    "    # Creates windows \n",
    "    val_ds = [pd.date_range(init, periods=n_ds_val_window, freq=freq) for init in val_init_ds]\n",
    "    val_ds = np.concatenate(val_ds)\n",
    "\n",
    "    # Cleans random windows from train mask\n",
    "    val_idx = train_mask_df.query('unique_id in @val_uids & ds in @val_ds').index\n",
    "    train_mask_df.loc[val_idx, 'sample_mask'] = 0\n",
    "    val_mask_df.loc[val_idx, 'sample_mask'] = 1\n",
    "    \n",
    "    return train_mask_df, val_mask_df, test_mask_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def scale_data(Y_df: pd.DataFrame, X_df: pd.DataFrame, \n",
    "                mask_df: pd.DataFrame, normalizer_y: str, \n",
    "                normalizer_x: str) -> Tuple[pd.DataFrame, pd.DataFrame, Scaler]:\n",
    "    \"\"\"\n",
    "    Scales input data accordingly to given normalizer parameters.\n",
    "                     \n",
    "    Parameters\n",
    "    ----------\n",
    "    Y_df: pd.DataFrame\n",
    "        Target time series with columns ['unique_id', 'ds', 'y'].\n",
    "    X_df: pd.DataFrame\n",
    "        Exogenous time series with columns ['unique_id', 'ds', 'y']\n",
    "    mask_df: pd.DataFrame\n",
    "        Mask dataframe.\n",
    "    normalizer_y: str\n",
    "        Normalizer for scaling Y_df.\n",
    "    normalizer_x: str\n",
    "        Normalizer for scaling X_df.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Y_df: pd.DataFrame\n",
    "        Scaled target time series.\n",
    "    X_df: pd.DataFrame\n",
    "        Scaled exogenous time series with columns.\n",
    "    scaler_y: Scaler\n",
    "        Scaler object for Y_df.\n",
    "    \"\"\"\n",
    "    mask = mask_df['available_mask'].values * mask_df['sample_mask'].values\n",
    "    \n",
    "    if normalizer_y is not None:\n",
    "        scaler_y = Scaler(normalizer=normalizer_y)\n",
    "        Y_df['y'] = scaler_y.scale(x=Y_df['y'].values, mask=mask)\n",
    "    else:\n",
    "        scaler_y = None\n",
    "\n",
    "    if normalizer_x is not None:\n",
    "        X_cols = [col for col in X_df.columns if col not in ['unique_id','ds']]\n",
    "        for col in X_cols:\n",
    "            scaler_x = Scaler(normalizer=normalizer_x)\n",
    "            X_df[col] = scaler_x.scale(x=X_df[col].values, mask=mask)\n",
    "\n",
    "    return Y_df, X_df, scaler_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def create_datasets(mc: dict, S_df: pd.DataFrame, \n",
    "                    Y_df: pd.DataFrame, X_df: pd.DataFrame, f_cols: list,\n",
    "                    ds_in_test: int, ds_in_val: int, verbose: bool=False) -> Tuple[BaseDataset, BaseDataset, BaseDataset, Scaler]:\n",
    "    \"\"\"\n",
    "    Creates train, validation and test datasets.\n",
    "                     \n",
    "    Parameters\n",
    "    ----------\n",
    "    mc: dict\n",
    "        Model configuration.\n",
    "    S_df: pd.DataFrame\n",
    "        Static exogenous variables with columns ['unique_id', 'ds'] \n",
    "        and static variables. \n",
    "    Y_df: pd.DataFrame\n",
    "        Target time series with columns ['unique_id', 'ds', 'y'].\n",
    "    X_df: pd.DataFrame\n",
    "        Exogenous time series with columns ['unique_id', 'ds', 'y']\n",
    "    f_cols: list\n",
    "        List of exogenous variables of the future.\n",
    "    ds_in_test: int\n",
    "        Number of ds in test.\n",
    "    ds_in_val: int\n",
    "        Number of ds in validation.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    train_dataset: BaseDataset\n",
    "        Train dataset.\n",
    "    valid_dataset: BaseDataset\n",
    "        Validation dataset.\n",
    "    test_dataset: BaseDataset\n",
    "        Test dataset.\n",
    "    scaler_y: Scaler\n",
    "        Scaler object for Y_df.\n",
    "    \"\"\"\n",
    "\n",
    "    #------------------------------------- Available and Validation Mask ------------------------------------#\n",
    "    train_mask_df, valid_mask_df, test_mask_df = get_mask_dfs(Y_df=Y_df,\n",
    "                                                              ds_in_val=ds_in_val,\n",
    "                                                              ds_in_test=ds_in_test)\n",
    "\n",
    "    #---------------------------------------------- Scale Data ----------------------------------------------#\n",
    "    Y_df, X_df, scaler_y = scale_data(Y_df=Y_df, X_df=X_df, mask_df=train_mask_df,\n",
    "                                      normalizer_y=mc['normalizer_y'], normalizer_x=mc['normalizer_x'])\n",
    "\n",
    "    #----------------------------------------- Declare Dataset and Loaders ----------------------------------#\n",
    "    \n",
    "    if mc['mode'] == 'simple':\n",
    "        train_dataset = WindowsDataset(S_df=S_df, Y_df=Y_df, X_df=X_df,\n",
    "                                       mask_df=train_mask_df, f_cols=f_cols,\n",
    "                                       input_size=int(mc['n_time_in']),\n",
    "                                       output_size=int(mc['n_time_out']),\n",
    "                                       sample_freq=int(mc['idx_to_sample_freq']),\n",
    "                                       complete_windows=mc['complete_windows'],\n",
    "                                       verbose=verbose)\n",
    "        \n",
    "        valid_dataset = WindowsDataset(S_df=S_df, Y_df=Y_df, X_df=X_df,\n",
    "                                       mask_df=valid_mask_df, f_cols=f_cols,\n",
    "                                       input_size=int(mc['n_time_in']),\n",
    "                                       output_size=int(mc['n_time_out']),\n",
    "                                       sample_freq=int(mc['val_idx_to_sample_freq']),\n",
    "                                       complete_windows=True,\n",
    "                                       verbose=verbose)\n",
    "        \n",
    "        test_dataset = WindowsDataset(S_df=S_df, Y_df=Y_df, X_df=X_df,\n",
    "                                      mask_df=test_mask_df, f_cols=f_cols,\n",
    "                                      input_size=int(mc['n_time_in']),\n",
    "                                      output_size=int(mc['n_time_out']),\n",
    "                                      sample_freq=int(mc['val_idx_to_sample_freq']),\n",
    "                                      complete_windows=True,\n",
    "                                      verbose=verbose)\n",
    "    if mc['mode'] == 'iterate_windows':\n",
    "        train_dataset = IterateWindowsDataset(S_df=S_df, Y_df=Y_df, X_df=X_df,\n",
    "                                              mask_df=train_mask_df, f_cols=f_cols,\n",
    "                                              input_size=int(mc['n_time_in']),\n",
    "                                              output_size=int(mc['n_time_out']),\n",
    "                                              verbose=verbose)\n",
    "        \n",
    "        valid_dataset = IterateWindowsDataset(S_df=S_df, Y_df=Y_df, X_df=X_df,\n",
    "                                              mask_df=valid_mask_df, f_cols=f_cols,\n",
    "                                              input_size=int(mc['n_time_in']),\n",
    "                                              output_size=int(mc['n_time_out']),\n",
    "                                              verbose=verbose)\n",
    "        \n",
    "        test_dataset = IterateWindowsDataset(S_df=S_df, Y_df=Y_df, X_df=X_df,\n",
    "                                             mask_df=test_mask_df, f_cols=f_cols,\n",
    "                                             input_size=int(mc['n_time_in']),\n",
    "                                             output_size=int(mc['n_time_out']),\n",
    "                                             verbose=verbose)   \n",
    "    \n",
    "    if mc['mode'] == 'full':\n",
    "        train_dataset = TimeSeriesDataset(S_df=S_df, Y_df=Y_df, X_df=X_df,\n",
    "                                          mask_df=train_mask_df, f_cols=f_cols,\n",
    "                                          input_size=int(mc['n_time_in']),\n",
    "                                          output_size=int(mc['n_time_out']),\n",
    "                                          verbose=verbose)\n",
    "        \n",
    "        valid_dataset = TimeSeriesDataset(S_df=S_df, Y_df=Y_df, X_df=X_df,\n",
    "                                          mask_df=valid_mask_df, f_cols=f_cols,\n",
    "                                          input_size=int(mc['n_time_in']),\n",
    "                                          output_size=int(mc['n_time_out']),\n",
    "                                          verbose=verbose)\n",
    "        \n",
    "        test_dataset = TimeSeriesDataset(S_df=S_df, Y_df=Y_df, X_df=X_df,\n",
    "                                         mask_df=test_mask_df, f_cols=f_cols,\n",
    "                                         input_size=int(mc['n_time_in']),\n",
    "                                         output_size=int(mc['n_time_out']),\n",
    "                                         verbose=verbose)        \n",
    "    \n",
    "    if ds_in_test == 0:\n",
    "        test_dataset = None\n",
    "\n",
    "    return train_dataset, valid_dataset, test_dataset, scaler_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def instantiate_loaders(mc: dict, \n",
    "                        train_dataset: BaseDataset, val_dataset: BaseDataset, \n",
    "                        test_dataset: BaseDataset) -> Tuple[DataLoader, DataLoader, DataLoader]:\n",
    "    \"\"\"\n",
    "    Creates train, validation and test loader classes.\n",
    "                     \n",
    "    Parameters\n",
    "    ----------\n",
    "    mc: dict\n",
    "        Model configuration.\n",
    "    train_dataset: BaseDataset\n",
    "        Train dataset.\n",
    "    val_dataset: BaseDataset\n",
    "        Validation dataset.\n",
    "    test_dataset: BaseDataset\n",
    "        Test dataset.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    train_loader: DataLoader\n",
    "        Train loader.\n",
    "    val_loader: DataLoader\n",
    "        Validation loader.\n",
    "    test_loader: DataLoader\n",
    "        Test loader.\n",
    "    \"\"\"\n",
    "\n",
    "    if mc['mode'] in ['simple', 'full'] :\n",
    "        n_windows = mc['n_windows'] if mc['mode']=='simple' else None\n",
    "        train_loader = TimeSeriesLoader(dataset=train_dataset,\n",
    "                                        batch_size=int(mc['batch_size']),\n",
    "                                        n_windows=n_windows,\n",
    "                                        eq_batch_size=False,\n",
    "                                        shuffle=True)\n",
    "        if val_dataset is not None:\n",
    "            val_loader = TimeSeriesLoader(dataset=val_dataset,\n",
    "                                        batch_size=1,\n",
    "                                        shuffle=False)\n",
    "        else:\n",
    "            val_loader = None\n",
    "\n",
    "        if test_dataset is not None:\n",
    "            test_loader = TimeSeriesLoader(dataset=test_dataset,\n",
    "                                        batch_size=1,\n",
    "                                        shuffle=False)\n",
    "        else:\n",
    "            test_loader = None\n",
    "\n",
    "    elif mc['mode'] == 'iterate_windows':\n",
    "        train_loader =DataLoader(dataset=train_dataset,\n",
    "                                 batch_size=int(mc['batch_size']),\n",
    "                                 shuffle=True,\n",
    "                                 drop_last=True)\n",
    "\n",
    "        if val_dataset is not None:\n",
    "            val_loader = DataLoader(dataset=val_dataset,\n",
    "                                    batch_size=1,\n",
    "                                    shuffle=False)\n",
    "        else:\n",
    "            val_loader = None\n",
    "\n",
    "        if test_dataset is not None:\n",
    "            test_loader = DataLoader(dataset=test_dataset,\n",
    "                                     batch_size=1,\n",
    "                                     shuffle=False)\n",
    "        else:\n",
    "            test_loader = None\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def instantiate_nbeats(mc: dict) -> NBEATS:\n",
    "    \"\"\"\n",
    "    Creates nbeats model.\n",
    "                     \n",
    "    Parameters\n",
    "    ----------\n",
    "    mc: dict\n",
    "        Model configuration.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model: NBEATS\n",
    "        Nbeats model.\n",
    "    \"\"\"\n",
    "    mc['n_mlp_units'] = len(mc['stack_types']) * [ mc['constant_n_layers'] * [int(mc['constant_n_mlp_units'])] ]\n",
    "    mc['n_layers'] =  len(mc['stack_types']) * [ mc['constant_n_layers'] ]\n",
    "    mc['n_blocks'] =  len(mc['stack_types']) * [ mc['constant_n_blocks'] ]\n",
    "\n",
    "    if mc['max_epochs'] is not None:\n",
    "        lr_decay_step_size = int(np.ceil(mc['max_epochs'] / mc['n_lr_decays']))\n",
    "    elif mc['max_steps'] is not None:\n",
    "        lr_decay_step_size = int(np.ceil(mc['max_steps'] / mc['n_lr_decays']))\n",
    "\n",
    "    model = NBEATS(n_time_in=int(mc['n_time_in']),\n",
    "                  n_time_out=int(mc['n_time_out']),\n",
    "                  n_x=mc['n_x'],\n",
    "                  n_s=mc['n_s'],\n",
    "                  n_s_hidden=int(mc['n_s_hidden']),\n",
    "                  n_x_hidden=int(mc['n_x_hidden']),\n",
    "                  shared_weights = mc['shared_weights'],\n",
    "                  initialization=mc['initialization'],\n",
    "                  activation=mc['activation'],\n",
    "                  stack_types=mc['stack_types'],\n",
    "                  n_blocks=mc['n_blocks'],\n",
    "                  n_layers=mc['n_layers'],\n",
    "                  n_mlp_units=mc['n_mlp_units'],\n",
    "                  batch_normalization = mc['batch_normalization'],\n",
    "                  dropout_prob_theta=mc['dropout_prob_theta'],\n",
    "                  learning_rate=float(mc['learning_rate']),\n",
    "                  lr_decay=float(mc['lr_decay']),\n",
    "                  lr_decay_step_size=lr_decay_step_size,\n",
    "                  weight_decay=mc['weight_decay'],\n",
    "                  loss_train=mc['loss_train'],\n",
    "                  loss_hypar=float(mc['loss_hypar']),\n",
    "                  loss_valid=mc['loss_valid'],\n",
    "                  frequency=mc['frequency'],\n",
    "                  random_seed=int(mc['random_seed']))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def instantiate_esrnn(mc: dict) -> ESRNN:  \n",
    "    \"\"\"\n",
    "    Creates esrnn model.\n",
    "                     \n",
    "    Parameters\n",
    "    ----------\n",
    "    mc: dict\n",
    "        Model configuration.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model: ESRNN\n",
    "        Esrnn model.\n",
    "    \"\"\"  \n",
    "    if mc['max_epochs'] is not None:\n",
    "        lr_decay_step_size = int(np.ceil(mc['max_epochs'] / mc['n_lr_decays']))\n",
    "    elif mc['max_steps'] is not None:\n",
    "        lr_decay_step_size = int(np.ceil(mc['max_steps'] / mc['n_lr_decays']))\n",
    "\n",
    "    model = ESRNN(# Architecture parameters\n",
    "                  n_series=mc['n_series'],\n",
    "                  n_x=mc['n_x'],\n",
    "                  n_s=mc['n_s'],\n",
    "                  sample_freq=int(mc['sample_freq']),\n",
    "                  input_size=int(mc['n_time_in']),\n",
    "                  output_size=int(mc['n_time_out']),\n",
    "                  es_component=mc['es_component'],\n",
    "                  cell_type=mc['cell_type'],\n",
    "                  state_hsize=int(mc['state_hsize']),\n",
    "                  dilations=mc['dilations'],\n",
    "                  add_nl_layer=mc['add_nl_layer'],\n",
    "                  # Optimization parameters                \n",
    "                  learning_rate=mc['learning_rate'],\n",
    "                  lr_scheduler_step_size=lr_decay_step_size,\n",
    "                  lr_decay=mc['lr_decay'],\n",
    "                  per_series_lr_multip=mc['per_series_lr_multip'],\n",
    "                  gradient_eps=mc['gradient_eps'],\n",
    "                  gradient_clipping_threshold=mc['gradient_clipping_threshold'],\n",
    "                  rnn_weight_decay=mc['rnn_weight_decay'],\n",
    "                  noise_std=mc['noise_std'],\n",
    "                  level_variability_penalty=mc['level_variability_penalty'],\n",
    "                  testing_percentile=mc['testing_percentile'],\n",
    "                  training_percentile=mc['training_percentile'],\n",
    "                  loss=mc['loss_train'],\n",
    "                  val_loss=mc['loss_valid'],\n",
    "                  seasonality=mc['seasonality']) \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def instantiate_rnn(mc: dict) -> ESRNN:  \n",
    "    \"\"\"\n",
    "    Creates esrnn model.\n",
    "                     \n",
    "    Parameters\n",
    "    ----------\n",
    "    mc: dict\n",
    "        Model configuration.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model: RNN\n",
    "        RNN model.\n",
    "    \"\"\"  \n",
    "    if mc['max_epochs'] is not None:\n",
    "        lr_decay_step_size = int(np.ceil(mc['max_epochs'] / mc['n_lr_decays']))\n",
    "    elif mc['max_steps'] is not None:\n",
    "        lr_decay_step_size = int(np.ceil(mc['max_steps'] / mc['n_lr_decays']))\n",
    "\n",
    "    model = RNN(# Architecture parameters\n",
    "                  input_size=int(mc['n_time_in']),\n",
    "                  output_size=int(mc['n_time_out']),\n",
    "                  n_x=mc['n_x'],\n",
    "                  n_s=mc['n_s'],\n",
    "                  sample_freq=int(mc['sample_freq']),\n",
    "                  cell_type=mc['cell_type'],\n",
    "                  state_hsize=int(mc['state_hsize']),\n",
    "                  dilations=mc['dilations'],\n",
    "                  add_nl_layer=mc['add_nl_layer'],\n",
    "                  # Optimization parameters\n",
    "                  learning_rate=mc['learning_rate'],\n",
    "                  lr_scheduler_step_size=lr_decay_step_size,\n",
    "                  lr_decay=mc['lr_decay'],\n",
    "                  gradient_eps=mc['gradient_eps'],\n",
    "                  gradient_clipping_threshold=mc['gradient_clipping_threshold'],\n",
    "                  weight_decay=mc['weight_decay'],\n",
    "                  noise_std=mc['noise_std'],\n",
    "                  loss_train=mc['loss_train'],\n",
    "                  loss_valid=mc['loss_valid'],\n",
    "                  loss_hypar=mc['loss_hypar'],\n",
    "                  frequency=mc['frequency'],\n",
    "                  random_seed=int(mc['random_seed']))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def instantiate_mqesrnn(mc: dict) -> MQESRNN: \n",
    "    \"\"\"\n",
    "    Creates mqesrnn model.\n",
    "                     \n",
    "    Parameters\n",
    "    ----------\n",
    "    mc: dict\n",
    "        Model configuration.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model: MQESRNN\n",
    "        Mqesrnn model.\n",
    "    \"\"\"  \n",
    "\n",
    "    if mc['max_epochs'] is not None:\n",
    "        lr_decay_step_size = int(np.ceil(mc['max_epochs'] / mc['n_lr_decays']))\n",
    "    elif mc['max_steps'] is not None:\n",
    "        lr_decay_step_size = int(np.ceil(mc['max_steps'] / mc['n_lr_decays']))\n",
    "\n",
    "    model = MQESRNN(# Architecture parameters\n",
    "                    n_series=mc['n_series'],\n",
    "                    n_x=mc['n_x'],\n",
    "                    n_s=mc['n_s'],\n",
    "                    idx_to_sample_freq=int(mc['idx_to_sample_freq']),\n",
    "                    input_size=int(mc['n_time_in']),\n",
    "                    output_size=int(mc['n_time_out']),\n",
    "                    es_component=mc['es_component'],\n",
    "                    cell_type=mc['cell_type'],\n",
    "                    state_hsize=int(mc['state_hsize']),\n",
    "                    dilations=mc['dilations'],\n",
    "                    add_nl_layer=mc['add_nl_layer'],\n",
    "                    # Optimization parameters                 \n",
    "                    learning_rate=mc['learning_rate'],\n",
    "                    lr_scheduler_step_size=lr_decay_step_size,\n",
    "                    lr_decay=mc['lr_decay'],\n",
    "                    gradient_eps=mc['gradient_eps'],\n",
    "                    gradient_clipping_threshold=mc['gradient_clipping_threshold'],\n",
    "                    rnn_weight_decay=mc['rnn_weight_decay'],\n",
    "                    noise_std=mc['noise_std'],\n",
    "                    testing_percentiles=list(mc['testing_percentiles']),\n",
    "                    training_percentiles=list(mc['training_percentiles']),\n",
    "                    loss=mc['loss_train'],\n",
    "                    val_loss=mc['loss_valid']) \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def instantiate_nhits(mc: dict) -> NHITS:\n",
    "    \"\"\"\n",
    "    Creates nhits model.\n",
    "                     \n",
    "    Parameters\n",
    "    ----------\n",
    "    mc: dict\n",
    "        Model configuration.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model: NHITS\n",
    "        Nhits model.\n",
    "    \"\"\"  \n",
    "    \n",
    "    mc['n_mlp_units'] = len(mc['stack_types']) * [ mc['constant_n_layers'] * [int(mc['constant_n_mlp_units'])] ]\n",
    "    mc['n_layers'] =  len(mc['stack_types']) * [ mc['constant_n_layers'] ]\n",
    "    mc['n_blocks'] =  len(mc['stack_types']) * [ mc['constant_n_blocks'] ]\n",
    "\n",
    "    if mc['max_epochs'] is not None:\n",
    "        lr_decay_step_size = int(np.ceil(mc['max_epochs'] / mc['n_lr_decays']))\n",
    "    elif mc['max_steps'] is not None:\n",
    "        lr_decay_step_size = int(np.ceil(mc['max_steps'] / mc['n_lr_decays']))\n",
    "\n",
    "    model = NHITS(n_time_in=int(mc['n_time_in']),\n",
    "                  n_time_out=int(mc['n_time_out']),\n",
    "                  n_x=mc['n_x'],\n",
    "                  n_s=mc['n_s'],\n",
    "                  n_s_hidden=int(mc['n_s_hidden']),\n",
    "                  n_x_hidden=int(mc['n_x_hidden']),\n",
    "                  shared_weights = mc['shared_weights'],\n",
    "                  initialization=mc['initialization'],\n",
    "                  activation=mc['activation'],\n",
    "                  stack_types=mc['stack_types'],\n",
    "                  n_blocks=mc['n_blocks'],\n",
    "                  n_layers=mc['n_layers'],\n",
    "                  n_mlp_units=mc['n_mlp_units'],\n",
    "                  n_pool_kernel_size=mc['n_pool_kernel_size'],\n",
    "                  n_freq_downsample=mc['n_freq_downsample'],\n",
    "                  pooling_mode=mc['pooling_mode'],\n",
    "                  interpolation_mode=mc['interpolation_mode'],\n",
    "                  batch_normalization = mc['batch_normalization'],\n",
    "                  dropout_prob_theta=mc['dropout_prob_theta'],\n",
    "                  learning_rate=float(mc['learning_rate']),\n",
    "                  lr_decay=float(mc['lr_decay']),\n",
    "                  lr_decay_step_size=lr_decay_step_size,\n",
    "                  weight_decay=mc['weight_decay'],\n",
    "                  loss_train=mc['loss_train'],\n",
    "                  loss_hypar=float(mc['loss_hypar']),\n",
    "                  loss_valid=mc['loss_valid'],\n",
    "                  frequency=mc['frequency'],\n",
    "                  random_seed=int(mc['random_seed']))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def instantiate_autoformer(mc: dict) -> Autoformer:\n",
    "    \"\"\"\n",
    "    Creates autoformer model.\n",
    "                     \n",
    "    Parameters\n",
    "    ----------\n",
    "    mc: dict\n",
    "        Model configuration.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model: Autoformer\n",
    "        Autoformer model.\n",
    "    \"\"\"  \n",
    "\n",
    "    if mc['max_epochs'] is not None:\n",
    "        lr_decay_step_size = int(np.ceil(mc['max_epochs'] / mc['n_lr_decays']))\n",
    "    elif mc['max_steps'] is not None:\n",
    "        lr_decay_step_size = int(np.ceil(mc['max_steps'] / mc['n_lr_decays']))\n",
    "    \n",
    "    model = Autoformer(seq_len=int(mc['seq_len']),\n",
    "                       label_len=int(mc['label_len']),\n",
    "                       pred_len=int(mc['pred_len']),\n",
    "                       output_attention=mc['output_attention'],\n",
    "                       enc_in=int(mc['enc_in']),\n",
    "                       dec_in=int(mc['dec_in']),\n",
    "                       d_model=int(mc['d_model']),\n",
    "                       c_out=int(mc['c_out']),\n",
    "                       embed = mc['embed'],\n",
    "                       freq=mc['freq'],\n",
    "                       dropout=mc['dropout'],\n",
    "                       factor=mc['factor'],\n",
    "                       n_heads=int(mc['n_heads']),\n",
    "                       d_ff=int(mc['d_ff']),\n",
    "                       moving_avg=int(mc['moving_avg']),\n",
    "                       activation=mc['activation'],\n",
    "                       e_layers=int(mc['e_layers']),\n",
    "                       d_layers=int(mc['d_layers']),\n",
    "                       learning_rate=float(mc['learning_rate']),\n",
    "                       lr_decay=float(mc['lr_decay']),\n",
    "                       lr_decay_step_size=lr_decay_step_size,\n",
    "                       weight_decay=mc['weight_decay'],\n",
    "                       loss_train=mc['loss_train'],\n",
    "                       loss_hypar=float(mc['loss_hypar']),\n",
    "                       loss_valid=mc['loss_valid'],\n",
    "                       random_seed=int(mc['random_seed']))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def instantiate_model(mc: dict) -> pl.LightningModule:\n",
    "    \"\"\"\n",
    "    Creates one of the models.\n",
    "    (nbeats, esrnn, mqesrnn, nhits, autoformer)\n",
    "                     \n",
    "    Parameters\n",
    "    ----------\n",
    "    mc: dict\n",
    "        Model configuration.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model: pl.LightningModule\n",
    "        Forecast model.\n",
    "    \"\"\"  \n",
    "    MODEL_DICT = {'nbeats': instantiate_nbeats,\n",
    "                  'esrnn': instantiate_esrnn,\n",
    "                  'rnn': instantiate_rnn,\n",
    "                  'mqesrnn': instantiate_mqesrnn,\n",
    "                  'nhits': instantiate_nhits,\n",
    "                  'autoformer': instantiate_autoformer}\n",
    "    return MODEL_DICT[mc['model']](mc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def predict(mc: dict, model: pl.LightningModule, \n",
    "            trainer: pl.Trainer, loader: DataLoader, \n",
    "            scaler_y: Scaler) -> Tuple[np.array, np.array, np.array, np.array]:\n",
    "    \"\"\"\n",
    "    Predicts results on dataset using trained model.\n",
    "                     \n",
    "    Parameters\n",
    "    ----------\n",
    "    mc: dict\n",
    "        Model configuration.\n",
    "    model: pl.LightningModule\n",
    "        Forecast model.\n",
    "    trainer: pl.Trainer\n",
    "        Trainer object.\n",
    "    loader: DataLoader\n",
    "        Data loader.\n",
    "    scaler_y: Scaler\n",
    "        Scaler object for target time series.   \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    y_true: np.array\n",
    "        True values from dataset.\n",
    "    y_hat: np.array\n",
    "        Predicted values from dataset.\n",
    "    mask: np.array \n",
    "        Masks for values.\n",
    "    meta_data: np.array \n",
    "        Metada from dataset.\n",
    "    \"\"\"  \n",
    "    outputs = trainer.predict(model, loader)\n",
    "    y_true, y_hat, mask = [t.cat(output).cpu().numpy() for output in zip(*outputs)]\n",
    "    meta_data = loader.dataset.meta_data\n",
    "\n",
    "    # Scale to original scale\n",
    "    if mc['normalizer_y'] is not None:\n",
    "        y_true_shape = y_true.shape\n",
    "        y_true = scaler_y.inv_scale(x=y_true.flatten())\n",
    "        y_true = np.reshape(y_true, y_true_shape)\n",
    "\n",
    "        y_hat = scaler_y.inv_scale(x=y_hat.flatten())\n",
    "        y_hat = np.reshape(y_hat, y_true_shape)\n",
    "\n",
    "    return y_true, y_hat, mask, meta_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def fit(mc: dict, Y_df: pd.DataFrame, X_df: pd.DataFrame =None, S_df: pd.DataFrame =None,\n",
    "        ds_in_val: int =0, ds_in_test: int =0,\n",
    "        f_cols: list =[], verbose: bool = False) -> Tuple[pl.LightningModule, pl.Trainer, \n",
    "                                                          DataLoader, DataLoader, Scaler] or pl.LightningModule:\n",
    "    \"\"\"\n",
    "    Traines model on given dataset.\n",
    "                     \n",
    "    Parameters\n",
    "    ----------\n",
    "    mc: dict\n",
    "        Model configuration.\n",
    "    Y_df: pd.DataFrame\n",
    "        Target time series with columns ['unique_id', 'ds', 'y'].\n",
    "    X_df: pd.DataFrame\n",
    "        Exogenous time series with columns ['unique_id', 'ds', 'y'].\n",
    "    S_df: pd.DataFrame\n",
    "        Static exogenous variables with columns ['unique_id', 'ds']. \n",
    "        and static variables. \n",
    "    ds_in_val: int\n",
    "        Number of ds in validation.\n",
    "    ds_in_test: int\n",
    "        Number of ds in test.\n",
    "    f_cols: list\n",
    "        List of exogenous variables of the future.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model: pl.LightningModule\n",
    "        Forecast model.\n",
    "    trainer: pl.Trainer\n",
    "        Trainer object.\n",
    "    val_loader: DataLoader\n",
    "        Validation loader.\n",
    "    test_loader: DataLoader\n",
    "        Test loader.\n",
    "    scaler_y: Scaler\n",
    "        Scaler object for target time series.   \n",
    "    \"\"\"   \n",
    "\n",
    "    # Protect inplace modifications\n",
    "    Y_df = Y_df.copy()\n",
    "    if X_df is not None:\n",
    "        X_df = X_df.copy()\n",
    "    if S_df is not None:\n",
    "        S_df = S_df.copy()        \n",
    "\n",
    "    #----------------------------------------------- Datasets -----------------------------------------------#\n",
    "    train_dataset, val_dataset, test_dataset, scaler_y = create_datasets(mc=mc,\n",
    "                                                                         S_df=S_df, Y_df=Y_df, X_df=X_df,\n",
    "                                                                         f_cols=f_cols,\n",
    "                                                                         ds_in_val=ds_in_val,\n",
    "                                                                         ds_in_test=ds_in_test,\n",
    "                                                                         verbose=verbose)\n",
    "    mc['n_x'], mc['n_s'] = train_dataset.get_n_variables()\n",
    "\n",
    "    #------------------------------------------- Instantiate & fit -------------------------------------------#\n",
    "    train_loader, val_loader, test_loader = instantiate_loaders(mc=mc,\n",
    "                                                                train_dataset=train_dataset,\n",
    "                                                                val_dataset=val_dataset,\n",
    "                                                                test_dataset=test_dataset)\n",
    "    model = instantiate_model(mc=mc)\n",
    "    callbacks = []\n",
    "    if mc['early_stop_patience'] and ds_in_val > 0:\n",
    "        early_stopping = pl.callbacks.EarlyStopping(monitor='val_loss', min_delta=1e-4, \n",
    "                                                    patience=mc['early_stop_patience'],\n",
    "                                                    verbose=verbose, \n",
    "                                                    mode='min') \n",
    "        callbacks=[early_stopping]\n",
    "    \n",
    "    gpus = -1 if t.cuda.is_available() else 0\n",
    "    trainer = pl.Trainer(max_epochs=mc['max_epochs'], \n",
    "                         max_steps=mc['max_steps'],\n",
    "                         check_val_every_n_epoch=mc['eval_freq'],\n",
    "                         gpus=gpus,\n",
    "                         callbacks=callbacks,\n",
    "                         enable_checkpointing=False,\n",
    "                         enable_progress_bar=verbose,\n",
    "                         enable_model_summary=verbose,\n",
    "                         logger=False)\n",
    "    \n",
    "    val_dataloaders = val_loader if ds_in_val > 0 else None\n",
    "    trainer.fit(model, train_loader, val_dataloaders)\n",
    "    \n",
    "    return model, trainer, val_loader, test_loader, scaler_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def model_fit_predict(mc: dict, \n",
    "                        S_df: pd.DataFrame, Y_df: pd.DataFrame, X_df: pd.DataFrame, \n",
    "                        f_cols: list, ds_in_val: int, ds_in_test: int, verbose: bool) -> dict:\n",
    "    \"\"\"\n",
    "    Traines model on train dataset, then calculates predictions\n",
    "    on test dataset.\n",
    "                     \n",
    "    Parameters\n",
    "    ----------\n",
    "    mc: dict\n",
    "        Model configuration.\n",
    "    Y_df: pd.DataFrame\n",
    "        Target time series with columns ['unique_id', 'ds', 'y'].\n",
    "    X_df: pd.DataFrame\n",
    "        Exogenous time series with columns ['unique_id', 'ds', 'y'].\n",
    "    f_cols: list\n",
    "        List of exogenous variables of the future.\n",
    "    ds_in_val: int\n",
    "        Number of ds in validation.\n",
    "    ds_in_test: int\n",
    "        Number of ds in test.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    results: dict\n",
    "        Dictionary with results of training and prediction on model.   \n",
    "    \"\"\"  \n",
    "\n",
    "    #------------------------------------------------ Fit ------------------------------------------------#\n",
    "    model, trainer, val_loader, test_loader, scaler_y = fit(\n",
    "        mc, S_df=S_df, Y_df=Y_df, X_df=X_df, \n",
    "        f_cols=[], ds_in_val=ds_in_val, ds_in_test=ds_in_test, verbose=verbose\n",
    "    )\n",
    "    #------------------------------------------------ Predict ------------------------------------------------#\n",
    "    results = {}\n",
    "\n",
    "    if ds_in_val > 0:\n",
    "        y_true, y_hat, mask, meta_data = predict(mc, model, trainer, val_loader, scaler_y)\n",
    "        val_values = (('val_y_true', y_true), ('val_y_hat', y_hat), ('val_mask', mask), ('val_meta_data', meta_data))\n",
    "        results.update(val_values)\n",
    "\n",
    "    # Predict test if available\n",
    "    if ds_in_test > 0:\n",
    "        y_true, y_hat, mask, meta_data = predict(mc, model, trainer, test_loader, scaler_y)\n",
    "        test_values = (('test_y_true', y_true), ('test_y_hat', y_hat), ('test_mask', mask), ('test_meta_data', meta_data))\n",
    "        results.update(test_values)\n",
    "\n",
    "    return results, model, trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def evaluate_model(mc: dict, loss_function_val: callable, loss_functions_test: dict, \n",
    "                   S_df: pd.DataFrame, Y_df: pd.DataFrame, X_df: pd.DataFrame, \n",
    "                   f_cols: list, ds_in_val: int, ds_in_test: int,\n",
    "                   return_forecasts: bool,\n",
    "                   return_model: bool,\n",
    "                   save_trials: bool,\n",
    "                   trials: Trials,\n",
    "                   results_dir: str,\n",
    "                   step_save_progress: int =5,\n",
    "                   loss_kwargs: list =None, verbose: bool=False) -> dict:\n",
    "    \"\"\"\n",
    "    Evaluate model on given dataset.\n",
    "                     \n",
    "    Parameters\n",
    "    ----------\n",
    "    mc: dictionary\n",
    "        Model configuration.\n",
    "    loss_function_val: function\n",
    "        Loss function used for validation.\n",
    "    loss_functions_test: Dictionary\n",
    "        Loss functions used for test. \n",
    "        (function name: string, function: fun)\n",
    "    S_df: pd.DataFrame\n",
    "        Static exogenous variables with columns ['unique_id', 'ds']. \n",
    "        and static variables. \n",
    "    Y_df: pd.DataFrame\n",
    "        Target time series with columns ['unique_id', 'ds', 'y'].\n",
    "    X_df: pd.DataFrame\n",
    "        Exogenous time series with columns ['unique_id', 'ds', 'y'].\n",
    "    f_cols: list\n",
    "        List of exogenous variables of the future.\n",
    "    ds_in_val: int\n",
    "        Number of ds in validation.\n",
    "    ds_in_test: int\n",
    "        Number of ds in test.\n",
    "    return_forecasts: bool\n",
    "        If true return forecast on test.\n",
    "    save_trials: bool    \n",
    "        If true save progres in file.\n",
    "    trials: hyperopt.Trials\n",
    "        Results from model evaluation.\n",
    "    results_dir: str\n",
    "        File path to save results.\n",
    "    step_save_progress: int\n",
    "        Every n-th step is saved in file.\n",
    "    loss_kwargs: List\n",
    "        Loss function arguments.    \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    results_output: dict\n",
    "        Dictionary with results of model evaluation.   \n",
    "    \"\"\" \n",
    "\n",
    "    if (save_trials) and (len(trials) % step_save_progress == 0):\n",
    "        trials_file = f'{results_dir}/trials.p'\n",
    "        with open(trials_file, \"wb\") as f:\n",
    "            pickle.dump(trials, f)\n",
    "    \n",
    "    if verbose:\n",
    "        print(47*'=' + '\\n')\n",
    "        print(pd.Series(mc))\n",
    "        print(47*'=' + '\\n')\n",
    "    \n",
    "    # Some asserts due to work in progress\n",
    "    n_series = Y_df['unique_id'].nunique()\n",
    "    if n_series > 1:\n",
    "        assert mc['normalizer_y'] is None, 'Data scaling not implemented with multiple time series'\n",
    "        assert mc['normalizer_x'] is None, 'Data scaling not implemented with multiple time series'\n",
    "\n",
    "    assert ds_in_test % mc['val_idx_to_sample_freq']==0, 'outsample size should be multiple of val_idx_to_sample_freq'\n",
    "\n",
    "    # Make predictions\n",
    "    start = time.time()\n",
    "    results, _, trainer = model_fit_predict(mc=mc,\n",
    "                                                S_df=S_df, \n",
    "                                                Y_df=Y_df,\n",
    "                                                X_df=X_df,\n",
    "                                                f_cols=f_cols,\n",
    "                                                ds_in_val=ds_in_val,\n",
    "                                                ds_in_test=ds_in_test,\n",
    "                                                verbose=verbose)\n",
    "    run_time = time.time() - start\n",
    "\n",
    "    # Evaluate predictions\n",
    "    val_loss = loss_function_val(y=results['val_y_true'], y_hat=results['val_y_hat'], weights=results['val_mask'], **loss_kwargs)\n",
    "\n",
    "    results_output = {'loss': val_loss,\n",
    "                      'mc': mc,\n",
    "                      'run_time': run_time,\n",
    "                      'status': STATUS_OK}\n",
    "\n",
    "    # Evaluation in test (if provided)\n",
    "    if ds_in_test > 0:\n",
    "        test_loss_dict = {}\n",
    "        for loss_name, loss_function in loss_functions_test.items():\n",
    "            test_loss_dict[loss_name] = loss_function(y=results['test_y_true'], y_hat=results['test_y_hat'], weights=results['test_mask'])\n",
    "        results_output['test_losses'] = test_loss_dict\n",
    "    \n",
    "    if return_forecasts and ds_in_test > 0:\n",
    "        forecasts_test = {}\n",
    "        test_values = (('test_y_true', results['test_y_true']), ('test_y_hat', results['test_y_hat']),\n",
    "                        ('test_mask', results['test_mask']), ('test_meta_data', results['test_meta_data']))\n",
    "        forecasts_test.update(test_values)\n",
    "        results_output['forecasts_test'] = forecasts_test\n",
    "\n",
    "    # Save model in best_trial\n",
    "    if len(trials) > 1:\n",
    "        current_best_loss = min(trials.losses()[:-1]) # :-1 since current run has None\n",
    "    else:\n",
    "        current_best_loss = np.inf\n",
    "        \n",
    "    if (return_model) and (val_loss < current_best_loss):\n",
    "        trainer.save_checkpoint(f\"{results_dir}/best_model.ckpt\")\n",
    "\n",
    "    return results_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "MODEL_DICT = {'nbeats': NBEATS,\n",
    "              'nhits': NHITS,\n",
    "              'rnn': RNN,\n",
    "              'esrnn': ESRNN,\n",
    "              'autoformer': Autoformer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def hyperopt_tunning(space: dict, hyperopt_max_evals: int, \n",
    "                     loss_function_val: callable, loss_functions_test: dict,\n",
    "                     S_df: pd.DataFrame, Y_df: pd.DataFrame, X_df: pd.DataFrame, \n",
    "                     f_cols: list, ds_in_val: int, ds_in_test: int,\n",
    "                     return_forecasts: bool,\n",
    "                     return_model: bool,\n",
    "                     save_trials: bool,\n",
    "                     results_dir: str,\n",
    "                     step_save_progress: int = 5,\n",
    "                     loss_kwargs: list =None, verbose: bool =False) -> Trials:\n",
    "    \"\"\"\n",
    "    Evaluates multiple models trained on given dataset.\n",
    "    Models are trained with different hyperparameters.\n",
    "    Hyperparameters are changed until function is minimized in\n",
    "    hyperparameter space. All models are trained and evaluated,\n",
    "    until function is minimized. \n",
    "                     \n",
    "    Parameters\n",
    "    ----------\n",
    "    space: Dictionary\n",
    "        Dictionary that contines hyperparameters that create space.\n",
    "    hyperopt_max_evals: int\n",
    "        Maximum number of evaluations.\n",
    "    loss_function_val: function\n",
    "        Loss function used for validation.\n",
    "    loss_functions_test: Dictionary\n",
    "        Loss functions used for test. \n",
    "        (function name: string, function: fun)\n",
    "    S_df: pd.DataFrame\n",
    "        Static exogenous variables with columns ['unique_id', 'ds']. \n",
    "        and static variables. \n",
    "    Y_df: pd.DataFrame\n",
    "        Target time series with columns ['unique_id', 'ds', 'y'].\n",
    "    X_df: pd.DataFrame\n",
    "        Exogenous time series with columns ['unique_id', 'ds', 'y'].\n",
    "    f_cols: list\n",
    "        List of exogenous variables of the future.\n",
    "    ds_in_val: int\n",
    "        Number of ds in validation.\n",
    "    ds_in_test: int\n",
    "        Number of ds in test.\n",
    "    return_forecasts: bool\n",
    "        If true return forecast on test.\n",
    "    return_model: bool\n",
    "        If true return models.\n",
    "    save_trials: bool    \n",
    "        If true save progres in file.\n",
    "    results_dir: str\n",
    "        File path to save results.\n",
    "    step_save_progress: int\n",
    "        Every n-th step is saved in file.\n",
    "    loss_kwargs: List\n",
    "        Loss function arguments.\n",
    "    verbose:\n",
    "        If true, will print summary of dataset, model and training.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    trials: Trials\n",
    "        Results from model evaluation.  \n",
    "    \"\"\" \n",
    "\n",
    "    assert ds_in_val > 0, 'Validation set is needed for tunning!'\n",
    "\n",
    "    if save_trials or return_model:\n",
    "        os.makedirs(results_dir, exist_ok = True)\n",
    "\n",
    "    trials = Trials()\n",
    "    fmin_objective = partial(evaluate_model, loss_function_val=loss_function_val, loss_functions_test=loss_functions_test,\n",
    "                             S_df=S_df, Y_df=Y_df, X_df=X_df, f_cols=f_cols,\n",
    "                             ds_in_val=ds_in_val, ds_in_test=ds_in_test,\n",
    "                             return_forecasts=return_forecasts, return_model=return_model,\n",
    "                             save_trials=save_trials, trials=trials,\n",
    "                             results_dir=results_dir,\n",
    "                             step_save_progress=step_save_progress,\n",
    "                             loss_kwargs=loss_kwargs or {}, verbose=verbose)\n",
    "\n",
    "    fmin(fmin_objective, space=space, algo=tpe.suggest, max_evals=hyperopt_max_evals, trials=trials, verbose=verbose)\n",
    "\n",
    "    # Saves final trials\n",
    "    if save_trials:\n",
    "        trials_file = f'{results_dir}/trials.p'\n",
    "        with open(trials_file, \"wb\") as f:\n",
    "            pickle.dump(trials, f)\n",
    "\n",
    "    if return_model:\n",
    "        # best_model.ckpt is generated with evaluate_model\n",
    "        model = MODEL_DICT[space['model']].load_from_checkpoint(f'{results_dir}/best_model.ckpt')\n",
    "        return model, trials\n",
    "    else:\n",
    "        return trials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Utils Examples\n",
    ">This part of notebook shows simple use of functions given in this notebook. EPF dataset in used in this experiments. Two experiments are shown where hyperparameter tunning is runned for two models NHITS and NBEATS.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "\n",
    "from neuralforecast.losses.numpy import mae, rmse\n",
    "from neuralforecast.auto import nhits_space, nbeats_space\n",
    "from neuralforecast.data.datasets.epf import EPF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ['NP']\n",
    "Y_df, X_df, S_df = EPF.load_groups(directory='data', groups=dataset)\n",
    "X_df = X_df[['unique_id', 'ds', 'week_day']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Returning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.018776 seconds\n",
      "INFO:hyperopt.tpe:TPE using 0 trials\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/Users/cchallu/opt/anaconda3/envs/neuralforecast/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "/Users/cchallu/opt/anaconda3/envs/neuralforecast/lib/python3.7/site-packages/torch/nn/functional.py:3635: UserWarning: Default upsampling behavior when mode=linear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode)\n",
      "/Users/cchallu/opt/anaconda3/envs/neuralforecast/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "/Users/cchallu/NIXTLA/neuralforecast/neuralforecast/data/tsloader.py:47: UserWarning: This class wraps the pytorch `DataLoader` with a special collate function. If you want to use yours simply use `DataLoader`. Removing collate_fn\n",
      "  'This class wraps the pytorch `DataLoader` with a '\n",
      "/Users/cchallu/opt/anaconda3/envs/neuralforecast/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, predict_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.025232 seconds\n",
      "INFO:hyperopt.tpe:TPE using 1/1 trials with best loss 798.387024\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    }
   ],
   "source": [
    "space = nhits_space(n_time_out=24, n_series=1, n_x=1, n_s=0, frequency='H')\n",
    "space['max_steps'] = hp.choice('max_steps', [1]) # Override max_steps for faster example\n",
    "model, trials = hyperopt_tunning(space=space, hyperopt_max_evals=2, loss_function_val=mae,\n",
    "                                 loss_functions_test={'mae': mae, 'rmse': rmse},\n",
    "                                 S_df=S_df, Y_df=Y_df, X_df=X_df, f_cols=[],\n",
    "                                 ds_in_val=7*24, ds_in_test=7*24, \n",
    "                                 return_forecasts=True, return_model=True, save_trials=True, \n",
    "                                 results_dir='./results/example', loss_kwargs={}, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NHITS(\n",
       "  (model): _NHITS(\n",
       "    (blocks): ModuleList(\n",
       "      (0): _NHITSBlock(\n",
       "        (pooling_layer): MaxPool1d(kernel_size=16, stride=16, padding=0, dilation=1, ceil_mode=True)\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=75, out_features=1024, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (3): ReLU()\n",
       "          (4): Linear(in_features=1024, out_features=49, bias=True)\n",
       "        )\n",
       "        (basis): _IdentityBasis()\n",
       "      )\n",
       "      (1): _NHITSBlock(\n",
       "        (pooling_layer): MaxPool1d(kernel_size=16, stride=16, padding=0, dilation=1, ceil_mode=True)\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=75, out_features=1024, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (3): ReLU()\n",
       "          (4): Linear(in_features=1024, out_features=49, bias=True)\n",
       "        )\n",
       "        (basis): _IdentityBasis()\n",
       "      )\n",
       "      (2): _NHITSBlock(\n",
       "        (pooling_layer): MaxPool1d(kernel_size=16, stride=16, padding=0, dilation=1, ceil_mode=True)\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=75, out_features=1024, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (3): ReLU()\n",
       "          (4): Linear(in_features=1024, out_features=49, bias=True)\n",
       "        )\n",
       "        (basis): _IdentityBasis()\n",
       "      )\n",
       "      (3): _NHITSBlock(\n",
       "        (pooling_layer): MaxPool1d(kernel_size=8, stride=8, padding=0, dilation=1, ceil_mode=True)\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=78, out_features=1024, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (3): ReLU()\n",
       "          (4): Linear(in_features=1024, out_features=50, bias=True)\n",
       "        )\n",
       "        (basis): _IdentityBasis()\n",
       "      )\n",
       "      (4): _NHITSBlock(\n",
       "        (pooling_layer): MaxPool1d(kernel_size=8, stride=8, padding=0, dilation=1, ceil_mode=True)\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=78, out_features=1024, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (3): ReLU()\n",
       "          (4): Linear(in_features=1024, out_features=50, bias=True)\n",
       "        )\n",
       "        (basis): _IdentityBasis()\n",
       "      )\n",
       "      (5): _NHITSBlock(\n",
       "        (pooling_layer): MaxPool1d(kernel_size=8, stride=8, padding=0, dilation=1, ceil_mode=True)\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=78, out_features=1024, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (3): ReLU()\n",
       "          (4): Linear(in_features=1024, out_features=50, bias=True)\n",
       "        )\n",
       "        (basis): _IdentityBasis()\n",
       "      )\n",
       "      (6): _NHITSBlock(\n",
       "        (pooling_layer): MaxPool1d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=True)\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=120, out_features=1024, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (3): ReLU()\n",
       "          (4): Linear(in_features=1024, out_features=72, bias=True)\n",
       "        )\n",
       "        (basis): _IdentityBasis()\n",
       "      )\n",
       "      (7): _NHITSBlock(\n",
       "        (pooling_layer): MaxPool1d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=True)\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=120, out_features=1024, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (3): ReLU()\n",
       "          (4): Linear(in_features=1024, out_features=72, bias=True)\n",
       "        )\n",
       "        (basis): _IdentityBasis()\n",
       "      )\n",
       "      (8): _NHITSBlock(\n",
       "        (pooling_layer): MaxPool1d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=True)\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=120, out_features=1024, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (3): ReLU()\n",
       "          (4): Linear(in_features=1024, out_features=72, bias=True)\n",
       "        )\n",
       "        (basis): _IdentityBasis()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without returning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.015480 seconds\n",
      "INFO:hyperopt.tpe:TPE using 0 trials\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/Users/cchallu/opt/anaconda3/envs/neuralforecast/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "/Users/cchallu/opt/anaconda3/envs/neuralforecast/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "/Users/cchallu/NIXTLA/neuralforecast/neuralforecast/data/tsloader.py:47: UserWarning: This class wraps the pytorch `DataLoader` with a special collate function. If you want to use yours simply use `DataLoader`. Removing collate_fn\n",
      "  'This class wraps the pytorch `DataLoader` with a '\n",
      "/Users/cchallu/opt/anaconda3/envs/neuralforecast/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, predict_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.017592 seconds\n",
      "INFO:hyperopt.tpe:TPE using 1/1 trials with best loss 23.877501\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    }
   ],
   "source": [
    "space = nbeats_space(n_time_out=24, n_series=1, n_x=1, n_s=0, frequency='H')\n",
    "space['max_steps'] = hp.choice('max_steps', [1]) # Override max_steps for faster example\n",
    "trials = hyperopt_tunning(space=space, hyperopt_max_evals=2, loss_function_val=mae,\n",
    "                          loss_functions_test={'mae': mae, 'rmse': rmse},\n",
    "                          S_df=S_df, Y_df=Y_df, X_df=X_df, f_cols=[],\n",
    "                          ds_in_val=7*24, ds_in_test=7*24, \n",
    "                          return_forecasts=True, return_model=False, save_trials=False, \n",
    "                          results_dir=None, loss_kwargs={}, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
