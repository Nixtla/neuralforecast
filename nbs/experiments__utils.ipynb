{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp experiments.utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Utils\n",
    "> Set of functions to easily perform experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "ENV_VARS = dict(OMP_NUM_THREADS='2',\n",
    "                OPENBLAS_NUM_THREADS='2',\n",
    "                MKL_NUM_THREADS='3',\n",
    "                VECLIB_MAXIMUM_THREADS='2',\n",
    "                NUMEXPR_NUM_THREADS='3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import os\n",
    "import pickle\n",
    "# Limit number of threads in numpy and others to avoid throttling\n",
    "os.environ.update(ENV_VARS)\n",
    "import random\n",
    "import time\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import torch as t\n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from neuralforecast.data.scalers import Scaler\n",
    "from neuralforecast.data.tsdataset import TimeSeriesDataset, WindowsDataset, IterateWindowsDataset\n",
    "from neuralforecast.data.tsloader import TimeSeriesLoader\n",
    "from neuralforecast.models.esrnn.esrnn import ESRNN\n",
    "from neuralforecast.models.esrnn.mqesrnn import MQESRNN\n",
    "from neuralforecast.models.nbeats.nbeats import NBEATS\n",
    "from neuralforecast.models.nhits.nhits import NHITS\n",
    "from neuralforecast.models.transformer.autoformer import Autoformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_mask_dfs(Y_df, ds_in_val, ds_in_test):\n",
    "    # train mask\n",
    "    train_mask_df = Y_df.copy()[['unique_id', 'ds']]\n",
    "    train_mask_df.sort_values(by=['unique_id', 'ds'], inplace=True)\n",
    "    train_mask_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    train_mask_df['sample_mask'] = 1\n",
    "    train_mask_df['available_mask'] = 1\n",
    "    \n",
    "    idx_out = train_mask_df.groupby('unique_id').tail(ds_in_val+ds_in_test).index\n",
    "    train_mask_df.loc[idx_out, 'sample_mask'] = 0\n",
    "    \n",
    "    # test mask\n",
    "    test_mask_df = train_mask_df.copy()\n",
    "    test_mask_df['sample_mask'] = 0\n",
    "    idx_test = test_mask_df.groupby('unique_id').tail(ds_in_test).index\n",
    "    test_mask_df.loc[idx_test, 'sample_mask'] = 1\n",
    "    \n",
    "    # validation mask\n",
    "    val_mask_df = train_mask_df.copy()\n",
    "    val_mask_df['sample_mask'] = 1\n",
    "    val_mask_df['sample_mask'] = val_mask_df['sample_mask'] - train_mask_df['sample_mask']\n",
    "    val_mask_df['sample_mask'] = val_mask_df['sample_mask'] - test_mask_df['sample_mask']\n",
    "\n",
    "    assert len(train_mask_df)==len(Y_df), \\\n",
    "        f'The mask_df length {len(train_mask_df)} is not equal to Y_df length {len(Y_df)}'\n",
    "    \n",
    "    return train_mask_df, val_mask_df, test_mask_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_random_mask_dfs(Y_df, ds_in_test, \n",
    "                        n_val_windows, n_ds_val_window,\n",
    "                        n_uids, freq):\n",
    "    \"\"\"\n",
    "    Generates train, test and random validation mask.\n",
    "    Train mask begins by avoiding ds_in_test\n",
    "    \n",
    "    Validation mask: 1) samples n_uids unique ids\n",
    "                     2) creates windows of size n_ds_val_window\n",
    "    Parameters\n",
    "    ----------\n",
    "    ds_in_test: int\n",
    "        Number of ds in test.\n",
    "    n_uids: int\n",
    "        Number of unique ids in validation.\n",
    "    n_val_windows: int\n",
    "        Number of windows for validation.\n",
    "    n_ds_val_window: int\n",
    "        Number of ds in each validation window.\n",
    "    periods: int  \n",
    "        ds_in_test multiplier.\n",
    "    freq: str\n",
    "        string that determines datestamp frequency, used in\n",
    "        random windows creation.\n",
    "    \"\"\"\n",
    "    np.random.seed(1)\n",
    "    #----------------------- Train mask -----------------------#\n",
    "    # Initialize masks\n",
    "    train_mask_df, val_mask_df, test_mask_df = get_mask_dfs(Y_df=Y_df,\n",
    "                                                            ds_in_val=0,\n",
    "                                                            ds_in_test=ds_in_test)\n",
    "    \n",
    "    assert val_mask_df['sample_mask'].sum()==0, 'Muerte'\n",
    "    \n",
    "    #----------------- Random Validation mask -----------------#\n",
    "    # Overwrite validation with random windows\n",
    "    uids = train_mask_df['unique_id'].unique()\n",
    "    val_uids = np.random.choice(uids, n_uids, replace=False)\n",
    "    \n",
    "    # Validation avoids test\n",
    "    idx_test = train_mask_df.groupby('unique_id').tail(ds_in_test).index\n",
    "    available_ds = train_mask_df.loc[~train_mask_df.index.isin(idx_test)]['ds'].unique()\n",
    "    val_init_ds = np.random.choice(available_ds, n_val_windows, replace=False)\n",
    "    \n",
    "    # Creates windows \n",
    "    val_ds = [pd.date_range(init, periods=n_ds_val_window, freq=freq) for init in val_init_ds]\n",
    "    val_ds = np.concatenate(val_ds)\n",
    "\n",
    "    # Cleans random windows from train mask\n",
    "    val_idx = train_mask_df.query('unique_id in @val_uids & ds in @val_ds').index\n",
    "    train_mask_df.loc[val_idx, 'sample_mask'] = 0\n",
    "    val_mask_df.loc[val_idx, 'sample_mask'] = 1\n",
    "    \n",
    "    return train_mask_df, val_mask_df, test_mask_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def scale_data(Y_df, X_df, mask_df, normalizer_y, normalizer_x):\n",
    "    mask = mask_df['available_mask'].values * mask_df['sample_mask'].values\n",
    "    \n",
    "    if normalizer_y is not None:\n",
    "        scaler_y = Scaler(normalizer=normalizer_y)\n",
    "        Y_df['y'] = scaler_y.scale(x=Y_df['y'].values, mask=mask)\n",
    "    else:\n",
    "        scaler_y = None\n",
    "\n",
    "    if normalizer_x is not None:\n",
    "        X_cols = [col for col in X_df.columns if col not in ['unique_id','ds']]\n",
    "        for col in X_cols:\n",
    "            scaler_x = Scaler(normalizer=normalizer_x)\n",
    "            X_df[col] = scaler_x.scale(x=X_df[col].values, mask=mask)\n",
    "\n",
    "    return Y_df, X_df, scaler_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def create_datasets(mc, S_df, Y_df, X_df, f_cols,\n",
    "                    ds_in_test, ds_in_val):\n",
    "    #------------------------------------- Available and Validation Mask ------------------------------------#\n",
    "    train_mask_df, valid_mask_df, test_mask_df = get_mask_dfs(Y_df=Y_df,\n",
    "                                                              ds_in_val=ds_in_val,\n",
    "                                                              ds_in_test=ds_in_test)\n",
    "\n",
    "    #---------------------------------------------- Scale Data ----------------------------------------------#\n",
    "    Y_df, X_df, scaler_y = scale_data(Y_df=Y_df, X_df=X_df, mask_df=train_mask_df,\n",
    "                                      normalizer_y=mc['normalizer_y'], normalizer_x=mc['normalizer_x'])\n",
    "\n",
    "    #----------------------------------------- Declare Dataset and Loaders ----------------------------------#\n",
    "    \n",
    "    if mc['mode'] == 'simple':\n",
    "        train_dataset = WindowsDataset(S_df=S_df, Y_df=Y_df, X_df=X_df,\n",
    "                                       mask_df=train_mask_df, f_cols=f_cols,\n",
    "                                       input_size=int(mc['n_time_in']),\n",
    "                                       output_size=int(mc['n_time_out']),\n",
    "                                       sample_freq=int(mc['idx_to_sample_freq']),\n",
    "                                       complete_windows=mc['complete_windows'],\n",
    "                                       verbose=True)\n",
    "        \n",
    "        valid_dataset = WindowsDataset(S_df=S_df, Y_df=Y_df, X_df=X_df,\n",
    "                                       mask_df=valid_mask_df, f_cols=f_cols,\n",
    "                                       input_size=int(mc['n_time_in']),\n",
    "                                       output_size=int(mc['n_time_out']),\n",
    "                                       sample_freq=int(mc['val_idx_to_sample_freq']),\n",
    "                                       complete_windows=True,\n",
    "                                       verbose=True)\n",
    "        \n",
    "        test_dataset = WindowsDataset(S_df=S_df, Y_df=Y_df, X_df=X_df,\n",
    "                                      mask_df=test_mask_df, f_cols=f_cols,\n",
    "                                      input_size=int(mc['n_time_in']),\n",
    "                                      output_size=int(mc['n_time_out']),\n",
    "                                      sample_freq=int(mc['val_idx_to_sample_freq']),\n",
    "                                      complete_windows=True,\n",
    "                                      verbose=True)\n",
    "    if mc['mode'] == 'iterate_windows':\n",
    "        train_dataset = IterateWindowsDataset(S_df=S_df, Y_df=Y_df, X_df=X_df,\n",
    "                                              mask_df=train_mask_df, f_cols=f_cols,\n",
    "                                              input_size=int(mc['n_time_in']),\n",
    "                                              output_size=int(mc['n_time_out']),\n",
    "                                              verbose=True)\n",
    "        \n",
    "        valid_dataset = IterateWindowsDataset(S_df=S_df, Y_df=Y_df, X_df=X_df,\n",
    "                                              mask_df=valid_mask_df, f_cols=f_cols,\n",
    "                                              input_size=int(mc['n_time_in']),\n",
    "                                              output_size=int(mc['n_time_out']),\n",
    "                                              verbose=True)\n",
    "        \n",
    "        test_dataset = IterateWindowsDataset(S_df=S_df, Y_df=Y_df, X_df=X_df,\n",
    "                                             mask_df=test_mask_df, f_cols=f_cols,\n",
    "                                             input_size=int(mc['n_time_in']),\n",
    "                                             output_size=int(mc['n_time_out']),\n",
    "                                             verbose=True)   \n",
    "    \n",
    "    if mc['mode'] == 'full':\n",
    "        train_dataset = TimeSeriesDataset(S_df=S_df, Y_df=Y_df, X_df=X_df,\n",
    "                                          mask_df=train_mask_df, f_cols=f_cols,\n",
    "                                          input_size=int(mc['n_time_in']),\n",
    "                                          output_size=int(mc['n_time_out']),\n",
    "                                          verbose=True)\n",
    "        \n",
    "        valid_dataset = TimeSeriesDataset(S_df=S_df, Y_df=Y_df, X_df=X_df,\n",
    "                                          mask_df=valid_mask_df, f_cols=f_cols,\n",
    "                                          input_size=int(mc['n_time_in']),\n",
    "                                          output_size=int(mc['n_time_out']),\n",
    "                                          verbose=True)\n",
    "        \n",
    "        test_dataset = TimeSeriesDataset(S_df=S_df, Y_df=Y_df, X_df=X_df,\n",
    "                                         mask_df=test_mask_df, f_cols=f_cols,\n",
    "                                         input_size=int(mc['n_time_in']),\n",
    "                                         output_size=int(mc['n_time_out']),\n",
    "                                         verbose=True)        \n",
    "    \n",
    "    if ds_in_test == 0:\n",
    "        test_dataset = None\n",
    "\n",
    "    return train_dataset, valid_dataset, test_dataset, scaler_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def instantiate_loaders(mc, train_dataset, val_dataset, test_dataset):\n",
    "    if mc['mode'] in ['simple', 'full'] :\n",
    "        train_loader = TimeSeriesLoader(dataset=train_dataset,\n",
    "                                        batch_size=int(mc['batch_size']),\n",
    "                                        n_windows=int(mc['n_windows']),\n",
    "                                        eq_batch_size=False,\n",
    "                                        shuffle=True)\n",
    "        if val_dataset is not None:\n",
    "            val_loader = TimeSeriesLoader(dataset=val_dataset,\n",
    "                                        batch_size=1,\n",
    "                                        shuffle=False)\n",
    "        else:\n",
    "            val_loader = None\n",
    "\n",
    "        if test_dataset is not None:\n",
    "            test_loader = TimeSeriesLoader(dataset=test_dataset,\n",
    "                                        batch_size=1,\n",
    "                                        shuffle=False)\n",
    "        else:\n",
    "            test_loader = None\n",
    "\n",
    "    elif mc['mode'] == 'iterate_windows':\n",
    "        train_loader =DataLoader(dataset=train_dataset,\n",
    "                                 batch_size=int(mc['batch_size']),\n",
    "                                 shuffle=True,\n",
    "                                 drop_last=True)\n",
    "\n",
    "        if val_dataset is not None:\n",
    "            val_loader = DataLoader(dataset=val_dataset,\n",
    "                                    batch_size=1,\n",
    "                                    shuffle=False)\n",
    "        else:\n",
    "            val_loader = None\n",
    "\n",
    "        if test_dataset is not None:\n",
    "            test_loader = DataLoader(dataset=test_dataset,\n",
    "                                     batch_size=1,\n",
    "                                     shuffle=False)\n",
    "        else:\n",
    "            test_loader = None\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def instantiate_nbeats(mc):\n",
    "    mc['n_mlp_units'] = len(mc['stack_types']) * [ mc['n_layers'] * [int(mc['n_mlp_units'])] ]\n",
    "    mc['n_layers'] =  len(mc['stack_types']) * [ mc['n_layers'] ]\n",
    "    mc['n_blocks'] =  len(mc['stack_types']) * [ mc['n_blocks'] ]\n",
    "\n",
    "    if mc['max_epochs'] is not None:\n",
    "        lr_decay_step_size = int(mc['max_epochs'] / mc['n_lr_decays'])\n",
    "    elif mc['max_steps'] is not None:\n",
    "        lr_decay_step_size = int(mc['max_steps'] / mc['n_lr_decays'])\n",
    "\n",
    "    model = NBEATS(n_time_in=int(mc['n_time_in']),\n",
    "                  n_time_out=int(mc['n_time_out']),\n",
    "                  n_x=mc['n_x'],\n",
    "                  n_s=mc['n_s'],\n",
    "                  n_s_hidden=int(mc['n_s_hidden']),\n",
    "                  n_x_hidden=int(mc['n_x_hidden']),\n",
    "                  shared_weights = mc['shared_weights'],\n",
    "                  initialization=mc['initialization'],\n",
    "                  activation=mc['activation'],\n",
    "                  stack_types=mc['stack_types'],\n",
    "                  n_blocks=mc['n_blocks'],\n",
    "                  n_layers=mc['n_layers'],\n",
    "                  n_mlp_units=mc['n_mlp_units'],\n",
    "                  batch_normalization = mc['batch_normalization'],\n",
    "                  dropout_prob_theta=mc['dropout_prob_theta'],\n",
    "                  learning_rate=float(mc['learning_rate']),\n",
    "                  lr_decay=float(mc['lr_decay']),\n",
    "                  lr_decay_step_size=lr_decay_step_size,\n",
    "                  weight_decay=mc['weight_decay'],\n",
    "                  loss_train=mc['loss_train'],\n",
    "                  loss_hypar=float(mc['loss_hypar']),\n",
    "                  loss_valid=mc['loss_valid'],\n",
    "                  frequency=mc['frequency'],\n",
    "                  random_seed=int(mc['random_seed']))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def instantiate_esrnn(mc):    \n",
    "    if mc['max_epochs'] is not None:\n",
    "        lr_decay_step_size = int(mc['max_epochs'] / mc['n_lr_decays'])\n",
    "    elif mc['max_steps'] is not None:\n",
    "        lr_decay_step_size = int(mc['max_steps'] / mc['n_lr_decays'])\n",
    "    model = ESRNN(# Architecture parameters\n",
    "                  n_series=mc['n_series'],\n",
    "                  n_x=mc['n_x'],\n",
    "                  n_s=mc['n_s'],\n",
    "                  sample_freq=int(mc['sample_freq']),\n",
    "                  input_size=int(mc['n_time_in']),\n",
    "                  output_size=int(mc['n_time_out']),\n",
    "                  es_component=mc['es_component'],\n",
    "                  cell_type=mc['cell_type'],\n",
    "                  state_hsize=int(mc['state_hsize']),\n",
    "                  dilations=mc['dilations'],\n",
    "                  add_nl_layer=mc['add_nl_layer'],\n",
    "                  # Optimization parameters                \n",
    "                  learning_rate=mc['learning_rate'],\n",
    "                  lr_scheduler_step_size=lr_decay_step_size,\n",
    "                  lr_decay=mc['lr_decay'],\n",
    "                  per_series_lr_multip=mc['per_series_lr_multip'],\n",
    "                  gradient_eps=mc['gradient_eps'],\n",
    "                  gradient_clipping_threshold=mc['gradient_clipping_threshold'],\n",
    "                  rnn_weight_decay=mc['rnn_weight_decay'],\n",
    "                  noise_std=mc['noise_std'],\n",
    "                  level_variability_penalty=mc['level_variability_penalty'],\n",
    "                  testing_percentile=mc['testing_percentile'],\n",
    "                  training_percentile=mc['training_percentile'],\n",
    "                  loss=mc['loss_train'],\n",
    "                  val_loss=mc['loss_valid'],\n",
    "                  seasonality=mc['seasonality']) \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def instantiate_mqesrnn(mc): \n",
    "    if mc['max_epochs'] is not None:\n",
    "        lr_decay_step_size = int(mc['max_epochs'] / mc['n_lr_decays'])\n",
    "    elif mc['max_steps'] is not None:\n",
    "        lr_decay_step_size = int(mc['max_steps'] / mc['n_lr_decays'])   \n",
    "    model = MQESRNN(# Architecture parameters\n",
    "                    n_series=mc['n_series'],\n",
    "                    n_x=mc['n_x'],\n",
    "                    n_s=mc['n_s'],\n",
    "                    idx_to_sample_freq=int(mc['idx_to_sample_freq']),\n",
    "                    input_size=int(mc['n_time_in']),\n",
    "                    output_size=int(mc['n_time_out']),\n",
    "                    es_component=mc['es_component'],\n",
    "                    cell_type=mc['cell_type'],\n",
    "                    state_hsize=int(mc['state_hsize']),\n",
    "                    dilations=mc['dilations'],\n",
    "                    add_nl_layer=mc['add_nl_layer'],\n",
    "                    # Optimization parameters                 \n",
    "                    learning_rate=mc['learning_rate'],\n",
    "                    lr_scheduler_step_size=lr_decay_step_size,\n",
    "                    lr_decay=mc['lr_decay'],\n",
    "                    gradient_eps=mc['gradient_eps'],\n",
    "                    gradient_clipping_threshold=mc['gradient_clipping_threshold'],\n",
    "                    rnn_weight_decay=mc['rnn_weight_decay'],\n",
    "                    noise_std=mc['noise_std'],\n",
    "                    testing_percentiles=list(mc['testing_percentiles']),\n",
    "                    training_percentiles=list(mc['training_percentiles']),\n",
    "                    loss=mc['loss_train'],\n",
    "                    val_loss=mc['loss_valid']) \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def instantiate_nhits(mc):\n",
    "    mc['n_mlp_units'] = len(mc['stack_types']) * [ mc['n_layers'] * [int(mc['n_mlp_units'])] ]\n",
    "    mc['n_layers'] =  len(mc['stack_types']) * [ mc['n_layers'] ]\n",
    "    mc['n_blocks'] =  len(mc['stack_types']) * [ mc['n_blocks'] ]\n",
    "\n",
    "    if mc['max_epochs'] is not None:\n",
    "        lr_decay_step_size = int(mc['max_epochs'] / mc['n_lr_decays'])\n",
    "    elif mc['max_steps'] is not None:\n",
    "        lr_decay_step_size = int(mc['max_steps'] / mc['n_lr_decays'])\n",
    "\n",
    "    model = NHITS(n_time_in=int(mc['n_time_in']),\n",
    "                  n_time_out=int(mc['n_time_out']),\n",
    "                  n_x=mc['n_x'],\n",
    "                  n_s=mc['n_s'],\n",
    "                  n_s_hidden=int(mc['n_s_hidden']),\n",
    "                  n_x_hidden=int(mc['n_x_hidden']),\n",
    "                  shared_weights = mc['shared_weights'],\n",
    "                  initialization=mc['initialization'],\n",
    "                  activation=mc['activation'],\n",
    "                  stack_types=mc['stack_types'],\n",
    "                  n_blocks=mc['n_blocks'],\n",
    "                  n_layers=mc['n_layers'],\n",
    "                  n_mlp_units=mc['n_mlp_units'],\n",
    "                  n_pool_kernel_size=mc['n_pool_kernel_size'],\n",
    "                  n_freq_downsample=mc['n_freq_downsample'],\n",
    "                  pooling_mode=mc['pooling_mode'],\n",
    "                  interpolation_mode=mc['interpolation_mode'],\n",
    "                  batch_normalization = mc['batch_normalization'],\n",
    "                  dropout_prob_theta=mc['dropout_prob_theta'],\n",
    "                  learning_rate=float(mc['learning_rate']),\n",
    "                  lr_decay=float(mc['lr_decay']),\n",
    "                  lr_decay_step_size=lr_decay_step_size,\n",
    "                  weight_decay=mc['weight_decay'],\n",
    "                  loss_train=mc['loss_train'],\n",
    "                  loss_hypar=float(mc['loss_hypar']),\n",
    "                  loss_valid=mc['loss_valid'],\n",
    "                  frequency=mc['frequency'],\n",
    "                  random_seed=int(mc['random_seed']))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def instantiate_autoformer(mc):\n",
    "\n",
    "    if mc['max_epochs'] is not None:\n",
    "        lr_decay_step_size = int(mc['max_epochs'] / mc['n_lr_decays'])\n",
    "    elif mc['max_steps'] is not None:\n",
    "        lr_decay_step_size = int(mc['max_steps'] / mc['n_lr_decays'])\n",
    "    \n",
    "    model = Autoformer(seq_len=int(mc['seq_len']),\n",
    "                       label_len=int(mc['label_len']),\n",
    "                       pred_len=int(mc['pred_len']),\n",
    "                       output_attention=mc['output_attention'],\n",
    "                       enc_in=int(mc['enc_in']),\n",
    "                       dec_in=int(mc['dec_in']),\n",
    "                       d_model=int(mc['d_model']),\n",
    "                       c_out=int(mc['c_out']),\n",
    "                       embed = mc['embed'],\n",
    "                       freq=mc['freq'],\n",
    "                       dropout=mc['dropout'],\n",
    "                       factor=mc['factor'],\n",
    "                       n_heads=int(mc['n_heads']),\n",
    "                       d_ff=int(mc['d_ff']),\n",
    "                       moving_avg=int(mc['moving_avg']),\n",
    "                       activation=mc['activation'],\n",
    "                       e_layers=int(mc['e_layers']),\n",
    "                       d_layers=int(mc['d_layers']),\n",
    "                       learning_rate=float(mc['learning_rate']),\n",
    "                       lr_decay=float(mc['lr_decay']),\n",
    "                       lr_decay_step_size=lr_decay_step_size,\n",
    "                       weight_decay=mc['weight_decay'],\n",
    "                       loss_train=mc['loss_train'],\n",
    "                       loss_hypar=float(mc['loss_hypar']),\n",
    "                       loss_valid=mc['loss_valid'],\n",
    "                       random_seed=int(mc['random_seed']))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def instantiate_model(mc):\n",
    "    MODEL_DICT = {'nbeats': instantiate_nbeats,\n",
    "                  'esrnn': instantiate_esrnn,\n",
    "                  'mqesrnn': instantiate_mqesrnn,\n",
    "                  'nhits': instantiate_nhits,\n",
    "                  'autoformer': instantiate_autoformer}\n",
    "    return MODEL_DICT[mc['model']](mc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def predict(mc, model, trainer, loader, scaler_y):\n",
    "    outputs = trainer.predict(model, loader)\n",
    "    y_true, y_hat, mask = [t.cat(output).cpu().numpy() for output in zip(*outputs)]\n",
    "    meta_data = loader.dataset.meta_data\n",
    "\n",
    "    # Scale to original scale\n",
    "    if mc['normalizer_y'] is not None:\n",
    "        y_true_shape = y_true.shape\n",
    "        y_true = scaler_y.inv_scale(x=y_true.flatten())\n",
    "        y_true = np.reshape(y_true, y_true_shape)\n",
    "\n",
    "        y_hat = scaler_y.inv_scale(x=y_hat.flatten())\n",
    "        y_hat = np.reshape(y_hat, y_true_shape)\n",
    "\n",
    "    return y_true, y_hat, mask, meta_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def model_fit_predict(mc, S_df, Y_df, X_df, f_cols, ds_in_val, ds_in_test):\n",
    "    \n",
    "    # Protect inplace modifications\n",
    "    Y_df = Y_df.copy()\n",
    "    if X_df is not None:\n",
    "        X_df = X_df.copy()\n",
    "    if S_df is not None:\n",
    "        S_df = S_df.copy()        \n",
    "\n",
    "    #----------------------------------------------- Datasets -----------------------------------------------#\n",
    "    train_dataset, val_dataset, test_dataset, scaler_y = create_datasets(mc=mc,\n",
    "                                                                         S_df=S_df, Y_df=Y_df, X_df=X_df,\n",
    "                                                                         f_cols=f_cols,\n",
    "                                                                         ds_in_val=ds_in_val,\n",
    "                                                                         ds_in_test=ds_in_test)\n",
    "    mc['n_x'], mc['n_s'] = train_dataset.get_n_variables()\n",
    "\n",
    "    #------------------------------------------- Instantiate & fit -------------------------------------------#\n",
    "    train_loader, val_loader, test_loader = instantiate_loaders(mc=mc,\n",
    "                                                                train_dataset=train_dataset,\n",
    "                                                                val_dataset=val_dataset,\n",
    "                                                                test_dataset=test_dataset)\n",
    "    model = instantiate_model(mc=mc)\n",
    "    callbacks = []\n",
    "    if mc['early_stop_patience']:\n",
    "        early_stopping = pl.callbacks.EarlyStopping(monitor='val_loss', min_delta=1e-4, \n",
    "                                                    patience=mc['early_stop_patience'],\n",
    "                                                    verbose=True, \n",
    "                                                    mode='min') \n",
    "        callbacks=[early_stopping]\n",
    "    \n",
    "    gpus = -1 if t.cuda.is_available() else 0\n",
    "    trainer = pl.Trainer(max_epochs=mc['max_epochs'], \n",
    "                         max_steps=mc['max_steps'],\n",
    "                         check_val_every_n_epoch=mc['eval_freq'],\n",
    "                         progress_bar_refresh_rate=1,\n",
    "                         gpus=gpus,\n",
    "                         callbacks=callbacks,\n",
    "                         checkpoint_callback=False,\n",
    "                         logger=False)\n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "    #------------------------------------------------ Predict ------------------------------------------------#\n",
    "    results = {}\n",
    "\n",
    "    if ds_in_val > 0:\n",
    "        y_true, y_hat, mask, meta_data = predict(mc, model, trainer, val_loader, scaler_y)\n",
    "        val_values = (('val_y_true', y_true), ('val_y_hat', y_hat), ('val_mask', mask), ('val_meta_data', meta_data))\n",
    "        results.update(val_values)\n",
    "\n",
    "        print(f\"VAL y_true.shape: {y_true.shape}\")\n",
    "        print(f\"VAL y_hat.shape: {y_hat.shape}\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "    # Predict test if available\n",
    "    if ds_in_test > 0:\n",
    "        y_true, y_hat, mask, meta_data = predict(mc, model, trainer, test_loader, scaler_y)\n",
    "        test_values = (('test_y_true', y_true), ('test_y_hat', y_hat), ('test_mask', mask), ('test_meta_data', meta_data))\n",
    "        results.update(test_values)\n",
    "\n",
    "        print(f\"TEST y_true.shape: {y_true.shape}\")\n",
    "        print(f\"TEST y_hat.shape: {y_hat.shape}\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def evaluate_model(mc, loss_function_val, loss_functions_test, \n",
    "                   S_df, Y_df, X_df, f_cols,\n",
    "                   ds_in_val, ds_in_test,\n",
    "                   return_forecasts,\n",
    "                   save_progress,\n",
    "                   trials,\n",
    "                   results_file,\n",
    "                   step_save_progress=5,\n",
    "                   loss_kwargs=None):\n",
    "\n",
    "    if (save_progress) and (len(trials) % step_save_progress == 0):\n",
    "        with open(results_file, \"wb\") as f:\n",
    "            pickle.dump(trials, f)\n",
    "    \n",
    "    print(47*'=' + '\\n')\n",
    "    print(pd.Series(mc))\n",
    "    print(47*'=' + '\\n')\n",
    "    \n",
    "    # Some asserts due to work in progress\n",
    "    n_series = Y_df['unique_id'].nunique()\n",
    "    if n_series > 1:\n",
    "        assert mc['normalizer_y'] is None, 'Data scaling not implemented with multiple time series'\n",
    "        assert mc['normalizer_x'] is None, 'Data scaling not implemented with multiple time series'\n",
    "\n",
    "    assert ds_in_test % mc['val_idx_to_sample_freq']==0, 'outsample size should be multiple of val_idx_to_sample_freq'\n",
    "\n",
    "    # Make predictions\n",
    "    start = time.time()\n",
    "    results = model_fit_predict(mc=mc,\n",
    "                                S_df=S_df, \n",
    "                                Y_df=Y_df,\n",
    "                                X_df=X_df,\n",
    "                                f_cols=f_cols,\n",
    "                                ds_in_val=ds_in_val,\n",
    "                                ds_in_test=ds_in_test)\n",
    "    run_time = time.time() - start\n",
    "\n",
    "    # Evaluate predictions\n",
    "    val_loss = loss_function_val(y=results['val_y_true'], y_hat=results['val_y_hat'], weights=results['val_mask'], **loss_kwargs)\n",
    "\n",
    "    results_output = {'loss': val_loss,\n",
    "                      'mc': mc,\n",
    "                      'run_time': run_time,\n",
    "                      'status': STATUS_OK}\n",
    "\n",
    "    # Evaluation in test (if provided)\n",
    "    if ds_in_test > 0:\n",
    "        test_loss_dict = {}\n",
    "        for loss_name, loss_function in loss_functions_test.items():\n",
    "            test_loss_dict[loss_name] = loss_function(y=results['test_y_true'], y_hat=results['test_y_hat'], weights=results['test_mask'])\n",
    "        results_output['test_losses'] = test_loss_dict\n",
    "\n",
    "    if return_forecasts and ds_in_test > 0:\n",
    "        forecasts_test = {}\n",
    "        test_values = (('test_y_true', results['test_y_true']), ('test_y_hat', results['test_y_hat']),\n",
    "                        ('test_mask', results['test_mask']), ('test_meta_data', results['test_meta_data']))\n",
    "        forecasts_test.update(test_values)\n",
    "        results_output['forecasts_test'] = forecasts_test\n",
    "\n",
    "    return results_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def hyperopt_tunning(space, hyperopt_max_evals, loss_function_val, loss_functions_test,\n",
    "                     S_df, Y_df, X_df, f_cols,\n",
    "                     ds_in_val, ds_in_test,\n",
    "                     return_forecasts,\n",
    "                     save_progress,\n",
    "                     results_file,\n",
    "                     step_save_progress=5,\n",
    "                     loss_kwargs=None):\n",
    "    assert ds_in_val > 0, 'Validation set is needed for tunning!'\n",
    "\n",
    "    trials = Trials()\n",
    "    fmin_objective = partial(evaluate_model, loss_function_val=loss_function_val, loss_functions_test=loss_functions_test,\n",
    "                             S_df=S_df, Y_df=Y_df, X_df=X_df, f_cols=f_cols,\n",
    "                             ds_in_val=ds_in_val, ds_in_test=ds_in_test,\n",
    "                             return_forecasts=return_forecasts, save_progress=save_progress, trials=trials,\n",
    "                             results_file=results_file,\n",
    "                             step_save_progress=step_save_progress,\n",
    "                             loss_kwargs=loss_kwargs or {})\n",
    "\n",
    "    fmin(fmin_objective, space=space, algo=tpe.suggest, max_evals=hyperopt_max_evals, trials=trials, verbose=True)\n",
    "\n",
    "    return trials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Utils Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from neuralforecast.losses.numpy import mae, rmse\n",
    "from neuralforecast.models.nhits.nhits import suggested_space as nhits_suggested_space\n",
    "from neuralforecast.models.nbeats.nbeats import suggested_space as nbeats_suggested_space\n",
    "from neuralforecast.data.datasets.epf import EPF, EPFInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ['NP']\n",
    "Y_df, X_df, S_df = EPF.load_groups(directory='data', groups=dataset)\n",
    "X_df = X_df[['unique_id', 'ds', 'week_day']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.015753 seconds\n",
      "INFO:hyperopt.tpe:TPE using 0 trials\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================      \n",
      "\n",
      "activation                                          ReLU\n",
      "batch_normalization                                False\n",
      "batch_size                                             1\n",
      "complete_windows                                    True\n",
      "dropout_prob_theta                                     0\n",
      "early_stop_patience                                   10\n",
      "eval_freq                                             50\n",
      "frequency                                              H\n",
      "idx_to_sample_freq                                     1\n",
      "initialization                              lecun_normal\n",
      "interpolation_mode                                linear\n",
      "learning_rate                                       0.01\n",
      "loss_hypar                                           0.5\n",
      "loss_train                                           MAE\n",
      "loss_valid                                           MAE\n",
      "lr_decay                                             0.5\n",
      "max_epochs                                          None\n",
      "max_steps                                             10\n",
      "mode                                              simple\n",
      "model                                              nhits\n",
      "n_blocks                                               3\n",
      "n_freq_downsample                            (24, 12, 1)\n",
      "n_layers                                               3\n",
      "n_lr_decays                                            3\n",
      "n_mlp_units                                          128\n",
      "n_pool_kernel_size                             (4, 4, 4)\n",
      "n_s                                                    0\n",
      "n_s_hidden                                             0\n",
      "n_time_in                                             72\n",
      "n_time_out                                            24\n",
      "n_windows                                            128\n",
      "n_x                                                    1\n",
      "n_x_hidden                                             1\n",
      "normalizer_x                                        None\n",
      "normalizer_y                                        None\n",
      "pooling_mode                                         max\n",
      "random_seed                                          7.0\n",
      "shared_weights                                     False\n",
      "stack_types               (identity, identity, identity)\n",
      "val_idx_to_sample_freq                                 1\n",
      "weight_decay                                           0\n",
      "dtype: object\n",
      "===============================================      \n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Train Validation splits\n",
      "\n",
      "INFO:root:                              ds                    \n",
      "                             min                 max\n",
      "unique_id sample_mask                               \n",
      "NP        0           2018-12-11 2018-12-24 23:00:00\n",
      "          1           2013-01-01 2018-12-10 23:00:00\n",
      "INFO:root:\n",
      "Total data \t\t\t52416 time stamps \n",
      "Available percentage=100.0, \t52416 time stamps \n",
      "Insample  percentage=99.36, \t52080 time stamps \n",
      "Outsample percentage=0.64, \t336 time stamps \n",
      "\n",
      "/Users/cchallu/NIXTLA/neuralforecast/neuralforecast/data/tsdataset.py:208: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  X.drop(['unique_id', 'ds'], 1, inplace=True)\n",
      "\n",
      "INFO:root:Train Validation splits\n",
      "\n",
      "INFO:root:                              ds                    \n",
      "                             min                 max\n",
      "unique_id sample_mask                               \n",
      "NP        0           2013-01-01 2018-12-24 23:00:00\n",
      "          1           2018-12-11 2018-12-17 23:00:00\n",
      "INFO:root:\n",
      "Total data \t\t\t52416 time stamps \n",
      "Available percentage=100.0, \t52416 time stamps \n",
      "Insample  percentage=0.32, \t168 time stamps \n",
      "Outsample percentage=99.68, \t52248 time stamps \n",
      "\n",
      "/Users/cchallu/NIXTLA/neuralforecast/neuralforecast/data/tsdataset.py:208: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  X.drop(['unique_id', 'ds'], 1, inplace=True)\n",
      "\n",
      "INFO:root:Train Validation splits\n",
      "\n",
      "INFO:root:                              ds                    \n",
      "                             min                 max\n",
      "unique_id sample_mask                               \n",
      "NP        0           2013-01-01 2018-12-17 23:00:00\n",
      "          1           2018-12-18 2018-12-24 23:00:00\n",
      "INFO:root:\n",
      "Total data \t\t\t52416 time stamps \n",
      "Available percentage=100.0, \t52416 time stamps \n",
      "Insample  percentage=0.32, \t168 time stamps \n",
      "Outsample percentage=99.68, \t52248 time stamps \n",
      "\n",
      "/Users/cchallu/NIXTLA/neuralforecast/neuralforecast/data/tsdataset.py:208: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  X.drop(['unique_id', 'ds'], 1, inplace=True)\n",
      "\n",
      "/Users/cchallu/opt/anaconda3/envs/neuralforecast/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=False)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=False)`.\n",
      "  f\"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will \"\n",
      "\n",
      "/Users/cchallu/opt/anaconda3/envs/neuralforecast/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:91: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=1)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "  f\"Setting `Trainer(progress_bar_refresh_rate={progress_bar_refresh_rate})` is deprecated in v1.5 and\"\n",
      "\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "\n",
      "  | Name  | Type   | Params\n",
      "---------------------------------\n",
      "0 | model | _NHITS | 523 K \n",
      "---------------------------------\n",
      "523 K     Trainable params\n",
      "0         Non-trainable params\n",
      "523 K     Total params\n",
      "2.095     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check: 0it [00:00, ?it/s]          \n",
      "Validation sanity check:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "  0%|          | 0/2 [00:01<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cchallu/opt/anaconda3/envs/neuralforecast/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "\n",
      "/Users/cchallu/opt/anaconda3/envs/neuralforecast/lib/python3.7/site-packages/torch/nn/functional.py:3635: UserWarning: Default upsampling behavior when mode=linear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode)\n",
      "\n",
      "/Users/cchallu/opt/anaconda3/envs/neuralforecast/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 0it [00:00, ?it/s]                         \n",
      "Training:   0%|          | 0/1 [00:00<?, ?it/s]      \n",
      "Epoch 0:   0%|          | 0/1 [00:00<?, ?it/s]       \n",
      "Epoch 0: 100%|##########| 1/1 [00:00<00:00, 10.97it/s, loss=3.74, train_loss_step=3.740]\n",
      "Epoch 0: 100%|##########| 1/1 [00:00<00:00, 10.59it/s, loss=3.74, train_loss_step=3.740, train_loss_epoch=3.740]\n",
      "Epoch 0:   0%|          | 0/1 [00:00<?, ?it/s, loss=3.74, train_loss_step=3.740, train_loss_epoch=3.740]        \n",
      "Epoch 1:   0%|          | 0/1 [00:00<?, ?it/s, loss=3.74, train_loss_step=3.740, train_loss_epoch=3.740]\n",
      "Epoch 1: 100%|##########| 1/1 [00:00<00:00, 15.75it/s, loss=47.5, train_loss_step=91.20, train_loss_epoch=3.740]\n",
      "Epoch 1: 100%|##########| 1/1 [00:00<00:00, 14.98it/s, loss=47.5, train_loss_step=91.20, train_loss_epoch=91.20]\n",
      "Epoch 1:   0%|          | 0/1 [00:00<?, ?it/s, loss=47.5, train_loss_step=91.20, train_loss_epoch=91.20]        \n",
      "Epoch 2:   0%|          | 0/1 [00:00<?, ?it/s, loss=47.5, train_loss_step=91.20, train_loss_epoch=91.20]\n",
      "Epoch 2: 100%|##########| 1/1 [00:00<00:00, 15.76it/s, loss=34.4, train_loss_step=8.380, train_loss_epoch=91.20]\n",
      "Epoch 2: 100%|##########| 1/1 [00:00<00:00, 14.90it/s, loss=34.4, train_loss_step=8.380, train_loss_epoch=8.380]\n",
      "Epoch 2:   0%|          | 0/1 [00:00<?, ?it/s, loss=34.4, train_loss_step=8.380, train_loss_epoch=8.380]        \n",
      "Epoch 3:   0%|          | 0/1 [00:00<?, ?it/s, loss=34.4, train_loss_step=8.380, train_loss_epoch=8.380]\n",
      "Epoch 3: 100%|##########| 1/1 [00:00<00:00, 15.54it/s, loss=27.6, train_loss_step=7.210, train_loss_epoch=8.380]\n",
      "Epoch 3: 100%|##########| 1/1 [00:00<00:00, 14.75it/s, loss=27.6, train_loss_step=7.210, train_loss_epoch=7.210]\n",
      "Epoch 3:   0%|          | 0/1 [00:00<?, ?it/s, loss=27.6, train_loss_step=7.210, train_loss_epoch=7.210]        \n",
      "Epoch 4:   0%|          | 0/1 [00:00<?, ?it/s, loss=27.6, train_loss_step=7.210, train_loss_epoch=7.210]\n",
      "Epoch 4: 100%|##########| 1/1 [00:00<00:00, 15.63it/s, loss=23, train_loss_step=4.580, train_loss_epoch=7.210]\n",
      "Epoch 4: 100%|##########| 1/1 [00:00<00:00, 14.90it/s, loss=23, train_loss_step=4.580, train_loss_epoch=4.580]\n",
      "Epoch 4:   0%|          | 0/1 [00:00<?, ?it/s, loss=23, train_loss_step=4.580, train_loss_epoch=4.580]        \n",
      "Epoch 5:   0%|          | 0/1 [00:00<?, ?it/s, loss=23, train_loss_step=4.580, train_loss_epoch=4.580]\n",
      "Epoch 5: 100%|##########| 1/1 [00:00<00:00, 15.71it/s, loss=19.9, train_loss_step=4.330, train_loss_epoch=4.580]\n",
      "Epoch 5: 100%|##########| 1/1 [00:00<00:00, 14.88it/s, loss=19.9, train_loss_step=4.330, train_loss_epoch=4.330]\n",
      "Epoch 5:   0%|          | 0/1 [00:00<?, ?it/s, loss=19.9, train_loss_step=4.330, train_loss_epoch=4.330]        \n",
      "Epoch 6:   0%|          | 0/1 [00:00<?, ?it/s, loss=19.9, train_loss_step=4.330, train_loss_epoch=4.330]\n",
      "Epoch 6: 100%|##########| 1/1 [00:00<00:00, 15.82it/s, loss=17.6, train_loss_step=4.110, train_loss_epoch=4.330]\n",
      "Epoch 6: 100%|##########| 1/1 [00:00<00:00, 15.03it/s, loss=17.6, train_loss_step=4.110, train_loss_epoch=4.110]\n",
      "Epoch 6:   0%|          | 0/1 [00:00<?, ?it/s, loss=17.6, train_loss_step=4.110, train_loss_epoch=4.110]        \n",
      "Epoch 7:   0%|          | 0/1 [00:00<?, ?it/s, loss=17.6, train_loss_step=4.110, train_loss_epoch=4.110]\n",
      "Epoch 7: 100%|##########| 1/1 [00:00<00:00, 14.76it/s, loss=15.9, train_loss_step=3.890, train_loss_epoch=4.110]\n",
      "Epoch 7: 100%|##########| 1/1 [00:00<00:00, 13.96it/s, loss=15.9, train_loss_step=3.890, train_loss_epoch=3.890]\n",
      "Epoch 7:   0%|          | 0/1 [00:00<?, ?it/s, loss=15.9, train_loss_step=3.890, train_loss_epoch=3.890]        \n",
      "Epoch 8:   0%|          | 0/1 [00:00<?, ?it/s, loss=15.9, train_loss_step=3.890, train_loss_epoch=3.890]\n",
      "Epoch 8: 100%|##########| 1/1 [00:00<00:00, 15.45it/s, loss=14.5, train_loss_step=3.270, train_loss_epoch=3.890]\n",
      "Epoch 8: 100%|##########| 1/1 [00:00<00:00, 14.71it/s, loss=14.5, train_loss_step=3.270, train_loss_epoch=3.270]\n",
      "Epoch 8:   0%|          | 0/1 [00:00<?, ?it/s, loss=14.5, train_loss_step=3.270, train_loss_epoch=3.270]        \n",
      "Epoch 9:   0%|          | 0/1 [00:00<?, ?it/s, loss=14.5, train_loss_step=3.270, train_loss_epoch=3.270]\n",
      "Epoch 9: 100%|##########| 1/1 [00:00<00:00, 15.72it/s, loss=13.4, train_loss_step=3.280, train_loss_epoch=3.270]\n",
      "Epoch 9: 100%|##########| 1/1 [00:00<00:00, 14.92it/s, loss=13.4, train_loss_step=3.280, train_loss_epoch=3.280]\n",
      "Epoch 9: 100%|##########| 1/1 [00:00<00:00, 14.29it/s, loss=13.4, train_loss_step=3.280, train_loss_epoch=3.280]\n",
      "  0%|          | 0/2 [00:01<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cchallu/NIXTLA/neuralforecast/neuralforecast/data/tsloader.py:47: UserWarning: This class wraps the pytorch `DataLoader` with a special collate function. If you want to use yours simply use `DataLoader`. Removing collate_fn\n",
      "  'This class wraps the pytorch `DataLoader` with a '\n",
      "\n",
      "/Users/cchallu/opt/anaconda3/envs/neuralforecast/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, predict_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting: 1it [00:00, ?it/s]                       \n",
      "Predicting: 100%|##########| 1/1 [00:00<?, ?it/s]    \n",
      "VAL y_true.shape: (145, 24)                          \n",
      "VAL y_hat.shape: (145, 24)                           \n",
      "Predicting: 1it [00:00, ?it/s]                       \n",
      "Predicting: 100%|##########| 1/1 [00:00<?, ?it/s]    \n",
      "TEST y_true.shape: (145, 24)                         \n",
      "TEST y_hat.shape: (145, 24)                          \n",
      " 50%|     | 1/2 [00:02<00:02,  2.06s/trial, best loss: 8.65079402923584]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.014779 seconds\n",
      "INFO:hyperopt.tpe:TPE using 1/1 trials with best loss 8.650794\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================                              \n",
      "\n",
      "activation                                          ReLU                     \n",
      "batch_normalization                                False\n",
      "batch_size                                             1\n",
      "complete_windows                                    True\n",
      "dropout_prob_theta                                     0\n",
      "early_stop_patience                                   10\n",
      "eval_freq                                             50\n",
      "frequency                                              H\n",
      "idx_to_sample_freq                                     1\n",
      "initialization                              lecun_normal\n",
      "interpolation_mode                                linear\n",
      "learning_rate                                      0.005\n",
      "loss_hypar                                           0.5\n",
      "loss_train                                           MAE\n",
      "loss_valid                                           MAE\n",
      "lr_decay                                             0.5\n",
      "max_epochs                                          None\n",
      "max_steps                                             10\n",
      "mode                                              simple\n",
      "model                                              nhits\n",
      "n_blocks                                               1\n",
      "n_freq_downsample                           (180, 60, 1)\n",
      "n_layers                                               2\n",
      "n_lr_decays                                            3\n",
      "n_mlp_units                                          256\n",
      "n_pool_kernel_size                             (2, 2, 2)\n",
      "n_s                                                    0\n",
      "n_s_hidden                                             0\n",
      "n_time_in                                             72\n",
      "n_time_out                                            24\n",
      "n_windows                                            512\n",
      "n_x                                                    1\n",
      "n_x_hidden                                             1\n",
      "normalizer_x                                        None\n",
      "normalizer_y                                        None\n",
      "pooling_mode                                         max\n",
      "random_seed                                         19.0\n",
      "shared_weights                                     False\n",
      "stack_types               (identity, identity, identity)\n",
      "val_idx_to_sample_freq                                 1\n",
      "weight_decay                                           0\n",
      "dtype: object\n",
      "===============================================                              \n",
      "\n",
      " 50%|     | 1/2 [00:02<00:02,  2.06s/trial, best loss: 8.65079402923584]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Train Validation splits\n",
      "\n",
      "INFO:root:                              ds                    \n",
      "                             min                 max\n",
      "unique_id sample_mask                               \n",
      "NP        0           2018-12-11 2018-12-24 23:00:00\n",
      "          1           2013-01-01 2018-12-10 23:00:00\n",
      "INFO:root:\n",
      "Total data \t\t\t52416 time stamps \n",
      "Available percentage=100.0, \t52416 time stamps \n",
      "Insample  percentage=99.36, \t52080 time stamps \n",
      "Outsample percentage=0.64, \t336 time stamps \n",
      "\n",
      "/Users/cchallu/NIXTLA/neuralforecast/neuralforecast/data/tsdataset.py:208: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  X.drop(['unique_id', 'ds'], 1, inplace=True)\n",
      "\n",
      "INFO:root:Train Validation splits\n",
      "\n",
      "INFO:root:                              ds                    \n",
      "                             min                 max\n",
      "unique_id sample_mask                               \n",
      "NP        0           2013-01-01 2018-12-24 23:00:00\n",
      "          1           2018-12-11 2018-12-17 23:00:00\n",
      "INFO:root:\n",
      "Total data \t\t\t52416 time stamps \n",
      "Available percentage=100.0, \t52416 time stamps \n",
      "Insample  percentage=0.32, \t168 time stamps \n",
      "Outsample percentage=99.68, \t52248 time stamps \n",
      "\n",
      "/Users/cchallu/NIXTLA/neuralforecast/neuralforecast/data/tsdataset.py:208: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  X.drop(['unique_id', 'ds'], 1, inplace=True)\n",
      "\n",
      "INFO:root:Train Validation splits\n",
      "\n",
      "INFO:root:                              ds                    \n",
      "                             min                 max\n",
      "unique_id sample_mask                               \n",
      "NP        0           2013-01-01 2018-12-17 23:00:00\n",
      "          1           2018-12-18 2018-12-24 23:00:00\n",
      "INFO:root:\n",
      "Total data \t\t\t52416 time stamps \n",
      "Available percentage=100.0, \t52416 time stamps \n",
      "Insample  percentage=0.32, \t168 time stamps \n",
      "Outsample percentage=99.68, \t52248 time stamps \n",
      "\n",
      "/Users/cchallu/NIXTLA/neuralforecast/neuralforecast/data/tsdataset.py:208: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  X.drop(['unique_id', 'ds'], 1, inplace=True)\n",
      "\n",
      "/Users/cchallu/opt/anaconda3/envs/neuralforecast/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=False)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=False)`.\n",
      "  f\"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will \"\n",
      "\n",
      "/Users/cchallu/opt/anaconda3/envs/neuralforecast/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:91: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=1)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "  f\"Setting `Trainer(progress_bar_refresh_rate={progress_bar_refresh_rate})` is deprecated in v1.5 and\"\n",
      "\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "\n",
      "  | Name  | Type   | Params\n",
      "---------------------------------\n",
      "0 | model | _NHITS | 361 K \n",
      "---------------------------------\n",
      "361 K     Trainable params\n",
      "0         Non-trainable params\n",
      "361 K     Total params\n",
      "1.447     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check: 0it [00:00, ?it/s]                                  \n",
      "Validation sanity check:   0%|          | 0/1 [00:00<?, ?it/s]               \n",
      " 50%|     | 1/2 [00:03<00:02,  2.06s/trial, best loss: 8.65079402923584]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cchallu/opt/anaconda3/envs/neuralforecast/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "\n",
      "/Users/cchallu/opt/anaconda3/envs/neuralforecast/lib/python3.7/site-packages/torch/nn/functional.py:3635: UserWarning: Default upsampling behavior when mode=linear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode)\n",
      "\n",
      "/Users/cchallu/opt/anaconda3/envs/neuralforecast/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 0it [00:00, ?it/s]                                                 \n",
      "Training:   0%|          | 0/1 [00:00<?, ?it/s]                              \n",
      "Epoch 0:   0%|          | 0/1 [00:00<?, ?it/s]                               \n",
      "Epoch 0: 100%|##########| 1/1 [00:00<00:00, 14.33it/s, loss=4.41, train_loss_step=4.410]\n",
      "Epoch 0: 100%|##########| 1/1 [00:00<00:00, 13.66it/s, loss=4.41, train_loss_step=4.410, train_loss_epoch=4.410]\n",
      "Epoch 0:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.41, train_loss_step=4.410, train_loss_epoch=4.410]        \n",
      "Epoch 1:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.41, train_loss_step=4.410, train_loss_epoch=4.410]\n",
      "Epoch 1: 100%|##########| 1/1 [00:00<00:00, 14.10it/s, loss=32.7, train_loss_step=61.00, train_loss_epoch=4.410]\n",
      "Epoch 1: 100%|##########| 1/1 [00:00<00:00, 13.46it/s, loss=32.7, train_loss_step=61.00, train_loss_epoch=61.00]\n",
      "Epoch 1:   0%|          | 0/1 [00:00<?, ?it/s, loss=32.7, train_loss_step=61.00, train_loss_epoch=61.00]        \n",
      "Epoch 2:   0%|          | 0/1 [00:00<?, ?it/s, loss=32.7, train_loss_step=61.00, train_loss_epoch=61.00]\n",
      "Epoch 2: 100%|##########| 1/1 [00:00<00:00, 14.32it/s, loss=24.8, train_loss_step=9.040, train_loss_epoch=61.00]\n",
      "Epoch 2: 100%|##########| 1/1 [00:00<00:00, 13.57it/s, loss=24.8, train_loss_step=9.040, train_loss_epoch=9.040]\n",
      "Epoch 2:   0%|          | 0/1 [00:00<?, ?it/s, loss=24.8, train_loss_step=9.040, train_loss_epoch=9.040]        \n",
      "Epoch 3:   0%|          | 0/1 [00:00<?, ?it/s, loss=24.8, train_loss_step=9.040, train_loss_epoch=9.040]\n",
      "Epoch 3: 100%|##########| 1/1 [00:00<00:00, 14.42it/s, loss=23.3, train_loss_step=18.60, train_loss_epoch=9.040]\n",
      "Epoch 3: 100%|##########| 1/1 [00:00<00:00, 13.67it/s, loss=23.3, train_loss_step=18.60, train_loss_epoch=18.60]\n",
      "Epoch 3:   0%|          | 0/1 [00:00<?, ?it/s, loss=23.3, train_loss_step=18.60, train_loss_epoch=18.60]        \n",
      "Epoch 4:   0%|          | 0/1 [00:00<?, ?it/s, loss=23.3, train_loss_step=18.60, train_loss_epoch=18.60]\n",
      "Epoch 4: 100%|##########| 1/1 [00:00<00:00, 14.10it/s, loss=21.7, train_loss_step=15.40, train_loss_epoch=18.60]\n",
      "Epoch 4: 100%|##########| 1/1 [00:00<00:00, 13.43it/s, loss=21.7, train_loss_step=15.40, train_loss_epoch=15.40]\n",
      "Epoch 4:   0%|          | 0/1 [00:00<?, ?it/s, loss=21.7, train_loss_step=15.40, train_loss_epoch=15.40]        \n",
      "Epoch 5:   0%|          | 0/1 [00:00<?, ?it/s, loss=21.7, train_loss_step=15.40, train_loss_epoch=15.40]\n",
      "Epoch 5: 100%|##########| 1/1 [00:00<00:00, 14.30it/s, loss=19.9, train_loss_step=11.00, train_loss_epoch=15.40]\n",
      "Epoch 5: 100%|##########| 1/1 [00:00<00:00, 13.48it/s, loss=19.9, train_loss_step=11.00, train_loss_epoch=11.00]\n",
      "Epoch 5:   0%|          | 0/1 [00:00<?, ?it/s, loss=19.9, train_loss_step=11.00, train_loss_epoch=11.00]        \n",
      "Epoch 6:   0%|          | 0/1 [00:00<?, ?it/s, loss=19.9, train_loss_step=11.00, train_loss_epoch=11.00]\n",
      "Epoch 6: 100%|##########| 1/1 [00:00<00:00, 13.44it/s, loss=18, train_loss_step=6.410, train_loss_epoch=11.00]\n",
      "Epoch 6: 100%|##########| 1/1 [00:00<00:00, 12.77it/s, loss=18, train_loss_step=6.410, train_loss_epoch=6.410]\n",
      "Epoch 6:   0%|          | 0/1 [00:00<?, ?it/s, loss=18, train_loss_step=6.410, train_loss_epoch=6.410]        \n",
      "Epoch 7:   0%|          | 0/1 [00:00<?, ?it/s, loss=18, train_loss_step=6.410, train_loss_epoch=6.410]\n",
      "Epoch 7: 100%|##########| 1/1 [00:00<00:00, 13.83it/s, loss=16.3, train_loss_step=4.910, train_loss_epoch=6.410]\n",
      "Epoch 7: 100%|##########| 1/1 [00:00<00:00, 13.16it/s, loss=16.3, train_loss_step=4.910, train_loss_epoch=4.910]\n",
      "Epoch 7:   0%|          | 0/1 [00:00<?, ?it/s, loss=16.3, train_loss_step=4.910, train_loss_epoch=4.910]        \n",
      "Epoch 8:   0%|          | 0/1 [00:00<?, ?it/s, loss=16.3, train_loss_step=4.910, train_loss_epoch=4.910]\n",
      "Epoch 8: 100%|##########| 1/1 [00:00<00:00, 14.30it/s, loss=14.9, train_loss_step=3.760, train_loss_epoch=4.910]\n",
      "Epoch 8: 100%|##########| 1/1 [00:00<00:00, 13.64it/s, loss=14.9, train_loss_step=3.760, train_loss_epoch=3.760]\n",
      "Epoch 8:   0%|          | 0/1 [00:00<?, ?it/s, loss=14.9, train_loss_step=3.760, train_loss_epoch=3.760]        \n",
      "Epoch 9:   0%|          | 0/1 [00:00<?, ?it/s, loss=14.9, train_loss_step=3.760, train_loss_epoch=3.760]\n",
      "Epoch 9: 100%|##########| 1/1 [00:00<00:00, 14.31it/s, loss=13.9, train_loss_step=3.990, train_loss_epoch=3.760]\n",
      "Epoch 9: 100%|##########| 1/1 [00:00<00:00, 13.57it/s, loss=13.9, train_loss_step=3.990, train_loss_epoch=3.990]\n",
      "Epoch 9: 100%|##########| 1/1 [00:00<00:00, 13.04it/s, loss=13.9, train_loss_step=3.990, train_loss_epoch=3.990]\n",
      " 50%|     | 1/2 [00:03<00:02,  2.06s/trial, best loss: 8.65079402923584]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cchallu/NIXTLA/neuralforecast/neuralforecast/data/tsloader.py:47: UserWarning: This class wraps the pytorch `DataLoader` with a special collate function. If you want to use yours simply use `DataLoader`. Removing collate_fn\n",
      "  'This class wraps the pytorch `DataLoader` with a '\n",
      "\n",
      "/Users/cchallu/opt/anaconda3/envs/neuralforecast/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, predict_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting: 1it [00:00, ?it/s]                                               \n",
      "Predicting: 100%|##########| 1/1 [00:00<?, ?it/s]                            \n",
      "VAL y_true.shape: (145, 24)                                                  \n",
      "VAL y_hat.shape: (145, 24)                                                   \n",
      "Predicting: 1it [00:00, ?it/s]                                               \n",
      "Predicting: 100%|##########| 1/1 [00:00<?, ?it/s]                            \n",
      "TEST y_true.shape: (145, 24)                                                 \n",
      "TEST y_hat.shape: (145, 24)                                                  \n",
      "100%|| 2/2 [00:04<00:00,  2.03s/trial, best loss: 8.560818672180176]\n"
     ]
    }
   ],
   "source": [
    "nhits_space = nhits_suggested_space(n_time_out=24, n_series=1, n_x=1, n_s=0, frequency='H')\n",
    "nhits_space['max_steps'] = hp.choice('max_steps', [10]) # Override max_steps for faster example\n",
    "trials = hyperopt_tunning(space=nhits_space, hyperopt_max_evals=2, loss_function_val=mae,\n",
    "                          loss_functions_test={'mae': mae, 'rmse': rmse},\n",
    "                          S_df=S_df, Y_df=Y_df, X_df=X_df, f_cols=[],\n",
    "                          ds_in_val=7*24, ds_in_test=7*24, return_forecasts=True, save_progress=False, results_file=None, loss_kwargs={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.013242 seconds\n",
      "INFO:hyperopt.tpe:TPE using 0 trials\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================      \n",
      "\n",
      "activation                                          ReLU\n",
      "batch_normalization                                False\n",
      "batch_size                                             1\n",
      "complete_windows                                    True\n",
      "dropout_prob_theta                                     0\n",
      "early_stop_patience                                   10\n",
      "eval_freq                                             50\n",
      "frequency                                              H\n",
      "idx_to_sample_freq                                     1\n",
      "initialization                              lecun_normal\n",
      "learning_rate                                     0.0001\n",
      "loss_hypar                                           0.5\n",
      "loss_train                                           MAE\n",
      "loss_valid                                           MAE\n",
      "lr_decay                                             0.5\n",
      "max_epochs                                          None\n",
      "max_steps                                             10\n",
      "mode                                              simple\n",
      "model                                             nbeats\n",
      "n_blocks                                               3\n",
      "n_layers                                               3\n",
      "n_lr_decays                                            3\n",
      "n_mlp_units                                          512\n",
      "n_s                                                    0\n",
      "n_s_hidden                                             0\n",
      "n_time_in                                             72\n",
      "n_time_out                                            24\n",
      "n_windows                                            512\n",
      "n_x                                                    1\n",
      "n_x_hidden                                             1\n",
      "normalizer_x                                        None\n",
      "normalizer_y                                        None\n",
      "random_seed                                          9.0\n",
      "shared_weights                                     False\n",
      "stack_types               (identity, identity, identity)\n",
      "val_idx_to_sample_freq                                 1\n",
      "weight_decay                                           0\n",
      "dtype: object\n",
      "===============================================      \n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Train Validation splits\n",
      "\n",
      "INFO:root:                              ds                    \n",
      "                             min                 max\n",
      "unique_id sample_mask                               \n",
      "NP        0           2018-12-11 2018-12-24 23:00:00\n",
      "          1           2013-01-01 2018-12-10 23:00:00\n",
      "INFO:root:\n",
      "Total data \t\t\t52416 time stamps \n",
      "Available percentage=100.0, \t52416 time stamps \n",
      "Insample  percentage=99.36, \t52080 time stamps \n",
      "Outsample percentage=0.64, \t336 time stamps \n",
      "\n",
      "/Users/cchallu/NIXTLA/neuralforecast/neuralforecast/data/tsdataset.py:208: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  X.drop(['unique_id', 'ds'], 1, inplace=True)\n",
      "\n",
      "INFO:root:Train Validation splits\n",
      "\n",
      "INFO:root:                              ds                    \n",
      "                             min                 max\n",
      "unique_id sample_mask                               \n",
      "NP        0           2013-01-01 2018-12-24 23:00:00\n",
      "          1           2018-12-11 2018-12-17 23:00:00\n",
      "INFO:root:\n",
      "Total data \t\t\t52416 time stamps \n",
      "Available percentage=100.0, \t52416 time stamps \n",
      "Insample  percentage=0.32, \t168 time stamps \n",
      "Outsample percentage=99.68, \t52248 time stamps \n",
      "\n",
      "/Users/cchallu/NIXTLA/neuralforecast/neuralforecast/data/tsdataset.py:208: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  X.drop(['unique_id', 'ds'], 1, inplace=True)\n",
      "\n",
      "INFO:root:Train Validation splits\n",
      "\n",
      "INFO:root:                              ds                    \n",
      "                             min                 max\n",
      "unique_id sample_mask                               \n",
      "NP        0           2013-01-01 2018-12-17 23:00:00\n",
      "          1           2018-12-18 2018-12-24 23:00:00\n",
      "INFO:root:\n",
      "Total data \t\t\t52416 time stamps \n",
      "Available percentage=100.0, \t52416 time stamps \n",
      "Insample  percentage=0.32, \t168 time stamps \n",
      "Outsample percentage=99.68, \t52248 time stamps \n",
      "\n",
      "/Users/cchallu/NIXTLA/neuralforecast/neuralforecast/data/tsdataset.py:208: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  X.drop(['unique_id', 'ds'], 1, inplace=True)\n",
      "\n",
      "/Users/cchallu/opt/anaconda3/envs/neuralforecast/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=False)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=False)`.\n",
      "  f\"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will \"\n",
      "\n",
      "/Users/cchallu/opt/anaconda3/envs/neuralforecast/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:91: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=1)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "  f\"Setting `Trainer(progress_bar_refresh_rate={progress_bar_refresh_rate})` is deprecated in v1.5 and\"\n",
      "\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "\n",
      "  | Name  | Type    | Params\n",
      "----------------------------------\n",
      "0 | model | _NBEATS | 5.9 M \n",
      "----------------------------------\n",
      "5.9 M     Trainable params\n",
      "0         Non-trainable params\n",
      "5.9 M     Total params\n",
      "23.799    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check: 0it [00:00, ?it/s]          \n",
      "Validation sanity check:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "  0%|          | 0/2 [00:01<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cchallu/opt/anaconda3/envs/neuralforecast/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "\n",
      "/Users/cchallu/opt/anaconda3/envs/neuralforecast/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 0it [00:00, ?it/s]                         \n",
      "Training:   0%|          | 0/1 [00:00<?, ?it/s]      \n",
      "Epoch 0:   0%|          | 0/1 [00:00<?, ?it/s]       \n",
      "Epoch 0: 100%|##########| 1/1 [00:00<00:00,  2.83it/s]\n",
      "Epoch 0: 100%|##########| 1/1 [00:00<00:00,  2.81it/s, loss=4.76, train_loss_step=4.760]\n",
      "Epoch 0: 100%|##########| 1/1 [00:00<00:00,  2.78it/s, loss=4.76, train_loss_step=4.760, train_loss_epoch=4.760]\n",
      "Epoch 0:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.76, train_loss_step=4.760, train_loss_epoch=4.760]        \n",
      "Epoch 1:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.76, train_loss_step=4.760, train_loss_epoch=4.760]\n",
      "Epoch 1: 100%|##########| 1/1 [00:00<00:00,  2.84it/s, loss=4.76, train_loss_step=4.760, train_loss_epoch=4.760]\n",
      "Epoch 1: 100%|##########| 1/1 [00:00<00:00,  2.81it/s, loss=4.22, train_loss_step=3.690, train_loss_epoch=4.760]\n",
      "Epoch 1: 100%|##########| 1/1 [00:00<00:00,  2.78it/s, loss=4.22, train_loss_step=3.690, train_loss_epoch=3.690]\n",
      "Epoch 1:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.22, train_loss_step=3.690, train_loss_epoch=3.690]        \n",
      "Epoch 2:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.22, train_loss_step=3.690, train_loss_epoch=3.690]\n",
      "Epoch 2: 100%|##########| 1/1 [00:00<00:00,  2.86it/s, loss=4.22, train_loss_step=3.690, train_loss_epoch=3.690]\n",
      "Epoch 2: 100%|##########| 1/1 [00:00<00:00,  2.84it/s, loss=4.15, train_loss_step=4.020, train_loss_epoch=3.690]\n",
      "Epoch 2: 100%|##########| 1/1 [00:00<00:00,  2.81it/s, loss=4.15, train_loss_step=4.020, train_loss_epoch=4.020]\n",
      "Epoch 2:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.15, train_loss_step=4.020, train_loss_epoch=4.020]        \n",
      "Epoch 3:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.15, train_loss_step=4.020, train_loss_epoch=4.020]\n",
      "Epoch 3: 100%|##########| 1/1 [00:00<00:00,  2.84it/s, loss=4.15, train_loss_step=4.020, train_loss_epoch=4.020]\n",
      "Epoch 3: 100%|##########| 1/1 [00:00<00:00,  2.81it/s, loss=4.04, train_loss_step=3.710, train_loss_epoch=4.020]\n",
      "Epoch 3: 100%|##########| 1/1 [00:00<00:00,  2.78it/s, loss=4.04, train_loss_step=3.710, train_loss_epoch=3.710]\n",
      "Epoch 3:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.04, train_loss_step=3.710, train_loss_epoch=3.710]        \n",
      "Epoch 4:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.04, train_loss_step=3.710, train_loss_epoch=3.710]\n",
      "Epoch 4: 100%|##########| 1/1 [00:00<00:00,  2.72it/s, loss=4.04, train_loss_step=3.710, train_loss_epoch=3.710]\n",
      "Epoch 4: 100%|##########| 1/1 [00:00<00:00,  2.69it/s, loss=3.99, train_loss_step=3.750, train_loss_epoch=3.710]\n",
      "Epoch 4: 100%|##########| 1/1 [00:00<00:00,  2.67it/s, loss=3.99, train_loss_step=3.750, train_loss_epoch=3.750]\n",
      "Epoch 4:   0%|          | 0/1 [00:00<?, ?it/s, loss=3.99, train_loss_step=3.750, train_loss_epoch=3.750]        \n",
      "Epoch 5:   0%|          | 0/1 [00:00<?, ?it/s, loss=3.99, train_loss_step=3.750, train_loss_epoch=3.750]\n",
      "Epoch 5: 100%|##########| 1/1 [00:00<00:00,  2.48it/s, loss=3.99, train_loss_step=3.750, train_loss_epoch=3.750]\n",
      "Epoch 5: 100%|##########| 1/1 [00:00<00:00,  2.46it/s, loss=3.95, train_loss_step=3.750, train_loss_epoch=3.750]\n",
      "Epoch 5: 100%|##########| 1/1 [00:00<00:00,  2.43it/s, loss=3.95, train_loss_step=3.750, train_loss_epoch=3.750]\n",
      "Epoch 5:   0%|          | 0/1 [00:00<?, ?it/s, loss=3.95, train_loss_step=3.750, train_loss_epoch=3.750]        \n",
      "Epoch 6:   0%|          | 0/1 [00:00<?, ?it/s, loss=3.95, train_loss_step=3.750, train_loss_epoch=3.750]\n",
      "Epoch 6: 100%|##########| 1/1 [00:00<00:00,  2.10it/s, loss=3.95, train_loss_step=3.750, train_loss_epoch=3.750]\n",
      "Epoch 6: 100%|##########| 1/1 [00:00<00:00,  2.07it/s, loss=3.88, train_loss_step=3.510, train_loss_epoch=3.750]\n",
      "Epoch 6: 100%|##########| 1/1 [00:00<00:00,  2.03it/s, loss=3.88, train_loss_step=3.510, train_loss_epoch=3.510]\n",
      "Epoch 6:   0%|          | 0/1 [00:00<?, ?it/s, loss=3.88, train_loss_step=3.510, train_loss_epoch=3.510]        \n",
      "Epoch 7:   0%|          | 0/1 [00:00<?, ?it/s, loss=3.88, train_loss_step=3.510, train_loss_epoch=3.510]\n",
      "Epoch 7: 100%|##########| 1/1 [00:00<00:00,  2.18it/s, loss=3.88, train_loss_step=3.510, train_loss_epoch=3.510]\n",
      "Epoch 7: 100%|##########| 1/1 [00:00<00:00,  2.16it/s, loss=3.83, train_loss_step=3.490, train_loss_epoch=3.510]\n",
      "Epoch 7: 100%|##########| 1/1 [00:00<00:00,  2.14it/s, loss=3.83, train_loss_step=3.490, train_loss_epoch=3.490]\n",
      "Epoch 7:   0%|          | 0/1 [00:00<?, ?it/s, loss=3.83, train_loss_step=3.490, train_loss_epoch=3.490]        \n",
      "Epoch 8:   0%|          | 0/1 [00:00<?, ?it/s, loss=3.83, train_loss_step=3.490, train_loss_epoch=3.490]\n",
      "Epoch 8: 100%|##########| 1/1 [00:00<00:00,  2.20it/s, loss=3.83, train_loss_step=3.490, train_loss_epoch=3.490]\n",
      "Epoch 8: 100%|##########| 1/1 [00:00<00:00,  2.18it/s, loss=3.77, train_loss_step=3.210, train_loss_epoch=3.490]\n",
      "Epoch 8: 100%|##########| 1/1 [00:00<00:00,  2.17it/s, loss=3.77, train_loss_step=3.210, train_loss_epoch=3.210]\n",
      "Epoch 8:   0%|          | 0/1 [00:00<?, ?it/s, loss=3.77, train_loss_step=3.210, train_loss_epoch=3.210]        \n",
      "Epoch 9:   0%|          | 0/1 [00:00<?, ?it/s, loss=3.77, train_loss_step=3.210, train_loss_epoch=3.210]\n",
      "Epoch 9: 100%|##########| 1/1 [00:00<00:00,  2.18it/s, loss=3.77, train_loss_step=3.210, train_loss_epoch=3.210]\n",
      "Epoch 9: 100%|##########| 1/1 [00:00<00:00,  2.16it/s, loss=3.72, train_loss_step=3.330, train_loss_epoch=3.210]\n",
      "Epoch 9: 100%|##########| 1/1 [00:00<00:00,  2.13it/s, loss=3.72, train_loss_step=3.330, train_loss_epoch=3.330]\n",
      "Epoch 9: 100%|##########| 1/1 [00:00<00:00,  2.11it/s, loss=3.72, train_loss_step=3.330, train_loss_epoch=3.330]\n",
      "  0%|          | 0/2 [00:05<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cchallu/NIXTLA/neuralforecast/neuralforecast/data/tsloader.py:47: UserWarning: This class wraps the pytorch `DataLoader` with a special collate function. If you want to use yours simply use `DataLoader`. Removing collate_fn\n",
      "  'This class wraps the pytorch `DataLoader` with a '\n",
      "\n",
      "/Users/cchallu/opt/anaconda3/envs/neuralforecast/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, predict_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting: 1it [00:00, ?it/s]                       \n",
      "Predicting: 100%|##########| 1/1 [00:00<?, ?it/s]    \n",
      "VAL y_true.shape: (145, 24)                          \n",
      "VAL y_hat.shape: (145, 24)                           \n",
      "Predicting: 1it [00:00, ?it/s]                       \n",
      "Predicting: 100%|##########| 1/1 [00:00<?, ?it/s]    \n",
      "TEST y_true.shape: (145, 24)                         \n",
      "TEST y_hat.shape: (145, 24)                          \n",
      " 50%|     | 1/2 [00:05<00:05,  5.58s/trial, best loss: 8.174054145812988]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.034675 seconds\n",
      "INFO:hyperopt.tpe:TPE using 1/1 trials with best loss 8.174054\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================                               \n",
      "\n",
      "activation                                          ReLU                      \n",
      "batch_normalization                                False\n",
      "batch_size                                             1\n",
      "complete_windows                                    True\n",
      "dropout_prob_theta                                     0\n",
      "early_stop_patience                                   10\n",
      "eval_freq                                             50\n",
      "frequency                                              H\n",
      "idx_to_sample_freq                                     1\n",
      "initialization                              lecun_normal\n",
      "learning_rate                                       0.01\n",
      "loss_hypar                                           0.5\n",
      "loss_train                                           MAE\n",
      "loss_valid                                           MAE\n",
      "lr_decay                                             0.5\n",
      "max_epochs                                          None\n",
      "max_steps                                             10\n",
      "mode                                              simple\n",
      "model                                             nbeats\n",
      "n_blocks                                               1\n",
      "n_layers                                               2\n",
      "n_lr_decays                                            3\n",
      "n_mlp_units                                          512\n",
      "n_s                                                    0\n",
      "n_s_hidden                                             0\n",
      "n_time_in                                             72\n",
      "n_time_out                                            24\n",
      "n_windows                                            512\n",
      "n_x                                                    1\n",
      "n_x_hidden                                             1\n",
      "normalizer_x                                        None\n",
      "normalizer_y                                        None\n",
      "random_seed                                         19.0\n",
      "shared_weights                                     False\n",
      "stack_types               (identity, identity, identity)\n",
      "val_idx_to_sample_freq                                 1\n",
      "weight_decay                                           0\n",
      "dtype: object\n",
      "===============================================                               \n",
      "\n",
      " 50%|     | 1/2 [00:05<00:05,  5.58s/trial, best loss: 8.174054145812988]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Train Validation splits\n",
      "\n",
      "INFO:root:                              ds                    \n",
      "                             min                 max\n",
      "unique_id sample_mask                               \n",
      "NP        0           2018-12-11 2018-12-24 23:00:00\n",
      "          1           2013-01-01 2018-12-10 23:00:00\n",
      "INFO:root:\n",
      "Total data \t\t\t52416 time stamps \n",
      "Available percentage=100.0, \t52416 time stamps \n",
      "Insample  percentage=99.36, \t52080 time stamps \n",
      "Outsample percentage=0.64, \t336 time stamps \n",
      "\n",
      "/Users/cchallu/NIXTLA/neuralforecast/neuralforecast/data/tsdataset.py:208: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  X.drop(['unique_id', 'ds'], 1, inplace=True)\n",
      "\n",
      "INFO:root:Train Validation splits\n",
      "\n",
      "INFO:root:                              ds                    \n",
      "                             min                 max\n",
      "unique_id sample_mask                               \n",
      "NP        0           2013-01-01 2018-12-24 23:00:00\n",
      "          1           2018-12-11 2018-12-17 23:00:00\n",
      "INFO:root:\n",
      "Total data \t\t\t52416 time stamps \n",
      "Available percentage=100.0, \t52416 time stamps \n",
      "Insample  percentage=0.32, \t168 time stamps \n",
      "Outsample percentage=99.68, \t52248 time stamps \n",
      "\n",
      "/Users/cchallu/NIXTLA/neuralforecast/neuralforecast/data/tsdataset.py:208: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  X.drop(['unique_id', 'ds'], 1, inplace=True)\n",
      "\n",
      "INFO:root:Train Validation splits\n",
      "\n",
      "INFO:root:                              ds                    \n",
      "                             min                 max\n",
      "unique_id sample_mask                               \n",
      "NP        0           2013-01-01 2018-12-17 23:00:00\n",
      "          1           2018-12-18 2018-12-24 23:00:00\n",
      "INFO:root:\n",
      "Total data \t\t\t52416 time stamps \n",
      "Available percentage=100.0, \t52416 time stamps \n",
      "Insample  percentage=0.32, \t168 time stamps \n",
      "Outsample percentage=99.68, \t52248 time stamps \n",
      "\n",
      "/Users/cchallu/NIXTLA/neuralforecast/neuralforecast/data/tsdataset.py:208: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  X.drop(['unique_id', 'ds'], 1, inplace=True)\n",
      "\n",
      "/Users/cchallu/opt/anaconda3/envs/neuralforecast/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=False)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=False)`.\n",
      "  f\"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will \"\n",
      "\n",
      "/Users/cchallu/opt/anaconda3/envs/neuralforecast/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:91: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=1)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "  f\"Setting `Trainer(progress_bar_refresh_rate={progress_bar_refresh_rate})` is deprecated in v1.5 and\"\n",
      "\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "\n",
      "  | Name  | Type    | Params\n",
      "----------------------------------\n",
      "0 | model | _NBEATS | 1.2 M \n",
      "----------------------------------\n",
      "1.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.2 M     Total params\n",
      "4.781     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check: 0it [00:00, ?it/s]                                   \n",
      "Validation sanity check:   0%|          | 0/1 [00:00<?, ?it/s]                \n",
      " 50%|     | 1/2 [00:07<00:05,  5.58s/trial, best loss: 8.174054145812988]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cchallu/opt/anaconda3/envs/neuralforecast/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "\n",
      "/Users/cchallu/opt/anaconda3/envs/neuralforecast/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 0it [00:00, ?it/s]                                                  \n",
      "Training:   0%|          | 0/1 [00:00<?, ?it/s]                               \n",
      "Epoch 0:   0%|          | 0/1 [00:00<?, ?it/s]                                \n",
      "Epoch 0: 100%|##########| 1/1 [00:00<00:00,  7.29it/s]                        \n",
      "Epoch 0: 100%|##########| 1/1 [00:00<00:00,  7.09it/s, loss=5.1, train_loss_step=5.100]\n",
      "Epoch 0: 100%|##########| 1/1 [00:00<00:00,  6.92it/s, loss=5.1, train_loss_step=5.100, train_loss_epoch=5.100]\n",
      "Epoch 0:   0%|          | 0/1 [00:00<?, ?it/s, loss=5.1, train_loss_step=5.100, train_loss_epoch=5.100]        \n",
      "Epoch 1:   0%|          | 0/1 [00:00<?, ?it/s, loss=5.1, train_loss_step=5.100, train_loss_epoch=5.100]\n",
      "Epoch 1: 100%|##########| 1/1 [00:00<00:00,  6.77it/s, loss=5.1, train_loss_step=5.100, train_loss_epoch=5.100]\n",
      "Epoch 1: 100%|##########| 1/1 [00:00<00:00,  6.59it/s, loss=149, train_loss_step=293.0, train_loss_epoch=5.100]\n",
      "Epoch 1: 100%|##########| 1/1 [00:00<00:00,  6.39it/s, loss=149, train_loss_step=293.0, train_loss_epoch=293.0]\n",
      "Epoch 1:   0%|          | 0/1 [00:00<?, ?it/s, loss=149, train_loss_step=293.0, train_loss_epoch=293.0]        \n",
      "Epoch 2:   0%|          | 0/1 [00:00<?, ?it/s, loss=149, train_loss_step=293.0, train_loss_epoch=293.0]\n",
      "Epoch 2: 100%|##########| 1/1 [00:00<00:00,  7.07it/s, loss=149, train_loss_step=293.0, train_loss_epoch=293.0]\n",
      "Epoch 2: 100%|##########| 1/1 [00:00<00:00,  6.89it/s, loss=105, train_loss_step=15.40, train_loss_epoch=293.0]\n",
      "Epoch 2: 100%|##########| 1/1 [00:00<00:00,  6.64it/s, loss=105, train_loss_step=15.40, train_loss_epoch=15.40]\n",
      "Epoch 2:   0%|          | 0/1 [00:00<?, ?it/s, loss=105, train_loss_step=15.40, train_loss_epoch=15.40]        \n",
      "Epoch 3:   0%|          | 0/1 [00:00<?, ?it/s, loss=105, train_loss_step=15.40, train_loss_epoch=15.40]\n",
      "Epoch 3: 100%|##########| 1/1 [00:00<00:00,  7.37it/s, loss=105, train_loss_step=15.40, train_loss_epoch=15.40]\n",
      "Epoch 3: 100%|##########| 1/1 [00:00<00:00,  7.17it/s, loss=85.6, train_loss_step=28.60, train_loss_epoch=15.40]\n",
      "Epoch 3: 100%|##########| 1/1 [00:00<00:00,  6.97it/s, loss=85.6, train_loss_step=28.60, train_loss_epoch=28.60]\n",
      "Epoch 3:   0%|          | 0/1 [00:00<?, ?it/s, loss=85.6, train_loss_step=28.60, train_loss_epoch=28.60]        \n",
      "Epoch 4:   0%|          | 0/1 [00:00<?, ?it/s, loss=85.6, train_loss_step=28.60, train_loss_epoch=28.60]\n",
      "Epoch 4: 100%|##########| 1/1 [00:00<00:00,  7.47it/s, loss=85.6, train_loss_step=28.60, train_loss_epoch=28.60]\n",
      "Epoch 4: 100%|##########| 1/1 [00:00<00:00,  7.28it/s, loss=71.4, train_loss_step=14.90, train_loss_epoch=28.60]\n",
      "Epoch 4: 100%|##########| 1/1 [00:00<00:00,  7.07it/s, loss=71.4, train_loss_step=14.90, train_loss_epoch=14.90]\n",
      "Epoch 4:   0%|          | 0/1 [00:00<?, ?it/s, loss=71.4, train_loss_step=14.90, train_loss_epoch=14.90]        \n",
      "Epoch 5:   0%|          | 0/1 [00:00<?, ?it/s, loss=71.4, train_loss_step=14.90, train_loss_epoch=14.90]\n",
      "Epoch 5: 100%|##########| 1/1 [00:00<00:00,  7.96it/s, loss=71.4, train_loss_step=14.90, train_loss_epoch=14.90]\n",
      "Epoch 5: 100%|##########| 1/1 [00:00<00:00,  7.74it/s, loss=73.2, train_loss_step=81.80, train_loss_epoch=14.90]\n",
      "Epoch 5: 100%|##########| 1/1 [00:00<00:00,  7.41it/s, loss=73.2, train_loss_step=81.80, train_loss_epoch=81.80]\n",
      "Epoch 5:   0%|          | 0/1 [00:00<?, ?it/s, loss=73.2, train_loss_step=81.80, train_loss_epoch=81.80]        \n",
      "Epoch 6:   0%|          | 0/1 [00:00<?, ?it/s, loss=73.2, train_loss_step=81.80, train_loss_epoch=81.80]\n",
      "Epoch 6: 100%|##########| 1/1 [00:00<00:00,  7.90it/s, loss=73.2, train_loss_step=81.80, train_loss_epoch=81.80]\n",
      "Epoch 6: 100%|##########| 1/1 [00:00<00:00,  7.70it/s, loss=64, train_loss_step=9.250, train_loss_epoch=81.80]  \n",
      "Epoch 6: 100%|##########| 1/1 [00:00<00:00,  7.44it/s, loss=64, train_loss_step=9.250, train_loss_epoch=9.250]\n",
      "Epoch 6:   0%|          | 0/1 [00:00<?, ?it/s, loss=64, train_loss_step=9.250, train_loss_epoch=9.250]        \n",
      "Epoch 7:   0%|          | 0/1 [00:00<?, ?it/s, loss=64, train_loss_step=9.250, train_loss_epoch=9.250]\n",
      "Epoch 7: 100%|##########| 1/1 [00:00<00:00,  8.20it/s, loss=64, train_loss_step=9.250, train_loss_epoch=9.250]\n",
      "Epoch 7: 100%|##########| 1/1 [00:00<00:00,  7.98it/s, loss=57.2, train_loss_step=9.460, train_loss_epoch=9.250]\n",
      "Epoch 7: 100%|##########| 1/1 [00:00<00:00,  7.66it/s, loss=57.2, train_loss_step=9.460, train_loss_epoch=9.460]\n",
      "Epoch 7:   0%|          | 0/1 [00:00<?, ?it/s, loss=57.2, train_loss_step=9.460, train_loss_epoch=9.460]        \n",
      "Epoch 8:   0%|          | 0/1 [00:00<?, ?it/s, loss=57.2, train_loss_step=9.460, train_loss_epoch=9.460]\n",
      "Epoch 8: 100%|##########| 1/1 [00:00<00:00,  7.97it/s, loss=57.2, train_loss_step=9.460, train_loss_epoch=9.460]\n",
      "Epoch 8: 100%|##########| 1/1 [00:00<00:00,  7.76it/s, loss=51.6, train_loss_step=7.160, train_loss_epoch=9.460]\n",
      "Epoch 8: 100%|##########| 1/1 [00:00<00:00,  7.53it/s, loss=51.6, train_loss_step=7.160, train_loss_epoch=7.160]\n",
      "Epoch 8:   0%|          | 0/1 [00:00<?, ?it/s, loss=51.6, train_loss_step=7.160, train_loss_epoch=7.160]        \n",
      "Epoch 9:   0%|          | 0/1 [00:00<?, ?it/s, loss=51.6, train_loss_step=7.160, train_loss_epoch=7.160]\n",
      "Epoch 9: 100%|##########| 1/1 [00:00<00:00,  8.09it/s, loss=51.6, train_loss_step=7.160, train_loss_epoch=7.160]\n",
      "Epoch 9: 100%|##########| 1/1 [00:00<00:00,  7.58it/s, loss=47.2, train_loss_step=6.870, train_loss_epoch=7.160]\n",
      "Epoch 9: 100%|##########| 1/1 [00:00<00:00,  6.98it/s, loss=47.2, train_loss_step=6.870, train_loss_epoch=6.870]\n",
      "Epoch 9: 100%|##########| 1/1 [00:00<00:00,  5.35it/s, loss=47.2, train_loss_step=6.870, train_loss_epoch=6.870]\n",
      " 50%|     | 1/2 [00:08<00:05,  5.58s/trial, best loss: 8.174054145812988]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cchallu/NIXTLA/neuralforecast/neuralforecast/data/tsloader.py:47: UserWarning: This class wraps the pytorch `DataLoader` with a special collate function. If you want to use yours simply use `DataLoader`. Removing collate_fn\n",
      "  'This class wraps the pytorch `DataLoader` with a '\n",
      "\n",
      "/Users/cchallu/opt/anaconda3/envs/neuralforecast/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, predict_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting: 1it [00:00, ?it/s]                                                \n",
      "Predicting: 100%|##########| 1/1 [00:00<?, ?it/s]                             \n",
      "VAL y_true.shape: (145, 24)                                                   \n",
      "VAL y_hat.shape: (145, 24)                                                    \n",
      "Predicting: 1it [00:00, ?it/s]                                                \n",
      "Predicting: 100%|##########| 1/1 [00:00<?, ?it/s]                             \n",
      "TEST y_true.shape: (145, 24)                                                  \n",
      "TEST y_hat.shape: (145, 24)                                                   \n",
      "100%|| 2/2 [00:08<00:00,  4.34s/trial, best loss: 8.174054145812988]\n"
     ]
    }
   ],
   "source": [
    "nbeats_space = nbeats_suggested_space(n_time_out=24, n_series=1, n_x=1, n_s=0, frequency='H')\n",
    "nbeats_space['max_steps'] = hp.choice('max_steps', [10]) # Override max_steps for faster example\n",
    "trials = hyperopt_tunning(space=nbeats_space, hyperopt_max_evals=2, loss_function_val=mae,\n",
    "                          loss_functions_test={'mae': mae, 'rmse': rmse},\n",
    "                          S_df=S_df, Y_df=Y_df, X_df=X_df, f_cols=[],\n",
    "                          ds_in_val=7*24, ds_in_test=7*24, return_forecasts=True, save_progress=False, results_file=None, loss_kwargs={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
