{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp experiments.utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Utils\n",
    "> This notebook contains a set of functions to easily perform experiments on time series datasets. In this notebook you can see functions for:\n",
    "1. Preparing dataset\n",
    "2. Loadnig models\n",
    "3. Training models\n",
    "4. Model evaluation\n",
    "5. Hyperparameter tunning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">In the next two scetion you can see enviroment variables and imports that are used for set of functions shown in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "ENV_VARS = dict(OMP_NUM_THREADS='2',\n",
    "                OPENBLAS_NUM_THREADS='2',\n",
    "                MKL_NUM_THREADS='3',\n",
    "                VECLIB_MAXIMUM_THREADS='2',\n",
    "                NUMEXPR_NUM_THREADS='3')\n",
    "\n",
    "import os\n",
    "# Limit number of threads in numpy and others to avoid throttling\n",
    "os.environ.update(ENV_VARS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import time\n",
    "import pickle\n",
    "from typing import Tuple\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch as t\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import logging\n",
    "import pytorch_lightning as pl\n",
    "pl.utilities.distributed.log.setLevel(logging.ERROR)\n",
    "\n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
    "\n",
    "from neuralforecast.data.scalers import Scaler\n",
    "from neuralforecast.data.tsdataset import (\n",
    "    TimeSeriesDataset, \n",
    "    WindowsDataset, \n",
    "    IterateWindowsDataset, \n",
    "    BaseDataset\n",
    ")\n",
    "\n",
    "from neuralforecast.data.tsloader import TimeSeriesLoader\n",
    "from neuralforecast.models.esrnn.esrnn import ESRNN\n",
    "from neuralforecast.models.rnn.rnn import RNN\n",
    "from neuralforecast.models.esrnn.mqesrnn import MQESRNN\n",
    "from neuralforecast.models.nbeats.nbeats import NBEATS\n",
    "from neuralforecast.models.nhits.nhits import NHITS\n",
    "from neuralforecast.models.mqnhits.mqnhits import MQNHITS\n",
    "from neuralforecast.models.transformer.autoformer import Autoformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions that represent basic use of neuralforecast library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_mask_dfs(Y_df: pd.DataFrame,\n",
    "                 ds_in_val: int, ds_in_test: int) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Generates train, test and validation mask.\n",
    "    Train mask begins by avoiding ds_in_test.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    Y_df: pd.DataFrame\n",
    "        Target time series with columns ['unique_id', 'ds', 'y'].\n",
    "    ds_in_val: int\n",
    "        Number of ds in validation.\n",
    "    ds_in_test: int\n",
    "        Number of ds in test.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    train_mask_df: pd.DataFrame\n",
    "        Train mask dataframe.\n",
    "    val_mask_df: pd.DataFrame\n",
    "        Validation mask dataframe.\n",
    "    test_mask_df: pd.DataFrame\n",
    "        Test mask dataframe.\n",
    "    \"\"\"\n",
    "\n",
    "    # train mask\n",
    "    train_mask_df = Y_df.copy()[['unique_id', 'ds']]\n",
    "    train_mask_df.sort_values(by=['unique_id', 'ds'], inplace=True)\n",
    "    train_mask_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    train_mask_df['sample_mask'] = 1\n",
    "    train_mask_df['available_mask'] = 1\n",
    "    \n",
    "    idx_out = train_mask_df.groupby('unique_id').tail(ds_in_val+ds_in_test).index\n",
    "    train_mask_df.loc[idx_out, 'sample_mask'] = 0\n",
    "    \n",
    "    # test mask\n",
    "    test_mask_df = train_mask_df.copy()\n",
    "    test_mask_df['sample_mask'] = 0\n",
    "    idx_test = test_mask_df.groupby('unique_id').tail(ds_in_test).index\n",
    "    test_mask_df.loc[idx_test, 'sample_mask'] = 1\n",
    "    \n",
    "    # validation mask\n",
    "    val_mask_df = train_mask_df.copy()\n",
    "    val_mask_df['sample_mask'] = 1\n",
    "    val_mask_df['sample_mask'] = val_mask_df['sample_mask'] - train_mask_df['sample_mask']\n",
    "    val_mask_df['sample_mask'] = val_mask_df['sample_mask'] - test_mask_df['sample_mask']\n",
    "\n",
    "    assert len(train_mask_df)==len(Y_df), \\\n",
    "        f'The mask_df length {len(train_mask_df)} is not equal to Y_df length {len(Y_df)}'\n",
    "    \n",
    "    return train_mask_df, val_mask_df, test_mask_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_random_mask_dfs(Y_df: pd.DataFrame, ds_in_test: int, \n",
    "                        n_val_windows: int, n_ds_val_window: int,\n",
    "                        n_uids: int, freq: str) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Generates train, test and random validation mask.\n",
    "    Train mask begins by avoiding ds_in_test\n",
    "    \n",
    "    Validation mask: 1) samples n_uids unique ids\n",
    "                     2) creates windows of size n_ds_val_window\n",
    "                     \n",
    "    Parameters\n",
    "    ----------\n",
    "    Y_df: pd.DataFrame\n",
    "        Target time series with columns ['unique_id', 'ds', 'y'].\n",
    "    ds_in_test: int\n",
    "        Number of ds in test.\n",
    "    n_val_windows: int\n",
    "        Number of windows for validation.\n",
    "    n_ds_val_window: int\n",
    "        Number of ds in each validation window.\n",
    "    n_uids: int\n",
    "        Number of unique ids in validation.\n",
    "    freq: str\n",
    "        string that determines datestamp frequency, used in\n",
    "        random windows creation.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    train_mask_df: pd.DataFrame\n",
    "        Train mask dataframe.\n",
    "    val_mask_df: pd.DataFrame\n",
    "        Validation mask dataframe.\n",
    "    test_mask_df: pd.DataFrame\n",
    "        Test mask dataframe.\n",
    "    \"\"\"\n",
    "    np.random.seed(1)\n",
    "    #----------------------- Train mask -----------------------#\n",
    "    # Initialize masks\n",
    "    train_mask_df, val_mask_df, test_mask_df = get_mask_dfs(Y_df=Y_df,\n",
    "                                                            ds_in_val=0,\n",
    "                                                            ds_in_test=ds_in_test)\n",
    "    \n",
    "    assert val_mask_df['sample_mask'].sum()==0, 'Muerte'\n",
    "    \n",
    "    #----------------- Random Validation mask -----------------#\n",
    "    # Overwrite validation with random windows\n",
    "    uids = train_mask_df['unique_id'].unique()\n",
    "    val_uids = np.random.choice(uids, n_uids, replace=False)\n",
    "    \n",
    "    # Validation avoids test\n",
    "    idx_test = train_mask_df.groupby('unique_id').tail(ds_in_test).index\n",
    "    available_ds = train_mask_df.loc[~train_mask_df.index.isin(idx_test)]['ds'].unique()\n",
    "    val_init_ds = np.random.choice(available_ds, n_val_windows, replace=False)\n",
    "    \n",
    "    # Creates windows \n",
    "    val_ds = [pd.date_range(init, periods=n_ds_val_window, freq=freq) for init in val_init_ds]\n",
    "    val_ds = np.concatenate(val_ds)\n",
    "\n",
    "    # Cleans random windows from train mask\n",
    "    val_idx = train_mask_df.query('unique_id in @val_uids & ds in @val_ds').index\n",
    "    train_mask_df.loc[val_idx, 'sample_mask'] = 0\n",
    "    val_mask_df.loc[val_idx, 'sample_mask'] = 1\n",
    "    \n",
    "    return train_mask_df, val_mask_df, test_mask_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def create_datasets(mc: dict, S_df: pd.DataFrame, \n",
    "                    Y_df: pd.DataFrame, X_df: pd.DataFrame, f_cols: list,\n",
    "                    ds_in_test: int, ds_in_val: int, verbose: bool=False) -> Tuple[BaseDataset, BaseDataset, BaseDataset, Scaler]:\n",
    "    \"\"\"\n",
    "    Creates train, validation and test datasets.\n",
    "                     \n",
    "    Parameters\n",
    "    ----------\n",
    "    mc: dict\n",
    "        Model configuration.\n",
    "    S_df: pd.DataFrame\n",
    "        Static exogenous variables with columns ['unique_id', 'ds'] \n",
    "        and static variables. \n",
    "    Y_df: pd.DataFrame\n",
    "        Target time series with columns ['unique_id', 'ds', 'y'].\n",
    "    X_df: pd.DataFrame\n",
    "        Exogenous time series with columns ['unique_id', 'ds', 'y']\n",
    "    f_cols: list\n",
    "        List of exogenous variables of the future.\n",
    "    ds_in_test: int\n",
    "        Number of ds in test.\n",
    "    ds_in_val: int\n",
    "        Number of ds in validation.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    train_dataset: BaseDataset\n",
    "        Train dataset.\n",
    "    valid_dataset: BaseDataset\n",
    "        Validation dataset.\n",
    "    test_dataset: BaseDataset\n",
    "        Test dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    #------------------------------------- Available and Validation Mask ------------------------------------#\n",
    "    train_mask_df, valid_mask_df, test_mask_df = get_mask_dfs(Y_df=Y_df,\n",
    "                                                              ds_in_val=ds_in_val,\n",
    "                                                              ds_in_test=ds_in_test)\n",
    "\n",
    "    #---------------------------------------------- Scaler ----------------------------------------------#\n",
    "    if mc['scaler'] is not None:\n",
    "        scaler = Scaler(technique=mc['scaler'])\n",
    "    else:\n",
    "        scaler = None\n",
    "\n",
    "    #----------------------------------------- Declare Dataset and Loaders ----------------------------------#\n",
    "    \n",
    "    if mc['mode'] == 'simple':\n",
    "        train_dataset = WindowsDataset(S_df=S_df, Y_df=Y_df, X_df=X_df,\n",
    "                                       mask_df=train_mask_df, f_cols=f_cols,\n",
    "                                       input_size=int(mc['n_time_in']),\n",
    "                                       output_size=int(mc['n_time_out']),\n",
    "                                       sample_freq=int(mc['idx_to_sample_freq']),\n",
    "                                       complete_windows=mc['complete_windows'],\n",
    "                                       scaler=scaler,\n",
    "                                       verbose=verbose)\n",
    "        \n",
    "        valid_dataset = WindowsDataset(S_df=S_df, Y_df=Y_df, X_df=X_df,\n",
    "                                       mask_df=valid_mask_df, f_cols=f_cols,\n",
    "                                       input_size=int(mc['n_time_in']),\n",
    "                                       output_size=int(mc['n_time_out']),\n",
    "                                       sample_freq=int(mc['val_idx_to_sample_freq']),\n",
    "                                       complete_windows=True,\n",
    "                                       scaler=scaler,\n",
    "                                       verbose=verbose)\n",
    "        \n",
    "        test_dataset = WindowsDataset(S_df=S_df, Y_df=Y_df, X_df=X_df,\n",
    "                                      mask_df=test_mask_df, f_cols=f_cols,\n",
    "                                      input_size=int(mc['n_time_in']),\n",
    "                                      output_size=int(mc['n_time_out']),\n",
    "                                      sample_freq=int(mc['val_idx_to_sample_freq']),\n",
    "                                      complete_windows=True,\n",
    "                                      scaler=scaler,\n",
    "                                      verbose=verbose)\n",
    "    if mc['mode'] == 'iterate_windows':\n",
    "        train_dataset = IterateWindowsDataset(S_df=S_df, Y_df=Y_df, X_df=X_df,\n",
    "                                              mask_df=train_mask_df, f_cols=f_cols,\n",
    "                                              input_size=int(mc['n_time_in']),\n",
    "                                              output_size=int(mc['n_time_out']),\n",
    "                                              scaler=scaler,\n",
    "                                              verbose=verbose)\n",
    "        \n",
    "        valid_dataset = IterateWindowsDataset(S_df=S_df, Y_df=Y_df, X_df=X_df,\n",
    "                                              mask_df=valid_mask_df, f_cols=f_cols,\n",
    "                                              input_size=int(mc['n_time_in']),\n",
    "                                              output_size=int(mc['n_time_out']),\n",
    "                                              scaler=scaler,\n",
    "                                              verbose=verbose)\n",
    "        \n",
    "        test_dataset = IterateWindowsDataset(S_df=S_df, Y_df=Y_df, X_df=X_df,\n",
    "                                             mask_df=test_mask_df, f_cols=f_cols,\n",
    "                                             input_size=int(mc['n_time_in']),\n",
    "                                             output_size=int(mc['n_time_out']),\n",
    "                                             scaler=scaler,\n",
    "                                             verbose=verbose)   \n",
    "    \n",
    "    if mc['mode'] == 'full':\n",
    "        train_dataset = TimeSeriesDataset(S_df=S_df, Y_df=Y_df, X_df=X_df,\n",
    "                                          mask_df=train_mask_df, f_cols=f_cols,\n",
    "                                          input_size=int(mc['n_time_in']),\n",
    "                                          output_size=int(mc['n_time_out']),\n",
    "                                          scaler=scaler,\n",
    "                                          verbose=verbose)\n",
    "        \n",
    "        valid_dataset = TimeSeriesDataset(S_df=S_df, Y_df=Y_df, X_df=X_df,\n",
    "                                          mask_df=valid_mask_df, f_cols=f_cols,\n",
    "                                          input_size=int(mc['n_time_in']),\n",
    "                                          output_size=int(mc['n_time_out']),\n",
    "                                          scaler=scaler,\n",
    "                                          verbose=verbose)\n",
    "        \n",
    "        test_dataset = TimeSeriesDataset(S_df=S_df, Y_df=Y_df, X_df=X_df,\n",
    "                                         mask_df=test_mask_df, f_cols=f_cols,\n",
    "                                         input_size=int(mc['n_time_in']),\n",
    "                                         output_size=int(mc['n_time_out']),\n",
    "                                         scaler=scaler,\n",
    "                                         verbose=verbose)        \n",
    "    \n",
    "    if ds_in_test == 0:\n",
    "        test_dataset = None\n",
    "\n",
    "    return train_dataset, valid_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def instantiate_loaders(mc: dict, \n",
    "                        train_dataset: BaseDataset, val_dataset: BaseDataset, \n",
    "                        test_dataset: BaseDataset) -> Tuple[DataLoader, DataLoader, DataLoader]:\n",
    "    \"\"\"\n",
    "    Creates train, validation and test loader classes.\n",
    "                     \n",
    "    Parameters\n",
    "    ----------\n",
    "    mc: dict\n",
    "        Model configuration.\n",
    "    train_dataset: BaseDataset\n",
    "        Train dataset.\n",
    "    val_dataset: BaseDataset\n",
    "        Validation dataset.\n",
    "    test_dataset: BaseDataset\n",
    "        Test dataset.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    train_loader: DataLoader\n",
    "        Train loader.\n",
    "    val_loader: DataLoader\n",
    "        Validation loader.\n",
    "    test_loader: DataLoader\n",
    "        Test loader.\n",
    "    \"\"\"\n",
    "\n",
    "    if mc['mode'] in ['simple', 'full'] :\n",
    "        n_windows = mc['n_windows'] if mc['mode']=='simple' else None\n",
    "        train_loader = TimeSeriesLoader(dataset=train_dataset,\n",
    "                                        batch_size=int(mc['batch_size']),\n",
    "                                        n_windows=n_windows,\n",
    "                                        eq_batch_size=False,\n",
    "                                        shuffle=True)\n",
    "        if val_dataset is not None:\n",
    "            val_loader = TimeSeriesLoader(dataset=val_dataset,\n",
    "                                        batch_size=1,\n",
    "                                        shuffle=False)\n",
    "        else:\n",
    "            val_loader = None\n",
    "\n",
    "        if test_dataset is not None:\n",
    "            test_loader = TimeSeriesLoader(dataset=test_dataset,\n",
    "                                        batch_size=1,\n",
    "                                        shuffle=False)\n",
    "        else:\n",
    "            test_loader = None\n",
    "\n",
    "    elif mc['mode'] == 'iterate_windows':\n",
    "        train_loader =DataLoader(dataset=train_dataset,\n",
    "                                 batch_size=int(mc['batch_size']),\n",
    "                                 shuffle=True,\n",
    "                                 drop_last=True)\n",
    "\n",
    "        if val_dataset is not None:\n",
    "            val_loader = DataLoader(dataset=val_dataset,\n",
    "                                    batch_size=1,\n",
    "                                    shuffle=False)\n",
    "        else:\n",
    "            val_loader = None\n",
    "\n",
    "        if test_dataset is not None:\n",
    "            test_loader = DataLoader(dataset=test_dataset,\n",
    "                                     batch_size=1,\n",
    "                                     shuffle=False)\n",
    "        else:\n",
    "            test_loader = None\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def instantiate_nbeats(mc: dict) -> NBEATS:\n",
    "    \"\"\"\n",
    "    Creates nbeats model.\n",
    "                     \n",
    "    Parameters\n",
    "    ----------\n",
    "    mc: dict\n",
    "        Model configuration.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model: NBEATS\n",
    "        Nbeats model.\n",
    "    \"\"\"\n",
    "    mc['n_mlp_units'] = len(mc['stack_types']) * [ mc['constant_n_layers'] * [int(mc['constant_n_mlp_units'])] ]\n",
    "    mc['n_layers'] =  len(mc['stack_types']) * [ mc['constant_n_layers'] ]\n",
    "    mc['n_blocks'] =  len(mc['stack_types']) * [ mc['constant_n_blocks'] ]\n",
    "\n",
    "    if mc['max_epochs'] is not None:\n",
    "        lr_decay_step_size = int(np.ceil(mc['max_epochs'] / mc['n_lr_decays']))\n",
    "    elif mc['max_steps'] is not None:\n",
    "        lr_decay_step_size = int(np.ceil(mc['max_steps'] / mc['n_lr_decays']))\n",
    "\n",
    "    model = NBEATS(n_time_in=int(mc['n_time_in']),\n",
    "                  n_time_out=int(mc['n_time_out']),\n",
    "                  n_x=mc['n_x'],\n",
    "                  n_s=mc['n_s'],\n",
    "                  n_s_hidden=int(mc['n_s_hidden']),\n",
    "                  n_x_hidden=int(mc['n_x_hidden']),\n",
    "                  shared_weights = mc['shared_weights'],\n",
    "                  initialization=mc['initialization'],\n",
    "                  activation=mc['activation'],\n",
    "                  stack_types=mc['stack_types'],\n",
    "                  n_blocks=mc['n_blocks'],\n",
    "                  n_layers=mc['n_layers'],\n",
    "                  n_mlp_units=mc['n_mlp_units'],\n",
    "                  batch_normalization = mc['batch_normalization'],\n",
    "                  dropout_prob_theta=mc['dropout_prob_theta'],\n",
    "                  learning_rate=float(mc['learning_rate']),\n",
    "                  lr_decay=float(mc['lr_decay']),\n",
    "                  lr_decay_step_size=lr_decay_step_size,\n",
    "                  weight_decay=mc['weight_decay'],\n",
    "                  loss_train=mc['loss_train'],\n",
    "                  loss_hypar=float(mc['loss_hypar']),\n",
    "                  loss_valid=mc['loss_valid'],\n",
    "                  frequency=mc['frequency'],\n",
    "                  random_seed=int(mc['random_seed']))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def instantiate_esrnn(mc: dict) -> ESRNN:  \n",
    "    \"\"\"\n",
    "    Creates esrnn model.\n",
    "                     \n",
    "    Parameters\n",
    "    ----------\n",
    "    mc: dict\n",
    "        Model configuration.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model: ESRNN\n",
    "        Esrnn model.\n",
    "    \"\"\"  \n",
    "    if mc['max_epochs'] is not None:\n",
    "        lr_decay_step_size = int(np.ceil(mc['max_epochs'] / mc['n_lr_decays']))\n",
    "    elif mc['max_steps'] is not None:\n",
    "        lr_decay_step_size = int(np.ceil(mc['max_steps'] / mc['n_lr_decays']))\n",
    "\n",
    "    model = ESRNN(# Architecture parameters\n",
    "                  n_series=mc['n_series'],\n",
    "                  n_x=mc['n_x'],\n",
    "                  n_s=mc['n_s'],\n",
    "                  sample_freq=int(mc['sample_freq']),\n",
    "                  input_size=int(mc['n_time_in']),\n",
    "                  output_size=int(mc['n_time_out']),\n",
    "                  es_component=mc['es_component'],\n",
    "                  cell_type=mc['cell_type'],\n",
    "                  state_hsize=int(mc['state_hsize']),\n",
    "                  dilations=mc['dilations'],\n",
    "                  add_nl_layer=mc['add_nl_layer'],\n",
    "                  # Optimization parameters                \n",
    "                  learning_rate=mc['learning_rate'],\n",
    "                  lr_scheduler_step_size=lr_decay_step_size,\n",
    "                  lr_decay=mc['lr_decay'],\n",
    "                  per_series_lr_multip=mc['per_series_lr_multip'],\n",
    "                  gradient_eps=mc['gradient_eps'],\n",
    "                  gradient_clipping_threshold=mc['gradient_clipping_threshold'],\n",
    "                  rnn_weight_decay=mc['rnn_weight_decay'],\n",
    "                  noise_std=mc['noise_std'],\n",
    "                  level_variability_penalty=mc['level_variability_penalty'],\n",
    "                  testing_percentile=mc['testing_percentile'],\n",
    "                  training_percentile=mc['training_percentile'],\n",
    "                  loss=mc['loss_train'],\n",
    "                  val_loss=mc['loss_valid'],\n",
    "                  seasonality=mc['seasonality']) \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def instantiate_rnn(mc: dict) -> ESRNN:  \n",
    "    \"\"\"\n",
    "    Creates esrnn model.\n",
    "                     \n",
    "    Parameters\n",
    "    ----------\n",
    "    mc: dict\n",
    "        Model configuration.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model: RNN\n",
    "        RNN model.\n",
    "    \"\"\"  \n",
    "    if mc['max_epochs'] is not None:\n",
    "        lr_decay_step_size = int(np.ceil(mc['max_epochs'] / mc['n_lr_decays']))\n",
    "    elif mc['max_steps'] is not None:\n",
    "        lr_decay_step_size = int(np.ceil(mc['max_steps'] / mc['n_lr_decays']))\n",
    "\n",
    "    model = RNN(# Architecture parameters\n",
    "                  input_size=int(mc['n_time_in']),\n",
    "                  output_size=int(mc['n_time_out']),\n",
    "                  n_x=mc['n_x'],\n",
    "                  n_s=mc['n_s'],\n",
    "                  sample_freq=int(mc['sample_freq']),\n",
    "                  cell_type=mc['cell_type'],\n",
    "                  state_hsize=int(mc['state_hsize']),\n",
    "                  dilations=mc['dilations'],\n",
    "                  add_nl_layer=mc['add_nl_layer'],\n",
    "                  # Optimization parameters\n",
    "                  learning_rate=mc['learning_rate'],\n",
    "                  lr_scheduler_step_size=lr_decay_step_size,\n",
    "                  lr_decay=mc['lr_decay'],\n",
    "                  gradient_eps=mc['gradient_eps'],\n",
    "                  gradient_clipping_threshold=mc['gradient_clipping_threshold'],\n",
    "                  weight_decay=mc['weight_decay'],\n",
    "                  noise_std=mc['noise_std'],\n",
    "                  loss_train=mc['loss_train'],\n",
    "                  loss_valid=mc['loss_valid'],\n",
    "                  loss_hypar=mc['loss_hypar'],\n",
    "                  frequency=mc['frequency'],\n",
    "                  random_seed=int(mc['random_seed']))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def instantiate_mqesrnn(mc: dict) -> MQESRNN: \n",
    "    \"\"\"\n",
    "    Creates mqesrnn model.\n",
    "                     \n",
    "    Parameters\n",
    "    ----------\n",
    "    mc: dict\n",
    "        Model configuration.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model: MQESRNN\n",
    "        Mqesrnn model.\n",
    "    \"\"\"  \n",
    "\n",
    "    if mc['max_epochs'] is not None:\n",
    "        lr_decay_step_size = int(np.ceil(mc['max_epochs'] / mc['n_lr_decays']))\n",
    "    elif mc['max_steps'] is not None:\n",
    "        lr_decay_step_size = int(np.ceil(mc['max_steps'] / mc['n_lr_decays']))\n",
    "\n",
    "    model = MQESRNN(# Architecture parameters\n",
    "                    n_series=mc['n_series'],\n",
    "                    n_x=mc['n_x'],\n",
    "                    n_s=mc['n_s'],\n",
    "                    idx_to_sample_freq=int(mc['idx_to_sample_freq']),\n",
    "                    input_size=int(mc['n_time_in']),\n",
    "                    output_size=int(mc['n_time_out']),\n",
    "                    es_component=mc['es_component'],\n",
    "                    cell_type=mc['cell_type'],\n",
    "                    state_hsize=int(mc['state_hsize']),\n",
    "                    dilations=mc['dilations'],\n",
    "                    add_nl_layer=mc['add_nl_layer'],\n",
    "                    # Optimization parameters                 \n",
    "                    learning_rate=mc['learning_rate'],\n",
    "                    lr_scheduler_step_size=lr_decay_step_size,\n",
    "                    lr_decay=mc['lr_decay'],\n",
    "                    gradient_eps=mc['gradient_eps'],\n",
    "                    gradient_clipping_threshold=mc['gradient_clipping_threshold'],\n",
    "                    rnn_weight_decay=mc['rnn_weight_decay'],\n",
    "                    noise_std=mc['noise_std'],\n",
    "                    testing_percentiles=list(mc['testing_percentiles']),\n",
    "                    training_percentiles=list(mc['training_percentiles']),\n",
    "                    loss=mc['loss_train'],\n",
    "                    val_loss=mc['loss_valid']) \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def instantiate_nhits(mc: dict) -> NHITS:\n",
    "    \"\"\"\n",
    "    Creates nhits model.\n",
    "                     \n",
    "    Parameters\n",
    "    ----------\n",
    "    mc: dict\n",
    "        Model configuration.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model: NHITS\n",
    "        Nhits model.\n",
    "    \"\"\"  \n",
    "    \n",
    "    mc['n_mlp_units'] = len(mc['stack_types']) * [ mc['constant_n_layers'] * [int(mc['constant_n_mlp_units'])] ]\n",
    "    mc['n_layers'] =  len(mc['stack_types']) * [ mc['constant_n_layers'] ]\n",
    "    mc['n_blocks'] =  len(mc['stack_types']) * [ mc['constant_n_blocks'] ]\n",
    "\n",
    "    if mc['max_epochs'] is not None:\n",
    "        lr_decay_step_size = int(np.ceil(mc['max_epochs'] / mc['n_lr_decays']))\n",
    "    elif mc['max_steps'] is not None:\n",
    "        lr_decay_step_size = int(np.ceil(mc['max_steps'] / mc['n_lr_decays']))\n",
    "\n",
    "    model = NHITS(n_time_in=int(mc['n_time_in']),\n",
    "                  n_time_out=int(mc['n_time_out']),\n",
    "                  n_x=mc['n_x'],\n",
    "                  n_s=mc['n_s'],\n",
    "                  n_s_hidden=int(mc['n_s_hidden']),\n",
    "                  n_x_hidden=int(mc['n_x_hidden']),\n",
    "                  shared_weights = mc['shared_weights'],\n",
    "                  initialization=mc['initialization'],\n",
    "                  activation=mc['activation'],\n",
    "                  stack_types=mc['stack_types'],\n",
    "                  n_blocks=mc['n_blocks'],\n",
    "                  n_layers=mc['n_layers'],\n",
    "                  n_mlp_units=mc['n_mlp_units'],\n",
    "                  n_pool_kernel_size=mc['n_pool_kernel_size'],\n",
    "                  n_freq_downsample=mc['n_freq_downsample'],\n",
    "                  pooling_mode=mc['pooling_mode'],\n",
    "                  interpolation_mode=mc['interpolation_mode'],\n",
    "                  batch_normalization = mc['batch_normalization'],\n",
    "                  dropout_prob_theta=mc['dropout_prob_theta'],\n",
    "                  learning_rate=float(mc['learning_rate']),\n",
    "                  lr_decay=float(mc['lr_decay']),\n",
    "                  lr_decay_step_size=lr_decay_step_size,\n",
    "                  weight_decay=mc['weight_decay'],\n",
    "                  loss_train=mc['loss_train'],\n",
    "                  loss_hypar=float(mc['loss_hypar']),\n",
    "                  loss_valid=mc['loss_valid'],\n",
    "                  frequency=mc['frequency'],\n",
    "                  random_seed=int(mc['random_seed']))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def instantiate_mqnhits(mc: dict) -> MQNHITS:\n",
    "    \"\"\"\n",
    "    Creates mqnhits model.\n",
    "                     \n",
    "    Parameters\n",
    "    ----------\n",
    "    mc: dict\n",
    "        Model configuration.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model: NHITS\n",
    "        Nhits model.\n",
    "    \"\"\"  \n",
    "    \n",
    "    mc['n_mlp_units'] = len(mc['stack_types']) * [ mc['constant_n_layers'] * [int(mc['constant_n_mlp_units'])] ]\n",
    "    mc['n_layers'] =  len(mc['stack_types']) * [ mc['constant_n_layers'] ]\n",
    "    mc['n_blocks'] =  len(mc['stack_types']) * [ mc['constant_n_blocks'] ]\n",
    "\n",
    "    if mc['max_epochs'] is not None:\n",
    "        lr_decay_step_size = int(np.ceil(mc['max_epochs'] / mc['n_lr_decays']))\n",
    "    elif mc['max_steps'] is not None:\n",
    "        lr_decay_step_size = int(np.ceil(mc['max_steps'] / mc['n_lr_decays']))\n",
    "\n",
    "    model = MQNHITS(n_time_in=int(mc['n_time_in']),\n",
    "                    n_time_out=int(mc['n_time_out']),\n",
    "                    quantiles=list(mc['quantiles']),\n",
    "                    n_x=mc['n_x'],\n",
    "                    n_s=mc['n_s'],\n",
    "                    n_s_hidden=int(mc['n_s_hidden']),\n",
    "                    n_x_hidden=int(mc['n_x_hidden']),\n",
    "                    shared_weights = mc['shared_weights'],\n",
    "                    initialization=mc['initialization'],\n",
    "                    activation=mc['activation'],\n",
    "                    stack_types=mc['stack_types'],\n",
    "                    n_blocks=mc['n_blocks'],\n",
    "                    n_layers=mc['n_layers'],\n",
    "                    n_mlp_units=mc['n_mlp_units'],\n",
    "                    n_pool_kernel_size=mc['n_pool_kernel_size'],\n",
    "                    n_freq_downsample=mc['n_freq_downsample'],\n",
    "                    pooling_mode=mc['pooling_mode'],\n",
    "                    interpolation_mode=mc['interpolation_mode'],\n",
    "                    batch_normalization = mc['batch_normalization'],\n",
    "                    dropout_prob_theta=mc['dropout_prob_theta'],\n",
    "                    learning_rate=float(mc['learning_rate']),\n",
    "                    lr_decay=float(mc['lr_decay']),\n",
    "                    lr_decay_step_size=lr_decay_step_size,\n",
    "                    weight_decay=mc['weight_decay'],\n",
    "                    loss_train=mc['loss_train'],\n",
    "                    loss_hypar=float(mc['loss_hypar']),\n",
    "                    loss_valid=mc['loss_valid'],\n",
    "                    frequency=mc['frequency'],\n",
    "                    random_seed=int(mc['random_seed']))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def instantiate_autoformer(mc: dict) -> Autoformer:\n",
    "    \"\"\"\n",
    "    Creates autoformer model.\n",
    "                     \n",
    "    Parameters\n",
    "    ----------\n",
    "    mc: dict\n",
    "        Model configuration.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model: Autoformer\n",
    "        Autoformer model.\n",
    "    \"\"\"  \n",
    "\n",
    "    if mc['max_epochs'] is not None:\n",
    "        lr_decay_step_size = int(np.ceil(mc['max_epochs'] / mc['n_lr_decays']))\n",
    "    elif mc['max_steps'] is not None:\n",
    "        lr_decay_step_size = int(np.ceil(mc['max_steps'] / mc['n_lr_decays']))\n",
    "    \n",
    "    model = Autoformer(seq_len=int(mc['seq_len']),\n",
    "                       label_len=int(mc['label_len']),\n",
    "                       pred_len=int(mc['pred_len']),\n",
    "                       output_attention=mc['output_attention'],\n",
    "                       enc_in=int(mc['enc_in']),\n",
    "                       dec_in=int(mc['dec_in']),\n",
    "                       d_model=int(mc['d_model']),\n",
    "                       c_out=int(mc['c_out']),\n",
    "                       embed = mc['embed'],\n",
    "                       freq=mc['freq'],\n",
    "                       dropout=mc['dropout'],\n",
    "                       factor=mc['factor'],\n",
    "                       n_heads=int(mc['n_heads']),\n",
    "                       d_ff=int(mc['d_ff']),\n",
    "                       moving_avg=int(mc['moving_avg']),\n",
    "                       activation=mc['activation'],\n",
    "                       e_layers=int(mc['e_layers']),\n",
    "                       d_layers=int(mc['d_layers']),\n",
    "                       learning_rate=float(mc['learning_rate']),\n",
    "                       lr_decay=float(mc['lr_decay']),\n",
    "                       lr_decay_step_size=lr_decay_step_size,\n",
    "                       weight_decay=mc['weight_decay'],\n",
    "                       loss_train=mc['loss_train'],\n",
    "                       loss_hypar=float(mc['loss_hypar']),\n",
    "                       loss_valid=mc['loss_valid'],\n",
    "                       random_seed=int(mc['random_seed']))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def instantiate_model(mc: dict) -> pl.LightningModule:\n",
    "    \"\"\"\n",
    "    Creates one of the models.\n",
    "    (nbeats, esrnn, mqesrnn, nhits, autoformer)\n",
    "                     \n",
    "    Parameters\n",
    "    ----------\n",
    "    mc: dict\n",
    "        Model configuration.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model: pl.LightningModule\n",
    "        Forecast model.\n",
    "    \"\"\"  \n",
    "    MODEL_DICT = {'nbeats': instantiate_nbeats,\n",
    "                  'esrnn': instantiate_esrnn,\n",
    "                  'rnn': instantiate_rnn,\n",
    "                  'mqesrnn': instantiate_mqesrnn,\n",
    "                  'nhits': instantiate_nhits,\n",
    "                  'mqnhits': instantiate_mqnhits,\n",
    "                  'autoformer': instantiate_autoformer}\n",
    "    return MODEL_DICT[mc['model']](mc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def predict(mc: dict, model: pl.LightningModule, \n",
    "            trainer: pl.Trainer, loader: DataLoader) -> Tuple[np.array, np.array, np.array, np.array]:\n",
    "    \"\"\"\n",
    "    Predicts results on dataset using trained model.\n",
    "                     \n",
    "    Parameters\n",
    "    ----------\n",
    "    mc: dict\n",
    "        Model configuration.\n",
    "    model: pl.LightningModule\n",
    "        Forecast model.\n",
    "    trainer: pl.Trainer\n",
    "        Trainer object.\n",
    "    loader: DataLoader\n",
    "        Data loader.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    y_true: np.array\n",
    "        True values from dataset.\n",
    "    y_hat: np.array\n",
    "        Predicted values from dataset.\n",
    "    mask: np.array \n",
    "        Masks for values.\n",
    "    meta_data: np.array \n",
    "        Metada from dataset.\n",
    "    \"\"\"  \n",
    "    outputs = trainer.predict(model, loader)\n",
    "    y_true, y_hat, mask, ts_idxs = [t.cat(output) for output in zip(*outputs)]\n",
    "\n",
    "    if mc['scaler'] is not None:\n",
    "        y_true = loader.dataset.scaler.inverse_transform_y(Y=y_true, ts_idxs=ts_idxs)\n",
    "        y_hat = loader.dataset.scaler.inverse_transform_y(Y=y_hat, ts_idxs=ts_idxs)\n",
    "\n",
    "    y_true = y_true.cpu().numpy()\n",
    "    y_hat = y_hat.cpu().numpy()\n",
    "    mask = mask.cpu().numpy()\n",
    "    ts_idxs = ts_idxs.cpu().numpy()\n",
    "\n",
    "    return y_true, y_hat, mask, ts_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def fit(mc: dict, Y_df: pd.DataFrame, X_df: pd.DataFrame =None, S_df: pd.DataFrame =None,\n",
    "        ds_in_val: int =0, ds_in_test: int =0,\n",
    "        f_cols: list =[], verbose: bool = False) -> Tuple[pl.LightningModule, pl.Trainer, \n",
    "                                                          DataLoader, DataLoader, Scaler] or pl.LightningModule:\n",
    "    \"\"\"\n",
    "    Traines model on given dataset.\n",
    "                     \n",
    "    Parameters\n",
    "    ----------\n",
    "    mc: dict\n",
    "        Model configuration.\n",
    "    Y_df: pd.DataFrame\n",
    "        Target time series with columns ['unique_id', 'ds', 'y'].\n",
    "    X_df: pd.DataFrame\n",
    "        Exogenous time series with columns ['unique_id', 'ds', 'y'].\n",
    "    S_df: pd.DataFrame\n",
    "        Static exogenous variables with columns ['unique_id', 'ds']. \n",
    "        and static variables. \n",
    "    ds_in_val: int\n",
    "        Number of ds in validation.\n",
    "    ds_in_test: int\n",
    "        Number of ds in test.\n",
    "    f_cols: list\n",
    "        List of exogenous variables of the future.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model: pl.LightningModule\n",
    "        Forecast model.\n",
    "    trainer: pl.Trainer\n",
    "        Trainer object.\n",
    "    val_loader: DataLoader\n",
    "        Validation loader.\n",
    "    test_loader: DataLoader\n",
    "        Test loader.\n",
    "    \"\"\"   \n",
    "\n",
    "    # Protect inplace modifications\n",
    "    Y_df = Y_df.copy()\n",
    "    if X_df is not None:\n",
    "        X_df = X_df.copy()\n",
    "    if S_df is not None:\n",
    "        S_df = S_df.copy()        \n",
    "\n",
    "    #----------------------------------------------- Datasets -----------------------------------------------#\n",
    "    train_dataset, val_dataset, test_dataset = create_datasets(mc=mc,\n",
    "                                                               S_df=S_df, Y_df=Y_df, X_df=X_df,\n",
    "                                                               f_cols=f_cols,\n",
    "                                                               ds_in_val=ds_in_val,\n",
    "                                                               ds_in_test=ds_in_test,\n",
    "                                                               verbose=verbose)\n",
    "    mc['n_x'], mc['n_s'] = train_dataset.get_n_variables()\n",
    "\n",
    "    #------------------------------------------- Instantiate & fit -------------------------------------------#\n",
    "    train_loader, val_loader, test_loader = instantiate_loaders(mc=mc,\n",
    "                                                                train_dataset=train_dataset,\n",
    "                                                                val_dataset=val_dataset,\n",
    "                                                                test_dataset=test_dataset)\n",
    "    model = instantiate_model(mc=mc)\n",
    "    callbacks = []\n",
    "    if mc['early_stop_patience'] and ds_in_val > 0:\n",
    "        early_stopping = pl.callbacks.EarlyStopping(monitor='val_loss', min_delta=1e-4, \n",
    "                                                    patience=mc['early_stop_patience'],\n",
    "                                                    verbose=verbose, \n",
    "                                                    mode='min') \n",
    "        callbacks=[early_stopping]\n",
    "    \n",
    "    gpus = -1 if t.cuda.is_available() else 0\n",
    "    trainer = pl.Trainer(max_epochs=mc['max_epochs'], \n",
    "                         max_steps=mc['max_steps'],\n",
    "                         check_val_every_n_epoch=mc['eval_freq'],\n",
    "                         gpus=gpus,\n",
    "                         callbacks=callbacks,\n",
    "                         enable_checkpointing=False,\n",
    "                         enable_progress_bar=verbose,\n",
    "                         enable_model_summary=verbose,\n",
    "                         logger=False)\n",
    "    \n",
    "    val_dataloaders = val_loader if ds_in_val > 0 else None\n",
    "    trainer.fit(model, train_loader, val_dataloaders)\n",
    "    \n",
    "    return model, trainer, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def model_fit_predict(mc: dict, \n",
    "                        S_df: pd.DataFrame, Y_df: pd.DataFrame, X_df: pd.DataFrame, \n",
    "                        f_cols: list, ds_in_val: int, ds_in_test: int, verbose: bool) -> dict:\n",
    "    \"\"\"\n",
    "    Traines model on train dataset, then calculates predictions\n",
    "    on test dataset.\n",
    "                     \n",
    "    Parameters\n",
    "    ----------\n",
    "    mc: dict\n",
    "        Model configuration.\n",
    "    Y_df: pd.DataFrame\n",
    "        Target time series with columns ['unique_id', 'ds', 'y'].\n",
    "    X_df: pd.DataFrame\n",
    "        Exogenous time series with columns ['unique_id', 'ds', 'y'].\n",
    "    f_cols: list\n",
    "        List of exogenous variables of the future.\n",
    "    ds_in_val: int\n",
    "        Number of ds in validation.\n",
    "    ds_in_test: int\n",
    "        Number of ds in test.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    results: dict\n",
    "        Dictionary with results of training and prediction on model.   \n",
    "    \"\"\"  \n",
    "\n",
    "    #------------------------------------------------ Fit ------------------------------------------------#\n",
    "    model, trainer, val_loader, test_loader = fit(\n",
    "        mc, S_df=S_df, Y_df=Y_df, X_df=X_df, \n",
    "        f_cols=[], ds_in_val=ds_in_val, ds_in_test=ds_in_test, verbose=verbose\n",
    "    )\n",
    "    #------------------------------------------------ Predict ------------------------------------------------#\n",
    "    results = {}\n",
    "\n",
    "    if ds_in_val > 0:\n",
    "        y_true, y_hat, mask, ts_idxs = predict(mc, model, trainer, val_loader)\n",
    "        val_values = (('val_y_true', y_true), ('val_y_hat', y_hat), \n",
    "                      ('val_mask', mask), ('val_meta_data', ts_idxs))\n",
    "        results.update(val_values)\n",
    "\n",
    "    # Predict test if available\n",
    "    if ds_in_test > 0:\n",
    "        y_true, y_hat, mask, ts_idxs = predict(mc, model, trainer, test_loader)\n",
    "        test_values = (('test_y_true', y_true), ('test_y_hat', y_hat), \n",
    "                       ('test_mask', mask), ('test_meta_data', ts_idxs))\n",
    "        results.update(test_values)\n",
    "\n",
    "    return results, model, trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def evaluate_model(mc: dict, loss_function_val: callable, loss_functions_test: dict, \n",
    "                   S_df: pd.DataFrame, Y_df: pd.DataFrame, X_df: pd.DataFrame, \n",
    "                   f_cols: list, ds_in_val: int, ds_in_test: int,\n",
    "                   return_forecasts: bool,\n",
    "                   return_model: bool,\n",
    "                   save_trials: bool,\n",
    "                   trials: Trials,\n",
    "                   results_dir: str,\n",
    "                   step_save_progress: int =5,\n",
    "                   loss_kwargs: list =None, verbose: bool=False) -> dict:\n",
    "    \"\"\"\n",
    "    Evaluate model on given dataset.\n",
    "                     \n",
    "    Parameters\n",
    "    ----------\n",
    "    mc: dictionary\n",
    "        Model configuration.\n",
    "    loss_function_val: function\n",
    "        Loss function used for validation.\n",
    "    loss_functions_test: Dictionary\n",
    "        Loss functions used for test. \n",
    "        (function name: string, function: fun)\n",
    "    S_df: pd.DataFrame\n",
    "        Static exogenous variables with columns ['unique_id', 'ds']. \n",
    "        and static variables. \n",
    "    Y_df: pd.DataFrame\n",
    "        Target time series with columns ['unique_id', 'ds', 'y'].\n",
    "    X_df: pd.DataFrame\n",
    "        Exogenous time series with columns ['unique_id', 'ds', 'y'].\n",
    "    f_cols: list\n",
    "        List of exogenous variables of the future.\n",
    "    ds_in_val: int\n",
    "        Number of ds in validation.\n",
    "    ds_in_test: int\n",
    "        Number of ds in test.\n",
    "    return_forecasts: bool\n",
    "        If true return forecast on test.\n",
    "    save_trials: bool    \n",
    "        If true save progres in file.\n",
    "    trials: hyperopt.Trials\n",
    "        Results from model evaluation.\n",
    "    results_dir: str\n",
    "        File path to save results.\n",
    "    step_save_progress: int\n",
    "        Every n-th step is saved in file.\n",
    "    loss_kwargs: List\n",
    "        Loss function arguments.    \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    results_output: dict\n",
    "        Dictionary with results of model evaluation.   \n",
    "    \"\"\" \n",
    "\n",
    "    if (save_trials) and (len(trials) % step_save_progress == 0):\n",
    "        trials_file = f'{results_dir}/trials.p'\n",
    "        with open(trials_file, \"wb\") as f:\n",
    "            pickle.dump(trials, f)\n",
    "    \n",
    "    if verbose:\n",
    "        print(47*'=' + '\\n')\n",
    "        print(pd.Series(mc))\n",
    "        print(47*'=' + '\\n')\n",
    "    \n",
    "    # Some asserts due to work in progress\n",
    "    n_series = Y_df['unique_id'].nunique()\n",
    "\n",
    "    assert ds_in_test % mc['val_idx_to_sample_freq']==0,\\\n",
    "        'outsample size should be multiple of val_idx_to_sample_freq'\n",
    "\n",
    "    # Make predictions\n",
    "    start = time.time()\n",
    "    results, _, trainer = model_fit_predict(mc=mc,\n",
    "                                                S_df=S_df, \n",
    "                                                Y_df=Y_df,\n",
    "                                                X_df=X_df,\n",
    "                                                f_cols=f_cols,\n",
    "                                                ds_in_val=ds_in_val,\n",
    "                                                ds_in_test=ds_in_test,\n",
    "                                                verbose=verbose)\n",
    "    run_time = time.time() - start\n",
    "\n",
    "    # Evaluate predictions\n",
    "    if mc['loss_valid'] == 'MQ':\n",
    "        val_loss = loss_function_val(y=results['val_y_true'], y_hat=results['val_y_hat'],\n",
    "                                     quantiles=np.array(mc['quantiles']),\n",
    "                                     weights=results['val_mask'])\n",
    "    else:\n",
    "        val_loss = loss_function_val(y=results['val_y_true'], y_hat=results['val_y_hat'], \n",
    "                                     weights=results['val_mask'], **loss_kwargs)\n",
    "\n",
    "    results_output = {'loss': val_loss,\n",
    "                      'mc': mc,\n",
    "                      'run_time': run_time,\n",
    "                      'status': STATUS_OK}\n",
    "\n",
    "    # Evaluation in test (if provided)\n",
    "    if ds_in_test > 0:\n",
    "        test_loss_dict = {}\n",
    "        for loss_name, loss_function in loss_functions_test.items():\n",
    "            if loss_name == 'MQ':\n",
    "                test_loss_dict[loss_name] = loss_function(y=results['test_y_true'], y_hat=results['test_y_hat'],\n",
    "                                                          quantiles=np.array(mc['quantiles']),\n",
    "                                                          weights=results['test_mask'])\n",
    "            else:\n",
    "                test_loss_dict[loss_name] = loss_function(y=results['test_y_true'], y_hat=results['test_y_hat'], \n",
    "                                                        weights=results['test_mask'])\n",
    "        results_output['test_losses'] = test_loss_dict\n",
    "    \n",
    "    if return_forecasts and ds_in_test > 0:\n",
    "        forecasts_test = {}\n",
    "        test_values = (('test_y_true', results['test_y_true']), ('test_y_hat', results['test_y_hat']),\n",
    "                        ('test_mask', results['test_mask']), ('test_meta_data', results['test_meta_data']))\n",
    "        forecasts_test.update(test_values)\n",
    "        results_output['forecasts_test'] = forecasts_test\n",
    "\n",
    "    # Save model in best_trial\n",
    "    if len(trials) > 1:\n",
    "        current_best_loss = min(trials.losses()[:-1]) # :-1 since current run has None\n",
    "    else:\n",
    "        current_best_loss = np.inf\n",
    "        \n",
    "    if (return_model) and (val_loss < current_best_loss):\n",
    "        trainer.save_checkpoint(f\"{results_dir}/best_model.ckpt\")\n",
    "\n",
    "    return results_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "MODEL_DICT = {'nbeats': NBEATS,\n",
    "              'nhits': NHITS,\n",
    "              'mqnhits': MQNHITS,\n",
    "              'rnn': RNN,\n",
    "              'esrnn': ESRNN,\n",
    "              'autoformer': Autoformer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def hyperopt_tunning(space: dict, hyperopt_max_evals: int, \n",
    "                     loss_function_val: callable, loss_functions_test: dict,\n",
    "                     S_df: pd.DataFrame, Y_df: pd.DataFrame, X_df: pd.DataFrame, \n",
    "                     f_cols: list, ds_in_val: int, ds_in_test: int,\n",
    "                     return_forecasts: bool,\n",
    "                     return_model: bool,\n",
    "                     save_trials: bool,\n",
    "                     results_dir: str,\n",
    "                     step_save_progress: int = 5,\n",
    "                     loss_kwargs: list =None, verbose: bool =False) -> Trials:\n",
    "    \"\"\"\n",
    "    Evaluates multiple models trained on given dataset.\n",
    "    Models are trained with different hyperparameters.\n",
    "    Hyperparameters are changed until function is minimized in\n",
    "    hyperparameter space. All models are trained and evaluated,\n",
    "    until function is minimized. \n",
    "                     \n",
    "    Parameters\n",
    "    ----------\n",
    "    space: Dictionary\n",
    "        Dictionary that contines hyperparameters that create space.\n",
    "    hyperopt_max_evals: int\n",
    "        Maximum number of evaluations.\n",
    "    loss_function_val: function\n",
    "        Loss function used for validation.\n",
    "    loss_functions_test: Dictionary\n",
    "        Loss functions used for test. \n",
    "        (function name: string, function: fun)\n",
    "    S_df: pd.DataFrame\n",
    "        Static exogenous variables with columns ['unique_id', 'ds']. \n",
    "        and static variables. \n",
    "    Y_df: pd.DataFrame\n",
    "        Target time series with columns ['unique_id', 'ds', 'y'].\n",
    "    X_df: pd.DataFrame\n",
    "        Exogenous time series with columns ['unique_id', 'ds', 'y'].\n",
    "    f_cols: list\n",
    "        List of exogenous variables of the future.\n",
    "    ds_in_val: int\n",
    "        Number of ds in validation.\n",
    "    ds_in_test: int\n",
    "        Number of ds in test.\n",
    "    return_forecasts: bool\n",
    "        If true return forecast on test.\n",
    "    return_model: bool\n",
    "        If true return models.\n",
    "    save_trials: bool    \n",
    "        If true save progres in file.\n",
    "    results_dir: str\n",
    "        File path to save results.\n",
    "    step_save_progress: int\n",
    "        Every n-th step is saved in file.\n",
    "    loss_kwargs: List\n",
    "        Loss function arguments.\n",
    "    verbose:\n",
    "        If true, will print summary of dataset, model and training.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    trials: Trials\n",
    "        Results from model evaluation.  \n",
    "    \"\"\" \n",
    "\n",
    "    assert ds_in_val > 0, 'Validation set is needed for tunning!'\n",
    "\n",
    "    if save_trials or return_model:\n",
    "        os.makedirs(results_dir, exist_ok = True)\n",
    "\n",
    "    trials = Trials()\n",
    "    fmin_objective = partial(evaluate_model, \n",
    "                             loss_function_val=loss_function_val, \n",
    "                             loss_functions_test=loss_functions_test,\n",
    "                             S_df=S_df, Y_df=Y_df, X_df=X_df, f_cols=f_cols,\n",
    "                             ds_in_val=ds_in_val, ds_in_test=ds_in_test,\n",
    "                             return_forecasts=return_forecasts, return_model=return_model,\n",
    "                             save_trials=save_trials, trials=trials,\n",
    "                             results_dir=results_dir,\n",
    "                             step_save_progress=step_save_progress,\n",
    "                             loss_kwargs=loss_kwargs or {}, verbose=verbose)\n",
    "\n",
    "    fmin(fmin_objective, space=space, algo=tpe.suggest, \n",
    "         max_evals=hyperopt_max_evals, trials=trials, verbose=verbose)\n",
    "\n",
    "    # Saves final trials\n",
    "    if save_trials:\n",
    "        trials_file = f'{results_dir}/trials.p'\n",
    "        with open(trials_file, \"wb\") as f:\n",
    "            pickle.dump(trials, f)\n",
    "\n",
    "    if return_model:\n",
    "        # best_model.ckpt is generated with evaluate_model\n",
    "        model = MODEL_DICT[space['model']].load_from_checkpoint(f'{results_dir}/best_model.ckpt')\n",
    "        return model, trials\n",
    "    else:\n",
    "        return trials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Utils Examples\n",
    ">This part of notebook shows simple use of functions given in this notebook. EPF dataset in used in this experiments. Two experiments are shown where hyperparameter tunning is runned for two models NHITS and NBEATS.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "\n",
    "from neuralforecast.losses.numpy import mae, rmse\n",
    "from neuralforecast.auto import nhits_space, nbeats_space\n",
    "from neuralforecast.data.datasets.epf import EPF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ['NP', 'FR']\n",
    "Y_df, X_df, S_df = EPF.load_groups(directory='data', groups=dataset)\n",
    "X_df = X_df[['unique_id', 'ds', 'week_day']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Returning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space = nhits_space(horizon=24) #, n_series=1, n_x=1, n_s=0, frequency='H')\n",
    "space['max_steps'] = hp.choice('max_steps', [1]) # Override max_steps for faster example\n",
    "# The suggested spaces are partial, here we complete them with data specific information\n",
    "space['n_series']   = hp.choice('n_series', [ Y_df['unique_id'].nunique() ])\n",
    "space['n_x']        = hp.choice('n_x', [ 0 if X_df is None else (X_df.shape[1]-2) ])\n",
    "space['n_s']        = hp.choice('n_s', [ 0 if S_df is None else (S_df.shape[1]-1) ])\n",
    "space['n_x_hidden'] = hp.choice('n_x_hidden', [ 0 if X_df is None else (X_df.shape[1]-2) ])\n",
    "space['n_s_hidden'] = hp.choice('n_s_hidden', [ 0 if S_df is None else (S_df.shape[1]-1) ])\n",
    "# Infers freq with first time series\n",
    "freq = pd.infer_freq(Y_df[Y_df['unique_id']==Y_df.unique_id.unique()[0]]['ds']) \n",
    "space['frequency']  = hp.choice('frequency', [ freq ])\n",
    "\n",
    "space['scaler']  = hp.choice('scaler', [ 'std' ]) # None\n",
    "\n",
    "model, trials = hyperopt_tunning(space=space, hyperopt_max_evals=2, loss_function_val=mae,\n",
    "                                 loss_functions_test={'mae': mae, 'rmse': rmse},\n",
    "                                 S_df=S_df, Y_df=Y_df, X_df=X_df, f_cols=[],\n",
    "                                 ds_in_val=7*24, ds_in_test=7*24, \n",
    "                                 return_forecasts=True, return_model=True, save_trials=True, \n",
    "                                 results_dir='./results/example', loss_kwargs={}, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials.trials[0]['result']['forecasts_test']['test_meta_data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = trials.trials[0]['result']['forecasts_test']['test_y_true']\n",
    "y_hat = trials.trials[0]['result']['forecasts_test']['test_y_hat']\n",
    "\n",
    "plt.plot(y_true[::24,:].flatten())\n",
    "plt.plot(y_hat[::24,:].flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.12 ('neuralforecast')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
