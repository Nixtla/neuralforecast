{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp experiments.utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Utils\n",
    "> This notebook contains a set of functions to easily perform experiments on time series datasets. In this notebook you can see functions for:\n",
    "1. Preparing dataset\n",
    "2. Loadnig models\n",
    "3. Training models\n",
    "4. Model evaluation\n",
    "5. Hyperparameter tunning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">In the next two scetion you can see enviroment variables and imports that are used for set of functions shown in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "ENV_VARS = dict(OMP_NUM_THREADS='2',\n",
    "                OPENBLAS_NUM_THREADS='2',\n",
    "                MKL_NUM_THREADS='3',\n",
    "                VECLIB_MAXIMUM_THREADS='2',\n",
    "                NUMEXPR_NUM_THREADS='3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import os\n",
    "import pickle\n",
    "# Limit number of threads in numpy and others to avoid throttling\n",
    "os.environ.update(ENV_VARS)\n",
    "import time\n",
    "from functools import partial\n",
    "from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import torch as t\n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from neuralforecast.data.scalers import Scaler\n",
    "from neuralforecast.data.tsdataset import TimeSeriesDataset, WindowsDataset, IterateWindowsDataset, BaseDataset\n",
    "from neuralforecast.data.tsloader import TimeSeriesLoader\n",
    "from neuralforecast.models.esrnn.esrnn import ESRNN\n",
    "from neuralforecast.models.esrnn.mqesrnn import MQESRNN\n",
    "from neuralforecast.models.nbeats.nbeats import NBEATS\n",
    "from neuralforecast.models.nhits.nhits import NHITS\n",
    "from neuralforecast.models.transformer.autoformer import Autoformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions that represent basic use of neuralforecast library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_mask_dfs(Y_df: pd.DataFrame,\n",
    "                 ds_in_val: int, ds_in_test: int) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Generates train, test and validation mask.\n",
    "    Train mask begins by avoiding ds_in_test.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    Y_df: pd.DataFrame\n",
    "        Target time series with columns ['unique_id', 'ds', 'y'].\n",
    "    ds_in_val: int\n",
    "        Number of ds in validation.\n",
    "    ds_in_test: int\n",
    "        Number of ds in test.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    train_mask_df: pd.DataFrame\n",
    "        Train mask dataframe.\n",
    "    val_mask_df: pd.DataFrame\n",
    "        Validation mask dataframe.\n",
    "    test_mask_df: pd.DataFrame\n",
    "        Test mask dataframe.\n",
    "    \"\"\"\n",
    "\n",
    "    # train mask\n",
    "    train_mask_df = Y_df.copy()[['unique_id', 'ds']]\n",
    "    train_mask_df.sort_values(by=['unique_id', 'ds'], inplace=True)\n",
    "    train_mask_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    train_mask_df['sample_mask'] = 1\n",
    "    train_mask_df['available_mask'] = 1\n",
    "    \n",
    "    idx_out = train_mask_df.groupby('unique_id').tail(ds_in_val+ds_in_test).index\n",
    "    train_mask_df.loc[idx_out, 'sample_mask'] = 0\n",
    "    \n",
    "    # test mask\n",
    "    test_mask_df = train_mask_df.copy()\n",
    "    test_mask_df['sample_mask'] = 0\n",
    "    idx_test = test_mask_df.groupby('unique_id').tail(ds_in_test).index\n",
    "    test_mask_df.loc[idx_test, 'sample_mask'] = 1\n",
    "    \n",
    "    # validation mask\n",
    "    val_mask_df = train_mask_df.copy()\n",
    "    val_mask_df['sample_mask'] = 1\n",
    "    val_mask_df['sample_mask'] = val_mask_df['sample_mask'] - train_mask_df['sample_mask']\n",
    "    val_mask_df['sample_mask'] = val_mask_df['sample_mask'] - test_mask_df['sample_mask']\n",
    "\n",
    "    assert len(train_mask_df)==len(Y_df), \\\n",
    "        f'The mask_df length {len(train_mask_df)} is not equal to Y_df length {len(Y_df)}'\n",
    "    \n",
    "    return train_mask_df, val_mask_df, test_mask_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_random_mask_dfs(Y_df: pd.DataFrame, ds_in_test: int, \n",
    "                        n_val_windows: int, n_ds_val_window: int,\n",
    "                        n_uids: int, freq: str) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Generates train, test and random validation mask.\n",
    "    Train mask begins by avoiding ds_in_test\n",
    "    \n",
    "    Validation mask: 1) samples n_uids unique ids\n",
    "                     2) creates windows of size n_ds_val_window\n",
    "                     \n",
    "    Parameters\n",
    "    ----------\n",
    "    Y_df: pd.DataFrame\n",
    "        Target time series with columns ['unique_id', 'ds', 'y'].\n",
    "    ds_in_test: int\n",
    "        Number of ds in test.\n",
    "    n_val_windows: int\n",
    "        Number of windows for validation.\n",
    "    n_ds_val_window: int\n",
    "        Number of ds in each validation window.\n",
    "    n_uids: int\n",
    "        Number of unique ids in validation.\n",
    "    freq: str\n",
    "        string that determines datestamp frequency, used in\n",
    "        random windows creation.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    train_mask_df: pd.DataFrame\n",
    "        Train mask dataframe.\n",
    "    val_mask_df: pd.DataFrame\n",
    "        Validation mask dataframe.\n",
    "    test_mask_df: pd.DataFrame\n",
    "        Test mask dataframe.\n",
    "    \"\"\"\n",
    "    np.random.seed(1)\n",
    "    #----------------------- Train mask -----------------------#\n",
    "    # Initialize masks\n",
    "    train_mask_df, val_mask_df, test_mask_df = get_mask_dfs(Y_df=Y_df,\n",
    "                                                            ds_in_val=0,\n",
    "                                                            ds_in_test=ds_in_test)\n",
    "    \n",
    "    assert val_mask_df['sample_mask'].sum()==0, 'Muerte'\n",
    "    \n",
    "    #----------------- Random Validation mask -----------------#\n",
    "    # Overwrite validation with random windows\n",
    "    uids = train_mask_df['unique_id'].unique()\n",
    "    val_uids = np.random.choice(uids, n_uids, replace=False)\n",
    "    \n",
    "    # Validation avoids test\n",
    "    idx_test = train_mask_df.groupby('unique_id').tail(ds_in_test).index\n",
    "    available_ds = train_mask_df.loc[~train_mask_df.index.isin(idx_test)]['ds'].unique()\n",
    "    val_init_ds = np.random.choice(available_ds, n_val_windows, replace=False)\n",
    "    \n",
    "    # Creates windows \n",
    "    val_ds = [pd.date_range(init, periods=n_ds_val_window, freq=freq) for init in val_init_ds]\n",
    "    val_ds = np.concatenate(val_ds)\n",
    "\n",
    "    # Cleans random windows from train mask\n",
    "    val_idx = train_mask_df.query('unique_id in @val_uids & ds in @val_ds').index\n",
    "    train_mask_df.loc[val_idx, 'sample_mask'] = 0\n",
    "    val_mask_df.loc[val_idx, 'sample_mask'] = 1\n",
    "    \n",
    "    return train_mask_df, val_mask_df, test_mask_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def scale_data(Y_df: pd.DataFrame, X_df: pd.DataFrame, \n",
    "                mask_df: pd.DataFrame, normalizer_y: str, \n",
    "                normalizer_x: str) -> Tuple[pd.DataFrame, pd.DataFrame, Scaler]:\n",
    "    \"\"\"\n",
    "    Scales input data accordingly to given normalizer parameters.\n",
    "                     \n",
    "    Parameters\n",
    "    ----------\n",
    "    Y_df: pd.DataFrame\n",
    "        Target time series with columns ['unique_id', 'ds', 'y'].\n",
    "    X_df: pd.DataFrame\n",
    "        Exogenous time series with columns ['unique_id', 'ds', 'y']\n",
    "    mask_df: pd.DataFrame\n",
    "        Mask dataframe.\n",
    "    normalizer_y: str\n",
    "        Normalizer for scaling Y_df.\n",
    "    normalizer_x: str\n",
    "        Normalizer for scaling X_df.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Y_df: pd.DataFrame\n",
    "        Scaled target time series.\n",
    "    X_df: pd.DataFrame\n",
    "        Scaled exogenous time series with columns.\n",
    "    scaler_y: Scaler\n",
    "        Scaler object for Y_df.\n",
    "    \"\"\"\n",
    "    mask = mask_df['available_mask'].values * mask_df['sample_mask'].values\n",
    "    \n",
    "    if normalizer_y is not None:\n",
    "        scaler_y = Scaler(normalizer=normalizer_y)\n",
    "        Y_df['y'] = scaler_y.scale(x=Y_df['y'].values, mask=mask)\n",
    "    else:\n",
    "        scaler_y = None\n",
    "\n",
    "    if normalizer_x is not None:\n",
    "        X_cols = [col for col in X_df.columns if col not in ['unique_id','ds']]\n",
    "        for col in X_cols:\n",
    "            scaler_x = Scaler(normalizer=normalizer_x)\n",
    "            X_df[col] = scaler_x.scale(x=X_df[col].values, mask=mask)\n",
    "\n",
    "    return Y_df, X_df, scaler_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def create_datasets(mc: dict, S_df: pd.DataFrame, \n",
    "                    Y_df: pd.DataFrame, X_df: pd.DataFrame, f_cols: list,\n",
    "                    ds_in_test: int, ds_in_val: int) -> Tuple[BaseDataset, BaseDataset, BaseDataset, Scaler]:\n",
    "    \"\"\"\n",
    "    Creates train, validation and test datasets.\n",
    "                     \n",
    "    Parameters\n",
    "    ----------\n",
    "    mc: dict\n",
    "        Model configuration.\n",
    "    S_df: pd.DataFrame\n",
    "        Static exogenous variables with columns ['unique_id', 'ds'] \n",
    "        and static variables. \n",
    "    Y_df: pd.DataFrame\n",
    "        Target time series with columns ['unique_id', 'ds', 'y'].\n",
    "    X_df: pd.DataFrame\n",
    "        Exogenous time series with columns ['unique_id', 'ds', 'y']\n",
    "    f_cols: list\n",
    "        List of exogenous variables of the future.\n",
    "    ds_in_test: int\n",
    "        Number of ds in test.\n",
    "    ds_in_val: int\n",
    "        Number of ds in validation.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    train_dataset: BaseDataset\n",
    "        Train dataset.\n",
    "    valid_dataset: BaseDataset\n",
    "        Validation dataset.\n",
    "    test_dataset: BaseDataset\n",
    "        Test dataset.\n",
    "    scaler_y: Scaler\n",
    "        Scaler object for Y_df.\n",
    "    \"\"\"\n",
    "\n",
    "    #------------------------------------- Available and Validation Mask ------------------------------------#\n",
    "    train_mask_df, valid_mask_df, test_mask_df = get_mask_dfs(Y_df=Y_df,\n",
    "                                                              ds_in_val=ds_in_val,\n",
    "                                                              ds_in_test=ds_in_test)\n",
    "\n",
    "    #---------------------------------------------- Scale Data ----------------------------------------------#\n",
    "    Y_df, X_df, scaler_y = scale_data(Y_df=Y_df, X_df=X_df, mask_df=train_mask_df,\n",
    "                                      normalizer_y=mc['normalizer_y'], normalizer_x=mc['normalizer_x'])\n",
    "\n",
    "    #----------------------------------------- Declare Dataset and Loaders ----------------------------------#\n",
    "    \n",
    "    if mc['mode'] == 'simple':\n",
    "        train_dataset = WindowsDataset(S_df=S_df, Y_df=Y_df, X_df=X_df,\n",
    "                                       mask_df=train_mask_df, f_cols=f_cols,\n",
    "                                       input_size=int(mc['n_time_in']),\n",
    "                                       output_size=int(mc['n_time_out']),\n",
    "                                       sample_freq=int(mc['idx_to_sample_freq']),\n",
    "                                       complete_windows=mc['complete_windows'],\n",
    "                                       verbose=True)\n",
    "        \n",
    "        valid_dataset = WindowsDataset(S_df=S_df, Y_df=Y_df, X_df=X_df,\n",
    "                                       mask_df=valid_mask_df, f_cols=f_cols,\n",
    "                                       input_size=int(mc['n_time_in']),\n",
    "                                       output_size=int(mc['n_time_out']),\n",
    "                                       sample_freq=int(mc['val_idx_to_sample_freq']),\n",
    "                                       complete_windows=True,\n",
    "                                       verbose=True)\n",
    "        \n",
    "        test_dataset = WindowsDataset(S_df=S_df, Y_df=Y_df, X_df=X_df,\n",
    "                                      mask_df=test_mask_df, f_cols=f_cols,\n",
    "                                      input_size=int(mc['n_time_in']),\n",
    "                                      output_size=int(mc['n_time_out']),\n",
    "                                      sample_freq=int(mc['val_idx_to_sample_freq']),\n",
    "                                      complete_windows=True,\n",
    "                                      verbose=True)\n",
    "    if mc['mode'] == 'iterate_windows':\n",
    "        train_dataset = IterateWindowsDataset(S_df=S_df, Y_df=Y_df, X_df=X_df,\n",
    "                                              mask_df=train_mask_df, f_cols=f_cols,\n",
    "                                              input_size=int(mc['n_time_in']),\n",
    "                                              output_size=int(mc['n_time_out']),\n",
    "                                              verbose=True)\n",
    "        \n",
    "        valid_dataset = IterateWindowsDataset(S_df=S_df, Y_df=Y_df, X_df=X_df,\n",
    "                                              mask_df=valid_mask_df, f_cols=f_cols,\n",
    "                                              input_size=int(mc['n_time_in']),\n",
    "                                              output_size=int(mc['n_time_out']),\n",
    "                                              verbose=True)\n",
    "        \n",
    "        test_dataset = IterateWindowsDataset(S_df=S_df, Y_df=Y_df, X_df=X_df,\n",
    "                                             mask_df=test_mask_df, f_cols=f_cols,\n",
    "                                             input_size=int(mc['n_time_in']),\n",
    "                                             output_size=int(mc['n_time_out']),\n",
    "                                             verbose=True)   \n",
    "    \n",
    "    if mc['mode'] == 'full':\n",
    "        train_dataset = TimeSeriesDataset(S_df=S_df, Y_df=Y_df, X_df=X_df,\n",
    "                                          mask_df=train_mask_df, f_cols=f_cols,\n",
    "                                          input_size=int(mc['n_time_in']),\n",
    "                                          output_size=int(mc['n_time_out']),\n",
    "                                          verbose=True)\n",
    "        \n",
    "        valid_dataset = TimeSeriesDataset(S_df=S_df, Y_df=Y_df, X_df=X_df,\n",
    "                                          mask_df=valid_mask_df, f_cols=f_cols,\n",
    "                                          input_size=int(mc['n_time_in']),\n",
    "                                          output_size=int(mc['n_time_out']),\n",
    "                                          verbose=True)\n",
    "        \n",
    "        test_dataset = TimeSeriesDataset(S_df=S_df, Y_df=Y_df, X_df=X_df,\n",
    "                                         mask_df=test_mask_df, f_cols=f_cols,\n",
    "                                         input_size=int(mc['n_time_in']),\n",
    "                                         output_size=int(mc['n_time_out']),\n",
    "                                         verbose=True)        \n",
    "    \n",
    "    if ds_in_test == 0:\n",
    "        test_dataset = None\n",
    "\n",
    "    return train_dataset, valid_dataset, test_dataset, scaler_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def instantiate_loaders(mc: dict, \n",
    "                        train_dataset: BaseDataset, val_dataset: BaseDataset, \n",
    "                        test_dataset: BaseDataset) -> Tuple[DataLoader, DataLoader, DataLoader]:\n",
    "    \"\"\"\n",
    "    Creates train, validation and test loader classes.\n",
    "                     \n",
    "    Parameters\n",
    "    ----------\n",
    "    mc: dict\n",
    "        Model configuration.\n",
    "    train_dataset: BaseDataset\n",
    "        Train dataset.\n",
    "    val_dataset: BaseDataset\n",
    "        Validation dataset.\n",
    "    test_dataset: BaseDataset\n",
    "        Test dataset.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    train_loader: DataLoader\n",
    "        Train loader.\n",
    "    val_loader: DataLoader\n",
    "        Validation loader.\n",
    "    test_loader: DataLoader\n",
    "        Test loader.\n",
    "    \"\"\"\n",
    "\n",
    "    if mc['mode'] in ['simple', 'full'] :\n",
    "        train_loader = TimeSeriesLoader(dataset=train_dataset,\n",
    "                                        batch_size=int(mc['batch_size']),\n",
    "                                        n_windows=int(mc['n_windows']),\n",
    "                                        eq_batch_size=False,\n",
    "                                        shuffle=True)\n",
    "        if val_dataset is not None:\n",
    "            val_loader = TimeSeriesLoader(dataset=val_dataset,\n",
    "                                        batch_size=1,\n",
    "                                        shuffle=False)\n",
    "        else:\n",
    "            val_loader = None\n",
    "\n",
    "        if test_dataset is not None:\n",
    "            test_loader = TimeSeriesLoader(dataset=test_dataset,\n",
    "                                        batch_size=1,\n",
    "                                        shuffle=False)\n",
    "        else:\n",
    "            test_loader = None\n",
    "\n",
    "    elif mc['mode'] == 'iterate_windows':\n",
    "        train_loader =DataLoader(dataset=train_dataset,\n",
    "                                 batch_size=int(mc['batch_size']),\n",
    "                                 shuffle=True,\n",
    "                                 drop_last=True)\n",
    "\n",
    "        if val_dataset is not None:\n",
    "            val_loader = DataLoader(dataset=val_dataset,\n",
    "                                    batch_size=1,\n",
    "                                    shuffle=False)\n",
    "        else:\n",
    "            val_loader = None\n",
    "\n",
    "        if test_dataset is not None:\n",
    "            test_loader = DataLoader(dataset=test_dataset,\n",
    "                                     batch_size=1,\n",
    "                                     shuffle=False)\n",
    "        else:\n",
    "            test_loader = None\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def instantiate_nbeats(mc: dict) -> NBEATS:\n",
    "    \"\"\"\n",
    "    Creates nbeats model.\n",
    "                     \n",
    "    Parameters\n",
    "    ----------\n",
    "    mc: dict\n",
    "        Model configuration.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model: NBEATS\n",
    "        Nbeats model.\n",
    "    \"\"\"\n",
    "    mc['n_mlp_units'] = len(mc['stack_types']) * [ mc['constant_n_layers'] * [int(mc['constant_n_mlp_units'])] ]\n",
    "    mc['n_layers'] =  len(mc['stack_types']) * [ mc['constant_n_layers'] ]\n",
    "    mc['n_blocks'] =  len(mc['stack_types']) * [ mc['constant_n_blocks'] ]\n",
    "\n",
    "    if mc['max_epochs'] is not None:\n",
    "        lr_decay_step_size = int(mc['max_epochs'] / mc['n_lr_decays'])\n",
    "    elif mc['max_steps'] is not None:\n",
    "        lr_decay_step_size = int(mc['max_steps'] / mc['n_lr_decays'])\n",
    "\n",
    "    model = NBEATS(n_time_in=int(mc['n_time_in']),\n",
    "                  n_time_out=int(mc['n_time_out']),\n",
    "                  n_x=mc['n_x'],\n",
    "                  n_s=mc['n_s'],\n",
    "                  n_s_hidden=int(mc['n_s_hidden']),\n",
    "                  n_x_hidden=int(mc['n_x_hidden']),\n",
    "                  shared_weights = mc['shared_weights'],\n",
    "                  initialization=mc['initialization'],\n",
    "                  activation=mc['activation'],\n",
    "                  stack_types=mc['stack_types'],\n",
    "                  n_blocks=mc['n_blocks'],\n",
    "                  n_layers=mc['n_layers'],\n",
    "                  n_mlp_units=mc['n_mlp_units'],\n",
    "                  batch_normalization = mc['batch_normalization'],\n",
    "                  dropout_prob_theta=mc['dropout_prob_theta'],\n",
    "                  learning_rate=float(mc['learning_rate']),\n",
    "                  lr_decay=float(mc['lr_decay']),\n",
    "                  lr_decay_step_size=lr_decay_step_size,\n",
    "                  weight_decay=mc['weight_decay'],\n",
    "                  loss_train=mc['loss_train'],\n",
    "                  loss_hypar=float(mc['loss_hypar']),\n",
    "                  loss_valid=mc['loss_valid'],\n",
    "                  frequency=mc['frequency'],\n",
    "                  random_seed=int(mc['random_seed']))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def instantiate_esrnn(mc: dict) -> ESRNN:  \n",
    "    \"\"\"\n",
    "    Creates esrnn model.\n",
    "                     \n",
    "    Parameters\n",
    "    ----------\n",
    "    mc: dict\n",
    "        Model configuration.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model: ESRNN\n",
    "        Esrnn model.\n",
    "    \"\"\"  \n",
    "    if mc['max_epochs'] is not None:\n",
    "        lr_decay_step_size = int(mc['max_epochs'] / mc['n_lr_decays'])\n",
    "    elif mc['max_steps'] is not None:\n",
    "        lr_decay_step_size = int(mc['max_steps'] / mc['n_lr_decays'])\n",
    "    model = ESRNN(# Architecture parameters\n",
    "                  n_series=mc['n_series'],\n",
    "                  n_x=mc['n_x'],\n",
    "                  n_s=mc['n_s'],\n",
    "                  sample_freq=int(mc['sample_freq']),\n",
    "                  input_size=int(mc['n_time_in']),\n",
    "                  output_size=int(mc['n_time_out']),\n",
    "                  es_component=mc['es_component'],\n",
    "                  cell_type=mc['cell_type'],\n",
    "                  state_hsize=int(mc['state_hsize']),\n",
    "                  dilations=mc['dilations'],\n",
    "                  add_nl_layer=mc['add_nl_layer'],\n",
    "                  # Optimization parameters                \n",
    "                  learning_rate=mc['learning_rate'],\n",
    "                  lr_scheduler_step_size=lr_decay_step_size,\n",
    "                  lr_decay=mc['lr_decay'],\n",
    "                  per_series_lr_multip=mc['per_series_lr_multip'],\n",
    "                  gradient_eps=mc['gradient_eps'],\n",
    "                  gradient_clipping_threshold=mc['gradient_clipping_threshold'],\n",
    "                  rnn_weight_decay=mc['rnn_weight_decay'],\n",
    "                  noise_std=mc['noise_std'],\n",
    "                  level_variability_penalty=mc['level_variability_penalty'],\n",
    "                  testing_percentile=mc['testing_percentile'],\n",
    "                  training_percentile=mc['training_percentile'],\n",
    "                  loss=mc['loss_train'],\n",
    "                  val_loss=mc['loss_valid'],\n",
    "                  seasonality=mc['seasonality']) \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def instantiate_mqesrnn(mc: dict) -> MQESRNN: \n",
    "    \"\"\"\n",
    "    Creates mqesrnn model.\n",
    "                     \n",
    "    Parameters\n",
    "    ----------\n",
    "    mc: dict\n",
    "        Model configuration.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model: MQESRNN\n",
    "        Mqesrnn model.\n",
    "    \"\"\"  \n",
    "\n",
    "    if mc['max_epochs'] is not None:\n",
    "        lr_decay_step_size = int(mc['max_epochs'] / mc['n_lr_decays'])\n",
    "    elif mc['max_steps'] is not None:\n",
    "        lr_decay_step_size = int(mc['max_steps'] / mc['n_lr_decays'])   \n",
    "    model = MQESRNN(# Architecture parameters\n",
    "                    n_series=mc['n_series'],\n",
    "                    n_x=mc['n_x'],\n",
    "                    n_s=mc['n_s'],\n",
    "                    idx_to_sample_freq=int(mc['idx_to_sample_freq']),\n",
    "                    input_size=int(mc['n_time_in']),\n",
    "                    output_size=int(mc['n_time_out']),\n",
    "                    es_component=mc['es_component'],\n",
    "                    cell_type=mc['cell_type'],\n",
    "                    state_hsize=int(mc['state_hsize']),\n",
    "                    dilations=mc['dilations'],\n",
    "                    add_nl_layer=mc['add_nl_layer'],\n",
    "                    # Optimization parameters                 \n",
    "                    learning_rate=mc['learning_rate'],\n",
    "                    lr_scheduler_step_size=lr_decay_step_size,\n",
    "                    lr_decay=mc['lr_decay'],\n",
    "                    gradient_eps=mc['gradient_eps'],\n",
    "                    gradient_clipping_threshold=mc['gradient_clipping_threshold'],\n",
    "                    rnn_weight_decay=mc['rnn_weight_decay'],\n",
    "                    noise_std=mc['noise_std'],\n",
    "                    testing_percentiles=list(mc['testing_percentiles']),\n",
    "                    training_percentiles=list(mc['training_percentiles']),\n",
    "                    loss=mc['loss_train'],\n",
    "                    val_loss=mc['loss_valid']) \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def instantiate_nhits(mc: dict) -> NHITS:\n",
    "    \"\"\"\n",
    "    Creates nhits model.\n",
    "                     \n",
    "    Parameters\n",
    "    ----------\n",
    "    mc: dict\n",
    "        Model configuration.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model: NHITS\n",
    "        Nhits model.\n",
    "    \"\"\"  \n",
    "    \n",
    "    mc['n_mlp_units'] = len(mc['stack_types']) * [ mc['constant_n_layers'] * [int(mc['constant_n_mlp_units'])] ]\n",
    "    mc['n_layers'] =  len(mc['stack_types']) * [ mc['constant_n_layers'] ]\n",
    "    mc['n_blocks'] =  len(mc['stack_types']) * [ mc['constant_n_blocks'] ]\n",
    "\n",
    "    if mc['max_epochs'] is not None:\n",
    "        lr_decay_step_size = int(mc['max_epochs'] / mc['n_lr_decays'])\n",
    "    elif mc['max_steps'] is not None:\n",
    "        lr_decay_step_size = int(mc['max_steps'] / mc['n_lr_decays'])\n",
    "\n",
    "    model = NHITS(n_time_in=int(mc['n_time_in']),\n",
    "                  n_time_out=int(mc['n_time_out']),\n",
    "                  n_x=mc['n_x'],\n",
    "                  n_s=mc['n_s'],\n",
    "                  n_s_hidden=int(mc['n_s_hidden']),\n",
    "                  n_x_hidden=int(mc['n_x_hidden']),\n",
    "                  shared_weights = mc['shared_weights'],\n",
    "                  initialization=mc['initialization'],\n",
    "                  activation=mc['activation'],\n",
    "                  stack_types=mc['stack_types'],\n",
    "                  n_blocks=mc['n_blocks'],\n",
    "                  n_layers=mc['n_layers'],\n",
    "                  n_mlp_units=mc['n_mlp_units'],\n",
    "                  n_pool_kernel_size=mc['n_pool_kernel_size'],\n",
    "                  n_freq_downsample=mc['n_freq_downsample'],\n",
    "                  pooling_mode=mc['pooling_mode'],\n",
    "                  interpolation_mode=mc['interpolation_mode'],\n",
    "                  batch_normalization = mc['batch_normalization'],\n",
    "                  dropout_prob_theta=mc['dropout_prob_theta'],\n",
    "                  learning_rate=float(mc['learning_rate']),\n",
    "                  lr_decay=float(mc['lr_decay']),\n",
    "                  lr_decay_step_size=lr_decay_step_size,\n",
    "                  weight_decay=mc['weight_decay'],\n",
    "                  loss_train=mc['loss_train'],\n",
    "                  loss_hypar=float(mc['loss_hypar']),\n",
    "                  loss_valid=mc['loss_valid'],\n",
    "                  frequency=mc['frequency'],\n",
    "                  random_seed=int(mc['random_seed']))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def instantiate_autoformer(mc: dict) -> Autoformer:\n",
    "    \"\"\"\n",
    "    Creates autoformer model.\n",
    "                     \n",
    "    Parameters\n",
    "    ----------\n",
    "    mc: dict\n",
    "        Model configuration.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model: Autoformer\n",
    "        Autoformer model.\n",
    "    \"\"\"  \n",
    "\n",
    "    if mc['max_epochs'] is not None:\n",
    "        lr_decay_step_size = int(mc['max_epochs'] / mc['n_lr_decays'])\n",
    "    elif mc['max_steps'] is not None:\n",
    "        lr_decay_step_size = int(mc['max_steps'] / mc['n_lr_decays'])\n",
    "    \n",
    "    model = Autoformer(seq_len=int(mc['seq_len']),\n",
    "                       label_len=int(mc['label_len']),\n",
    "                       pred_len=int(mc['pred_len']),\n",
    "                       output_attention=mc['output_attention'],\n",
    "                       enc_in=int(mc['enc_in']),\n",
    "                       dec_in=int(mc['dec_in']),\n",
    "                       d_model=int(mc['d_model']),\n",
    "                       c_out=int(mc['c_out']),\n",
    "                       embed = mc['embed'],\n",
    "                       freq=mc['freq'],\n",
    "                       dropout=mc['dropout'],\n",
    "                       factor=mc['factor'],\n",
    "                       n_heads=int(mc['n_heads']),\n",
    "                       d_ff=int(mc['d_ff']),\n",
    "                       moving_avg=int(mc['moving_avg']),\n",
    "                       activation=mc['activation'],\n",
    "                       e_layers=int(mc['e_layers']),\n",
    "                       d_layers=int(mc['d_layers']),\n",
    "                       learning_rate=float(mc['learning_rate']),\n",
    "                       lr_decay=float(mc['lr_decay']),\n",
    "                       lr_decay_step_size=lr_decay_step_size,\n",
    "                       weight_decay=mc['weight_decay'],\n",
    "                       loss_train=mc['loss_train'],\n",
    "                       loss_hypar=float(mc['loss_hypar']),\n",
    "                       loss_valid=mc['loss_valid'],\n",
    "                       random_seed=int(mc['random_seed']))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def instantiate_model(mc: dict) -> pl.LightningModule:\n",
    "    \"\"\"\n",
    "    Creates one of the models.\n",
    "    (nbeats, esrnn, mqesrnn, nhits, autoformer)\n",
    "                     \n",
    "    Parameters\n",
    "    ----------\n",
    "    mc: dict\n",
    "        Model configuration.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model: pl.LightningModule\n",
    "        Forecast model.\n",
    "    \"\"\"  \n",
    "    MODEL_DICT = {'nbeats': instantiate_nbeats,\n",
    "                  'esrnn': instantiate_esrnn,\n",
    "                  'mqesrnn': instantiate_mqesrnn,\n",
    "                  'nhits': instantiate_nhits,\n",
    "                  'autoformer': instantiate_autoformer}\n",
    "    return MODEL_DICT[mc['model']](mc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def predict(mc: dict, model: pl.LightningModule, \n",
    "            trainer: pl.Trainer, loader: DataLoader, \n",
    "            scaler_y: Scaler) -> Tuple[np.array, np.array, np.array, np.array]:\n",
    "    \"\"\"\n",
    "    Predicts results on dataset using trained model.\n",
    "                     \n",
    "    Parameters\n",
    "    ----------\n",
    "    mc: dict\n",
    "        Model configuration.\n",
    "    model: pl.LightningModule\n",
    "        Forecast model.\n",
    "    trainer: pl.Trainer\n",
    "        Trainer object.\n",
    "    loader: DataLoader\n",
    "        Data loader.\n",
    "    scaler_y: Scaler\n",
    "        Scaler object for target time series.   \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    y_true: np.array\n",
    "        True values from dataset.\n",
    "    y_hat: np.array\n",
    "        Predicted values from dataset.\n",
    "    mask: np.array \n",
    "        Masks for values.\n",
    "    meta_data: np.array \n",
    "        Metada from dataset.\n",
    "    \"\"\"  \n",
    "    outputs = trainer.predict(model, loader)\n",
    "    y_true, y_hat, mask = [t.cat(output).cpu().numpy() for output in zip(*outputs)]\n",
    "    meta_data = loader.dataset.meta_data\n",
    "\n",
    "    # Scale to original scale\n",
    "    if mc['normalizer_y'] is not None:\n",
    "        y_true_shape = y_true.shape\n",
    "        y_true = scaler_y.inv_scale(x=y_true.flatten())\n",
    "        y_true = np.reshape(y_true, y_true_shape)\n",
    "\n",
    "        y_hat = scaler_y.inv_scale(x=y_hat.flatten())\n",
    "        y_hat = np.reshape(y_hat, y_true_shape)\n",
    "\n",
    "    return y_true, y_hat, mask, meta_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def fit(mc: dict, Y_df: pd.DataFrame, X_df: pd.DataFrame =None, S_df: pd.DataFrame =None,\n",
    "        ds_in_val: int =0, ds_in_test: int =0,\n",
    "        f_cols: list =[], \n",
    "        only_model: bool =True) -> Tuple[pl.LightningModule, pl.Trainer, \n",
    "                                        DataLoader, DataLoader, Scaler] or pl.LightningModule:\n",
    "    \"\"\"\n",
    "    Traines model on given dataset.\n",
    "                     \n",
    "    Parameters\n",
    "    ----------\n",
    "    mc: dict\n",
    "        Model configuration.\n",
    "    Y_df: pd.DataFrame\n",
    "        Target time series with columns ['unique_id', 'ds', 'y'].\n",
    "    X_df: pd.DataFrame\n",
    "        Exogenous time series with columns ['unique_id', 'ds', 'y'].\n",
    "    S_df: pd.DataFrame\n",
    "        Static exogenous variables with columns ['unique_id', 'ds']. \n",
    "        and static variables. \n",
    "    ds_in_val: int\n",
    "        Number of ds in validation.\n",
    "    ds_in_test: int\n",
    "        Number of ds in test.\n",
    "    f_cols: list\n",
    "        List of exogenous variables of the future.\n",
    "    only_model: bool\n",
    "        If true only model will be returned.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model: pl.LightningModule\n",
    "        Forecast model.\n",
    "    trainer: pl.Trainer\n",
    "        Trainer object.\n",
    "    val_loader: DataLoader\n",
    "        Validation loader.\n",
    "    test_loader: DataLoader\n",
    "        Test loader.\n",
    "    scaler_y: Scaler\n",
    "        Scaler object for target time series.   \n",
    "    \"\"\"   \n",
    "\n",
    "    # Protect inplace modifications\n",
    "    Y_df = Y_df.copy()\n",
    "    if X_df is not None:\n",
    "        X_df = X_df.copy()\n",
    "    if S_df is not None:\n",
    "        S_df = S_df.copy()        \n",
    "\n",
    "    #----------------------------------------------- Datasets -----------------------------------------------#\n",
    "    train_dataset, val_dataset, test_dataset, scaler_y = create_datasets(mc=mc,\n",
    "                                                                         S_df=S_df, Y_df=Y_df, X_df=X_df,\n",
    "                                                                         f_cols=f_cols,\n",
    "                                                                         ds_in_val=ds_in_val,\n",
    "                                                                         ds_in_test=ds_in_test)\n",
    "    mc['n_x'], mc['n_s'] = train_dataset.get_n_variables()\n",
    "\n",
    "    #------------------------------------------- Instantiate & fit -------------------------------------------#\n",
    "    train_loader, val_loader, test_loader = instantiate_loaders(mc=mc,\n",
    "                                                                train_dataset=train_dataset,\n",
    "                                                                val_dataset=val_dataset,\n",
    "                                                                test_dataset=test_dataset)\n",
    "    model = instantiate_model(mc=mc)\n",
    "    callbacks = []\n",
    "    if mc['early_stop_patience'] and ds_in_val > 0:\n",
    "        early_stopping = pl.callbacks.EarlyStopping(monitor='val_loss', min_delta=1e-4, \n",
    "                                                    patience=mc['early_stop_patience'],\n",
    "                                                    verbose=True, \n",
    "                                                    mode='min') \n",
    "        callbacks=[early_stopping]\n",
    "    \n",
    "    gpus = -1 if t.cuda.is_available() else 0\n",
    "    trainer = pl.Trainer(max_epochs=mc['max_epochs'], \n",
    "                         max_steps=mc['max_steps'],\n",
    "                         check_val_every_n_epoch=mc['eval_freq'],\n",
    "                         progress_bar_refresh_rate=1,\n",
    "                         gpus=gpus,\n",
    "                         callbacks=callbacks,\n",
    "                         checkpoint_callback=False,\n",
    "                         logger=False)\n",
    "    \n",
    "    val_dataloaders = val_loader if ds_in_val > 0 else None\n",
    "    trainer.fit(model, train_loader, val_dataloaders)\n",
    "    \n",
    "    if only_model:\n",
    "        return model\n",
    "    \n",
    "    return model, trainer, val_loader, test_loader, scaler_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def model_fit_predict(mc: dict, \n",
    "                        S_df: pd.DataFrame, Y_df: pd.DataFrame, X_df: pd.DataFrame, \n",
    "                        f_cols: list, ds_in_val: int, ds_in_test: int) -> dict:\n",
    "    \"\"\"\n",
    "    Traines model on train dataset, then calculates predictions\n",
    "    on test dataset.\n",
    "                     \n",
    "    Parameters\n",
    "    ----------\n",
    "    mc: dict\n",
    "        Model configuration.\n",
    "    Y_df: pd.DataFrame\n",
    "        Target time series with columns ['unique_id', 'ds', 'y'].\n",
    "    X_df: pd.DataFrame\n",
    "        Exogenous time series with columns ['unique_id', 'ds', 'y'].\n",
    "    f_cols: list\n",
    "        List of exogenous variables of the future.\n",
    "    ds_in_val: int\n",
    "        Number of ds in validation.\n",
    "    ds_in_test: int\n",
    "        Number of ds in test.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    results: dict\n",
    "        Dictionary with results of training and prediction on model.   \n",
    "    \"\"\"  \n",
    "\n",
    "    #------------------------------------------------ Fit ------------------------------------------------#\n",
    "    model, trainer, val_loader, test_loader, scaler_y = fit(\n",
    "        mc, S_df=S_df, Y_df=Y_df, X_df=X_df, \n",
    "        f_cols=[], ds_in_val=ds_in_val, ds_in_test=ds_in_val,\n",
    "        only_model=False\n",
    "    )\n",
    "    #------------------------------------------------ Predict ------------------------------------------------#\n",
    "    results = {}\n",
    "\n",
    "    if ds_in_val > 0:\n",
    "        y_true, y_hat, mask, meta_data = predict(mc, model, trainer, val_loader, scaler_y)\n",
    "        val_values = (('val_y_true', y_true), ('val_y_hat', y_hat), ('val_mask', mask), ('val_meta_data', meta_data))\n",
    "        results.update(val_values)\n",
    "\n",
    "        print(f\"VAL y_true.shape: {y_true.shape}\")\n",
    "        print(f\"VAL y_hat.shape: {y_hat.shape}\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "    # Predict test if available\n",
    "    if ds_in_test > 0:\n",
    "        y_true, y_hat, mask, meta_data = predict(mc, model, trainer, test_loader, scaler_y)\n",
    "        test_values = (('test_y_true', y_true), ('test_y_hat', y_hat), ('test_mask', mask), ('test_meta_data', meta_data))\n",
    "        results.update(test_values)\n",
    "\n",
    "        print(f\"TEST y_true.shape: {y_true.shape}\")\n",
    "        print(f\"TEST y_hat.shape: {y_hat.shape}\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def evaluate_model(mc: dict, loss_function_val: callable, loss_functions_test: dict, \n",
    "                   S_df: pd.DataFrame, Y_df: pd.DataFrame, X_df: pd.DataFrame, \n",
    "                   f_cols: list, ds_in_val: int, ds_in_test: int,\n",
    "                   return_forecasts: bool,\n",
    "                   save_progress: bool,\n",
    "                   trials: Trials,\n",
    "                   results_file: str,\n",
    "                   step_save_progress: int =5,\n",
    "                   loss_kwargs: list =None) -> dict:\n",
    "    \"\"\"\n",
    "    Evaluate model on given dataset.\n",
    "                     \n",
    "    Parameters\n",
    "    ----------\n",
    "    mc: dictionary\n",
    "        Model configuration.\n",
    "    loss_function_val: function\n",
    "        Loss function used for validation.\n",
    "    loss_functions_test: Dictionary\n",
    "        Loss functions used for test. \n",
    "        (function name: string, function: fun)\n",
    "    S_df: pd.DataFrame\n",
    "        Static exogenous variables with columns ['unique_id', 'ds']. \n",
    "        and static variables. \n",
    "    Y_df: pd.DataFrame\n",
    "        Target time series with columns ['unique_id', 'ds', 'y'].\n",
    "    X_df: pd.DataFrame\n",
    "        Exogenous time series with columns ['unique_id', 'ds', 'y'].\n",
    "    f_cols: list\n",
    "        List of exogenous variables of the future.\n",
    "    ds_in_val: int\n",
    "        Number of ds in validation.\n",
    "    ds_in_test: int\n",
    "        Number of ds in test.\n",
    "    return_forecasts: bool\n",
    "        If true return forecast on test.\n",
    "    save_progress: bool    \n",
    "        If true save progres in file.\n",
    "    trials: hyperopt.Trials\n",
    "        Results from model evaluation.\n",
    "    results_file: str\n",
    "        File path to save results.\n",
    "    step_save_progress: int\n",
    "        Every n-th step is saved in file.\n",
    "    loss_kwargs: List\n",
    "        Loss function arguments.    \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    results_output: dict\n",
    "        Dictionary with results of model evaluation.   \n",
    "    \"\"\" \n",
    "\n",
    "    if (save_progress) and (len(trials) % step_save_progress == 0):\n",
    "        with open(results_file, \"wb\") as f:\n",
    "            pickle.dump(trials, f)\n",
    "    \n",
    "    print(47*'=' + '\\n')\n",
    "    print(pd.Series(mc))\n",
    "    print(47*'=' + '\\n')\n",
    "    \n",
    "    # Some asserts due to work in progress\n",
    "    n_series = Y_df['unique_id'].nunique()\n",
    "    if n_series > 1:\n",
    "        assert mc['normalizer_y'] is None, 'Data scaling not implemented with multiple time series'\n",
    "        assert mc['normalizer_x'] is None, 'Data scaling not implemented with multiple time series'\n",
    "\n",
    "    assert ds_in_test % mc['val_idx_to_sample_freq']==0, 'outsample size should be multiple of val_idx_to_sample_freq'\n",
    "\n",
    "    # Make predictions\n",
    "    start = time.time()\n",
    "    results = model_fit_predict(mc=mc,\n",
    "                                S_df=S_df, \n",
    "                                Y_df=Y_df,\n",
    "                                X_df=X_df,\n",
    "                                f_cols=f_cols,\n",
    "                                ds_in_val=ds_in_val,\n",
    "                                ds_in_test=ds_in_test)\n",
    "    run_time = time.time() - start\n",
    "\n",
    "    # Evaluate predictions\n",
    "    val_loss = loss_function_val(y=results['val_y_true'], y_hat=results['val_y_hat'], weights=results['val_mask'], **loss_kwargs)\n",
    "\n",
    "    results_output = {'loss': val_loss,\n",
    "                      'mc': mc,\n",
    "                      'run_time': run_time,\n",
    "                      'status': STATUS_OK}\n",
    "\n",
    "    # Evaluation in test (if provided)\n",
    "    if ds_in_test > 0:\n",
    "        test_loss_dict = {}\n",
    "        for loss_name, loss_function in loss_functions_test.items():\n",
    "            test_loss_dict[loss_name] = loss_function(y=results['test_y_true'], y_hat=results['test_y_hat'], weights=results['test_mask'])\n",
    "        results_output['test_losses'] = test_loss_dict\n",
    "\n",
    "    if return_forecasts and ds_in_test > 0:\n",
    "        forecasts_test = {}\n",
    "        test_values = (('test_y_true', results['test_y_true']), ('test_y_hat', results['test_y_hat']),\n",
    "                        ('test_mask', results['test_mask']), ('test_meta_data', results['test_meta_data']))\n",
    "        forecasts_test.update(test_values)\n",
    "        results_output['forecasts_test'] = forecasts_test\n",
    "\n",
    "    return results_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def hyperopt_tunning(space: dict, hyperopt_max_evals: int, \n",
    "                     loss_function_val: callable, loss_functions_test: dict,\n",
    "                     S_df: pd.DataFrame, Y_df: pd.DataFrame, X_df: pd.DataFrame, \n",
    "                     f_cols: list, ds_in_val: int, ds_in_test: int,\n",
    "                     return_forecasts: bool,\n",
    "                     save_progress: bool,\n",
    "                     results_file: str,\n",
    "                     step_save_progress: int =5,\n",
    "                     loss_kwargs: list =None) -> Trials:\n",
    "    \"\"\"\n",
    "    Evaluates multiple models trained on given dataset.\n",
    "    Models are trained with different hyperparameters.\n",
    "    Hyperparameters are changed until function is minimized in\n",
    "    hyperparameter space. All models are trained and evaluated,\n",
    "    until function is minimized. \n",
    "                     \n",
    "    Parameters\n",
    "    ----------\n",
    "    space: Dictionary\n",
    "        Dictionary that contines hyperparameters that create space.\n",
    "    hyperopt_max_evals: int\n",
    "        Maximum number of evaluations.\n",
    "    loss_function_val: function\n",
    "        Loss function used for validation.\n",
    "    loss_functions_test: Dictionary\n",
    "        Loss functions used for test. \n",
    "        (function name: string, function: fun)\n",
    "    S_df: pd.DataFrame\n",
    "        Static exogenous variables with columns ['unique_id', 'ds']. \n",
    "        and static variables. \n",
    "    Y_df: pd.DataFrame\n",
    "        Target time series with columns ['unique_id', 'ds', 'y'].\n",
    "    X_df: pd.DataFrame\n",
    "        Exogenous time series with columns ['unique_id', 'ds', 'y'].\n",
    "    f_cols: list\n",
    "        List of exogenous variables of the future.\n",
    "    ds_in_val: int\n",
    "        Number of ds in validation.\n",
    "    ds_in_test: int\n",
    "        Number of ds in test.\n",
    "    return_forecasts: bool\n",
    "        If true return forecast on test.\n",
    "    save_progress: bool    \n",
    "        If true save progres in file.\n",
    "    results_file: str\n",
    "        File path to save results.\n",
    "    step_save_progress: int\n",
    "        Every n-th step is saved in file.\n",
    "    loss_kwargs: List\n",
    "        Loss function arguments.    \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    trials: Trials\n",
    "        Results from model evaluation.  \n",
    "    \"\"\" \n",
    "\n",
    "    assert ds_in_val > 0, 'Validation set is needed for tunning!'\n",
    "\n",
    "    trials = Trials()\n",
    "    fmin_objective = partial(evaluate_model, loss_function_val=loss_function_val, loss_functions_test=loss_functions_test,\n",
    "                             S_df=S_df, Y_df=Y_df, X_df=X_df, f_cols=f_cols,\n",
    "                             ds_in_val=ds_in_val, ds_in_test=ds_in_test,\n",
    "                             return_forecasts=return_forecasts, save_progress=save_progress, trials=trials,\n",
    "                             results_file=results_file,\n",
    "                             step_save_progress=step_save_progress,\n",
    "                             loss_kwargs=loss_kwargs or {})\n",
    "\n",
    "    fmin(fmin_objective, space=space, algo=tpe.suggest, max_evals=hyperopt_max_evals, trials=trials, verbose=True)\n",
    "\n",
    "    return trials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Utils Examples\n",
    ">This part of notebook shows simple use of functions given in this notebook. EPF dataset in used in this experiments. Two experiments are shown where hyperparameter tunning is runned for two models NHITS and NBEATS.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "\n",
    "from neuralforecast.losses.numpy import mae, rmse\n",
    "from neuralforecast.models.nhits.nhits import suggested_space as nhits_suggested_space\n",
    "from neuralforecast.models.nbeats.nbeats import suggested_space as nbeats_suggested_space\n",
    "from neuralforecast.data.datasets.epf import EPF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ['NP']\n",
    "Y_df, X_df, S_df = EPF.load_groups(directory='data', groups=dataset)\n",
    "X_df = X_df[['unique_id', 'ds', 'week_day']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                                                                                            | 0/2 [00:00<?, ?trial/s, best loss=?]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.011583 seconds\n",
      "INFO:hyperopt.tpe:TPE using 0 trials\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A                                                                                                                                                                                                    ===============================================\n",
      "\n",
      "\n",
      "  0%|                                                                                                                                                            | 0/2 [00:00<?, ?trial/s, best loss=?]\u001b[A\n",
      "\u001b[A                                                                                                                                                                                                    activation                                          ReLU\n",
      "batch_normalization                                False\n",
      "batch_size                                             1\n",
      "complete_windows                                    True\n",
      "constant_n_blocks                                      1\n",
      "constant_n_layers                                      2\n",
      "constant_n_mlp_units                                 512\n",
      "dropout_prob_theta                                     0\n",
      "early_stop_patience                                   10\n",
      "eval_freq                                             50\n",
      "frequency                                              H\n",
      "idx_to_sample_freq                                     1\n",
      "initialization                              lecun_normal\n",
      "interpolation_mode                                linear\n",
      "learning_rate                                       0.01\n",
      "loss_hypar                                           0.5\n",
      "loss_train                                           MAE\n",
      "loss_valid                                           MAE\n",
      "lr_decay                                             0.5\n",
      "max_epochs                                          None\n",
      "max_steps                                             10\n",
      "mode                                              simple\n",
      "model                                              nhits\n",
      "n_freq_downsample                             (60, 8, 1)\n",
      "n_lr_decays                                            3\n",
      "n_pool_kernel_size                             (1, 1, 1)\n",
      "n_s                                                    0\n",
      "n_s_hidden                                             0\n",
      "n_time_in                                             48\n",
      "n_time_out                                            24\n",
      "n_windows                                             32\n",
      "n_x                                                    1\n",
      "n_x_hidden                                             1\n",
      "normalizer_x                                        None\n",
      "normalizer_y                                        None\n",
      "pooling_mode                                         max\n",
      "random_seed                                         18.0\n",
      "shared_weights                                     False\n",
      "stack_types               (identity, identity, identity)\n",
      "val_idx_to_sample_freq                                 1\n",
      "weight_decay                                           0\n",
      "dtype: object\n",
      "\n",
      "  0%|                                                                                                                                                            | 0/2 [00:00<?, ?trial/s, best loss=?]\u001b[A\n",
      "\u001b[A                                                                                                                                                                                                    ===============================================\n",
      "\n",
      "\n",
      "  0%|                                                                                                                                                            | 0/2 [00:00<?, ?trial/s, best loss=?]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Train Validation splits\n",
      "\n",
      "INFO:root:                              ds                    \n",
      "                             min                 max\n",
      "unique_id sample_mask                               \n",
      "NP        0           2018-12-11 2018-12-24 23:00:00\n",
      "          1           2013-01-01 2018-12-10 23:00:00\n",
      "INFO:root:\n",
      "Total data \t\t\t52416 time stamps \n",
      "Available percentage=100.0, \t52416 time stamps \n",
      "Insample  percentage=99.36, \t52080 time stamps \n",
      "Outsample percentage=0.64, \t336 time stamps \n",
      "\n",
      "/Users/fedex/projects/neuralforecast/neuralforecast/data/tsdataset.py:208: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  X.drop(['unique_id', 'ds'], 1, inplace=True)\n",
      "\n",
      "INFO:root:Train Validation splits\n",
      "\n",
      "INFO:root:                              ds                    \n",
      "                             min                 max\n",
      "unique_id sample_mask                               \n",
      "NP        0           2013-01-01 2018-12-24 23:00:00\n",
      "          1           2018-12-11 2018-12-17 23:00:00\n",
      "INFO:root:\n",
      "Total data \t\t\t52416 time stamps \n",
      "Available percentage=100.0, \t52416 time stamps \n",
      "Insample  percentage=0.32, \t168 time stamps \n",
      "Outsample percentage=99.68, \t52248 time stamps \n",
      "\n",
      "/Users/fedex/projects/neuralforecast/neuralforecast/data/tsdataset.py:208: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  X.drop(['unique_id', 'ds'], 1, inplace=True)\n",
      "\n",
      "INFO:root:Train Validation splits\n",
      "\n",
      "INFO:root:                              ds                    \n",
      "                             min                 max\n",
      "unique_id sample_mask                               \n",
      "NP        0           2013-01-01 2018-12-17 23:00:00\n",
      "          1           2018-12-18 2018-12-24 23:00:00\n",
      "INFO:root:\n",
      "Total data \t\t\t52416 time stamps \n",
      "Available percentage=100.0, \t52416 time stamps \n",
      "Insample  percentage=0.32, \t168 time stamps \n",
      "Outsample percentage=99.68, \t52248 time stamps \n",
      "\n",
      "/Users/fedex/projects/neuralforecast/neuralforecast/data/tsdataset.py:208: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  X.drop(['unique_id', 'ds'], 1, inplace=True)\n",
      "\n",
      "/Users/fedex/opt/miniconda3/envs/neuralforecast/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=False)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=False)`.\n",
      "  f\"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will \"\n",
      "\n",
      "/Users/fedex/opt/miniconda3/envs/neuralforecast/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:91: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=1)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "  f\"Setting `Trainer(progress_bar_refresh_rate={progress_bar_refresh_rate})` is deprecated in v1.5 and\"\n",
      "\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "\n",
      "  | Name  | Type   | Params\n",
      "---------------------------------\n",
      "0 | model | _NHITS | 1.1 M \n",
      "---------------------------------\n",
      "1.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.1 M     Total params\n",
      "4.248     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fedex/opt/miniconda3/envs/neuralforecast/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "\n",
      "/Users/fedex/opt/miniconda3/envs/neuralforecast/lib/python3.7/site-packages/torch/nn/functional.py:3635: UserWarning: Default upsampling behavior when mode=linear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode)\n",
      "\n",
      "/Users/fedex/opt/miniconda3/envs/neuralforecast/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76ee7de573a8456abe2685b103eb5011",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fedex/projects/neuralforecast/neuralforecast/data/tsloader.py:47: UserWarning: This class wraps the pytorch `DataLoader` with a special collate function. If you want to use yours simply use `DataLoader`. Removing collate_fn\n",
      "  'This class wraps the pytorch `DataLoader` with a '\n",
      "\n",
      "/Users/fedex/opt/miniconda3/envs/neuralforecast/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, predict_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cea530f103e44128b7818e713135171",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 1it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A                                                                                                                                                                                                    VAL y_true.shape: (145, 24)\n",
      "\n",
      "  0%|                                                                                                                                                            | 0/2 [00:01<?, ?trial/s, best loss=?]\u001b[A\n",
      "\u001b[A                                                                                                                                                                                                    VAL y_hat.shape: (145, 24)\n",
      "\n",
      "  0%|                                                                                                                                                            | 0/2 [00:01<?, ?trial/s, best loss=?]\u001b[A"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54a40e7cf1d3436ba86cdd41ac61ac67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 1it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A                                                                                                                                                                                                    TEST y_true.shape: (145, 24)\n",
      "\n",
      "  0%|                                                                                                                                                            | 0/2 [00:01<?, ?trial/s, best loss=?]\u001b[A\n",
      "\u001b[A                                                                                                                                                                                                    TEST y_hat.shape: (145, 24)\n",
      "\n",
      "  0%|                                                                                                                                                            | 0/2 [00:01<?, ?trial/s, best loss=?]\u001b[A\n",
      " 50%|                                                                 | 1/2 [00:01<00:01,  1.16s/trial, best loss: 9.431559562683105]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.012864 seconds\n",
      "INFO:hyperopt.tpe:TPE using 1/1 trials with best loss 9.431560\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A                                                                                                                                                                                                    ===============================================\n",
      "\n",
      "\n",
      " 50%|                                                                 | 1/2 [00:01<00:01,  1.16s/trial, best loss: 9.431559562683105]\u001b[A\n",
      "\u001b[A                                                                                                                                                                                                    activation                                          ReLU\n",
      "batch_normalization                                False\n",
      "batch_size                                             1\n",
      "complete_windows                                    True\n",
      "constant_n_blocks                                      3\n",
      "constant_n_layers                                      2\n",
      "constant_n_mlp_units                                 256\n",
      "dropout_prob_theta                                     0\n",
      "early_stop_patience                                   10\n",
      "eval_freq                                             50\n",
      "frequency                                              H\n",
      "idx_to_sample_freq                                     1\n",
      "initialization                              lecun_normal\n",
      "interpolation_mode                                linear\n",
      "learning_rate                                     0.0001\n",
      "loss_hypar                                           0.5\n",
      "loss_train                                           MAE\n",
      "loss_valid                                           MAE\n",
      "lr_decay                                             0.5\n",
      "max_epochs                                          None\n",
      "max_steps                                             10\n",
      "mode                                              simple\n",
      "model                                              nhits\n",
      "n_freq_downsample                             (60, 8, 1)\n",
      "n_lr_decays                                            3\n",
      "n_pool_kernel_size                             (8, 8, 8)\n",
      "n_s                                                    0\n",
      "n_s_hidden                                             0\n",
      "n_time_in                                            120\n",
      "n_time_out                                            24\n",
      "n_windows                                            256\n",
      "n_x                                                    1\n",
      "n_x_hidden                                             1\n",
      "normalizer_x                                        None\n",
      "normalizer_y                                        None\n",
      "pooling_mode                                         max\n",
      "random_seed                                         16.0\n",
      "shared_weights                                     False\n",
      "stack_types               (identity, identity, identity)\n",
      "val_idx_to_sample_freq                                 1\n",
      "weight_decay                                           0\n",
      "dtype: object\n",
      "\n",
      " 50%|                                                                 | 1/2 [00:01<00:01,  1.16s/trial, best loss: 9.431559562683105]\u001b[A\n",
      "\u001b[A                                                                                                                                                                                                    ===============================================\n",
      "\n",
      "\n",
      " 50%|                                                                 | 1/2 [00:01<00:01,  1.16s/trial, best loss: 9.431559562683105]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Train Validation splits\n",
      "\n",
      "INFO:root:                              ds                    \n",
      "                             min                 max\n",
      "unique_id sample_mask                               \n",
      "NP        0           2018-12-11 2018-12-24 23:00:00\n",
      "          1           2013-01-01 2018-12-10 23:00:00\n",
      "INFO:root:\n",
      "Total data \t\t\t52416 time stamps \n",
      "Available percentage=100.0, \t52416 time stamps \n",
      "Insample  percentage=99.36, \t52080 time stamps \n",
      "Outsample percentage=0.64, \t336 time stamps \n",
      "\n",
      "/Users/fedex/projects/neuralforecast/neuralforecast/data/tsdataset.py:208: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  X.drop(['unique_id', 'ds'], 1, inplace=True)\n",
      "\n",
      "INFO:root:Train Validation splits\n",
      "\n",
      "INFO:root:                              ds                    \n",
      "                             min                 max\n",
      "unique_id sample_mask                               \n",
      "NP        0           2013-01-01 2018-12-24 23:00:00\n",
      "          1           2018-12-11 2018-12-17 23:00:00\n",
      "INFO:root:\n",
      "Total data \t\t\t52416 time stamps \n",
      "Available percentage=100.0, \t52416 time stamps \n",
      "Insample  percentage=0.32, \t168 time stamps \n",
      "Outsample percentage=99.68, \t52248 time stamps \n",
      "\n",
      "/Users/fedex/projects/neuralforecast/neuralforecast/data/tsdataset.py:208: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  X.drop(['unique_id', 'ds'], 1, inplace=True)\n",
      "\n",
      "INFO:root:Train Validation splits\n",
      "\n",
      "INFO:root:                              ds                    \n",
      "                             min                 max\n",
      "unique_id sample_mask                               \n",
      "NP        0           2013-01-01 2018-12-17 23:00:00\n",
      "          1           2018-12-18 2018-12-24 23:00:00\n",
      "INFO:root:\n",
      "Total data \t\t\t52416 time stamps \n",
      "Available percentage=100.0, \t52416 time stamps \n",
      "Insample  percentage=0.32, \t168 time stamps \n",
      "Outsample percentage=99.68, \t52248 time stamps \n",
      "\n",
      "/Users/fedex/projects/neuralforecast/neuralforecast/data/tsdataset.py:208: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  X.drop(['unique_id', 'ds'], 1, inplace=True)\n",
      "\n",
      "/Users/fedex/opt/miniconda3/envs/neuralforecast/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=False)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=False)`.\n",
      "  f\"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will \"\n",
      "\n",
      "/Users/fedex/opt/miniconda3/envs/neuralforecast/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:91: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=1)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "  f\"Setting `Trainer(progress_bar_refresh_rate={progress_bar_refresh_rate})` is deprecated in v1.5 and\"\n",
      "\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "\n",
      "  | Name  | Type   | Params\n",
      "---------------------------------\n",
      "0 | model | _NHITS | 1.3 M \n",
      "---------------------------------\n",
      "1.3 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.3 M     Total params\n",
      "5.040     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fedex/opt/miniconda3/envs/neuralforecast/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "\n",
      "/Users/fedex/opt/miniconda3/envs/neuralforecast/lib/python3.7/site-packages/torch/nn/functional.py:3635: UserWarning: Default upsampling behavior when mode=linear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode)\n",
      "\n",
      "/Users/fedex/opt/miniconda3/envs/neuralforecast/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa3ab7b4dbed46cd8c267fcb0c92dfa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fedex/projects/neuralforecast/neuralforecast/data/tsloader.py:47: UserWarning: This class wraps the pytorch `DataLoader` with a special collate function. If you want to use yours simply use `DataLoader`. Removing collate_fn\n",
      "  'This class wraps the pytorch `DataLoader` with a '\n",
      "\n",
      "/Users/fedex/opt/miniconda3/envs/neuralforecast/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, predict_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "709eb466fd1f448ebe5e961e8725ac52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 1it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A                                                                                                                                                                                                    VAL y_true.shape: (145, 24)\n",
      "\n",
      " 50%|                                                                 | 1/2 [00:02<00:01,  1.16s/trial, best loss: 9.431559562683105]\u001b[A\n",
      "\u001b[A                                                                                                                                                                                                    VAL y_hat.shape: (145, 24)\n",
      "\n",
      " 50%|                                                                 | 1/2 [00:02<00:01,  1.16s/trial, best loss: 9.431559562683105]\u001b[A"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "513dd707ae0d4412a78f4e719d05f0a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 1it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A                                                                                                                                                                                                    TEST y_true.shape: (145, 24)\n",
      "\n",
      " 50%|                                                                 | 1/2 [00:02<00:01,  1.16s/trial, best loss: 9.431559562683105]\u001b[A\n",
      "\u001b[A                                                                                                                                                                                                    TEST y_hat.shape: (145, 24)\n",
      "\n",
      " 50%|                                                                 | 1/2 [00:02<00:01,  1.16s/trial, best loss: 9.431559562683105]\u001b[A\n",
      "100%|| 2/2 [00:02<00:00,  1.46s/trial, best loss: 8.655431747436523]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "nhits_space = nhits_suggested_space(n_time_out=24, n_series=1, n_x=1, n_s=0, frequency='H')\n",
    "nhits_space['max_steps'] = hp.choice('max_steps', [10]) # Override max_steps for faster example\n",
    "trials = hyperopt_tunning(space=nhits_space, hyperopt_max_evals=2, loss_function_val=mae,\n",
    "                          loss_functions_test={'mae': mae, 'rmse': rmse},\n",
    "                          S_df=S_df, Y_df=Y_df, X_df=X_df, f_cols=[],\n",
    "                          ds_in_val=7*24, ds_in_test=7*24, \n",
    "                          return_forecasts=True, save_progress=False, \n",
    "                          results_file=None, loss_kwargs={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Train Validation splits\n",
      "\n",
      "INFO:root:                              ds                    \n",
      "                             min                 max\n",
      "unique_id sample_mask                               \n",
      "NP        1           2013-01-01 2018-12-24 23:00:00\n",
      "INFO:root:\n",
      "Total data \t\t\t52416 time stamps \n",
      "Available percentage=100.0, \t52416 time stamps \n",
      "Insample  percentage=100.0, \t52416 time stamps \n",
      "Outsample percentage=0.0, \t0 time stamps \n",
      "\n",
      "/Users/fedex/projects/neuralforecast/neuralforecast/data/tsdataset.py:208: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  X.drop(['unique_id', 'ds'], 1, inplace=True)\n",
      "INFO:root:Train Validation splits\n",
      "\n",
      "INFO:root:                              ds                    \n",
      "                             min                 max\n",
      "unique_id sample_mask                               \n",
      "NP        0           2013-01-01 2018-12-24 23:00:00\n",
      "INFO:root:\n",
      "Total data \t\t\t52416 time stamps \n",
      "Available percentage=100.0, \t52416 time stamps \n",
      "Insample  percentage=0.0, \t0 time stamps \n",
      "Outsample percentage=100.0, \t52416 time stamps \n",
      "\n",
      "/Users/fedex/projects/neuralforecast/neuralforecast/data/tsdataset.py:208: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  X.drop(['unique_id', 'ds'], 1, inplace=True)\n",
      "INFO:root:Train Validation splits\n",
      "\n",
      "INFO:root:                              ds                    \n",
      "                             min                 max\n",
      "unique_id sample_mask                               \n",
      "NP        0           2013-01-01 2018-12-24 23:00:00\n",
      "INFO:root:\n",
      "Total data \t\t\t52416 time stamps \n",
      "Available percentage=100.0, \t52416 time stamps \n",
      "Insample  percentage=0.0, \t0 time stamps \n",
      "Outsample percentage=100.0, \t52416 time stamps \n",
      "\n",
      "/Users/fedex/projects/neuralforecast/neuralforecast/data/tsdataset.py:208: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  X.drop(['unique_id', 'ds'], 1, inplace=True)\n",
      "/Users/fedex/opt/miniconda3/envs/neuralforecast/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=False)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=False)`.\n",
      "  f\"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will \"\n",
      "/Users/fedex/opt/miniconda3/envs/neuralforecast/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:91: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=1)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "  f\"Setting `Trainer(progress_bar_refresh_rate={progress_bar_refresh_rate})` is deprecated in v1.5 and\"\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/Users/fedex/opt/miniconda3/envs/neuralforecast/lib/python3.7/site-packages/pytorch_lightning/trainer/configuration_validator.py:122: UserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\"You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\")\n",
      "\n",
      "  | Name  | Type   | Params\n",
      "---------------------------------\n",
      "0 | model | _NHITS | 1.9 M \n",
      "---------------------------------\n",
      "1.9 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 M     Total params\n",
      "7.405     Total estimated model params size (MB)\n",
      "/Users/fedex/opt/miniconda3/envs/neuralforecast/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c40f04cab8e4ec6ac967d4519931bd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fedex/opt/miniconda3/envs/neuralforecast/lib/python3.7/site-packages/torch/nn/functional.py:3635: UserWarning: Default upsampling behavior when mode=linear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode)\n"
     ]
    }
   ],
   "source": [
    "best_model = fit(mc=trials.best_trial['result']['mc'], Y_df=Y_df, S_df=S_df, X_df=X_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                            | 0/2 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.008880 seconds\n",
      "INFO:hyperopt.tpe:TPE using 0 trials\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================                                                                                                                                                        \n",
      "\n",
      "activation                                          ReLU                                                                                                                                               \n",
      "batch_normalization                                False\n",
      "batch_size                                             1\n",
      "complete_windows                                    True\n",
      "constant_n_blocks                                      3\n",
      "constant_n_layers                                      3\n",
      "constant_n_mlp_units                                1024\n",
      "dropout_prob_theta                                     0\n",
      "early_stop_patience                                   10\n",
      "eval_freq                                             50\n",
      "frequency                                              H\n",
      "idx_to_sample_freq                                     1\n",
      "initialization                              lecun_normal\n",
      "learning_rate                                      0.005\n",
      "loss_hypar                                           0.5\n",
      "loss_train                                           MAE\n",
      "loss_valid                                           MAE\n",
      "lr_decay                                             0.5\n",
      "max_epochs                                          None\n",
      "max_steps                                             10\n",
      "mode                                              simple\n",
      "model                                             nbeats\n",
      "n_lr_decays                                            3\n",
      "n_s                                                    0\n",
      "n_s_hidden                                             0\n",
      "n_time_in                                            120\n",
      "n_time_out                                            24\n",
      "n_windows                                             32\n",
      "n_x                                                    1\n",
      "n_x_hidden                                             1\n",
      "normalizer_x                                        None\n",
      "normalizer_y                                        None\n",
      "random_seed                                         14.0\n",
      "shared_weights                                     False\n",
      "stack_types               (identity, identity, identity)\n",
      "val_idx_to_sample_freq                                 1\n",
      "weight_decay                                           0\n",
      "dtype: object\n",
      "===============================================                                                                                                                                                        \n",
      "\n",
      "  0%|                                                                                                                                                            | 0/2 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Train Validation splits\n",
      "\n",
      "INFO:root:                              ds                    \n",
      "                             min                 max\n",
      "unique_id sample_mask                               \n",
      "NP        0           2018-12-11 2018-12-24 23:00:00\n",
      "          1           2013-01-01 2018-12-10 23:00:00\n",
      "INFO:root:\n",
      "Total data \t\t\t52416 time stamps \n",
      "Available percentage=100.0, \t52416 time stamps \n",
      "Insample  percentage=99.36, \t52080 time stamps \n",
      "Outsample percentage=0.64, \t336 time stamps \n",
      "\n",
      "/Users/fedex/projects/neuralforecast/neuralforecast/data/tsdataset.py:208: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  X.drop(['unique_id', 'ds'], 1, inplace=True)\n",
      "\n",
      "INFO:root:Train Validation splits\n",
      "\n",
      "INFO:root:                              ds                    \n",
      "                             min                 max\n",
      "unique_id sample_mask                               \n",
      "NP        0           2013-01-01 2018-12-24 23:00:00\n",
      "          1           2018-12-11 2018-12-17 23:00:00\n",
      "INFO:root:\n",
      "Total data \t\t\t52416 time stamps \n",
      "Available percentage=100.0, \t52416 time stamps \n",
      "Insample  percentage=0.32, \t168 time stamps \n",
      "Outsample percentage=99.68, \t52248 time stamps \n",
      "\n",
      "/Users/fedex/projects/neuralforecast/neuralforecast/data/tsdataset.py:208: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  X.drop(['unique_id', 'ds'], 1, inplace=True)\n",
      "\n",
      "INFO:root:Train Validation splits\n",
      "\n",
      "INFO:root:                              ds                    \n",
      "                             min                 max\n",
      "unique_id sample_mask                               \n",
      "NP        0           2013-01-01 2018-12-17 23:00:00\n",
      "          1           2018-12-18 2018-12-24 23:00:00\n",
      "INFO:root:\n",
      "Total data \t\t\t52416 time stamps \n",
      "Available percentage=100.0, \t52416 time stamps \n",
      "Insample  percentage=0.32, \t168 time stamps \n",
      "Outsample percentage=99.68, \t52248 time stamps \n",
      "\n",
      "/Users/fedex/projects/neuralforecast/neuralforecast/data/tsdataset.py:208: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  X.drop(['unique_id', 'ds'], 1, inplace=True)\n",
      "\n",
      "/Users/fedex/opt/miniconda3/envs/neuralforecast/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=False)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=False)`.\n",
      "  f\"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will \"\n",
      "\n",
      "/Users/fedex/opt/miniconda3/envs/neuralforecast/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:91: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=1)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "  f\"Setting `Trainer(progress_bar_refresh_rate={progress_bar_refresh_rate})` is deprecated in v1.5 and\"\n",
      "\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "\n",
      "  | Name  | Type    | Params\n",
      "----------------------------------\n",
      "0 | model | _NBEATS | 22.7 M\n",
      "----------------------------------\n",
      "22.7 M    Trainable params\n",
      "0         Non-trainable params\n",
      "22.7 M    Total params\n",
      "90.654    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fedex/opt/miniconda3/envs/neuralforecast/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "\n",
      "/Users/fedex/opt/miniconda3/envs/neuralforecast/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0700ae2914094cf587246d5fbd4a3c2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fedex/projects/neuralforecast/neuralforecast/data/tsloader.py:47: UserWarning: This class wraps the pytorch `DataLoader` with a special collate function. If you want to use yours simply use `DataLoader`. Removing collate_fn\n",
      "  'This class wraps the pytorch `DataLoader` with a '\n",
      "\n",
      "/Users/fedex/opt/miniconda3/envs/neuralforecast/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, predict_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bb71c8bb1d940bb853ac05208ce5c95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 1it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAL y_true.shape: (145, 24)                                                                                                                                                                            \n",
      "VAL y_hat.shape: (145, 24)                                                                                                                                                                             \n",
      "  0%|                                                                                                                                                            | 0/2 [00:03<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17ac8b888a864755a7d60fc5cab794dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 1it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST y_true.shape: (145, 24)                                                                                                                                                                           \n",
      "TEST y_hat.shape: (145, 24)                                                                                                                                                                            \n",
      " 50%|                                                                 | 1/2 [00:03<00:03,  3.67s/trial, best loss: 10.357765197753906]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.011300 seconds\n",
      "INFO:hyperopt.tpe:TPE using 1/1 trials with best loss 10.357765\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================                                                                                                                                                        \n",
      "\n",
      "activation                                          ReLU                                                                                                                                               \n",
      "batch_normalization                                False\n",
      "batch_size                                             1\n",
      "complete_windows                                    True\n",
      "constant_n_blocks                                      1\n",
      "constant_n_layers                                      2\n",
      "constant_n_mlp_units                                1024\n",
      "dropout_prob_theta                                     0\n",
      "early_stop_patience                                   10\n",
      "eval_freq                                             50\n",
      "frequency                                              H\n",
      "idx_to_sample_freq                                     1\n",
      "initialization                              lecun_normal\n",
      "learning_rate                                      0.005\n",
      "loss_hypar                                           0.5\n",
      "loss_train                                           MAE\n",
      "loss_valid                                           MAE\n",
      "lr_decay                                             0.5\n",
      "max_epochs                                          None\n",
      "max_steps                                             10\n",
      "mode                                              simple\n",
      "model                                             nbeats\n",
      "n_lr_decays                                            3\n",
      "n_s                                                    0\n",
      "n_s_hidden                                             0\n",
      "n_time_in                                             48\n",
      "n_time_out                                            24\n",
      "n_windows                                            512\n",
      "n_x                                                    1\n",
      "n_x_hidden                                             1\n",
      "normalizer_x                                        None\n",
      "normalizer_y                                        None\n",
      "random_seed                                         20.0\n",
      "shared_weights                                     False\n",
      "stack_types               (identity, identity, identity)\n",
      "val_idx_to_sample_freq                                 1\n",
      "weight_decay                                           0\n",
      "dtype: object\n",
      "===============================================                                                                                                                                                        \n",
      "\n",
      " 50%|                                                                 | 1/2 [00:03<00:03,  3.67s/trial, best loss: 10.357765197753906]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Train Validation splits\n",
      "\n",
      "INFO:root:                              ds                    \n",
      "                             min                 max\n",
      "unique_id sample_mask                               \n",
      "NP        0           2018-12-11 2018-12-24 23:00:00\n",
      "          1           2013-01-01 2018-12-10 23:00:00\n",
      "INFO:root:\n",
      "Total data \t\t\t52416 time stamps \n",
      "Available percentage=100.0, \t52416 time stamps \n",
      "Insample  percentage=99.36, \t52080 time stamps \n",
      "Outsample percentage=0.64, \t336 time stamps \n",
      "\n",
      "/Users/fedex/projects/neuralforecast/neuralforecast/data/tsdataset.py:208: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  X.drop(['unique_id', 'ds'], 1, inplace=True)\n",
      "\n",
      "INFO:root:Train Validation splits\n",
      "\n",
      "INFO:root:                              ds                    \n",
      "                             min                 max\n",
      "unique_id sample_mask                               \n",
      "NP        0           2013-01-01 2018-12-24 23:00:00\n",
      "          1           2018-12-11 2018-12-17 23:00:00\n",
      "INFO:root:\n",
      "Total data \t\t\t52416 time stamps \n",
      "Available percentage=100.0, \t52416 time stamps \n",
      "Insample  percentage=0.32, \t168 time stamps \n",
      "Outsample percentage=99.68, \t52248 time stamps \n",
      "\n",
      "/Users/fedex/projects/neuralforecast/neuralforecast/data/tsdataset.py:208: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  X.drop(['unique_id', 'ds'], 1, inplace=True)\n",
      "\n",
      "INFO:root:Train Validation splits\n",
      "\n",
      "INFO:root:                              ds                    \n",
      "                             min                 max\n",
      "unique_id sample_mask                               \n",
      "NP        0           2013-01-01 2018-12-17 23:00:00\n",
      "          1           2018-12-18 2018-12-24 23:00:00\n",
      "INFO:root:\n",
      "Total data \t\t\t52416 time stamps \n",
      "Available percentage=100.0, \t52416 time stamps \n",
      "Insample  percentage=0.32, \t168 time stamps \n",
      "Outsample percentage=99.68, \t52248 time stamps \n",
      "\n",
      "/Users/fedex/projects/neuralforecast/neuralforecast/data/tsdataset.py:208: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  X.drop(['unique_id', 'ds'], 1, inplace=True)\n",
      "\n",
      "/Users/fedex/opt/miniconda3/envs/neuralforecast/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=False)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=False)`.\n",
      "  f\"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will \"\n",
      "\n",
      "/Users/fedex/opt/miniconda3/envs/neuralforecast/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:91: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=1)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "  f\"Setting `Trainer(progress_bar_refresh_rate={progress_bar_refresh_rate})` is deprecated in v1.5 and\"\n",
      "\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "\n",
      "  | Name  | Type    | Params\n",
      "----------------------------------\n",
      "0 | model | _NBEATS | 3.7 M \n",
      "----------------------------------\n",
      "3.7 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.7 M     Total params\n",
      "14.968    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fedex/opt/miniconda3/envs/neuralforecast/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "\n",
      "/Users/fedex/opt/miniconda3/envs/neuralforecast/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3186ffafddd4c7ab7cb0a9ca8de1ee5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fedex/projects/neuralforecast/neuralforecast/data/tsloader.py:47: UserWarning: This class wraps the pytorch `DataLoader` with a special collate function. If you want to use yours simply use `DataLoader`. Removing collate_fn\n",
      "  'This class wraps the pytorch `DataLoader` with a '\n",
      "\n",
      "/Users/fedex/opt/miniconda3/envs/neuralforecast/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, predict_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "700ae857490e48a2933b62376730dfce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 1it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAL y_true.shape: (145, 24)                                                                                                                                                                            \n",
      "VAL y_hat.shape: (145, 24)                                                                                                                                                                             \n",
      " 50%|                                                                 | 1/2 [00:07<00:03,  3.67s/trial, best loss: 10.357765197753906]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f85b7bec03da42618c18ebc28d1eac5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 1it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST y_true.shape: (145, 24)                                                                                                                                                                           \n",
      "TEST y_hat.shape: (145, 24)                                                                                                                                                                            \n",
      "100%|| 2/2 [00:07<00:00,  3.81s/trial, best loss: 8.38829517364502]\n"
     ]
    }
   ],
   "source": [
    "nbeats_space = nbeats_suggested_space(n_time_out=24, n_series=1, n_x=1, n_s=0, frequency='H')\n",
    "nbeats_space['max_steps'] = hp.choice('max_steps', [10]) # Override max_steps for faster example\n",
    "trials = hyperopt_tunning(space=nbeats_space, hyperopt_max_evals=2, loss_function_val=mae,\n",
    "                          loss_functions_test={'mae': mae, 'rmse': rmse},\n",
    "                          S_df=S_df, Y_df=Y_df, X_df=X_df, f_cols=[],\n",
    "                          ds_in_val=7*24, ds_in_test=7*24, \n",
    "                          return_forecasts=True, save_progress=False, \n",
    "                          results_file=None, loss_kwargs={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Train Validation splits\n",
      "\n",
      "INFO:root:                              ds                    \n",
      "                             min                 max\n",
      "unique_id sample_mask                               \n",
      "NP        1           2013-01-01 2018-12-24 23:00:00\n",
      "INFO:root:\n",
      "Total data \t\t\t52416 time stamps \n",
      "Available percentage=100.0, \t52416 time stamps \n",
      "Insample  percentage=100.0, \t52416 time stamps \n",
      "Outsample percentage=0.0, \t0 time stamps \n",
      "\n",
      "/Users/fedex/projects/neuralforecast/neuralforecast/data/tsdataset.py:208: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  X.drop(['unique_id', 'ds'], 1, inplace=True)\n",
      "INFO:root:Train Validation splits\n",
      "\n",
      "INFO:root:                              ds                    \n",
      "                             min                 max\n",
      "unique_id sample_mask                               \n",
      "NP        0           2013-01-01 2018-12-24 23:00:00\n",
      "INFO:root:\n",
      "Total data \t\t\t52416 time stamps \n",
      "Available percentage=100.0, \t52416 time stamps \n",
      "Insample  percentage=0.0, \t0 time stamps \n",
      "Outsample percentage=100.0, \t52416 time stamps \n",
      "\n",
      "/Users/fedex/projects/neuralforecast/neuralforecast/data/tsdataset.py:208: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  X.drop(['unique_id', 'ds'], 1, inplace=True)\n",
      "INFO:root:Train Validation splits\n",
      "\n",
      "INFO:root:                              ds                    \n",
      "                             min                 max\n",
      "unique_id sample_mask                               \n",
      "NP        0           2013-01-01 2018-12-24 23:00:00\n",
      "INFO:root:\n",
      "Total data \t\t\t52416 time stamps \n",
      "Available percentage=100.0, \t52416 time stamps \n",
      "Insample  percentage=0.0, \t0 time stamps \n",
      "Outsample percentage=100.0, \t52416 time stamps \n",
      "\n",
      "/Users/fedex/projects/neuralforecast/neuralforecast/data/tsdataset.py:208: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  X.drop(['unique_id', 'ds'], 1, inplace=True)\n",
      "/Users/fedex/opt/miniconda3/envs/neuralforecast/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=False)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=False)`.\n",
      "  f\"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will \"\n",
      "/Users/fedex/opt/miniconda3/envs/neuralforecast/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:91: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=1)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "  f\"Setting `Trainer(progress_bar_refresh_rate={progress_bar_refresh_rate})` is deprecated in v1.5 and\"\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/Users/fedex/opt/miniconda3/envs/neuralforecast/lib/python3.7/site-packages/pytorch_lightning/trainer/configuration_validator.py:122: UserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\"You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\")\n",
      "\n",
      "  | Name  | Type    | Params\n",
      "----------------------------------\n",
      "0 | model | _NBEATS | 3.7 M \n",
      "----------------------------------\n",
      "3.7 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.7 M     Total params\n",
      "14.968    Total estimated model params size (MB)\n",
      "/Users/fedex/opt/miniconda3/envs/neuralforecast/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c73804c0c1f248c0a99fff0f21717a73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_model = fit(mc=trials.best_trial['result']['mc'], Y_df=Y_df, S_df=S_df, X_df=X_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
