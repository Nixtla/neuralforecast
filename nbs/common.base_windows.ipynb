{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524620c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp common._base_windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15392f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fa25a4",
   "metadata": {},
   "source": [
    "# BaseWindows\n",
    "\n",
    "> The `BaseWindows` class contains standard methods shared across window-based neural networks; in contrast to recurrent neural networks these models commit to a fixed sequence length input. The class is represented by `MLP`, and other more sophisticated architectures like `NBEATS`, and `NHITS`.<br><br>The standard methods include data preprocessing `_normalization`, optimization utilities like parameter initialization, `training_step`, `validation_step`, and shared `fit` and `predict` methods.These shared methods enable all the `neuralforecast.models` compatibility with the `core.NeuralForecast` wrapper class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2508f7a9-1433-4ad8-8f2f-0078c6ed6c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from fastcore.test import test_eq\n",
    "from nbdev.showdoc import show_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44065066-e72a-431f-938f-1528adef9fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import TQDMProgressBar\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "from neuralforecast.common._scalers import TemporalNorm\n",
    "from neuralforecast.tsdataset import TimeSeriesDataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce70cd14-ecb1-4205-8511-fecbd26c8408",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BaseWindows(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, \n",
    "                 h,\n",
    "                 input_size,\n",
    "                 loss,\n",
    "                 valid_loss,\n",
    "                 learning_rate,\n",
    "                 max_steps,\n",
    "                 val_check_steps,\n",
    "                 batch_size,\n",
    "                 windows_batch_size,\n",
    "                 valid_batch_size,\n",
    "                 step_size=1,\n",
    "                 num_lr_decays=0,\n",
    "                 early_stop_patience_steps=-1,\n",
    "                 scaler_type='identity',\n",
    "                 futr_exog_list=None,\n",
    "                 hist_exog_list=None,\n",
    "                 stat_exog_list=None,\n",
    "                 num_workers_loader=0,\n",
    "                 drop_last_loader=False,\n",
    "                 random_seed=1,\n",
    "                 alias=None,\n",
    "                 **trainer_kwargs):\n",
    "        super(BaseWindows, self).__init__()\n",
    "\n",
    "        self.save_hyperparameters() # Allows instantiation from a checkpoint from class\n",
    "        self.random_seed = random_seed\n",
    "        pl.seed_everything(self.random_seed, workers=True)\n",
    "\n",
    "        # Padder to complete train windows, \n",
    "        # example y=[1,2,3,4,5] h=3 -> last y_output = [5,0,0]\n",
    "        self.h = h\n",
    "        self.input_size = input_size\n",
    "        self.padder = nn.ConstantPad1d(padding=(0, self.h), value=0)\n",
    "\n",
    "        # Loss\n",
    "        self.loss = loss\n",
    "        if valid_loss is None:\n",
    "            self.valid_loss = loss\n",
    "        else:\n",
    "            self.valid_loss = valid_loss\n",
    "        self.train_trajectories = []\n",
    "        self.valid_trajectories = []\n",
    "\n",
    "        # Valid batch_size\n",
    "        self.batch_size = batch_size\n",
    "        if valid_batch_size is None:\n",
    "            self.valid_batch_size = batch_size\n",
    "        else:\n",
    "            self.valid_batch_size = valid_batch_size\n",
    "        \n",
    "        # Optimization\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_steps = max_steps\n",
    "        self.num_lr_decays = num_lr_decays\n",
    "        self.lr_decay_steps = max(max_steps // self.num_lr_decays, 1) if self.num_lr_decays > 0 else 10e7\n",
    "        self.early_stop_patience_steps = early_stop_patience_steps\n",
    "        self.val_check_steps = val_check_steps\n",
    "        self.windows_batch_size = windows_batch_size\n",
    "        self.step_size = step_size\n",
    "\n",
    "        # Scaler\n",
    "        self.scaler = TemporalNorm(scaler_type=scaler_type, dim=1) # Time dimension is 1.\n",
    "\n",
    "        # Variables\n",
    "        self.futr_exog_list = futr_exog_list if futr_exog_list is not None else []\n",
    "        self.hist_exog_list = hist_exog_list if hist_exog_list is not None else []\n",
    "        self.stat_exog_list = stat_exog_list if stat_exog_list is not None else []\n",
    "\n",
    "        # Fit arguments\n",
    "        self.val_size = 0\n",
    "        self.test_size = 0\n",
    "\n",
    "        # Model state\n",
    "        self.decompose_forecast = False\n",
    "\n",
    "        ## Trainer arguments ##\n",
    "        # Max steps, validation steps and check_val_every_n_epoch\n",
    "        if 'max_epochs' in trainer_kwargs.keys():\n",
    "            warnings.warn('max_epochs will be deprecated, use max_steps instead.')\n",
    "        else:\n",
    "            trainer_kwargs = {**trainer_kwargs,\n",
    "                              **{'max_steps': max_steps}}\n",
    "\n",
    "        if 'max_epochs' in trainer_kwargs.keys():\n",
    "            warnings.warn('max_epochs will be deprecated, use max_steps instead.')\n",
    "\n",
    "        # Callbacks\n",
    "        if trainer_kwargs.get('callbacks', None) is None:\n",
    "            callbacks = [TQDMProgressBar()]\n",
    "            # Early stopping\n",
    "            if self.early_stop_patience_steps > 0:\n",
    "                callbacks += [EarlyStopping(monitor='ptl/val_loss',\n",
    "                                            patience=self.early_stop_patience_steps)]\n",
    "\n",
    "            trainer_kwargs['callbacks'] = callbacks\n",
    "\n",
    "        # Add GPU accelerator if available\n",
    "        if trainer_kwargs.get('accelerator', None) is None:\n",
    "            if torch.cuda.is_available():\n",
    "                trainer_kwargs['accelerator'] = \"gpu\"\n",
    "        if trainer_kwargs.get('devices', None) is None:\n",
    "            if torch.cuda.is_available():\n",
    "                trainer_kwargs['devices'] = -1\n",
    "\n",
    "        # Avoid saturating local memory, disabled fit model checkpoints\n",
    "        if trainer_kwargs.get('enable_checkpointing', None) is None:\n",
    "            trainer_kwargs['enable_checkpointing'] = False\n",
    "\n",
    "        self.trainer_kwargs = trainer_kwargs\n",
    "\n",
    "        # DataModule arguments\n",
    "        self.num_workers_loader = num_workers_loader\n",
    "        self.drop_last_loader = drop_last_loader\n",
    "        # used by on_validation_epoch_end hook\n",
    "        self.validation_step_outputs = []\n",
    "        self.alias = alias\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return type(self).__name__ if self.alias is None else self.alias\n",
    "\n",
    "    def on_fit_start(self):\n",
    "        torch.manual_seed(self.random_seed)\n",
    "        np.random.seed(self.random_seed)\n",
    "        random.seed(self.random_seed)\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        scheduler = {'scheduler': torch.optim.lr_scheduler.StepLR(optimizer=optimizer,\n",
    "                                                                  step_size=self.lr_decay_steps,\n",
    "                                                                  gamma=0.5),\n",
    "                     'frequency': 1,\n",
    "                     'interval': 'step'}\n",
    "        return {'optimizer': optimizer, 'lr_scheduler': scheduler}\n",
    "\n",
    "    def _create_windows(self, batch, step):\n",
    "        # Parse common data\n",
    "        window_size = self.input_size + self.h\n",
    "        temporal_cols = batch['temporal_cols']\n",
    "        temporal = batch['temporal']\n",
    "\n",
    "        if step == 'train':\n",
    "            if self.val_size + self.test_size > 0:\n",
    "                cutoff = -self.val_size - self.test_size\n",
    "                temporal = temporal[:, :, :cutoff]\n",
    "\n",
    "            temporal = self.padder(temporal)\n",
    "            windows = temporal.unfold(dimension=-1, \n",
    "                                      size=window_size, \n",
    "                                      step=self.step_size)\n",
    "\n",
    "            # [B, C, Ws, L+H] 0, 1, 2, 3\n",
    "            # -> [B * Ws, L+H, C] 0, 2, 3, 1\n",
    "            windows_per_serie = windows.shape[2]\n",
    "            windows = windows.permute(0, 2, 3, 1).contiguous()\n",
    "            windows = windows.reshape(-1, window_size, len(temporal_cols))\n",
    "\n",
    "            # Sample and Available conditions\n",
    "            available_idx = temporal_cols.get_loc('available_mask')\n",
    "            sample_condition = windows[:, -self.h:, available_idx]\n",
    "            sample_condition = torch.sum(sample_condition, axis=1)\n",
    "            available_condition = windows[:, :-self.h, available_idx]\n",
    "            available_condition = torch.sum(available_condition, axis=1)\n",
    "            final_condition = (sample_condition > 0) & (available_condition > 0)\n",
    "            windows = windows[final_condition]\n",
    "\n",
    "            # Parse Static data to match windows\n",
    "            # [B, S_in] -> [B, Ws, S_in] -> [B*Ws, S_in]\n",
    "            static = batch.get('static', None)\n",
    "            static_cols=batch.get('static_cols', None)\n",
    "            if static is not None:\n",
    "                static = torch.repeat_interleave(static, \n",
    "                                    repeats=windows_per_serie, dim=0)\n",
    "                static = static[final_condition]\n",
    "\n",
    "            # Protection of empty windows\n",
    "            if final_condition.sum() == 0:\n",
    "                raise Exception('No windows available for training')\n",
    "\n",
    "            # Sample windows\n",
    "            n_windows = len(windows)\n",
    "            if self.windows_batch_size is not None:\n",
    "                w_idxs = np.random.choice(n_windows, \n",
    "                                          size=self.windows_batch_size,\n",
    "                                          replace=(n_windows < self.windows_batch_size))\n",
    "                windows = windows[w_idxs]\n",
    "                \n",
    "                if static is not None:\n",
    "                    static = static[w_idxs]\n",
    "\n",
    "            # think about interaction available * sample mask\n",
    "            # [B, C, Ws, L+H]\n",
    "            windows_batch = dict(temporal=windows,\n",
    "                                 temporal_cols=temporal_cols,\n",
    "                                 static=static,\n",
    "                                 static_cols=static_cols)\n",
    "            return windows_batch\n",
    "\n",
    "        elif step in ['predict', 'val']:\n",
    "\n",
    "            if step == 'predict':\n",
    "                predict_step_size = self.predict_step_size\n",
    "                cutoff = - self.input_size - self.test_size\n",
    "                temporal = batch['temporal'][:, :, cutoff:]\n",
    "\n",
    "            elif step == 'val':\n",
    "                predict_step_size = self.step_size\n",
    "                cutoff = -self.input_size - self.val_size - self.test_size\n",
    "                if self.test_size > 0:\n",
    "                    temporal = batch['temporal'][:, :, cutoff:-self.test_size]\n",
    "                else:\n",
    "                    temporal = batch['temporal'][:, :, cutoff:]\n",
    "\n",
    "            if (step=='predict') and (self.test_size==0) and (len(self.futr_exog_list)==0):\n",
    "                temporal = self.padder(temporal)\n",
    "\n",
    "            windows = temporal.unfold(dimension=-1,\n",
    "                                      size=window_size,\n",
    "                                      step=predict_step_size)\n",
    "\n",
    "            # [batch, channels, windows, window_size] 0, 1, 2, 3\n",
    "            # -> [batch * windows, window_size, channels] 0, 2, 3, 1\n",
    "            windows_per_serie = windows.shape[2]\n",
    "            windows = windows.permute(0, 2, 3, 1).contiguous()\n",
    "            windows = windows.reshape(-1, window_size, len(temporal_cols))\n",
    "\n",
    "            static = batch.get('static', None)\n",
    "            static_cols=batch.get('static_cols', None)\n",
    "            if static is not None:\n",
    "                static = torch.repeat_interleave(static, \n",
    "                                    repeats=windows_per_serie, dim=0)\n",
    "\n",
    "            windows_batch = dict(temporal=windows,\n",
    "                                 temporal_cols=temporal_cols,\n",
    "                                 static=static,\n",
    "                                 static_cols=static_cols)\n",
    "            return windows_batch\n",
    "        else:\n",
    "            raise ValueError(f'Unknown step {step}')\n",
    "            \n",
    "    def _normalization(self, windows):\n",
    "        # windows are already filtered by train/validation/test\n",
    "        # from the `create_windows_method` nor leakage risk\n",
    "        temporal = windows['temporal']                  # B, L+H, C\n",
    "        temporal_cols = windows['temporal_cols'].copy() # B, L+H, C\n",
    "\n",
    "        # To avoid leakage uses only the lags\n",
    "        temporal_data_cols = temporal_cols.drop('available_mask').tolist()\n",
    "        temporal_data = temporal[:, :, temporal_cols.get_indexer(temporal_data_cols)]\n",
    "        temporal_mask = temporal[:, :, temporal_cols.get_loc('available_mask')].clone()\n",
    "        temporal_mask[:, -self.h:] = 0.0\n",
    "\n",
    "        # Normalize. self.scaler stores the shift and scale for inverse transform\n",
    "        temporal_mask = temporal_mask.unsqueeze(-1) # Add channel dimension for scaler.transform.\n",
    "        temporal_data = self.scaler.transform(x=temporal_data, mask=temporal_mask)\n",
    "\n",
    "        # Replace values in windows dict\n",
    "        temporal[:, :, temporal_cols.get_indexer(temporal_data_cols)] = temporal_data\n",
    "        windows['temporal'] = temporal\n",
    "\n",
    "        return windows\n",
    "\n",
    "    def _inv_normalization(self, y_hat, temporal_cols):\n",
    "        # Receives window predictions [B, H, output]\n",
    "        # Broadcasts outputs and inverts normalization\n",
    "\n",
    "        # Add C dimension\n",
    "        if y_hat.ndim == 2:\n",
    "            remove_dimension = True\n",
    "            y_hat = y_hat.unsqueeze(-1)\n",
    "        else:\n",
    "            remove_dimension = False\n",
    "\n",
    "        temporal_data_cols = temporal_cols.drop('available_mask')\n",
    "        y_scale = self.scaler.x_scale[:,:,temporal_data_cols.get_indexer(['y'])]\n",
    "        y_loc = self.scaler.x_shift[:,:,temporal_data_cols.get_indexer(['y'])]\n",
    "\n",
    "        y_scale = torch.repeat_interleave(y_scale, repeats=y_hat.shape[-1], dim=-1)\n",
    "        y_loc = torch.repeat_interleave(y_loc, repeats=y_hat.shape[-1], dim=-1)\n",
    "\n",
    "        y_hat = self.scaler.inverse_transform(z=y_hat, x_scale=y_scale, x_shift=y_loc)\n",
    "\n",
    "        if remove_dimension:\n",
    "            y_hat = y_hat.squeeze(-1)\n",
    "            y_loc = y_loc.squeeze(-1)\n",
    "            y_scale = y_scale.squeeze(-1)\n",
    "\n",
    "        return y_hat, y_loc, y_scale\n",
    "\n",
    "    def _parse_windows(self, batch, windows):\n",
    "        # Filter insample lags from outsample horizon\n",
    "        y_idx = batch['temporal_cols'].get_loc('y')\n",
    "        mask_idx = batch['temporal_cols'].get_loc('available_mask')\n",
    "        insample_y = windows['temporal'][:, :-self.h, y_idx]\n",
    "        insample_mask = windows['temporal'][:, :-self.h, mask_idx]\n",
    "        outsample_y = windows['temporal'][:, -self.h:, y_idx]\n",
    "        outsample_mask = windows['temporal'][:, -self.h:, mask_idx]\n",
    "\n",
    "        # Filter historic exogenous variables\n",
    "        if len(self.hist_exog_list):\n",
    "            hist_exog_idx = windows['temporal_cols'].get_indexer(self.hist_exog_list)\n",
    "            hist_exog = windows['temporal'][:, :-self.h, hist_exog_idx]\n",
    "        else:\n",
    "            hist_exog = None\n",
    "        \n",
    "        # Filter future exogenous variables\n",
    "        if len(self.futr_exog_list):\n",
    "            futr_exog_idx = windows['temporal_cols'].get_indexer(self.futr_exog_list)\n",
    "            futr_exog = windows['temporal'][:, :, futr_exog_idx]\n",
    "        else:\n",
    "            futr_exog = None\n",
    "        # Filter static variables\n",
    "        if len(self.stat_exog_list):\n",
    "            static_idx = windows['static_cols'].get_indexer(self.stat_exog_list)\n",
    "            stat_exog = windows['static'][:, static_idx]\n",
    "        else:\n",
    "            stat_exog = None\n",
    "\n",
    "        return insample_y, insample_mask, outsample_y, outsample_mask, \\\n",
    "               hist_exog, futr_exog, stat_exog\n",
    "\n",
    "    def training_step(self, batch, batch_idx):        \n",
    "        # Create and normalize windows [Ws, L+H, C]\n",
    "        windows = self._create_windows(batch, step='train')\n",
    "        original_outsample_y = torch.clone(windows['temporal'][:,-self.h:,0])\n",
    "        windows = self._normalization(windows=windows)\n",
    "\n",
    "        # Parse windows\n",
    "        insample_y, insample_mask, outsample_y, outsample_mask, \\\n",
    "               hist_exog, futr_exog, stat_exog = self._parse_windows(batch, windows)\n",
    "\n",
    "        windows_batch = dict(insample_y=insample_y, # [Ws, L]\n",
    "                             insample_mask=insample_mask, # [Ws, L]\n",
    "                             futr_exog=futr_exog, # [Ws, L+H]\n",
    "                             hist_exog=hist_exog, # [Ws, L]\n",
    "                             stat_exog=stat_exog) # [Ws, 1]\n",
    "\n",
    "        # Model Predictions\n",
    "        output = self(windows_batch)\n",
    "        if self.loss.is_distribution_output:\n",
    "            _, y_loc, y_scale = self._inv_normalization(y_hat=outsample_y,\n",
    "                                            temporal_cols=batch['temporal_cols'])\n",
    "            outsample_y = original_outsample_y\n",
    "            distr_args = self.loss.scale_decouple(output=output, loc=y_loc, scale=y_scale)\n",
    "            loss = self.loss(y=outsample_y, distr_args=distr_args, mask=outsample_mask)\n",
    "        else:\n",
    "            loss = self.loss(y=outsample_y, y_hat=output, mask=outsample_mask)\n",
    "\n",
    "        self.log('train_loss', loss, prog_bar=True, on_epoch=True)\n",
    "        self.train_trajectories.append((self.global_step, float(loss)))\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        if self.val_size == 0:\n",
    "            return np.nan\n",
    "        \n",
    "        # Create and normalize windows [Ws, L+H, C]\n",
    "        windows = self._create_windows(batch, step='val')\n",
    "        original_outsample_y = torch.clone(windows['temporal'][:,-self.h:,0])\n",
    "        windows = self._normalization(windows=windows)\n",
    "\n",
    "        # Parse windows\n",
    "        insample_y, insample_mask, outsample_y, outsample_mask, \\\n",
    "               hist_exog, futr_exog, stat_exog = self._parse_windows(batch, windows)\n",
    "\n",
    "        windows_batch = dict(insample_y=insample_y, # [Ws, L]\n",
    "                             insample_mask=insample_mask, # [Ws, L]\n",
    "                             futr_exog=futr_exog, # [Ws, L+H]\n",
    "                             hist_exog=hist_exog, # [Ws, L]\n",
    "                             stat_exog=stat_exog) # [Ws, 1]\n",
    "\n",
    "        # Model Predictions\n",
    "        output = self(windows_batch)\n",
    "        if self.loss.is_distribution_output:\n",
    "            _, y_loc, y_scale = self._inv_normalization(y_hat=outsample_y,\n",
    "                                            temporal_cols=batch['temporal_cols'])\n",
    "            outsample_y = original_outsample_y\n",
    "            distr_args = self.loss.scale_decouple(output=output, loc=y_loc, scale=y_scale)\n",
    "            _, sample_mean, quants  = self.loss.sample(distr_args=distr_args)\n",
    "\n",
    "            if str(type(self.valid_loss)) in\\\n",
    "                [\"<class 'neuralforecast.losses.pytorch.sCRPS'>\", \"<class 'neuralforecast.losses.pytorch.MQLoss'>\"]:\n",
    "                output = quants\n",
    "            elif str(type(self.valid_loss)) in [\"<class 'neuralforecast.losses.pytorch.relMSE'>\"]:\n",
    "                output = torch.unsqueeze(sample_mean, dim=-1) # [N,H,1] -> [N,H]\n",
    "\n",
    "        # Validation Loss evaluation\n",
    "        if self.valid_loss.is_distribution_output:\n",
    "            valid_loss = self.valid_loss(y=outsample_y, distr_args=distr_args, mask=outsample_mask)\n",
    "        else:\n",
    "            valid_loss = self.valid_loss(y=outsample_y, y_hat=output, mask=outsample_mask)\n",
    "\n",
    "        self.log('valid_loss', valid_loss, prog_bar=True, on_epoch=True)\n",
    "        self.validation_step_outputs.append(valid_loss)\n",
    "        return valid_loss\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        if self.val_size == 0:\n",
    "            return\n",
    "        avg_loss = torch.stack(self.validation_step_outputs).mean()\n",
    "        self.log(\"ptl/val_loss\", avg_loss)\n",
    "        self.valid_trajectories.append((self.global_step, float(avg_loss)))\n",
    "        self.validation_step_outputs.clear() # free memory (compute `avg_loss` per epoch) \n",
    "    \n",
    "    def predict_step(self, batch, batch_idx):        \n",
    "        # Create and normalize windows [Ws, L+H, C]\n",
    "        windows = self._create_windows(batch, step='predict')\n",
    "        windows = self._normalization(windows=windows)\n",
    "\n",
    "        # Parse windows\n",
    "        insample_y, insample_mask, _, _, \\\n",
    "               hist_exog, futr_exog, stat_exog = self._parse_windows(batch, windows)\n",
    "\n",
    "        windows_batch = dict(insample_y=insample_y, # [Ws, L]\n",
    "                             insample_mask=insample_mask, # [Ws, L]\n",
    "                             futr_exog=futr_exog, # [Ws, L+H]\n",
    "                             hist_exog=hist_exog, # [Ws, L]\n",
    "                             stat_exog=stat_exog) # [Ws, 1]\n",
    "\n",
    "        # Model Predictions\n",
    "        output = self(windows_batch)\n",
    "        if self.loss.is_distribution_output:\n",
    "            _, y_loc, y_scale = self._inv_normalization(y_hat=output[0],\n",
    "                                            temporal_cols=batch['temporal_cols'])\n",
    "            distr_args = self.loss.scale_decouple(output=output, loc=y_loc, scale=y_scale)\n",
    "            _, sample_mean, quants = self.loss.sample(distr_args=distr_args)\n",
    "            y_hat = torch.concat((sample_mean, quants), axis=2)\n",
    "\n",
    "            if self.loss.return_params:\n",
    "                distr_args = torch.stack(distr_args, dim=-1)\n",
    "                distr_args = torch.reshape(distr_args, (len(windows[\"temporal\"]), self.h, -1))\n",
    "                y_hat = torch.concat((y_hat, distr_args), axis=2)\n",
    "        else:\n",
    "            y_hat, _, _ = self._inv_normalization(y_hat=output,\n",
    "                                            temporal_cols=batch['temporal_cols'])\n",
    "        return y_hat\n",
    "    \n",
    "    def fit(self, dataset, val_size=0, test_size=0, random_seed=None):\n",
    "        \"\"\" Fit.\n",
    "\n",
    "        The `fit` method, optimizes the neural network's weights using the\n",
    "        initialization parameters (`learning_rate`, `windows_batch_size`, ...)\n",
    "        and the `loss` function as defined during the initialization. \n",
    "        Within `fit` we use a PyTorch Lightning `Trainer` that\n",
    "        inherits the initialization's `self.trainer_kwargs`, to customize\n",
    "        its inputs, see [PL's trainer arguments](https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.trainer.trainer.Trainer.html?highlight=trainer).\n",
    "\n",
    "        The method is designed to be compatible with SKLearn-like classes\n",
    "        and in particular to be compatible with the StatsForecast library.\n",
    "\n",
    "        By default the `model` is not saving training checkpoints to protect \n",
    "        disk memory, to get them change `enable_checkpointing=True` in `__init__`.\n",
    "\n",
    "        **Parameters:**<br>\n",
    "        `dataset`: NeuralForecast's `TimeSeriesDataset`, see [documentation](https://nixtla.github.io/neuralforecast/tsdataset.html).<br>\n",
    "        `val_size`: int, validation size for temporal cross-validation.<br>\n",
    "        `test_size`: int, test size for temporal cross-validation.<br>\n",
    "        \"\"\"\n",
    "        # Restart random seed\n",
    "        if random_seed is None:\n",
    "            random_seed = self.random_seed\n",
    "        torch.manual_seed(random_seed)\n",
    "        \n",
    "        self.val_size = val_size\n",
    "        self.test_size = test_size\n",
    "        datamodule = TimeSeriesDataModule(\n",
    "            dataset=dataset, \n",
    "            batch_size=self.batch_size,\n",
    "            valid_batch_size=self.valid_batch_size,\n",
    "            num_workers=self.num_workers_loader,\n",
    "            drop_last=self.drop_last_loader\n",
    "        )\n",
    "\n",
    "        ### Check validation every steps ###\n",
    "        steps_in_epoch = np.ceil(dataset.n_groups / self.batch_size)\n",
    "\n",
    "        # In v1.6.5 of PL, val_check_interval can be used for multiple validation steps\n",
    "        # within one epoch (steps_in_epoch > self.val_check_steps)\n",
    "        if steps_in_epoch > self.val_check_steps:\n",
    "            val_check_interval = self.val_check_steps / steps_in_epoch \n",
    "            check_val_every_n_epoch = 1\n",
    "        # Use check_val_every_n_epoch to check validation at end of some epochs,\n",
    "        # closest to self.val_check_steps.\n",
    "        else:\n",
    "            val_check_interval = None\n",
    "            check_val_every_n_epoch = int(np.round(self.val_check_steps / steps_in_epoch))\n",
    "\n",
    "        self.trainer_kwargs['val_check_interval'] = val_check_interval\n",
    "        self.trainer_kwargs['check_val_every_n_epoch'] = check_val_every_n_epoch\n",
    "\n",
    "        trainer = pl.Trainer(**self.trainer_kwargs)\n",
    "        trainer.fit(self, datamodule=datamodule)\n",
    "\n",
    "    def predict(self, dataset, test_size=None, step_size=1, random_seed=None, **data_module_kwargs):\n",
    "        \"\"\" Predict.\n",
    "\n",
    "        Neural network prediction with PL's `Trainer` execution of `predict_step`.\n",
    "\n",
    "        **Parameters:**<br>\n",
    "        `dataset`: NeuralForecast's `TimeSeriesDataset`, see [documentation](https://nixtla.github.io/neuralforecast/tsdataset.html).<br>\n",
    "        `test_size`: int=None, test size for temporal cross-validation.<br>\n",
    "        `step_size`: int=1, Step size between each window.<br>\n",
    "        `**data_module_kwargs`: PL's TimeSeriesDataModule args, see [documentation](https://pytorch-lightning.readthedocs.io/en/1.6.1/extensions/datamodules.html#using-a-datamodule).\n",
    "        \"\"\"\n",
    "        # Restart random seed\n",
    "        if random_seed is None:\n",
    "            random_seed = self.random_seed\n",
    "        torch.manual_seed(random_seed)\n",
    "\n",
    "        self.predict_step_size = step_size\n",
    "        self.decompose_forecast = False\n",
    "        datamodule = TimeSeriesDataModule(dataset=dataset,\n",
    "                                          valid_batch_size=self.valid_batch_size,\n",
    "                                          **data_module_kwargs)\n",
    "\n",
    "        # Protect when case of multiple gpu. PL does not support return preds with multiple gpu.\n",
    "        pred_trainer_kwargs = self.trainer_kwargs.copy()\n",
    "        if (pred_trainer_kwargs.get('accelerator', None) == \"gpu\") and (torch.cuda.device_count() > 1):\n",
    "            pred_trainer_kwargs['devices'] = [0]\n",
    "\n",
    "        trainer = pl.Trainer(**pred_trainer_kwargs)\n",
    "        fcsts = trainer.predict(self, datamodule=datamodule)        \n",
    "        fcsts = torch.vstack(fcsts).numpy().flatten()\n",
    "        fcsts = fcsts.reshape(-1, len(self.loss.output_names))\n",
    "        return fcsts\n",
    "\n",
    "    def decompose(self, dataset, step_size=1, random_seed=None, **data_module_kwargs):\n",
    "        \"\"\" Decompose Predictions.\n",
    "\n",
    "        Decompose the predictions through the network's layers.\n",
    "        Available methods are `ESRNN`, `NHITS`, `NBEATS`, and `NBEATSx`.\n",
    "\n",
    "        **Parameters:**<br>\n",
    "        `dataset`: NeuralForecast's `TimeSeriesDataset`, see [documentation here](https://nixtla.github.io/neuralforecast/tsdataset.html).<br>\n",
    "        `step_size`: int=1, step size between each window of temporal data.<br>\n",
    "        `**data_module_kwargs`: PL's TimeSeriesDataModule args, see [documentation](https://pytorch-lightning.readthedocs.io/en/1.6.1/extensions/datamodules.html#using-a-datamodule).\n",
    "        \"\"\"\n",
    "        # Restart random seed\n",
    "        if random_seed is None:\n",
    "            random_seed = self.random_seed\n",
    "        torch.manual_seed(random_seed)\n",
    "\n",
    "        self.predict_step_size = step_size\n",
    "        self.decompose_forecast = True\n",
    "        datamodule = TimeSeriesDataModule(dataset=dataset,\n",
    "                                          valid_batch_size=self.valid_batch_size,\n",
    "                                          **data_module_kwargs)\n",
    "        trainer = pl.Trainer(**self.trainer_kwargs)\n",
    "        fcsts = trainer.predict(self, datamodule=datamodule)\n",
    "        self.decompose_forecast = False # Default decomposition back to false\n",
    "        return torch.vstack(fcsts).numpy()\n",
    "\n",
    "    def forward(self, insample_y, insample_mask):\n",
    "        raise NotImplementedError('forward')\n",
    "\n",
    "    def set_test_size(self, test_size):\n",
    "        self.test_size = test_size\n",
    "\n",
    "    def save(self, path):\n",
    "        \"\"\" BaseWindows.save\n",
    "\n",
    "        Save the fitted model to disk.\n",
    "\n",
    "        **Parameters:**<br>\n",
    "        `path`: str, path to save the model.<br>\n",
    "        \"\"\"\n",
    "        self.trainer.save_checkpoint(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1712ea15",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(BaseWindows, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48063f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(BaseWindows.fit, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75529be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(BaseWindows.predict, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f8315d",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(BaseWindows.decompose, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fd48a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
