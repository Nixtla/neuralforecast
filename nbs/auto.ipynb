{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from hyperopt import hp\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from neuralforecast.experiments.utils import hyperopt_tunning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class AutoBaseModel(object):\n",
    "    def __init__(self, n_time_out):\n",
    "        super(AutoBaseModel, self).__init__()\n",
    "\n",
    "        self.n_time_out = n_time_out\n",
    "\n",
    "    def fit(self, Y_df, X_df, S_df, hyperopt_steps, loss_function_val, n_ts_val, results_dir,\n",
    "            save_trials=False, loss_functions_test=None, n_ts_test=0, return_test_forecast=False, verbose=False):\n",
    "\n",
    "        # Override space with data specific information\n",
    "        self.space['n_series'] = hp.choice('n_series', [ Y_df['unique_id'].nunique() ])\n",
    "        self.space['n_x'] = hp.choice('n_x', [ 0 if X_df is None else (X_df.shape[1]-2) ])\n",
    "        self.space['n_s'] = hp.choice('n_s', [ 0 if S_df is None else (S_df.shape[1]-1) ])\n",
    "        self.space['n_x_hidden'] = hp.choice('n_x_hidden', [ 0 if X_df is None else (X_df.shape[1]-2) ])\n",
    "        self.space['n_s_hidden'] = hp.choice('n_s_hidden', [ 0 if S_df is None else (S_df.shape[1]-1) ])\n",
    "        self.space['frequency'] = hp.choice('frequency', [ pd.infer_freq(Y_df['ds']) ])\n",
    "\n",
    "        self.model, self.trials = hyperopt_tunning(space=self.space,\n",
    "                                                   hyperopt_max_evals=hyperopt_steps,\n",
    "                                                   loss_function_val=loss_function_val,\n",
    "                                                   loss_functions_test=loss_functions_test,\n",
    "                                                   S_df=S_df, Y_df=Y_df, X_df=X_df, \n",
    "                                                   f_cols=[], ds_in_val=n_ts_val, \n",
    "                                                   ds_in_test=n_ts_test,\n",
    "                                                   return_forecasts=return_test_forecast,\n",
    "                                                   return_model=True,\n",
    "                                                   save_trials=save_trials,\n",
    "                                                   results_dir=results_dir,\n",
    "                                                   step_save_progress=5,\n",
    "                                                   verbose=verbose)\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def forecast(self, Y_df: pd.DataFrame, X_df: pd.DataFrame = None, S_df: pd.DataFrame = None, \n",
    "                 batch_size: int =1, trainer: pl.Trainer =None) -> pd.DataFrame:\n",
    "        \n",
    "        return self.model.forecast(Y_df=Y_df, X_df=X_df, S_df=S_df, batch_size=batch_size, trainer=trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoNHITS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class NHITS(AutoBaseModel):\n",
    "    def __init__(self, n_time_out, space=None):\n",
    "        super(NHITS, self).__init__(n_time_out)\n",
    "\n",
    "        if space is None:\n",
    "            space = nhits_space(n_time_out=n_time_out)\n",
    "        self.space = space\n",
    "\n",
    "\n",
    "def nhits_space(n_time_out: int, n_series: int = None, n_x: int = None, \n",
    "                n_s: int = None, \n",
    "                frequency: str = None) -> dict:\n",
    "    \"\"\"\n",
    "    Suggested hyperparameters search space for tuning. To be used with hyperopt library.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_time_out: int\n",
    "        Forecasting horizon.\n",
    "    n_series: int\n",
    "        Number of time-series.\n",
    "    n_x: int\n",
    "        Number of exogenous variables.\n",
    "    n_s: int\n",
    "        Number of static variables.\n",
    "    frequency: str\n",
    "        Frequency of time-seris.\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    space: Dict\n",
    "        Dictionary with search space for hyperopt library.\n",
    "    \"\"\"\n",
    "\n",
    "    space= {# Architecture parameters\n",
    "        'model':'nhits',\n",
    "        'mode': 'simple',\n",
    "        'n_time_in': hp.choice('n_time_in', [2*n_time_out, 3*n_time_out, 5*n_time_out]),\n",
    "        'n_time_out': hp.choice('n_time_out', [n_time_out]),\n",
    "        'n_x': hp.choice('n_x', [n_x]),\n",
    "        'n_s': hp.choice('n_s', [n_s]),\n",
    "        'shared_weights': hp.choice('shared_weights', [False]),\n",
    "        'activation': hp.choice('activation', ['ReLU']),\n",
    "        'initialization':  hp.choice('initialization', ['lecun_normal']),\n",
    "        'stack_types': hp.choice('stack_types', [ 3*['identity'] ]),\n",
    "        'constant_n_blocks': hp.choice('n_blocks', [ 1, 3 ]), # Constant n_blocks across stacks\n",
    "        'constant_n_layers': hp.choice('n_layers', [ 2, 3 ]), # Constant n_layers across stacks\n",
    "        'constant_n_mlp_units': hp.choice('n_mlp_units', [ 128, 256, 512, 1024 ]), # Constant n_mlp_units across stacks\n",
    "        'n_x_hidden': hp.choice('n_x_hidden', [n_x] ),\n",
    "        'n_s_hidden': hp.choice('n_s_hidden', [n_s] ),\n",
    "        'n_pool_kernel_size': hp.choice('n_pool_kernel_size', [ 3*[1], 3*[2], 3*[4], 3*[8], [8, 4, 1], [16, 8, 1] ]),\n",
    "        'n_freq_downsample': hp.choice('n_freq_downsample', [ [168, 24, 1], [24, 12, 1],\n",
    "                                                                [180, 60, 1], [60, 8, 1],\n",
    "                                                                [40, 20, 1] ]),\n",
    "        'pooling_mode': hp.choice('pooling_mode', [ 'max' ]),\n",
    "        'interpolation_mode': hp.choice('interpolation_mode', [ 'linear' ]),\n",
    "        # Regularization and optimization parameters\n",
    "        'batch_normalization': hp.choice('batch_normalization', [False]),\n",
    "        'dropout_prob_theta': hp.choice('dropout_prob_theta', [ 0 ]),\n",
    "        'learning_rate': hp.choice('learning_rate', [0.0001, 0.001, 0.005, 0.01]),\n",
    "        'lr_decay': hp.choice('lr_decay', [0.5] ),\n",
    "        'n_lr_decays': hp.choice('n_lr_decays', [3]), \n",
    "        'weight_decay': hp.choice('weight_decay', [0] ),\n",
    "        'max_epochs': hp.choice('max_epochs', [None]),\n",
    "        'max_steps': hp.choice('max_steps', [1_000, 3_000, 5_000]),\n",
    "        'early_stop_patience': hp.choice('early_stop_patience', [10]),\n",
    "        'eval_freq': hp.choice('eval_freq', [50]),\n",
    "        'loss_train': hp.choice('loss', ['MAE']),\n",
    "        'loss_hypar': hp.choice('loss_hypar', [0.5]),   \n",
    "        'loss_valid': hp.choice('loss_valid', ['MAE']),\n",
    "        # Data parameters\n",
    "        'normalizer_y': hp.choice('normalizer_y', [None]),\n",
    "        'normalizer_x': hp.choice('normalizer_x', [None]),\n",
    "        'complete_windows':  hp.choice('complete_windows', [True]),\n",
    "        'frequency': hp.choice('frequency', [frequency]),\n",
    "        'idx_to_sample_freq': hp.choice('idx_to_sample_freq', [1]),\n",
    "        'val_idx_to_sample_freq': hp.choice('val_idx_to_sample_freq', [1]),\n",
    "        'batch_size': hp.choice('batch_size', [1]),\n",
    "        'n_windows': hp.choice('n_windows', [32, 64, 128, 256, 512]),\n",
    "        'random_seed': hp.quniform('random_seed', 1, 20, 1)}\n",
    "\n",
    "    return space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoNBEATS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class NBEATS(AutoBaseModel):\n",
    "    def __init__(self, n_time_out, space=None):\n",
    "        super(NBEATS, self).__init__(n_time_out)\n",
    "\n",
    "        if space is None:\n",
    "            space = nbeats_space(n_time_out=n_time_out)\n",
    "        self.space = space\n",
    "\n",
    "def nbeats_space(n_time_out: int, n_series: int = None, \n",
    "                 n_x: int = None, n_s: int = None, \n",
    "                 frequency: str = None) -> dict:\n",
    "    \"\"\"\n",
    "    Suggested hyperparameters search space for tuning. To be used with hyperopt library.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_time_out: int\n",
    "        Forecasting horizon.\n",
    "    n_series: int\n",
    "        Number of time-series.\n",
    "    n_x: int\n",
    "        Number of exogenous variables.\n",
    "    n_s: int\n",
    "        Number of static variables.\n",
    "    frequency: str\n",
    "        Frequency of time-seris.\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    space: Dict\n",
    "        Dictionary with search space for hyperopt library.\n",
    "    \"\"\"\n",
    "\n",
    "    space= {# Architecture parameters\n",
    "        'model':'nbeats',\n",
    "        'mode': 'simple',\n",
    "        'n_time_in': hp.choice('n_time_in', [2*n_time_out, 3*n_time_out, 5*n_time_out]),\n",
    "        'n_time_out': hp.choice('n_time_out', [n_time_out]),\n",
    "        'n_x': hp.choice('n_x', [n_x]),\n",
    "        'n_s': hp.choice('n_s', [n_s]),\n",
    "        'shared_weights': hp.choice('shared_weights', [False]),\n",
    "        'activation': hp.choice('activation', ['ReLU']),\n",
    "        'initialization':  hp.choice('initialization', ['lecun_normal']),\n",
    "        'stack_types': hp.choice('stack_types', [ 3*['identity'] ]),\n",
    "        'constant_n_blocks': hp.choice('n_blocks', [ 1, 3 ]), # Constant n_blocks across stacks\n",
    "        'constant_n_layers': hp.choice('n_layers', [ 2, 3 ]), # Constant n_layers across stacks\n",
    "        'constant_n_mlp_units': hp.choice('n_mlp_units', [ 128, 256, 512, 1024 ]), # Constant n_mlp_units across stacks\n",
    "        'n_x_hidden': hp.choice('n_x_hidden', [n_x] ),\n",
    "        'n_s_hidden': hp.choice('n_s_hidden', [n_s] ),\n",
    "        # Regularization and optimization parameters\n",
    "        'batch_normalization': hp.choice('batch_normalization', [False]),\n",
    "        'dropout_prob_theta': hp.choice('dropout_prob_theta', [ 0 ]),\n",
    "        'learning_rate': hp.choice('learning_rate', [0.0001, 0.001, 0.005, 0.01]),\n",
    "        'lr_decay': hp.choice('lr_decay', [0.5] ),\n",
    "        'n_lr_decays': hp.choice('n_lr_decays', [3]), \n",
    "        'weight_decay': hp.choice('weight_decay', [0] ),\n",
    "        'max_epochs': hp.choice('max_epochs', [None]),\n",
    "        'max_steps': hp.choice('max_steps', [1_000, 3_000, 5_000]),\n",
    "        'early_stop_patience': hp.choice('early_stop_patience', [10]),\n",
    "        'eval_freq': hp.choice('eval_freq', [50]),\n",
    "        'loss_train': hp.choice('loss', ['MAE']),\n",
    "        'loss_hypar': hp.choice('loss_hypar', [0.5]),                \n",
    "        'loss_valid': hp.choice('loss_valid', ['MAE']),\n",
    "        # Data parameters\n",
    "        'normalizer_y': hp.choice('normalizer_y', [None]),\n",
    "        'normalizer_x': hp.choice('normalizer_x', [None]),\n",
    "        'complete_windows':  hp.choice('complete_windows', [True]),\n",
    "        'frequency': hp.choice('frequency', [frequency]),\n",
    "        'idx_to_sample_freq': hp.choice('idx_to_sample_freq', [1]),\n",
    "        'val_idx_to_sample_freq': hp.choice('val_idx_to_sample_freq', [1]),\n",
    "        'batch_size': hp.choice('batch_size', [1]),\n",
    "        'n_windows': hp.choice('n_windows', [32, 64, 128, 256, 512]),\n",
    "        'random_seed': hp.quniform('random_seed', 1, 20, 1)}\n",
    "\n",
    "    return space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class RNN(AutoBaseModel):\n",
    "    def __init__(self, n_time_out, space=None):\n",
    "        super(RNN, self).__init__(n_time_out)\n",
    "\n",
    "        if space is None:\n",
    "            space = rnn_space(n_time_out=n_time_out)\n",
    "        self.space = space\n",
    "\n",
    "def rnn_space(n_time_out: int, n_series: int = None, n_x: int = None, \n",
    "              n_s: int = None, \n",
    "              frequency: str = None) -> dict:\n",
    "    \"\"\"\n",
    "    Suggested hyperparameters search space for tuning. To be used with hyperopt library.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_time_out: int\n",
    "        Forecasting horizon.\n",
    "    n_series: int\n",
    "        Number of time-series.\n",
    "    n_x: int\n",
    "        Number of exogenous variables.\n",
    "    n_s: int\n",
    "        Number of static variables.\n",
    "    frequency: str\n",
    "        Frequency of time-seris.\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    space: Dict\n",
    "        Dictionary with search space for hyperopt library.\n",
    "    \"\"\"\n",
    "\n",
    "    space= {# Architecture parameters\n",
    "        'model':'rnn',\n",
    "        'mode': 'full',\n",
    "        'n_time_in': hp.choice('n_time_in', [1*n_time_out, 2*n_time_out, 3*n_time_out]),\n",
    "        'n_time_out': hp.choice('n_time_out', [n_time_out]),\n",
    "        'n_x': hp.choice('n_x', [n_x]),\n",
    "        'n_s': hp.choice('n_s', [n_s]),\n",
    "        'cell_type': hp.choice('cell_type', ['LSTM', 'GRU']),\n",
    "        'state_hsize': hp.choice('state_hsize', [10, 20, 50, 100]),\n",
    "        'dilations': hp.choice('dilations', [ [[1, 2]], [[1, 2, 4, 8]], [[1,2],[4,8]] ]),\n",
    "        'add_nl_layer': hp.choice('add_nl_layer', [ False ]),\n",
    "        'sample_freq': hp.choice('sample_freq', [n_time_out]),\n",
    "        # Regularization and optimization parameters\n",
    "        'learning_rate': hp.choice('learning_rate', [0.0001, 0.001, 0.005, 0.01, 0.05, 0.1]),\n",
    "        'lr_decay': hp.choice('lr_decay', [0.5] ),\n",
    "        'n_lr_decays': hp.choice('n_lr_decays', [3]), \n",
    "        'gradient_eps': hp.choice('gradient_eps', [1e-8]),\n",
    "        'gradient_clipping_threshold': hp.choice('gradient_clipping_threshold', [10]),\n",
    "        'weight_decay': hp.choice('weight_decay', [0]),\n",
    "        'noise_std': hp.choice('noise_std', [0.001]),\n",
    "        'max_epochs': hp.choice('max_epochs', [None]),\n",
    "        'max_steps': hp.choice('max_steps', [500, 1000]),\n",
    "        'early_stop_patience': hp.choice('early_stop_patience', [10]),\n",
    "        'eval_freq': hp.choice('eval_freq', [50]),\n",
    "        'loss_train': hp.choice('loss', ['MAE']),\n",
    "        'loss_hypar': hp.choice('loss_hypar', [0.5]),                \n",
    "        'loss_valid': hp.choice('loss_valid', ['MAE']),\n",
    "        # Data parameters\n",
    "        'normalizer_y': hp.choice('normalizer_y', [None]),\n",
    "        'normalizer_x': hp.choice('normalizer_x', [None]),\n",
    "        'complete_windows':  hp.choice('complete_windows', [True]),\n",
    "        'frequency': hp.choice('frequency', [frequency]),\n",
    "        'idx_to_sample_freq': hp.choice('idx_to_sample_freq', [1]),\n",
    "        'val_idx_to_sample_freq': hp.choice('val_idx_to_sample_freq', [1]),\n",
    "        'batch_size': hp.choice('batch_size', [16, 32, 64]),\n",
    "        'n_windows': hp.choice('n_windows', [None]),\n",
    "        'random_seed': hp.quniform('random_seed', 1, 20, 1)}\n",
    "\n",
    "    return space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoNF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def auto(config_dict: dict, \n",
    "         Y_df: pd.DataFrame, X_df: pd.DataFrame, S_df: pd.DataFrame,\n",
    "         loss_function_val: callable, loss_functions_test: dict, \n",
    "         forecast_horizon: int, ts_in_val: int, ts_in_test: int,\n",
    "         results_dir: str,\n",
    "         return_forecasts: bool = False,\n",
    "         test_auto: bool = False, verbose: bool = False):\n",
    "    \"\"\"\n",
    "    Auto hyperparameter tuning function.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    config_dict: Dict\n",
    "        Dictionary with configuration. Keys should be name of models.\n",
    "        For each model specify the hyperparameter space \n",
    "        (None will use default suggested space), hyperopt steps and timeout.\n",
    "    Y_df: pd.DataFrame\n",
    "        Target time series with columns ['unique_id', 'ds', 'y'].\n",
    "    X_df: pd.DataFrame\n",
    "        Exogenous time series with columns ['unique_id', 'ds', 'y'].\n",
    "    S_df: pd.DataFrame\n",
    "        Static exogenous variables with columns ['unique_id', 'ds']. \n",
    "        and static variables. \n",
    "    loss_function_val: function\n",
    "        Loss function used for validation.\n",
    "    loss_functions_test: Dictionary\n",
    "        Loss functions used for test evaluation. \n",
    "        (function name: string, function: fun)\n",
    "    forecast_horizon: int\n",
    "        Forecast horizon\n",
    "    ts_in_val: int\n",
    "        Number of timestamps in validation.\n",
    "    ts_in_test: int\n",
    "        Number of timestamps in test.\n",
    "    return_forecasts: bool\n",
    "        If true return forecast on test.\n",
    "    return_model: bool\n",
    "        If true return model.\n",
    "    test_auto: bool\n",
    "        If true, will only run one training step and hyperopt iteration for each model. \n",
    "        For testing purposes, to ensure your pipeline will finish running, without waiting.\n",
    "    verbose:\n",
    "        If true, will print summary of dataset, model and training.\n",
    "    \"\"\"\n",
    "\n",
    "    if test_auto:\n",
    "        print('WARNING: test_auto=True, MODELS WILL NOT BE TRAINED PROPERLY!')\n",
    "\n",
    "    # Data characteristics\n",
    "    n_series = Y_df['unique_id'].nunique()\n",
    "    n_x = len(X_df.columns)-2 if X_df is not None else 0\n",
    "    n_s = len(S_df.columns)-1 if S_df is not None else 0\n",
    "    frequency = pd.infer_freq(Y_df['ds'])\n",
    "\n",
    "    # Hyperopt\n",
    "    models = config_dict.keys()\n",
    "    output_dict = {}\n",
    "    best_model = None\n",
    "    best_loss = np.inf\n",
    "    for model_str in models:\n",
    "        print('MODEL:', model_str)\n",
    "        model_config = config_dict[model_str]\n",
    "\n",
    "        hyperopt_steps = model_config['hyperopt_steps'] if (test_auto==False) else 1\n",
    "\n",
    "        if model_config['space'] is None:\n",
    "            # If hyperparameter space is None, use predefined space for the model\n",
    "            # Available spaces, [N-BEATS, N-HiTS, RNN]\n",
    "            space = instantiate_space(model=model_str, n_time_out=forecast_horizon, \n",
    "                                      n_series=n_series, n_x=n_x, n_s=n_s, \n",
    "                                      frequency=frequency, test=test_auto)\n",
    "        else:\n",
    "            space = model_config['space']\n",
    "        \n",
    "        # Run automated hyperparameter optimization\n",
    "        results_dir_model = f'{results_dir}/{model_str}'\n",
    "        model, trials = nf.experiments.utils.hyperopt_tunning(space=space,\n",
    "                                                                hyperopt_max_evals=hyperopt_steps,\n",
    "                                                                loss_function_val=loss_function_val,\n",
    "                                                                loss_functions_test=loss_functions_test,\n",
    "                                                                S_df=S_df, Y_df=Y_df, X_df=X_df, \n",
    "                                                                f_cols=[], ds_in_val=ts_in_val, \n",
    "                                                                ds_in_test=ts_in_test,\n",
    "                                                                return_forecasts=return_forecasts,\n",
    "                                                                return_model=True,\n",
    "                                                                save_trials=False,\n",
    "                                                                results_dir=results_dir_model,\n",
    "                                                                step_save_progress=0,\n",
    "                                                                verbose=verbose)\n",
    "\n",
    "        model_output = {'best_mc': trials.best_trial['result']['mc'],\n",
    "                        'run_time': trials.best_trial['result']['run_time'],\n",
    "                        'best_val_loss': trials.best_trial['result']['loss']}\n",
    "\n",
    "        # Return model\n",
    "        model_output['model'] = model\n",
    "\n",
    "        # Return test losses\n",
    "        if ts_in_test > 0:\n",
    "            model_output['best_test_loss'] = trials.best_trial['result']['test_losses']\n",
    "\n",
    "        # Return test forecasts\n",
    "        if (return_forecasts) and (ts_in_test > 0):\n",
    "            model_output['y_hat'] = trials.best_trial['result']['forecasts_test']['test_y_hat']\n",
    "            model_output['y_true'] = trials.best_trial['result']['forecasts_test']['test_y_true']\n",
    "\n",
    "        # Improvement\n",
    "        optimization_times = [trials.trials[0]['result']['loss']]\n",
    "        optimization_losses = [trials.trials[0]['result']['run_time']]\n",
    "        for i in range(1, len(trials)):\n",
    "            loss = trials.trials[i]['result']['loss']\n",
    "            time = trials.trials[i]['result']['run_time']\n",
    "\n",
    "            if loss > np.min(optimization_losses):\n",
    "                loss = np.min(optimization_losses)\n",
    "            optimization_losses.append(loss)\n",
    "            optimization_times.append(np.sum(optimization_times)+time)\n",
    "\n",
    "        model_output['optimization_losses'] = optimization_losses\n",
    "        model_output['optimization_times'] = optimization_times\n",
    "    \n",
    "        # Append to dict\n",
    "        output_dict[model_str] = model_output\n",
    "\n",
    "        if trials.best_trial['result']['loss'] < best_loss:\n",
    "            best_model = model\n",
    "            best_loss = trials.best_trial['result']['loss']\n",
    "\n",
    "    return best_model, output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def instantiate_space(model, n_time_out, n_series, n_x, n_s, frequency, test):\n",
    "    assert model in ['nbeats', 'nhits', 'rnn'], f'Invalid model {model}'\n",
    "    \n",
    "    if model == 'nbeats':\n",
    "        space = nbeats_space(n_time_out=n_time_out, n_series=n_series, \n",
    "                             n_x=n_x, n_s=n_s, frequency=frequency)\n",
    "    elif model == 'nhits':\n",
    "        space = nhits_space(n_time_out=n_time_out, n_series=n_series, \n",
    "                            n_x=n_x, n_s=n_s, frequency=frequency)\n",
    "    elif model == 'rnn':\n",
    "        space = rnn_space(n_time_out=n_time_out, n_series=n_series,\n",
    "                          n_x=n_x, n_s=n_s, frequency=frequency)\n",
    "\n",
    "    if test:\n",
    "        space['max_steps'] = hp.choice('max_steps', [1])\n",
    "        space['max_epochs'] = hp.choice('max_epochs', [None])\n",
    "    \n",
    "    return space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The forecasting task we selected is to predict the number of patients with influenza-like illnesses from the [US CDC dataset](https://gis.cdc.gov/grasp/fluview/fluportaldashboard.html), the dataset contains 7 target variables, and has 966 weeks of history.\n",
    "\n",
    "We will be creating point forecasts with N-BEATS, N-HiTS and RNN models. The predictive features will be the autoregressive features. More information on the dataset can be found in the [N-HiTS paper](https://arxiv.org/abs/2201.12886).\n",
    "\n",
    "Table of Contents\n",
    "1.   [Installing NeuralForecast Library](#cell-1)\n",
    "2.   [Data Loading and Processing](#cell-2)\n",
    "3.   [Define Hyperparameter Space](#cell-3)\n",
    "4.   [Hyperparameter Tuning](#cell-4)\n",
    "5.   [Evaluate Results](#cell-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"cell-1\"></a>\n",
    "### 1. Installing Neuralforecast library\n",
    "\n",
    "You can install the released version of NeuralForecast from the Python package index with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%capture\n",
    "#!pip install neuralforecast\n",
    "#!pip install matplotlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import neuralforecast as nf\n",
    "from neuralforecast.data.datasets.long_horizon import LongHorizon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"cell-2\"></a>\n",
    "### 2. Data Loading and Processing\n",
    "\n",
    "For this example we keep 10% of the observations as validation and use the latest 20% of the observations as the test set. To do so we use the sample_mask and declare the windows that will be used to train, and validate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_df, _, _ = LongHorizon.load(directory='./', group='ILI')\n",
    "Y_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_series = len(Y_df.unique_id.unique())\n",
    "n_time = len(Y_df.ds.unique()) # dataset is balanced\n",
    "\n",
    "n_ts_test = 193\n",
    "n_ts_val = 97\n",
    "\n",
    "print('n_time', n_time)\n",
    "print('n_series', n_series)\n",
    "print('n_ts_test', n_ts_test)\n",
    "print('n_ts_val', n_ts_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_id \\in ['% WEIGHTED ILI', '%UNWEIGHTED ILI', 'AGE 0-4', \n",
    "#                'AGE 5-24', 'ILITOTAL', 'NUM. OF PROVIDERS', 'OT']\n",
    "y_plot = Y_df[Y_df.unique_id=='% WEIGHTED ILI'].y.values\n",
    "x_plot = pd.to_datetime(Y_df[Y_df.unique_id=='% WEIGHTED ILI'].ds).values\n",
    "\n",
    "plt.plot(x_plot, y_plot)\n",
    "plt.axvline(x_plot[n_time-n_ts_val-n_ts_test], color='black', linestyle='-.')\n",
    "plt.axvline(x_plot[n_time-n_ts_test], color='black', linestyle='-.')\n",
    "plt.ylabel('Weighted ILI [ratio]')\n",
    "plt.xlabel('Date')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.0 auto.NHITS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_nhits = NHITS(n_time_out=24)\n",
    "auto_nhits.space['max_steps'] = hp.choice('max_steps', [1]) # Override max_steps for faster example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_nhits.fit(Y_df=Y_df, X_df=None, S_df=None, hyperopt_steps=2,\n",
    "               n_ts_val=n_ts_val,\n",
    "               n_ts_test=n_ts_test,\n",
    "               results_dir='./results/autonhits',\n",
    "               save_trials=True,\n",
    "               loss_function_val=nf.losses.numpy.mae,\n",
    "               loss_functions_test={'mae':nf.losses.numpy.mae,\n",
    "                                    'mse':nf.losses.numpy.mse},\n",
    "               return_test_forecast=True,\n",
    "               verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts = auto_nhits.forecast(Y_df=Y_df)\n",
    "forecasts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 auto.NBEATS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_nbeats = NBEATS(n_time_out=24)\n",
    "auto_nbeats.space['max_steps'] = hp.choice('max_steps', [1]) # Override max_steps for faster example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_nbeats.fit(Y_df=Y_df, X_df=None, S_df=None, hyperopt_steps=2,\n",
    "                n_ts_val=n_ts_val,\n",
    "                n_ts_test=n_ts_test,\n",
    "                results_dir='./results/autonbeats',\n",
    "                save_trials=True,\n",
    "                loss_function_val=nf.losses.numpy.mae,\n",
    "                loss_functions_test={'mae':nf.losses.numpy.mae,\n",
    "                                     'mse':nf.losses.numpy.mse},\n",
    "                return_test_forecast=True,\n",
    "                verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts = auto_nbeats.forecast(Y_df=Y_df)\n",
    "forecasts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 auto.RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_rnn = RNN(n_time_out=24)\n",
    "auto_rnn.space['max_steps'] = hp.choice('max_steps', [1]) # Override max_steps for faster example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_rnn.fit(Y_df=Y_df, X_df=None, S_df=None, hyperopt_steps=2,\n",
    "             n_ts_val=n_ts_val,\n",
    "             n_ts_test=n_ts_test,\n",
    "             results_dir='./results/autornn',\n",
    "             save_trials=True,\n",
    "             loss_function_val=nf.losses.numpy.mae,\n",
    "             loss_functions_test={'mae':nf.losses.numpy.mae,\n",
    "                                  'mse':nf.losses.numpy.mse},\n",
    "             return_test_forecast=True,\n",
    "             verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts = auto_rnn.forecast(Y_df=Y_df)\n",
    "forecasts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"cell-3\"></a>\n",
    "### 3. Define Hyperparameter Space\n",
    "\n",
    "A temporal train-evaluation split procedure allows us to estimate the model’s generalization performance on future data unseen by the model. We use the train set to optimize the model parameters, and the validation  and test sets to evaluate the accuracy of the model’s predictions.\n",
    "\n",
    "In this case we set the space to `None`, that implicitly uses the predefined model space, but the space can be specified as a dictionary following the conventions of the [Hyperopt package](https://github.com/hyperopt/hyperopt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dict = {'nbeats':\n",
    "                       {'space': None, # Use default\n",
    "                        'hyperopt_steps': 5,\n",
    "                        'timeout': 60*1\n",
    "                       },\n",
    "               'nhits':\n",
    "                       {'space': None,  # Use default\n",
    "                        'hyperopt_steps': 5,\n",
    "                        'timeout': 60*1\n",
    "                       },\n",
    "                'rnn':\n",
    "                       {'space': None,  # Use default\n",
    "                        'hyperopt_steps': 5,\n",
    "                        'timeout': 60*1\n",
    "                       }\n",
    "              }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"cell-4\"></a>\n",
    "### 4. Hyperparameter Tuning\n",
    "\n",
    "A temporal train-validation-test (676,97,193) split procedure allows us to estimate the model’s generalization performance on future data unseen by the model. We use the train set to optimize the model parameters, and the validation  and test sets to evaluate the accuracy of the model’s predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_horizon = 24\n",
    "best_model, results = auto(config_dict=config_dict,\n",
    "                           Y_df=Y_df, X_df=None, S_df=None,\n",
    "                           loss_function_val=nf.losses.numpy.mae, \n",
    "                           loss_functions_test={'mae':nf.losses.numpy.mae,\n",
    "                                                'mse':nf.losses.numpy.mse},\n",
    "                           forecast_horizon=forecast_horizon, ts_in_val=n_ts_val, ts_in_test=n_ts_test,\n",
    "                           results_dir='./results/auto',\n",
    "                           return_forecasts=True,\n",
    "                           test_auto=True,\n",
    "                           verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time   = results['nbeats']['optimization_times']\n",
    "losses = results['nbeats']['optimization_losses']\n",
    "plt.plot(time, losses)\n",
    "plt.xlabel('segs')\n",
    "plt.ylabel('val loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"cell-5\"></a>\n",
    "### 5. Evaluate Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we wrangle the numpy predictions to evaluate and plot the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_nhits  = results['nhits']['y_hat']#.reshape(n_series, forecast_horizon, ts_in_test)\n",
    "y_hat_nbeats = results['nbeats']['y_hat']#.reshape(n_series, forecast_horizon, ts_in_test)\n",
    "y_hat_rnn    = results['rnn']['y_hat']#.reshape(n_series, forecast_horizon, ts_in_test)\n",
    "y_true       = results['nbeats']['y_true']#.reshape(forecast_horizon,n_series, -1)\n",
    "\n",
    "print('\\n Original Shapes')\n",
    "print('1. y_hat_nhits.shape', y_hat_nhits.shape)\n",
    "print('1. y_hat_nbeats.shape', y_hat_nbeats.shape)\n",
    "print('1. y_hat_rnn.shape', y_hat_rnn.shape)\n",
    "print('1. y_true.shape', y_true.shape)\n",
    "\n",
    "y_hat_nbeats = results['nbeats']['y_hat'].reshape((n_series,\n",
    "                                                   170, forecast_horizon))\n",
    "y_hat_nhits = results['nhits']['y_hat'].reshape((n_series,\n",
    "                                                 170, forecast_horizon))\n",
    "y_true = results['nbeats']['y_true'].reshape((n_series,\n",
    "                                              170, forecast_horizon))\n",
    "\n",
    "print('\\n Wrangled Shapes')\n",
    "print('2. y_hat_nhits.shape', y_hat_nhits.shape)\n",
    "print('2. y_hat_nbeats.shape', y_hat_nbeats.shape)\n",
    "print('2. y_hat_rnn.shape', y_hat_rnn.shape)\n",
    "print('2. y_true.shape', y_true.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_idx = 0\n",
    "u_idx = 0\n",
    "\n",
    "plt.plot(y_true[u_idx,w_idx,:], label='True Signal')\n",
    "plt.plot(y_hat_nbeats[u_idx,w_idx,:], label='N-BEATS')\n",
    "plt.plot(y_hat_nhits[u_idx,w_idx,:], label='N-HiTS')\n",
    "#plt.plot(y_true[:,0,2], label='True')\n",
    "#plt.plot(best_nbeats[::24,:].flatten(), label='N-BEATS')\n",
    "#plt.plot(best_rnn[::24,:].flatten(), label='RNN')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Y_df.unique_id.unique()', Y_df.unique_id.unique())\n",
    "ver = Y_df[Y_df.unique_id=='% WEIGHTED ILI']\n",
    "\n",
    "plt.plot(ver.y[n_time-193:n_time-193+24])\n",
    "plt.ylabel('% WEIGHTED ILI')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"cell-5\"></a>\n",
    "### 5. Evaluate Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
