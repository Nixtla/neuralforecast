{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from hyperopt import hp\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from neuralforecast.experiments.utils import hyperopt_tunning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class AutoBaseModel(object):\n",
    "    def __init__(self, horizon):\n",
    "        super(AutoBaseModel, self).__init__()\n",
    "\n",
    "        self.horizon = horizon\n",
    "\n",
    "    def fit(self, Y_df, X_df, S_df, hyperopt_steps, loss_function_val, n_ts_val, results_dir,\n",
    "            save_trials=False, loss_functions_test=None, n_ts_test=0, return_test_forecast=False, verbose=False):\n",
    "\n",
    "        # The suggested spaces are partial, here we complete them with data specific information\n",
    "        self.space['n_series']   = hp.choice('n_series', [ Y_df['unique_id'].nunique() ])\n",
    "        self.space['n_x']        = hp.choice('n_x', [ 0 if X_df is None else (X_df.shape[1]-2) ])\n",
    "        self.space['n_s']        = hp.choice('n_s', [ 0 if S_df is None else (S_df.shape[1]-1) ])\n",
    "        self.space['n_x_hidden'] = hp.choice('n_x_hidden', [ 0 if X_df is None else (X_df.shape[1]-2) ])\n",
    "        self.space['n_s_hidden'] = hp.choice('n_s_hidden', [ 0 if S_df is None else (S_df.shape[1]-1) ])\n",
    "\n",
    "        # Infers freq with first time series\n",
    "        freq = pd.infer_freq(Y_df[Y_df['unique_id']==Y_df.unique_id.unique()[0]]['ds']) \n",
    "        self.space['frequency']  = hp.choice('frequency', [ freq ])\n",
    "\n",
    "        self.model, self.trials = hyperopt_tunning(space=self.space,\n",
    "                                                   hyperopt_max_evals=hyperopt_steps,\n",
    "                                                   loss_function_val=loss_function_val,\n",
    "                                                   loss_functions_test=loss_functions_test,\n",
    "                                                   S_df=S_df, Y_df=Y_df, X_df=X_df, \n",
    "                                                   f_cols=[], ds_in_val=n_ts_val, \n",
    "                                                   ds_in_test=n_ts_test,\n",
    "                                                   return_forecasts=return_test_forecast,\n",
    "                                                   return_model=True,\n",
    "                                                   save_trials=save_trials,\n",
    "                                                   results_dir=results_dir,\n",
    "                                                   step_save_progress=5,\n",
    "                                                   verbose=verbose)\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def forecast(self, Y_df: pd.DataFrame, X_df: pd.DataFrame = None, S_df: pd.DataFrame = None, \n",
    "                 batch_size: int =1, trainer: pl.Trainer =None) -> pd.DataFrame:\n",
    "        \n",
    "        return self.model.forecast(Y_df=Y_df, X_df=X_df, S_df=S_df, batch_size=batch_size, trainer=trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoNHITS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class NHITS(AutoBaseModel):\n",
    "    def __init__(self, horizon, space=None):\n",
    "        super(NHITS, self).__init__(horizon)\n",
    "\n",
    "        if space is None:\n",
    "            space = nhits_space(horizon=horizon)\n",
    "        self.space = space\n",
    "\n",
    "\n",
    "def nhits_space(horizon: int) -> dict:\n",
    "    \"\"\"\n",
    "    Suggested hyperparameters search space for tuning. To be used with hyperopt library.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    horizon: int\n",
    "        Forecasting horizon.\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    space: Dict\n",
    "        Dictionary with search space for hyperopt library.\n",
    "    \"\"\"\n",
    "\n",
    "    space= {# Architecture parameters\n",
    "            'model':'nhits',\n",
    "            'mode': 'simple',\n",
    "            'n_time_in': hp.choice('n_time_in', [2*horizon, 3*horizon, 5*horizon]),\n",
    "            'n_time_out': hp.choice('n_time_out', [horizon]),\n",
    "            'shared_weights': hp.choice('shared_weights', [False]),\n",
    "            'activation': hp.choice('activation', ['ReLU']),\n",
    "            'initialization':  hp.choice('initialization', ['lecun_normal']),\n",
    "            'stack_types': hp.choice('stack_types', [ 3*['identity'] ]),\n",
    "            'constant_n_blocks': hp.choice('n_blocks', [ 1, 3 ]), # Constant n_blocks across stacks\n",
    "            'constant_n_layers': hp.choice('n_layers', [ 2, 3 ]), # Constant n_layers across stacks\n",
    "            'constant_n_mlp_units': hp.choice('n_mlp_units', [ 128, 256, 512, 1024 ]), # Constant n_mlp_units across stacks\n",
    "            'n_pool_kernel_size': hp.choice('n_pool_kernel_size', [ 3*[1], 3*[2], 3*[4], 3*[8], [8, 4, 1], [16, 8, 1] ]),\n",
    "            'n_freq_downsample': hp.choice('n_freq_downsample', [ [168, 24, 1], [24, 12, 1],\n",
    "                                                                     [180, 60, 1], [60, 8, 1],\n",
    "                                                                     [40, 20, 1] ]),\n",
    "            'pooling_mode': hp.choice('pooling_mode', [ 'max' ]),\n",
    "            'interpolation_mode': hp.choice('interpolation_mode', [ 'linear' ]),\n",
    "            # Regularization and optimization parameters\n",
    "            'batch_normalization': hp.choice('batch_normalization', [False]),\n",
    "            'dropout_prob_theta': hp.choice('dropout_prob_theta', [ 0 ]),\n",
    "            'learning_rate': hp.choice('learning_rate', [0.0001, 0.001, 0.005, 0.01]),\n",
    "            'lr_decay': hp.choice('lr_decay', [0.5] ),\n",
    "            'n_lr_decays': hp.choice('n_lr_decays', [3]), \n",
    "            'weight_decay': hp.choice('weight_decay', [0] ),\n",
    "            'max_epochs': hp.choice('max_epochs', [None]),\n",
    "            'max_steps': hp.choice('max_steps', [1_000, 3_000, 5_000]),\n",
    "            'early_stop_patience': hp.choice('early_stop_patience', [10]),\n",
    "            'eval_freq': hp.choice('eval_freq', [50]),\n",
    "            'loss_train': hp.choice('loss', ['MAE']),\n",
    "            'loss_hypar': hp.choice('loss_hypar', [0.5]),   \n",
    "            'loss_valid': hp.choice('loss_valid', ['MAE']),\n",
    "            # Data parameters\n",
    "            'normalizer_y': hp.choice('normalizer_y', [None]),\n",
    "            'normalizer_x': hp.choice('normalizer_x', [None]),\n",
    "            'complete_windows':  hp.choice('complete_windows', [True]),\n",
    "            'idx_to_sample_freq': hp.choice('idx_to_sample_freq', [1]),\n",
    "            'val_idx_to_sample_freq': hp.choice('val_idx_to_sample_freq', [1]),\n",
    "            'batch_size': hp.choice('batch_size', [1]),\n",
    "            'n_windows': hp.choice('n_windows', [32, 64, 128, 256, 512]),\n",
    "            'random_seed': hp.quniform('random_seed', 1, 20, 1)}\n",
    "\n",
    "    return space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoNBEATS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class NBEATS(AutoBaseModel):\n",
    "    def __init__(self, horizon, space=None):\n",
    "        super(NBEATS, self).__init__(horizon)\n",
    "\n",
    "        if space is None:\n",
    "            space = nbeats_space(horizon=horizon)\n",
    "        self.space = space\n",
    "\n",
    "def nbeats_space(horizon: int) -> dict:\n",
    "    \"\"\"\n",
    "    Suggested hyperparameters search space for tuning. To be used with hyperopt library.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    horizon: int\n",
    "        Forecasting horizon.\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    space: Dict\n",
    "        Dictionary with search space for hyperopt library.\n",
    "    \"\"\"\n",
    "\n",
    "    space= {# Architecture parameters\n",
    "            'model':'nbeats',\n",
    "            'mode': 'simple',\n",
    "            'n_time_in': hp.choice('n_time_in', [2*horizon, 3*horizon, 5*horizon]),\n",
    "            'n_time_out': hp.choice('n_time_out', [horizon]),\n",
    "            'shared_weights': hp.choice('shared_weights', [False]),\n",
    "            'activation': hp.choice('activation', ['ReLU']),\n",
    "            'initialization':  hp.choice('initialization', ['lecun_normal']),\n",
    "            'stack_types': hp.choice('stack_types', [ 3*['identity'] ]),\n",
    "            'constant_n_blocks': hp.choice('n_blocks', [ 1, 3 ]), # Constant n_blocks across stacks\n",
    "            'constant_n_layers': hp.choice('n_layers', [ 2, 3 ]), # Constant n_layers across stacks\n",
    "            'constant_n_mlp_units': hp.choice('n_mlp_units', [ 128, 256, 512, 1024 ]), # Constant n_mlp_units across stacks\n",
    "            # Regularization and optimization parameters\n",
    "            'batch_normalization': hp.choice('batch_normalization', [False]),\n",
    "            'dropout_prob_theta': hp.choice('dropout_prob_theta', [ 0 ]),\n",
    "            'learning_rate': hp.choice('learning_rate', [0.0001, 0.001, 0.005, 0.01]),\n",
    "            'lr_decay': hp.choice('lr_decay', [0.5] ),\n",
    "            'n_lr_decays': hp.choice('n_lr_decays', [3]), \n",
    "            'weight_decay': hp.choice('weight_decay', [0] ),\n",
    "            'max_epochs': hp.choice('max_epochs', [None]),\n",
    "            'max_steps': hp.choice('max_steps', [1_000, 3_000, 5_000]),\n",
    "            'early_stop_patience': hp.choice('early_stop_patience', [10]),\n",
    "            'eval_freq': hp.choice('eval_freq', [50]),\n",
    "            'loss_train': hp.choice('loss', ['MAE']),\n",
    "            'loss_hypar': hp.choice('loss_hypar', [0.5]),                \n",
    "            'loss_valid': hp.choice('loss_valid', ['MAE']),\n",
    "            # Data parameters\n",
    "            'normalizer_y': hp.choice('normalizer_y', [None]),\n",
    "            'normalizer_x': hp.choice('normalizer_x', [None]),\n",
    "            'complete_windows':  hp.choice('complete_windows', [True]),\n",
    "            'idx_to_sample_freq': hp.choice('idx_to_sample_freq', [1]),\n",
    "            'val_idx_to_sample_freq': hp.choice('val_idx_to_sample_freq', [1]),\n",
    "            'batch_size': hp.choice('batch_size', [1]),\n",
    "            'n_windows': hp.choice('n_windows', [32, 64, 128, 256, 512]),\n",
    "            'random_seed': hp.quniform('random_seed', 1, 20, 1)}\n",
    "\n",
    "    return space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class RNN(AutoBaseModel):\n",
    "    def __init__(self, horizon, space=None):\n",
    "        super(RNN, self).__init__(horizon)\n",
    "\n",
    "        if space is None:\n",
    "            space = rnn_space(horizon=horizon)\n",
    "        self.space = space\n",
    "\n",
    "def rnn_space(horizon: int) -> dict:\n",
    "    \"\"\"\n",
    "    Suggested hyperparameters search space for tuning. To be used with hyperopt library.\n",
    "    This space is not complete for training, will be completed automatically within\n",
    "    the fit method of the AutoBaseModels.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        horizon: int\n",
    "            Forecasting horizon\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        space: Dict\n",
    "            Dictionary with search space for hyperopt library.\n",
    "    \"\"\"\n",
    "\n",
    "    space= {# Architecture parameters\n",
    "            'model':'rnn',\n",
    "            'mode': 'full',\n",
    "            'n_time_in': hp.choice('n_time_in', [1*horizon, 2*horizon, 3*horizon]),\n",
    "            'n_time_out': hp.choice('n_time_out', [horizon]),\n",
    "            'cell_type': hp.choice('cell_type', ['LSTM', 'GRU']),\n",
    "            'state_hsize': hp.choice('state_hsize', [10, 20, 50, 100]),\n",
    "            'dilations': hp.choice('dilations', [ [[1, 2]], [[1, 2, 4, 8]], [[1,2],[4,8]] ]),\n",
    "            'add_nl_layer': hp.choice('add_nl_layer', [ False ]),\n",
    "            'sample_freq': hp.choice('sample_freq', [1]),\n",
    "            # Regularization and optimization parameters\n",
    "            'learning_rate': hp.choice('learning_rate', [0.0001, 0.001, 0.005, 0.01, 0.05, 0.1]),\n",
    "            'lr_decay': hp.choice('lr_decay', [0.5] ),\n",
    "            'n_lr_decays': hp.choice('n_lr_decays', [3]), \n",
    "            'gradient_eps': hp.choice('gradient_eps', [1e-8]),\n",
    "            'gradient_clipping_threshold': hp.choice('gradient_clipping_threshold', [10]),\n",
    "            'weight_decay': hp.choice('weight_decay', [0]),\n",
    "            'noise_std': hp.choice('noise_std', [0.001]),\n",
    "            'max_epochs': hp.choice('max_epochs', [None]),\n",
    "            'max_steps': hp.choice('max_steps', [500, 1000]),\n",
    "            'early_stop_patience': hp.choice('early_stop_patience', [10]),\n",
    "            'eval_freq': hp.choice('eval_freq', [50]),\n",
    "            'loss_train': hp.choice('loss', ['MAE']),\n",
    "            'loss_hypar': hp.choice('loss_hypar', [0.5]),                \n",
    "            'loss_valid': hp.choice('loss_valid', ['MAE']),\n",
    "            # Data parameters\n",
    "            'normalizer_y': hp.choice('normalizer_y', [None]),\n",
    "            'normalizer_x': hp.choice('normalizer_x', [None]),\n",
    "            'complete_windows':  hp.choice('complete_windows', [True]),\n",
    "            'idx_to_sample_freq': hp.choice('idx_to_sample_freq', [1]),\n",
    "            'val_idx_to_sample_freq': hp.choice('val_idx_to_sample_freq', [1]),\n",
    "            'batch_size': hp.choice('batch_size', [16, 32, 64]),\n",
    "            'n_windows': hp.choice('n_windows', [None]),\n",
    "            'random_seed': hp.quniform('random_seed', 1, 20, 1)}\n",
    "\n",
    "    return space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoNF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "MODEL_DICT = {'nbeats': NBEATS,\n",
    "              'nhits': NHITS,\n",
    "              'rnn': RNN}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class AutoNF(object):\n",
    "    def __init__(self, models, horizon):\n",
    "        super(AutoNF, self).__init__()\n",
    "        if isinstance(models, list):\n",
    "            self.config_dict = {model: dict(space=None) for model in models}\n",
    "        else:\n",
    "            self.config_dict = models\n",
    "        self.horizon = horizon\n",
    "\n",
    "    \"\"\"\n",
    "    The AutoNF class is an automated machine learning class that simultaneously explores hyperparameters \n",
    "    and optimizes the supported models.\n",
    "\n",
    "    AutoNF selects from a curated set of well-performing neural forecasting models {N-BEATSx, N-HiTS, RNN} by \n",
    "    tunning their hyperparameters with a shared optimization toolkit, using rolling window cross-validation.\n",
    "    The method helps to improve the comparability across model baselines and make the models \n",
    "    available for non-Machine Learning experts.\n",
    "\n",
    "    The AutoNF class inherits the optimized neural forecast `fit` and `predict` methods.\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        models: List or Dict\n",
    "            List of models or Dictionary with configuration. \n",
    "            Keys should be name of models.\n",
    "            For each model specify the hyperparameter space \n",
    "            (None will use default suggested space), hyperopt steps and timeout.\n",
    "        horizon: int\n",
    "            Forecast horizon\n",
    "    \"\"\"\n",
    "\n",
    "    def fit(self, \n",
    "            Y_df: pd.DataFrame, X_df: pd.DataFrame, S_df: pd.DataFrame,\n",
    "            loss_function_val: callable, loss_functions_test: dict, \n",
    "            n_ts_val: int, n_ts_test: int,\n",
    "            results_dir: str,\n",
    "            hyperopt_steps: int = None,\n",
    "            return_forecasts: bool = False,\n",
    "            verbose: bool = False):\n",
    "        \"\"\"\n",
    "        This function automatically fits and selects best performing model from\n",
    "        the config_dict. \n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            Y_df: pd.DataFrame\n",
    "                Target time series with columns ['unique_id', 'ds', 'y'].\n",
    "            X_df: pd.DataFrame\n",
    "                Exogenous time series with columns ['unique_id', 'ds', 'y'].\n",
    "            S_df: pd.DataFrame\n",
    "                Static exogenous variables with columns ['unique_id', 'ds']. \n",
    "                and static variables. \n",
    "            loss_function_val: function\n",
    "                Loss function used for validation.\n",
    "            loss_functions_test: Dictionary\n",
    "                Loss functions used for test evaluation. \n",
    "                (function name: string, function: fun)\n",
    "            ts_in_val: int\n",
    "                Number of timestamps in validation.\n",
    "            ts_in_test: int\n",
    "                Number of timestamps in test.\n",
    "            hyperopt_steps: int\n",
    "                Number of hyperopt steps.\n",
    "            return_forecasts: bool\n",
    "                If true return forecast on test.\n",
    "            verbose:\n",
    "                If true, will print summary of dataset, model and training.\n",
    "        \"\"\"\n",
    "\n",
    "        models = self.config_dict.keys()\n",
    "        assert all(model in MODEL_DICT for model in models), \\\n",
    "                f'One of the models in model_config is not correct. Models available are {MODEL_DICT.keys()}.'\n",
    "\n",
    "        # Hyperopt\n",
    "        output_dict = {}\n",
    "        best_model  = None\n",
    "        best_loss   = np.inf\n",
    "        for model_str in models:\n",
    "            print('MODEL: ', model_str)\n",
    "            model_config = self.config_dict[model_str]\n",
    "            \n",
    "            # Run automated hyperparameter optimization\n",
    "            if hyperopt_steps is None:\n",
    "                hyperopt_steps = model_config['hyperopt_steps']\n",
    "            results_dir_model = f'{results_dir}/{model_str}'\n",
    "            model = MODEL_DICT[model_str](horizon=self.horizon, space=model_config['space'])\n",
    "\n",
    "            model.fit(Y_df=Y_df, X_df=X_df, S_df=S_df, hyperopt_steps=hyperopt_steps,\n",
    "                      n_ts_val=n_ts_val,\n",
    "                      n_ts_test=n_ts_test,\n",
    "                      results_dir=results_dir_model,\n",
    "                      save_trials=False,\n",
    "                      loss_function_val=loss_function_val,\n",
    "                      loss_functions_test=loss_functions_test,\n",
    "                      return_test_forecast=return_forecasts,\n",
    "                      verbose=verbose)\n",
    "\n",
    "            # Save results in dict\n",
    "            trials = model.trials\n",
    "\n",
    "            model_output = {'best_mc': trials.best_trial['result']['mc'],\n",
    "                            'run_time': trials.best_trial['result']['run_time'],\n",
    "                            'best_val_loss': trials.best_trial['result']['loss']}\n",
    "\n",
    "            # Return model\n",
    "            model_output['model'] = model\n",
    "\n",
    "            # Return test losses\n",
    "            if n_ts_test > 0:\n",
    "                model_output['best_test_loss'] = trials.best_trial['result']['test_losses']\n",
    "\n",
    "            # Return test forecasts\n",
    "            if (return_forecasts) and (n_ts_test > 0):\n",
    "                model_output['y_hat'] = trials.best_trial['result']['forecasts_test']['test_y_hat']\n",
    "                model_output['y_true'] = trials.best_trial['result']['forecasts_test']['test_y_true']\n",
    "\n",
    "            # Improvement\n",
    "            optimization_times = [trials.trials[0]['result']['loss']]\n",
    "            optimization_losses = [trials.trials[0]['result']['run_time']]\n",
    "            for i in range(1, len(trials)):\n",
    "                loss = trials.trials[i]['result']['loss']\n",
    "                time = trials.trials[i]['result']['run_time']\n",
    "\n",
    "                if loss > np.min(optimization_losses):\n",
    "                    loss = np.min(optimization_losses)\n",
    "                optimization_losses.append(loss)\n",
    "                optimization_times.append(np.sum(optimization_times)+time)\n",
    "\n",
    "            model_output['optimization_losses'] = optimization_losses\n",
    "            model_output['optimization_times'] = optimization_times\n",
    "        \n",
    "            # Append to dict\n",
    "            output_dict[model_str] = model_output\n",
    "\n",
    "            if trials.best_trial['result']['loss'] < best_loss:\n",
    "                best_model = model\n",
    "                best_loss = trials.best_trial['result']['loss']\n",
    "\n",
    "        self.best_model = best_model\n",
    "        self.results_dict = output_dict\n",
    "\n",
    "    def forecast(self, Y_df: pd.DataFrame, X_df: pd.DataFrame = None, S_df: pd.DataFrame = None, \n",
    "                 batch_size: int =1, trainer: pl.Trainer =None) -> pd.DataFrame:\n",
    "        \n",
    "        return self.best_model.forecast(Y_df=Y_df, X_df=X_df, S_df=S_df, \n",
    "                                        batch_size=batch_size, trainer=trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The forecasting task we selected is to predict the number of patients with influenza-like illnesses from the [US CDC dataset](https://gis.cdc.gov/grasp/fluview/fluportaldashboard.html), the dataset contains 7 target variables, and has 966 weeks of history.\n",
    "\n",
    "We will be creating point forecasts with N-BEATS, N-HiTS and RNN models. The predictive features will be the autoregressive features. More information on the dataset can be found in the [N-HiTS paper](https://arxiv.org/abs/2201.12886).\n",
    "\n",
    "Table of Contents\n",
    "1.   [Installing NeuralForecast Library](#cell-1)\n",
    "2.   [Data Loading and Processing](#cell-2)\n",
    "3.   [Define Hyperparameter Space](#cell-3)\n",
    "4.   [Hyperparameter Tuning](#cell-4)\n",
    "5.   [Evaluate Results](#cell-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"cell-1\"></a>\n",
    "### 1. Installing Neuralforecast library\n",
    "\n",
    "You can install the released version of NeuralForecast from the Python package index with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%capture\n",
    "#!pip install neuralforecast\n",
    "#!pip install matplotlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import neuralforecast as nf\n",
    "from neuralforecast.data.datasets.long_horizon import LongHorizon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"cell-2\"></a>\n",
    "### 2. Data Loading and Processing\n",
    "\n",
    "For this example we keep 10% of the observations as validation and use the latest 20% of the observations as the test set. To do so we use the sample_mask and declare the windows that will be used to train, and validate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_df, _, _ = LongHorizon.load(directory='./', group='ILI')\n",
    "Y_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_series = len(Y_df.unique_id.unique())\n",
    "n_time = len(Y_df.ds.unique()) # dataset is balanced\n",
    "\n",
    "n_ts_test = 193\n",
    "n_ts_val = 97\n",
    "\n",
    "print('n_time', n_time)\n",
    "print('n_series', n_series)\n",
    "print('n_ts_test', n_ts_test)\n",
    "print('n_ts_val', n_ts_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_id \\in ['% WEIGHTED ILI', '%UNWEIGHTED ILI', 'AGE 0-4', \n",
    "#                'AGE 5-24', 'ILITOTAL', 'NUM. OF PROVIDERS', 'OT']\n",
    "y_plot = Y_df[Y_df.unique_id=='% WEIGHTED ILI'].y.values\n",
    "x_plot = pd.to_datetime(Y_df[Y_df.unique_id=='% WEIGHTED ILI'].ds).values\n",
    "\n",
    "plt.plot(x_plot, y_plot)\n",
    "plt.axvline(x_plot[n_time-n_ts_val-n_ts_test], color='black', linestyle='-.')\n",
    "plt.axvline(x_plot[n_time-n_ts_test], color='black', linestyle='-.')\n",
    "plt.ylabel('Weighted ILI [ratio]')\n",
    "plt.xlabel('Date')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.0 auto.NHITS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_nhits = NHITS(horizon=24)\n",
    "auto_nhits.space['max_steps'] = hp.choice('max_steps', [1]) # Override max_steps for faster example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_nhits.fit(Y_df=Y_df, X_df=None, S_df=None, hyperopt_steps=2,\n",
    "               n_ts_val=n_ts_val,\n",
    "               n_ts_test=n_ts_test,\n",
    "               results_dir='./results/autonhits',\n",
    "               save_trials=True,\n",
    "               loss_function_val=nf.losses.numpy.mae,\n",
    "               loss_functions_test={'mae':nf.losses.numpy.mae,\n",
    "                                    'mse':nf.losses.numpy.mse},\n",
    "               return_test_forecast=True,\n",
    "               verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts = auto_nhits.forecast(Y_df=Y_df)\n",
    "forecasts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 auto.NBEATS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_nbeats = NBEATS(horizon=24)\n",
    "auto_nbeats.space['max_steps'] = hp.choice('max_steps', [1]) # Override max_steps for faster example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_nbeats.fit(Y_df=Y_df, X_df=None, S_df=None, hyperopt_steps=2,\n",
    "                n_ts_val=n_ts_val,\n",
    "                n_ts_test=n_ts_test,\n",
    "                results_dir='./results/autonbeats',\n",
    "                save_trials=True,\n",
    "                loss_function_val=nf.losses.numpy.mae,\n",
    "                loss_functions_test={'mae':nf.losses.numpy.mae,\n",
    "                                     'mse':nf.losses.numpy.mse},\n",
    "                return_test_forecast=True,\n",
    "                verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts = auto_nbeats.forecast(Y_df=Y_df)\n",
    "forecasts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 auto.RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_rnn = RNN(horizon=24)\n",
    "auto_rnn.space['max_steps'] = hp.choice('max_steps', [1]) # Override max_steps for faster example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_rnn.fit(Y_df=Y_df, X_df=None, S_df=None, hyperopt_steps=2,\n",
    "             n_ts_val=n_ts_val,\n",
    "             n_ts_test=n_ts_test,\n",
    "             results_dir='./results/autornn',\n",
    "             save_trials=True,\n",
    "             loss_function_val=nf.losses.numpy.mae,\n",
    "             loss_functions_test={'mae':nf.losses.numpy.mae,\n",
    "                                  'mse':nf.losses.numpy.mse},\n",
    "             return_test_forecast=True,\n",
    "             verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts = auto_rnn.forecast(Y_df=Y_df)\n",
    "forecasts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. AutoNF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"cell-3\"></a>\n",
    "### Define Hyperparameter Space\n",
    "\n",
    "A temporal train-evaluation split procedure allows us to estimate the model’s generalization performance on future data unseen by the model. We use the train set to optimize the model parameters, and the validation  and test sets to evaluate the accuracy of the model’s predictions.\n",
    "\n",
    "In this case we set the space to `None`, that implicitly uses the predefined model space, but the space can be specified as a dictionary following the conventions of the [Hyperopt package](https://github.com/hyperopt/hyperopt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_horizon = 24\n",
    "\n",
    "nhits_space_dict = nhits_space(horizon=forecast_horizon)\n",
    "nhits_space_dict['max_steps'] = hp.choice('max_steps', [10])\n",
    "nhits_space_dict['max_epochs'] = hp.choice('max_epochs', [None])\n",
    "\n",
    "nbeats_space_dict = nbeats_space(horizon=forecast_horizon)\n",
    "nbeats_space_dict['max_steps'] = hp.choice('max_steps', [10])\n",
    "nbeats_space_dict['max_epochs'] = hp.choice('max_epochs', [None])\n",
    "\n",
    "rnn_space_dict = rnn_space(horizon=forecast_horizon)\n",
    "rnn_space_dict['max_steps'] = hp.choice('max_steps', [10])\n",
    "rnn_space_dict['max_epochs'] = hp.choice('max_epochs', [None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dict = {'nbeats': dict(space=nhits_space_dict, hyperopt_steps=3), # Use space=None for default dict\n",
    "               'nhits':  dict(space=nbeats_space_dict, hyperopt_steps=3), # Use space=None for default dict\n",
    "               'rnn':    dict(space=rnn_space_dict, hyperopt_steps=3) # Use space=None for default dict\n",
    "              }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"cell-4\"></a>\n",
    "### Hyperparameter Tuning\n",
    "\n",
    "A temporal train-validation-test (676,97,193) split procedure allows us to estimate the model’s generalization performance on future data unseen by the model. We use the train set to optimize the model parameters, and the validation  and test sets to evaluate the accuracy of the model’s predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_horizon = 24\n",
    "\n",
    "model = AutoNF(models=config_dict, horizon=forecast_horizon)\n",
    "\n",
    "model.fit(Y_df=Y_df, X_df=None, S_df=None,\n",
    "          loss_function_val=nf.losses.numpy.mae, \n",
    "          loss_functions_test={'mae':nf.losses.numpy.mae,\n",
    "                               'mse':nf.losses.numpy.mse},\n",
    "          n_ts_val=n_ts_val,\n",
    "          n_ts_test=n_ts_test,\n",
    "          results_dir='./results/auto',\n",
    "          return_forecasts=True,\n",
    "          verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Val losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('MAE')\n",
    "print('NHITS:\\t', model.results_dict['nhits']['best_val_loss'])\n",
    "print('NBEATS:\\t', model.results_dict['nbeats']['best_val_loss'])\n",
    "print('RNN:\\t', model.results_dict['rnn']['best_val_loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Val losses trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time   = model.results_dict['nbeats']['optimization_times']\n",
    "losses = model.results_dict['nbeats']['optimization_losses']\n",
    "plt.plot(time, losses)\n",
    "plt.xlabel('segs')\n",
    "plt.ylabel('val loss')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"cell-5\"></a>\n",
    "### 5. Evaluate Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we wrangle the numpy predictions to evaluate and plot the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asserting equal outputs between nbeats and rnn\n",
    "assert((model.results_dict['nbeats']['y_true'] == model.results_dict['rnn']['y_true']).mean() == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_nhits  = model.results_dict['nhits']['y_hat']\n",
    "y_hat_nbeats = model.results_dict['nbeats']['y_hat']\n",
    "y_hat_rnn    = model.results_dict['rnn']['y_hat']\n",
    "y_true       = model.results_dict['nbeats']['y_true']\n",
    "\n",
    "print('\\n Shapes')\n",
    "print('1. y_hat_nhits.shape (N,T,H) \\t', y_hat_nhits.shape)\n",
    "print('1. y_hat_nbeats.shape (N,T,H)\\t', y_hat_nbeats.shape)\n",
    "print('1. y_hat_rnn.shape (N,T,H)\\t', y_hat_rnn.shape)\n",
    "print('1. y_true.shape (N,T,H)\\t\\t', y_true.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_idx = 0\n",
    "u_idx = 0\n",
    "\n",
    "plt.plot(y_true[u_idx,w_idx,:], label='True Signal')\n",
    "plt.plot(y_hat_nbeats[u_idx,w_idx,:], label='N-BEATS')\n",
    "plt.plot(y_hat_nhits[u_idx,w_idx,:], label='N-HiTS')\n",
    "plt.plot(y_hat_rnn[u_idx,w_idx,:], label='RNN')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Weighted ILI [ratio]')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passing only models name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_horizon = 24\n",
    "\n",
    "model = AutoNF(models=['nhits', 'rnn', 'nbeats'], horizon=forecast_horizon)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
