{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling time series data is an important preprocessing step when using neural forecasting methods for several reasons:\n",
    "\n",
    "1. **Convergence speed**: Neural forecasting models tend to converge faster when the features are on a similar scale.\n",
    "2. **Avoiding vanishing or exploding gradients**: some architectures, such as recurrent neural networks (RNNs), are sensitive to the scale of input data. If the input values are too large, it could lead to exploding gradients, where the gradients become too large and the model becomes unstable. Conversely, very small input values could lead to vanishing gradients, where weight updates during training are negligible and the training fails to converge.\n",
    "3. **Ensuring consistent scale**: Neural forecasting models have shared global parameters for the all time series of the task. In cases where time series have different scale, scaling ensures that no particular time series dominates the learning process.\n",
    "4. **Improving generalization**: time series with consistent scale can lead to smoother loss surfaces. Moreover, scaling helps to homogenize the distribution of the input data, which can also improve generalization by avoiding out-of-range values.\n",
    "\n",
    "The `Neuralforecast` library integrates two types of temporal scaling:\n",
    "\n",
    "* **Time Series Scaling**: scaling each time series using all its data on the train set before start training the model. This is done by using the `local_scaler_type` parameter of the `Neuralforecast` core class.\n",
    "* **Window scaling (TemporalNorm)**: scaling each input window separetly for each element of the batch at every training iteration. This is done by using the `scaler_type` parameter of each model class.\n",
    "\n",
    "In this notebook, we will demonstrate how to scale the time series data with both methods on an Eletricity Price Forecasting (EPF) task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run these experiments using GPU with Google Colab.\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/Nixtla/neuralforecast/blob/main/nbs/examples/Time_Series_Scaling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install `Neuralforecast`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install neuralforecast\n",
    "!pip install hyperopt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `df` dataframe contains the target and exogenous variables past information to train the model. The `unique_id` column identifies the markets, `ds` contains the datestamps, and `y` the electricity price. For future variables, we include a forecast of how much electricity will be produced (`gen_forecast`), system load (`system_laod`), and day of week (`week_day`). Both the electricity system demand and offer impact the price significantly, including these variables to the model greatly improve performance, as we demonstrate in Olivares et al. (2022).\n",
    "\n",
    "The `futr_df` dataframe includes the information of the future exogenous variables for the period we want to forecast (in this case, 24 hours after the end of the train dataset `df`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://datasets-nixtla.s3.amazonaws.com/EPF_FR_BE.csv')\n",
    "df['ds'] = pd.to_datetime(df['ds'])\n",
    "\n",
    "futr_df = pd.read_csv('https://datasets-nixtla.s3.amazonaws.com/EPF_FR_BE_futr.csv')\n",
    "futr_df['ds'] = pd.to_datetime(futr_df['ds'])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that `y` and the exogenous variables are on largely different scales. Next, we show two methods to scale the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Time Series Scaling with `Neuralforecast` class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most widely used approches for scaling time series is to treat it as a pre-processing step, where each time series and temporal exogenous variables are scaled based on their entire information in the train set. Models are then trained on the scaled data.\n",
    "\n",
    "To simplify pipelines, we added a scaling functionality to the `Neuralforecast` class. Each time series will be scaled before training the model with either `fit` or `cross_validation`, and scaling statistics are stored. The class then uses the stored statistics to scale the forecasts back to the original scale before returning the forecasts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.a. Instantiate model and `Neuralforecast` class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we will use the `TimesNet` model, recently proposed in [Wu, Haixu, et al. (2022)](https://arxiv.org/abs/2210.02186). First instantiate the model with the desired parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralforecast.models import TimesNet\n",
    "from neuralforecast.core import NeuralForecast\n",
    "\n",
    "import logging\n",
    "logging.getLogger(\"pytorch_lightning\").setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon = 24 # day-ahead daily forecast\n",
    "model = TimesNet(h = horizon,                                                 # Horizon\n",
    "                 input_size = 5*horizon,                                      # Length of input window\n",
    "                 max_steps = 100,                                             # Training iterations\n",
    "                 top_k = 3,                                                   # Number of periods (for FFT).\n",
    "                 num_kernels = 3,                                             # Number of kernels for Inception module\n",
    "                 batch_size = 2,                                              # Number of time series per batch\n",
    "                 windows_batch_size = 32,                                     # Number of windows per batch\n",
    "                 learning_rate = 0.001,                                       # Learning rate\n",
    "                 futr_exog_list = ['gen_forecast', 'system_load','week_day'], # Future exogenous variables\n",
    "                 scaler_type = None)                                          # We use the Core scaling method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model by instantiating a `NeuralForecast` object and using the `fit` method. The `local_scaler_type` parameter is used to specify the type of scaling to be used. In this case, we will use `standard`, which scales the data to have zero mean and unit variance.Other supported scalers are `minmax`, `robust`, `robust-iqr`, `minmax`, and `boxcox`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nf = NeuralForecast(models=[model], freq='H', local_scaler_type='standard')\n",
    "nf.fit(df=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.b Forecast and plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, use the `predict` method to forecast the day-ahead prices. The `Neuralforecast` class handles the inverse normalization, forecasts are returned in the original scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_hat_df = nf.predict(futr_df=futr_df)\n",
    "Y_hat_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plot_df = df[df['unique_id']=='FR'].tail(24*5).reset_index(drop=True)\n",
    "Y_hat_df = Y_hat_df.reset_index(drop=False)\n",
    "Y_hat_df = Y_hat_df[Y_hat_df['unique_id']=='FR']\n",
    "\n",
    "plot_df = pd.concat([plot_df, Y_hat_df ]).set_index('ds') # Concatenate the train and forecast dataframes\n",
    "\n",
    "plot_df[['y', 'TimesNet']].plot(linewidth=2)\n",
    "plt.axvline('2016-11-01', color='red')\n",
    "plt.ylabel('Price [EUR/MWh]', fontsize=12)\n",
    "plt.xlabel('Date', fontsize=12)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{.callout-important}\n",
    "The inverse scaling is performed by the `Neuralforecast` class before returning the final forecasts. Therefore, the hyperparmater selection with `Auto` models and validation loss for early stopping or model selection are performed on the scaled data. Different types of scaling with the `Neuralforecast` class can't be automatically compared with `Auto` models.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Temporal Window normalization during training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temporal normalization scales each instance of the batch separately at the window level. It is performed at each training iteration for each window of the batch, for both target variable and temporal exogenous covariates. For more details, see [Olivares et al. (2023)](https://arxiv.org/abs/2305.07089) and https://nixtla.github.io/neuralforecast/common.scalers.html."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.a. Instantiate model and `Neuralforecast` class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temporal normalization is specified by the `scaler_type` argument. Currently, it is only supported for Windows-based models (`NHITS`, `NBEATS`, `MLP`, `TimesNet`, and all Transformers). In this example, we use the `TimesNet` model and `robust` scaler, recently proposed by Wu, Haixu, et al. (2022). First instantiate the model with the desired parameters.\n",
    "\n",
    "Visit https://nixtla.github.io/neuralforecast/common.scalers.html for a complete list of supported scalers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon = 24 # day-ahead daily forecast\n",
    "model = TimesNet(h = horizon,                                                 # Horizon\n",
    "                 input_size = 5*horizon,                                      # Length of input window\n",
    "                 max_steps = 100,                                             # Training iterations\n",
    "                 top_k = 3,                                                   # Number of periods (for FFT).\n",
    "                 num_kernels = 3,                                             # Number of kernels for Inception module\n",
    "                 batch_size = 2,                                              # Number of time series per batch\n",
    "                 windows_batch_size = 32,                                     # Number of windows per batch\n",
    "                 learning_rate = 0.001,                                       # Learning rate\n",
    "                 futr_exog_list = ['gen_forecast', 'system_load','week_day'], # Future exogenous variables\n",
    "                 scaler_type = 'robust')                                      # Robust scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model by instantiating a `NeuralForecast` object and using the `fit` method. Note that `local_scaler_type` has `None` as default to avoid scaling the data before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nf = NeuralForecast(models=[model], freq='H')\n",
    "nf.fit(df=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.b Forecast and plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, use the `predict` method to forecast the day-ahead prices. The forecasts are returned in the original scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_hat_df = nf.predict(futr_df=futr_df)\n",
    "Y_hat_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plot_df = df[df['unique_id']=='FR'].tail(24*5).reset_index(drop=True)\n",
    "Y_hat_df = Y_hat_df.reset_index(drop=False)\n",
    "Y_hat_df = Y_hat_df[Y_hat_df['unique_id']=='FR']\n",
    "\n",
    "plot_df = pd.concat([plot_df, Y_hat_df ]).set_index('ds') # Concatenate the train and forecast dataframes\n",
    "\n",
    "plot_df[['y', 'TimesNet']].plot(linewidth=2)\n",
    "plt.axvline('2016-11-01', color='red')\n",
    "plt.ylabel('Price [EUR/MWh]', fontsize=12)\n",
    "plt.xlabel('Date', fontsize=12)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{.callout-important}\n",
    "For most applications, models with temporal normalization (section 4) produced more accurate forecasts than time series scaling (section 3). However, with temporal normalization models lose the information of the relative level between different windows. In some cases this global information within time series is crucial, for instance when an exogenous variables contains the dosage of a medication. In these cases, time series scaling (section 3) is preferred.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- [Kin G. Olivares, David Luo, Cristian Challu, Stefania La Vattiata, Max Mergenthaler, Artur Dubrawski (2023). \"HINT: Hierarchical Mixture Networks For Coherent Probabilistic Forecasting\". International Conference on Machine Learning (ICML). Workshop on Structured Probabilistic Inference & Generative Modeling. Available at https://arxiv.org/abs/2305.07089.](https://arxiv.org/abs/2305.07089)\n",
    "- [Wu, Haixu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, and Mingsheng Long. \"Timesnet: Temporal 2d-variation modeling for general time series analysis.\", ICLR 2023](https://openreview.net/forum?id=ju_Uqw384Oq)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
