{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to add new Models to NeuralForecast\n",
    "> Tutorial on how to add new models to NeuralForecast"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{.callout-warning}\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "This Guide assumes advanced familiarity with NeuralForecast.\n",
    "\n",
    "We highly recommend reading first the Getting Started and the NeuralForecast Map tutorials!\n",
    "\n",
    "Additionally, refer to the [CONTRIBUTING guide](https://github.com/Nixtla/neuralforecast/blob/main/CONTRIBUTING.md) for the basics how to contribute to NeuralForecast.\n",
    "\n",
    ":::"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial is aimed for contributors that would like to add a new model to the NeuralForecast library.\n",
    "\n",
    "We would like to encourage the community to contribute with new models to the library.\n",
    "\n",
    "The existing modules of the library already take care of all common aspects of the optimization and training, selection, and evaluation of deep learning models. The `core` class simplify building entire pipelines (for both industry and academia) on any dataset with user-friendly methods such as `fit` and `predict`.\n",
    "\n",
    "**Adding a new model to the NeuralForecast is simpler than building a new PyTorch model from scratch** \n",
    "\n",
    "**It has the following additional advantages:****\n",
    "\n",
    "* Existing modules in NeuralForecast already implement most aspects that are necessary for training and evaluating deep learning models.\n",
    "* Integrated with PyTorch-Lightning and Tune libraries for efficient optimization and distributed computation.\n",
    "* The `BaseModel` classes provide common optimization components such as early stopping, learning rate schedulers, among others.\n",
    "* Scheduled automatic performance tests on Github to ensure quality standards.\n",
    "* Easy performance and computation comparison with the other models in the library.\n",
    "* Exposure to a large community of users and contributors. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will present the steps following an example of a simplified `MLP` model (with no exogenous covariates)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Determine the model type (Base class)\n",
    "\n",
    "\n",
    "The library contains **three** types of base models: `BaseWindows`, `BaseRecurrent`, and `BaseMultivariate`. \n",
    "\n",
    "Examples:\n",
    "\n",
    "* `BaseWindows`: `MLP`, `NBEATS`, `NHITS`, `TFT`, `PatchTST`\n",
    "* `BaseRecurrent`: `RNN`, `LSTM`, `GRU`\n",
    "* `BaseMultivariate`: `StemGNN`\n",
    "\n",
    "The main difference between the three types is the sampling procedure and input batch for the `forward` method, which determines the type of model. \n",
    "\n",
    "### a. Sampling process of the three different types of models\n",
    "\n",
    "During training, all base models receive a sample of time series of the dataset from the `TimeSeriesLoader` module. The main difference between the three types is the creation of the batch, which is then sent to the `forward` method.\n",
    "\n",
    "*`BaseWindows`*:  The `BaseWindows` models will sample `windows_batch_size` individual windows of size `input_size+h`. This class is designed for windows-based models, that predict the `h` future values based on a fixed short history of size `input_size`. This family of models includes the `MLP`, `NBEATS`, `NHITS`, and most unviariate Transformer based models such as `TFT` and `PatchTST`.\n",
    "\n",
    "*`BaseRecurrent`*: The `BaseRecurrent` models will directly use the `batch_size` time series and pass them to the model (will only use `input_size` to shorten history for truncated-backpropagation). They are designed for recurrent-based models that have infinite memory, such as `RNN`, `LSTM`, among others.\n",
    "\n",
    "*`BaseMultivariate`*: Finally, the `BaseMultivariate` model will receive the complete set of time series from the loader. It will then sample `batch_size` timestamps, and get the windows of size `input_size+h` containing all the time series, starting at those timestamps. This base class is designed for multivariate models, such as `StemGNN`, that model the interactions between time series interactions for forecasting."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Hyperparameters of the three different types of models\n",
    "\n",
    "Get familiar with the hyperparameters specified in the base class correspoding to your model. All classes share common hyperparameters such as `h` (horizon), `input_size`, and optimization hyperparameters such as `learning_rate`, `max_steps`, among others. Additionally, each base class has particular hyperparameters to control the sampling process. The following list presents these hyperparameters for each base class:\n",
    "\n",
    "`BaseWindows`:\n",
    "\n",
    " * `batch_size` (bs): number of time series sampled by the loader during training.\n",
    " * `valid_batch_size` (v_bs): number of time series sampled by the loader during inference (validation and test).\n",
    " * `windows_batch_size` (w_bs): number of individual windows sampled during training (from the previous time series) to form the batch.\n",
    " * `inference_windows_batch_size` (i_bs): number of individual windows sampled during inference to form each batch. Used to control the GPU memory.\n",
    "\n",
    "`BaseRecurrent`:\n",
    "    \n",
    " * `batch_size` (bs): number of time series sampled by the loader during training. Will be directly passed to the model.\n",
    " * `input_size` (L): usually defaulted to -1, can be used to shorten the history during training (sampling timestamps, similarly to `BaseWindows`) for truncated-backpropagation.\n",
    " * `inference_input_size` (i_L): length of historical data used during inference (starting from the forecast creation date and going backwards). Used to control the GPU memory.\n",
    " \n",
    "\n",
    "`BaseMultivariate`:\n",
    "    \n",
    " * `batch_size` (bs): number of windows sampled by the class during training (loader sends all the dataset).\n",
    " * `n_series` (n_ts): number of time series in the dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Input and output batch shapes\n",
    "\n",
    "The `forward` method of the base classes receive a batch of data in a dictionary with the following keys:\n",
    "\n",
    "- `insample_y`: historic values of the time series.\n",
    "- `insample_mask`: mask indicating the available values of the time series (1 if available, 0 if missing).\n",
    "- `futr_exog`: future exogenous covariates (if any).\n",
    "- `hist_exog`: historic exogenous covariates (if any).\n",
    "- `stat_exog`: static exogenous covariates (if any).\n",
    "\n",
    "The shape of each of these tensors depends on the base class, the following table presents the shape for each one:\n",
    "\n",
    "| `tensor`        | `BaseWindows`            | `BaseRecurrent`                 | `BaseMultivariate`             |\n",
    "|-----------------|--------------------------|---------------------------------|--------------------------------|\n",
    "| `insample_y`    | (`w_bs`, `L`)            | (`bs`, `seq_len`, 1)            | (`bs`,`L`, `n_ts`)             |\n",
    "| `insample_mask` | (`w_bs`, `L`)            | (`bs`, `seq_len`, 1)            | (`bs`,`L`, `n_ts`)             |\n",
    "| `futr_exog`     | (`w_bs`, `L`+`h`, `n_f`) | (`bs`, `n_f`, `seq_len`, 1+`h`) | (`bs`, `n_f`, `L`+`h`, `n_ts`) |\n",
    "| `hist_exog`     | (`w_bs`, `L`, `n_h`)     | (`bs`, `n_h`, `seq_len`, 1)     | (`bs`, `n_h`, `L`, `n_ts`)     |\n",
    "| `stat_exog`     | (`w_bs`,`n_s`)           | (`bs`,`n_s`)                    | (`n_ts`, `n_s`)                |\n",
    "\n",
    "The `forward` function should return a single tensor with the forecasts of the next `h` timestamps for each window (`w_bs`) or batch (`bs`). Use the attributes of the `loss` class to automatically parse the output to the correct shape (see the `MLP` example below).  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{.callout-tip}\n",
    "\n",
    "Since we are using `nbdev`, you can easily add prints to the code and see the shapes of the tensors during training.\n",
    "\n",
    ":::"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create the model file and class"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Model class\n",
    "\n",
    "The next step is creating the model class. The main steps are:\n",
    "\n",
    "1. Create the file in the `nbs` folder (https://github.com/Nixtla/neuralforecast/tree/main/nbs).\n",
    "2. Add the header of the `nbdev` file.\n",
    "3. Import libraries in the file. \n",
    "4. Define the `__init__` method with the inhereted and particular hyperparameters of the model and instantiates the architecture.\n",
    "5. Define the `forward` method, that recieves the input batch dictionary and returns the forecast."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, add the following two cells on top of the `nbdev` file, and add the dependencies of the model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "#| default_exp models.mlp\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "#| export\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from neuralforecast.losses.pytorch import MAE\n",
    "from neuralforecast.common._base_windows import BaseWindows\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{.callout-tip}\n",
    "\n",
    "Don't forget to add the `#| export` tag on this cell.\n",
    "\n",
    ":::"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, create class and the init method. The following example shows the init method of the simplified `MLP` model class.\n",
    "\n",
    "The `loss` class contains an `outputsize_multiplier` attribute, to automatically adjust the output size of the forecast. For example, for the Multi-quantile loss (`MQLoss`), the model needs to output each quantile for each horizon."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "#| export\n",
    "class MLP(BaseWindows): # <<---- Inherit from the BaseWindows\n",
    "    def __init__(self,\n",
    "                 # Inhereted hyperparameters with no defaults\n",
    "                 h,\n",
    "                 input_size,\n",
    "                 # Model specific hyperparameters\n",
    "                 num_layers = 2,\n",
    "                 hidden_size = 1024,\n",
    "                 # Inhereted hyperparameters with defaults\n",
    "                 exclude_insample_y = False,\n",
    "                 loss = MAE(),\n",
    "                 valid_loss = None,\n",
    "                 max_steps: int = 1000,\n",
    "                 learning_rate: float = 1e-3,\n",
    "                 num_lr_decays: int = -1,\n",
    "                 early_stop_patience_steps: int =-1,\n",
    "                 val_check_steps: int = 100,\n",
    "                 batch_size: int = 32,\n",
    "                 valid_batch_size: Optional[int] = None,\n",
    "                 windows_batch_size = 1024,\n",
    "                 inference_windows_batch_size = -1,\n",
    "                 step_size: int = 1,\n",
    "                 scaler_type: str = 'identity',\n",
    "                 random_seed: int = 1,\n",
    "                 num_workers_loader: int = 0,\n",
    "                 drop_last_loader: bool = False,\n",
    "                 **trainer_kwargs):\n",
    "    # Inherit BaseWindows class\n",
    "    super(MLP, self).__init__(h=h,\n",
    "                              input_size=input_size,\n",
    "                              ...,\n",
    "                              random_seed=random_seed,\n",
    "                              **trainer_kwargs)\n",
    "\n",
    "    # Architecture\n",
    "    self.num_layers = num_layers\n",
    "    self.hidden_size = hidden_size\n",
    "\n",
    "    # MultiLayer Perceptron\n",
    "    layers = [nn.Linear(in_features=input_size, out_features=hidden_size)]\n",
    "    layers += [nn.ReLU()]\n",
    "    for i in range(num_layers - 1):\n",
    "        layers += [nn.Linear(in_features=hidden_size, out_features=hidden_size)]\n",
    "        layers += [nn.ReLU()]\n",
    "    self.mlp = nn.ModuleList(layers)\n",
    "\n",
    "    # Adapter with Loss dependent dimensions\n",
    "    self.out = nn.Linear(in_features=hidden_size, \n",
    "                         out_features=h * self.loss.outputsize_multiplier) ## <<--- Use outputsize_multiplier to adjust output size\n",
    "\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{.callout-tip}\n",
    "\n",
    "Don't forget to add the `#| export` tag on each cell.\n",
    "\n",
    ":::\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, define the `forward` step for your model. Note how the `reshape` method is used to adjust the output shape of the model to the expected shape of the forecast defined above. **Finally, always include `y_pred = self.loss.domain_map(y_pred)` at the end of the function**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "    #| export\n",
    "    def forward(self, windows_batch):\n",
    "        # Parse windows_batch\n",
    "        insample_y = windows_batch['insample_y'].clone()\n",
    "        # MLP\n",
    "        y_pred = self.mlp(y_pred)\n",
    "        # Reshape and map to loss domain\n",
    "        y_pred = y_pred.reshape(batch_size, self.h, self.loss.outputsize_multiplier)\n",
    "        y_pred = self.loss.domain_map(y_pred)\n",
    "        return y_pred\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{.callout-tip}\n",
    "\n",
    "Larger architectures, such as Transformers, might require splitting the `forward` by using intermediate functions.\n",
    "\n",
    ":::\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Tests and documentation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nbdev` allows for testing and documenting the model during the development process. It allow users to iterate the development within the notebook, testing the code in the same environment. Refer to existing models, such as the real MLP model [here](https://github.com/Nixtla/neuralforecast/blob/main/nbs/models.mlp.ipynb). These files already contain the tests, documentation, and usage examples that were used during the development process."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Core class and additional files\n",
    "\n",
    "Finally, add the model to the `core` class and additional files. This process should be done after exporting the model with the `nbdev_export` command.\n",
    "\n",
    "1. Manually add the model in the following [init file](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/models/__init__.py).\n",
    "2. Add the model to the `core` class, using the `nbdev` file [here](https://github.com/Nixtla/neuralforecast/blob/main/nbs/core.ipynb):\n",
    "    \n",
    "    a. Add the model to the initial model list:\n",
    "    ```python\n",
    "    from neuralforecast.models import (\n",
    "    GRU, LSTM, RNN, TCN, DilatedRNN,\n",
    "    MLP, NHITS, NBEATS, NBEATSx,\n",
    "    TFT, VanillaTransformer,\n",
    "    Informer, Autoformer, FEDformer,\n",
    "    StemGNN, PatchTST\n",
    "    )\n",
    "    ```\n",
    "    b. Add the model to the `MODEL_FILENAME_DICT` dictionary (used for the `save` and `load` functions)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Upload to GitHub\n",
    "\n",
    "Congratulations! Following the steps above the model is ready to be used in the library. \n",
    "\n",
    "Follow the steps in our contributing guide to upload the model to GitHub: [here](https://github.com/Nixtla/neuralforecast/blob/main/CONTRIBUTING.md).\n",
    "\n",
    "One of the maintainers will review the PR, request changes if necessary, and merge it to the library."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
