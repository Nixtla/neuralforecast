{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer models\n",
    "> Tutorial on how to train and forecast Transformer models."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer models, originally proposed for applications in natural language processing, have seen increasing adoption in the field of time series forecasting. The transformative power of these models lies in their novel architecture that relies heavily on the self-attention mechanism, which helps the model to focus on different parts of the input sequence to make predictions, while capturing long-range dependencies within the data. In the context of time series forecasting, Transformer models leverage this self-attention mechanism to identify relevant information across different periods in the time series, making them exceptionally effective in predicting future values for complex and noisy sequences.\n",
    "\n",
    "Long horizon forecasting consists of predicting a large number of timestamps. It is a challenging task because of the *volatility* of the predictions and the *computational complexity*. To solve this problem, recent studies proposed a variety of Transformer-based models. \n",
    "\n",
    "The Neuralforecast library includes implementations of the following popular recent models: `Informer` (Zhou, H. et al. 2021), `Autoformer` (Wu et al. 2021), `FEDformer` (Zhou, T. et al. 2022), and `PatchTST` (Nie et al. 2023).\n",
    "\n",
    "Our implementation of all these models are univariate, meaning that only autoregressive values of each feature are used for forecasting. **We observed that these unvivariate models are more accurate and faster than their multivariate couterpart**.\n",
    "\n",
    "In this notebook we will show how to:\n",
    "* Load the [ETTm2](https://github.com/zhouhaoyi/ETDataset) benchmark dataset, used in the academic literature.\n",
    "* Train models\n",
    "* Forecast the test set\n",
    "\n",
    "**The results achieved in this notebook outperform the original self-reported results in the respective original paper, with a fraction of the computational cost. Additionally, all models are trained with the default recommended parameters, results can be further improved using our `auto` models with automatic hyperparameter selection.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run these experiments using GPU with Google Colab.\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/Nixtla/neuralforecast/blob/main/nbs/examples/LongHorizon_with_Transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install neuralforecast datasetsforecast"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load ETTm2 Data\n",
    "\n",
    "The `LongHorizon` class will automatically download the complete ETTm2 dataset and process it.\n",
    "\n",
    "It return three Dataframes: `Y_df` contains the values for the target variables, `X_df` contains exogenous calendar features and `S_df` contains static features for each time-series (none for ETTm2). For this example we will only use `Y_df`.\n",
    "\n",
    "If you want to use your own data just replace `Y_df`. Be sure to use a long format and have a simmilar structure than our data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from datasetsforecast.long_horizon import LongHorizon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this to your own data to try the model\n",
    "Y_df, _, _ = LongHorizon.load(directory='./', group='ETTm2')\n",
    "Y_df['ds'] = pd.to_datetime(Y_df['ds'])\n",
    "\n",
    "n_time = len(Y_df.ds.unique())\n",
    "val_size = int(.2 * n_time)\n",
    "test_size = int(.2 * n_time)\n",
    "\n",
    "Y_df.groupby('unique_id').head(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train models\n",
    "\n",
    "We will train models using the `cross_validation` method, which allows users to automatically simulate multiple historic forecasts (in the test set).\n",
    "\n",
    "The `cross_validation` method will use the validation set for hyperparameter selection and early stopping, and will then produce the forecasts for the test set.\n",
    "\n",
    "First, instantiate each model in the `models` list, specifying the `horizon`, `input_size`, and training iterations.\n",
    "\n",
    "(NOTE: The `FEDformer` model was excluded due to extremely long training times.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from neuralforecast.core import NeuralForecast\n",
    "from neuralforecast.models import Informer, Autoformer, FEDformer, PatchTST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "horizon = 96 # 24hrs = 4 * 15 min.\n",
    "models = [Informer(h=horizon,                 # Forecasting horizon\n",
    "                input_size=horizon,           # Input size\n",
    "                max_steps=1000,               # Number of training iterations\n",
    "                val_check_steps=100,          # Compute validation loss every 100 steps\n",
    "                early_stop_patience_steps=3), # Stop training if validation loss does not improve\n",
    "          Autoformer(h=horizon,\n",
    "                input_size=horizon,\n",
    "                max_steps=1000,\n",
    "                val_check_steps=100,\n",
    "                early_stop_patience_steps=3),\n",
    "          PatchTST(h=horizon,\n",
    "                input_size=horizon,\n",
    "                max_steps=1000,\n",
    "                val_check_steps=100,\n",
    "                early_stop_patience_steps=3),\n",
    "         ]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{.callout-tip}\n",
    "Check our `auto` models for automatic hyperparameter optimization.\n",
    ":::"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate a `NeuralForecast` object with the following required parameters:\n",
    "\n",
    "* `models`: a list of models.\n",
    "\n",
    "* `freq`: a string indicating the frequency of the data. (See [panda's available frequencies](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases).)\n",
    "\n",
    "Second, use the `cross_validation` method, specifying the dataset (`Y_df`), validation size and test size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "nf = NeuralForecast(\n",
    "    models=models,\n",
    "    freq='15min')\n",
    "\n",
    "Y_hat_df = nf.cross_validation(df=Y_df,\n",
    "                               val_size=val_size,\n",
    "                               test_size=test_size,\n",
    "                               n_windows=None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `cross_validation` method will return the forecasts for each model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_hat_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate Results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we plot the forecasts on the test set for the `OT` variable for all models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_plot = Y_hat_df[Y_hat_df['unique_id']=='OT'] # OT dataset\n",
    "cutoffs = Y_hat_df['cutoff'].unique()[::horizon]\n",
    "Y_plot = Y_plot[Y_hat_df['cutoff'].isin(cutoffs)]\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.plot(Y_plot['ds'], Y_plot['y'], label='True')\n",
    "plt.plot(Y_plot['ds'], Y_plot['Informer'], label='Informer')\n",
    "plt.plot(Y_plot['ds'], Y_plot['Autoformer'], label='Autoformer')\n",
    "plt.plot(Y_plot['ds'], Y_plot['PatchTST'], label='PatchTST')\n",
    "plt.xlabel('Datestamp')\n",
    "plt.ylabel('OT')\n",
    "plt.grid()\n",
    "plt.legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we compute the test errors using the Mean Absolute Error (MAE):\n",
    "\n",
    "$\\qquad MAE = \\frac{1}{Windows * Horizon} \\sum_{\\tau} |y_{\\tau} - \\hat{y}_{\\tau}| \\qquad$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralforecast.losses.numpy import mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_informer = mae(Y_hat_df['y'], Y_hat_df['Informer'])\n",
    "mae_autoformer = mae(Y_hat_df['y'], Y_hat_df['Autoformer'])\n",
    "mae_patchtst = mae(Y_hat_df['y'], Y_hat_df['PatchTST'])\n",
    "\n",
    "print(f'Informer: {mae_informer:.3f}')\n",
    "print(f'Autoformer: {mae_autoformer:.3f}')\n",
    "print(f'PatchTST: {mae_patchtst:.3f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference, we can check the performance when compared to self-reported performance in their respective papers.\n",
    "\n",
    "| Horizon   | PatchTST     | AutoFormer | Informer | ARIMA \n",
    "|---        |---           |---         |---       |---\n",
    "|  96       |  **0.256**   |   0.339    |  0.453   | 0.301 \n",
    "|  192      |  0.296       |   0.340    |  0.563   | 0.345 \n",
    "|  336      |  0.329       |   0.372    |  0.887   | 0.386 \n",
    "|  720      |  0.385       |   0.419    |  1.388   | 0.445 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We proposed an alternative model for long-horizon forecasting, the `NHITS`, based on feed-forward networks in (Challu et al. 2023). It achieves on par performance with `PatchTST`, with a fraction of the computational cost. The `NHITS` tutorial is available [here](https://nixtla.github.io/neuralforecast/examples/longhorizon_with_nhits.html)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[Zhou, H., Zhang, S., Peng, J., Zhang, S., Li, J., Xiong, H., & Zhang, W. (2021, May). Informer: Beyond efficient transformer for long sequence time-series forecasting. In Proceedings of the AAAI conference on artificial intelligence (Vol. 35, No. 12, pp. 11106-11115)](https://ojs.aaai.org/index.php/AAAI/article/view/17325)\n",
    "\n",
    "[Wu, H., Xu, J., Wang, J., & Long, M. (2021). Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting. Advances in Neural Information Processing Systems, 34, 22419-22430.](https://proceedings.neurips.cc/paper/2021/hash/bcc0d400288793e8bdcd7c19a8ac0c2b-Abstract.html)\n",
    "\n",
    "[Zhou, T., Ma, Z., Wen, Q., Wang, X., Sun, L., & Jin, R. (2022, June). Fedformer: Frequency enhanced decomposed transformer for long-term series forecasting. In International Conference on Machine Learning (pp. 27268-27286). PMLR.](https://proceedings.mlr.press/v162/zhou22g.html)\n",
    "\n",
    "\n",
    "[Nie, Y., Nguyen, N. H., Sinthong, P., & Kalagnanam, J. (2022). A Time Series is Worth 64 Words: Long-term Forecasting with Transformers.](https://arxiv.org/pdf/2211.14730.pdf)\n",
    "\n",
    "[Cristian Challu, Kin G. Olivares, Boris N. Oreshkin, Federico Garza, Max Mergenthaler-Canseco, Artur Dubrawski (2021). NHITS: Neural Hierarchical Interpolation for Time Series Forecasting. Accepted at AAAI 2023.](https://arxiv.org/abs/2201.12886)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
