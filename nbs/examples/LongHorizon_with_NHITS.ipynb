{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# â€¢ Long-Horizon Forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Long-horizon forecasting is challenging because of the *volatility* of the predictions and the *computational complexity*. To solve this problem we created the [NHITS](https://arxiv.org/abs/2201.12886) model and made the code available [NeuralForecast library](https://nixtla.github.io/neuralforecast/models.nhits.html). `NHITS` specializes its partial outputs in the different frequencies of the time series through hierarchical interpolation and multi-rate input\n",
    "processing. \n",
    "\n",
    "In this notebook we show how to use N-HiTS on the [ETTm2](https://github.com/zhouhaoyi/ETDataset) benchmark dataset. This data set includes data points for 2 Electricity Transformers at 2 stations, including load, oil temperature.\n",
    "\n",
    "We will show you how to load data, train, and perform automatic hyperparameter tuning, **to achieve SoTA performance**, outperforming even the latest Transformer architectures for a fraction of their computational cost (50x faster)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run these experiments using GPU with Google Colab.\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/Nixtla/neuralforecast/blob/main/nbs/examples/LongHorizon_with_NHITS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installing NeuralForecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install neuralforecast datasetsforecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ray import tune\n",
    "\n",
    "from neuralforecast.auto import AutoNHITS\n",
    "from neuralforecast.core import NeuralForecast\n",
    "\n",
    "from neuralforecast.losses.pytorch import MAE\n",
    "from neuralforecast.losses.numpy import mae, mse\n",
    "from datasetsforecast.long_horizon import LongHorizon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger(\"pytorch_lightning\").setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example will automatically run on GPUs if available. **Make sure** cuda is available. (If you need help to put this into production send us an email or join or community, we also offer a fully hosted solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load ETTm2 Data\n",
    "\n",
    "The `LongHorizon` class will automatically download the complete ETTm2 dataset and process it.\n",
    "\n",
    "It return three Dataframes: `Y_df` contains the values for the target variables, `X_df` contains exogenous calendar features and `S_df` contains static features for each time-series (none for ETTm2). For this example we will only use `Y_df`.\n",
    "\n",
    "If you want to use your own data just replace `Y_df`. Be sure to use a long format and have a simmilar structure than our data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this to your own data to try the model\n",
    "Y_df, _, _ = LongHorizon.load(directory='./', group='ETTm2')\n",
    "Y_df['ds'] = pd.to_datetime(Y_df['ds'])\n",
    "\n",
    "# For this excercise we are going to take 20% of the DataSet\n",
    "n_time = len(Y_df.ds.unique())\n",
    "val_size = int(.2 * n_time)\n",
    "test_size = int(.2 * n_time)\n",
    "\n",
    "Y_df.groupby('unique_id').head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to plot the temperature of the transformer \n",
    "# and marking the validation and train splits\n",
    "u_id = 'HUFL'\n",
    "x_plot = pd.to_datetime(Y_df[Y_df.unique_id==u_id].ds)\n",
    "y_plot = Y_df[Y_df.unique_id==u_id].y.values\n",
    "\n",
    "x_val = x_plot[n_time - val_size - test_size]\n",
    "x_test = x_plot[n_time - test_size]\n",
    "\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.plot(x_plot, y_plot)\n",
    "plt.xlabel('Date', fontsize=17)\n",
    "plt.ylabel('HUFL [15 min temperature]', fontsize=17)\n",
    "\n",
    "plt.axvline(x_val, color='black', linestyle='-.')\n",
    "plt.axvline(x_test, color='black', linestyle='-.')\n",
    "plt.text(x_val, 5, '  Validation', fontsize=12)\n",
    "plt.text(x_test, 5, '  Test', fontsize=12)\n",
    "\n",
    "plt.grid()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_df.unique_id.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Hyperparameter Space\n",
    "\n",
    "The `AutoNHITS` class contains a pre-defined suggested hyperparameter space, built for the [Hyperopt library](https://iopscience.iop.org/article/10.1088/1749-4699/8/1/014008/meta).  This function only needs dataframe specific information such as the number of series and frequency. Feel free to play around with this space.\n",
    "\n",
    "The parameter `h` is used to specify the desired forecasting horizon. To replicate results for other horizons from the paper, just change this value!\n",
    "\n",
    "The  `AutoNHITS.default_config` attribute contains a suggested hyperparameter space. Here we send the paper's config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon = 96 # 24hrs = 4 * 15 min.\n",
    "\n",
    "# Use your own config or AutoNHITS.default_config\n",
    "nhits_config = {\n",
    "       \"learning_rate\": tune.choice([1e-3]),                                     # Initial Learning rate\n",
    "       \"max_steps\": tune.choice([1000]),                                         # Number of SGD steps\n",
    "       \"input_size\": tune.choice([5 * horizon]),                                 # input_size = multiplier * horizon\n",
    "       \"batch_size\": tune.choice([7]),                                           # Number of series in windows\n",
    "       \"windows_batch_size\": tune.choice([256]),                                 # Number of windows in batch\n",
    "       \"n_pool_kernel_size\": tune.choice([[2, 2, 2], [16, 8, 1]]),               # MaxPool's Kernelsize\n",
    "       \"n_freq_downsample\": tune.choice([[168, 24, 1], [24, 12, 1], [1, 1, 1]]), # Interpolation expressivity ratios\n",
    "       \"activation\": tune.choice(['ReLU']),                                      # Type of non-linear activation\n",
    "       \"n_blocks\":  tune.choice([[1, 1, 1]]),                                    # Blocks per each 3 stacks\n",
    "       \"mlp_units\":  tune.choice([[[512, 512], [512, 512], [512, 512]]]),        # 2 512-Layers per block for each stack\n",
    "       \"interpolation_mode\": tune.choice(['linear']),                            # Type of multi-step interpolation\n",
    "       \"random_seed\": tune.randint(1, 10),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can override the NHiTS `config` for each hyperparameter.\n",
    "\n",
    "Notice that *1000 Stochastic Gradient Steps* are enough to achieve SoTA performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer to http://hyperopt.github.io/hyperopt/ for more information on the different space options, such as lists and continous intervals.m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Hyperparameter Tuning\n",
    "\n",
    "The function hyperopt_tunning will perform the automatic hyperparameter optimization using the Hyperopt library for any of our implemented models, on the specified space, on any dataframes Y_df and with custom validation and test losses.\n",
    "\n",
    "This function will split the data in Y_df based on the number of timestamps for the validation and test sets, specified with ds_in_val and ds_in_test. Each configuration will be trained on the train split, evaluated on the validation and test sets with the desired loss functions.\n",
    "\n",
    "Use the hyperopt_max_evals parameter to change the number of configurations explored (5 is enough for achieving SoTA performance, but more iterations will further improve results)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Fit and predict\n",
    "fcst = NeuralForecast(\n",
    "    models=[AutoNHITS(h=horizon, config=nhits_config, \n",
    "                      num_samples=5)], # control of hyperopt samples\n",
    "    freq='15min')\n",
    "\n",
    "fcst_df = fcst.cross_validation(df=Y_df, val_size=val_size,\n",
    "                                test_size=test_size, n_windows=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate Results\n",
    "\n",
    "The `AutoNHITS` class contains a `.results` tune attribute. For each configuration explored, with information of the hyperparameter optimization. \n",
    "It contains the validation loss, and best validation hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcst.models[0].results.get_best_result().config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = fcst_df.y.values\n",
    "y_hat = fcst_df['AutoNHITS'].values\n",
    "\n",
    "n_series = len(Y_df.unique_id.unique())\n",
    "\n",
    "y_true = y_true.reshape(n_series, -1, horizon)\n",
    "y_hat = y_hat.reshape(n_series, -1, horizon)\n",
    "\n",
    "print('Parsed results')\n",
    "print('2. y_true.shape (n_series, n_windows, n_time_out):\\t', y_true.shape)\n",
    "print('2. y_hat.shape  (n_series, n_windows, n_time_out):\\t', y_hat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=3, ncols=1, figsize=(10, 11))\n",
    "fig.tight_layout()\n",
    "\n",
    "series = ['HUFL','HULL','LUFL','LULL','MUFL','MULL','OT']\n",
    "series_idx = 3\n",
    "\n",
    "for idx, w_idx in enumerate([200, 300, 400]):\n",
    "  axs[idx].plot(y_true[series_idx, w_idx,:],label='True')\n",
    "  axs[idx].plot(y_hat[series_idx, w_idx,:],label='Forecast')\n",
    "  axs[idx].grid()\n",
    "  axs[idx].set_ylabel(series[series_idx]+f' window {w_idx}', \n",
    "                      fontsize=17)\n",
    "  if idx==2:\n",
    "    axs[idx].set_xlabel('Forecast Horizon', fontsize=17)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we compute the test errors for the two metrics of interest (which are also available in the `trials` object):\n",
    "\n",
    "$\\qquad MAE = \\frac{1}{Windows * Horizon} \\sum_{\\tau} |y_{\\tau} - \\hat{y}_{\\tau}| \\qquad$ and $\\qquad MSE = \\frac{1}{Windows * Horizon} \\sum_{\\tau} (y_{\\tau} - \\hat{y}_{\\tau})^{2} \\qquad$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('MAE: ', mae(y_hat, y_true))\n",
    "print('MSE: ', mse(y_hat, y_true))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference we can check the performance when compared\n",
    "to previous 'state-of-the-art' long-horizon Transformer-based forecasting methods from the [N-HiTS paper](https://arxiv.org/abs/2201.12886). To recover or improve the paper results try setting `hyperopt_max_evals=30` in [Hyperparameter Tuning](#cell-4).\n",
    "\n",
    "Mean Absolute Error (MAE):\n",
    "\n",
    "| Horizon   | N-HiTS       | AutoFormer | InFormer | ARIMA \n",
    "|---        |---           |---         |---       |---\n",
    "|  96       |  **0.255**     |   0.339    |  0.453   | 0.301 \n",
    "|  192      |  0.305       |   0.340    |  0.563   | 0.345 \n",
    "|  336      |  0.346       |   0.372    |  0.887   | 0.386 \n",
    "|  720      |  0.426       |   0.419    |  1.388   | 0.445 \n",
    "\n",
    "Mean Squared Error (MSE):\n",
    "\n",
    "| Horizon   | N-HiTS       | AutoFormer | InFormer | ARIMA \n",
    "|---        |---           |---         |---       |---\n",
    "|  96       |  **0.176**     |   0.255    |  0.365   | 0.225 \n",
    "|  192      |  0.245       |   0.281    |  0.533   | 0.298 \n",
    "|  336      |  0.295       |   0.339    |  1.363   | 0.370 \n",
    "|  720      |  0.401       |   0.422    |  3.379   | 0.478 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('neuralforecast')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
