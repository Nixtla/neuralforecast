{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long-Horizon Forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Long-horizon forecasting is challenging because of the *volatility* of the predictions and the *computational complexity*. To solve this problem we created the [NHITS](https://arxiv.org/abs/2201.12886) model and made the code available [NeuralForecast library](https://nixtla.github.io/neuralforecast/models.nhits.html). `NHITS` specializes its partial outputs in the different frequencies of the time series through hierarchical interpolation and multi-rate input\n",
    "processing. \n",
    "\n",
    "In this notebook we show how to use N-HiTS on the [ETTm2](https://github.com/zhouhaoyi/ETDataset) benchmark dataset. This data set includes data points for 2 Electricity Transformers at 2 stations, including load, oil temperature.\n",
    "\n",
    "We will show you how to load data, train, and perform automatic hyperparameter tuning, **to achieve SoTA performance**, outperforming even the latest Transformer architectures for a fraction of their computational cost (50x faster)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run these experiments using GPU with Google Colab.\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/Nixtla/neuralforecast/blob/main/nbs/examples/LongHorizon_with_NHITS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installing NeuralForecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install neuralforecast datasetsforecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ray import tune\n",
    "\n",
    "from neuralforecast.auto import AutoNHITS\n",
    "from neuralforecast.core import NeuralForecast\n",
    "\n",
    "from neuralforecast.losses.pytorch import MAE\n",
    "from neuralforecast.losses.numpy import mae, mse\n",
    "from datasetsforecast.long_horizon import LongHorizon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger(\"pytorch_lightning\").setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example will automatically run on GPUs if available. **Make sure** cuda is available. (If you need help to put this into production send us an email or join or community, we also offer a fully hosted solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load ETTm2 Data\n",
    "\n",
    "The `LongHorizon` class will automatically download the complete ETTm2 dataset and process it.\n",
    "\n",
    "It return three Dataframes: `Y_df` contains the values for the target variables, `X_df` contains exogenous calendar features and `S_df` contains static features for each time-series (none for ETTm2). For this example we will only use `Y_df`.\n",
    "\n",
    "If you want to use your own data just replace `Y_df`. Be sure to use a long format and have a simmilar structure than our data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this to your own data to try the model\n",
    "Y_df, _, _ = LongHorizon.load(directory='./', group='ETTm2')\n",
    "Y_df['ds'] = pd.to_datetime(Y_df['ds'])\n",
    "\n",
    "# For this excercise we are going to take 20% of the DataSet\n",
    "n_time = len(Y_df.ds.unique())\n",
    "val_size = int(.2 * n_time)\n",
    "test_size = int(.2 * n_time)\n",
    "\n",
    "Y_df.groupby('unique_id').head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to plot the temperature of the transformer \n",
    "# and marking the validation and train splits\n",
    "u_id = 'HUFL'\n",
    "x_plot = pd.to_datetime(Y_df[Y_df.unique_id==u_id].ds)\n",
    "y_plot = Y_df[Y_df.unique_id==u_id].y.values\n",
    "\n",
    "x_val = x_plot[n_time - val_size - test_size]\n",
    "x_test = x_plot[n_time - test_size]\n",
    "\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.plot(x_plot, y_plot)\n",
    "plt.xlabel('Date', fontsize=17)\n",
    "plt.ylabel('HUFL [15 min temperature]', fontsize=17)\n",
    "\n",
    "plt.axvline(x_val, color='black', linestyle='-.')\n",
    "plt.axvline(x_test, color='black', linestyle='-.')\n",
    "plt.text(x_val, 5, '  Validation', fontsize=12)\n",
    "plt.text(x_test, 5, '  Test', fontsize=12)\n",
    "\n",
    "plt.grid()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_df.unique_id.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Hyperparameter Space\n",
    "\n",
    "The `AutoNHITS` class contains a pre-defined suggested hyperparameter space, built for the [Hyperopt library](https://iopscience.iop.org/article/10.1088/1749-4699/8/1/014008/meta).  This function only needs dataframe specific information such as the number of series and frequency. Feel free to play around with this space.\n",
    "\n",
    "The parameter `h` is used to specify the desired forecasting horizon. To replicate results for other horizons from the paper, just change this value!\n",
    "\n",
    "The  `AutoNHITS.default_config` attribute contains a suggested hyperparameter space. Here we send the paper's config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon = 96 # 24hrs = 4 * 15 min.\n",
    "\n",
    "# Use your own config or AutoNHITS.default_config\n",
    "nhits_config = {\n",
    "       \"learning_rate\": tune.choice([1e-3]),                                     # Initial Learning rate\n",
    "       \"max_steps\": tune.choice([1000]),                                         # Number of SGD steps\n",
    "       \"input_size\": tune.choice([5 * horizon]),                                 # input_size = multiplier * horizon\n",
    "       \"batch_size\": tune.choice([7]),                                           # Number of series in windows\n",
    "       \"windows_batch_size\": tune.choice([256]),                                 # Number of windows in batch\n",
    "       \"n_pool_kernel_size\": tune.choice([[2, 2, 2], [16, 8, 1]]),               # MaxPool's Kernelsize\n",
    "       \"n_freq_downsample\": tune.choice([[168, 24, 1], [24, 12, 1], [1, 1, 1]]), # Interpolation expressivity ratios\n",
    "       \"activation\": tune.choice(['ReLU']),                                      # Type of non-linear activation\n",
    "       \"n_blocks\":  tune.choice([[1, 1, 1]]),                                    # Blocks per each 3 stacks\n",
    "       \"mlp_units\":  tune.choice([[[512, 512], [512, 512], [512, 512]]]),        # 2 512-Layers per block for each stack\n",
    "       \"interpolation_mode\": tune.choice(['linear']),                            # Type of multi-step interpolation\n",
    "       \"random_seed\": tune.randint(1, 10),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can override the NHiTS `config` for each hyperparameter.\n",
    "\n",
    "Notice that *1000 Stochastic Gradient Steps* are enough to achieve SoTA performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{.callout-tip}\n",
    "Refer to https://docs.ray.io/en/latest/tune/index.html for more information on the different space options, such as lists and continous intervals.m\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fit the model by instantiating a `NeuralForecast` object with the following required parameters:\n",
    "\n",
    "* `models`: a list of models.\n",
    "\n",
    "* `freq`: a string indicating the frequency of the data. (See [panda's available frequencies](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases).)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `cross_validation` method allows the user to simulate multiple historic forecasts, greatly simplifying pipelines by replacing for loops with `fit` and `predict` methods.\n",
    "\n",
    "With time series data, Cross Validation is done by defining a sliding window across the historical data and predicting the period following it. This form of cross-validation allows us to arrive at a better estimation of our modelâ€™s predictive abilities across a wider range of temporal instances while also keeping the data in the training set contiguous as is required by our models.\n",
    "\n",
    "The `cross_validation` method will use the validation set for hyperparameter selection, and will then produce the forecasts for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Fit and predict\n",
    "fcst = NeuralForecast(\n",
    "    models=[AutoNHITS(h=horizon, config=nhits_config, \n",
    "                      num_samples=5)], # control of hyperopt samples\n",
    "    freq='15min')\n",
    "\n",
    "fcst_df = fcst.cross_validation(df=Y_df, val_size=val_size,\n",
    "                                test_size=test_size, n_windows=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate Results\n",
    "\n",
    "The `AutoNHITS` class contains a `.results` tune attribute. For each configuration explored, with information of the hyperparameter optimization. \n",
    "It contains the validation loss, and best validation hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcst.models[0].results.get_best_result().config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = fcst_df.y.values\n",
    "y_hat = fcst_df['AutoNHITS'].values\n",
    "\n",
    "n_series = len(Y_df.unique_id.unique())\n",
    "\n",
    "y_true = y_true.reshape(n_series, -1, horizon)\n",
    "y_hat = y_hat.reshape(n_series, -1, horizon)\n",
    "\n",
    "print('Parsed results')\n",
    "print('2. y_true.shape (n_series, n_windows, n_time_out):\\t', y_true.shape)\n",
    "print('2. y_hat.shape  (n_series, n_windows, n_time_out):\\t', y_hat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=3, ncols=1, figsize=(10, 11))\n",
    "fig.tight_layout()\n",
    "\n",
    "series = ['HUFL','HULL','LUFL','LULL','MUFL','MULL','OT']\n",
    "series_idx = 3\n",
    "\n",
    "for idx, w_idx in enumerate([200, 300, 400]):\n",
    "  axs[idx].plot(y_true[series_idx, w_idx,:],label='True')\n",
    "  axs[idx].plot(y_hat[series_idx, w_idx,:],label='Forecast')\n",
    "  axs[idx].grid()\n",
    "  axs[idx].set_ylabel(series[series_idx]+f' window {w_idx}', \n",
    "                      fontsize=17)\n",
    "  if idx==2:\n",
    "    axs[idx].set_xlabel('Forecast Horizon', fontsize=17)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we compute the test errors for the two metrics of interest (which are also available in the `trials` object):\n",
    "\n",
    "$\\qquad MAE = \\frac{1}{Windows * Horizon} \\sum_{\\tau} |y_{\\tau} - \\hat{y}_{\\tau}| \\qquad$ and $\\qquad MSE = \\frac{1}{Windows * Horizon} \\sum_{\\tau} (y_{\\tau} - \\hat{y}_{\\tau})^{2} \\qquad$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('MAE: ', mae(y_hat, y_true))\n",
    "print('MSE: ', mse(y_hat, y_true))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference we can check the performance when compared\n",
    "to previous 'state-of-the-art' long-horizon Transformer-based forecasting methods from the [N-HiTS paper](https://arxiv.org/abs/2201.12886). To recover or improve the paper results try setting `hyperopt_max_evals=30` in [Hyperparameter Tuning](#cell-4).\n",
    "\n",
    "Mean Absolute Error (MAE):\n",
    "\n",
    "| Horizon   | N-HiTS       | AutoFormer | InFormer | ARIMA \n",
    "|---        |---           |---         |---       |---\n",
    "|  96       |  **0.255**     |   0.339    |  0.453   | 0.301 \n",
    "|  192      |  0.305       |   0.340    |  0.563   | 0.345 \n",
    "|  336      |  0.346       |   0.372    |  0.887   | 0.386 \n",
    "|  720      |  0.426       |   0.419    |  1.388   | 0.445 \n",
    "\n",
    "Mean Squared Error (MSE):\n",
    "\n",
    "| Horizon   | N-HiTS       | AutoFormer | InFormer | ARIMA \n",
    "|---        |---           |---         |---       |---\n",
    "|  96       |  **0.176**     |   0.255    |  0.365   | 0.225 \n",
    "|  192      |  0.245       |   0.281    |  0.533   | 0.298 \n",
    "|  336      |  0.295       |   0.339    |  1.363   | 0.370 \n",
    "|  720      |  0.401       |   0.422    |  3.379   | 0.478 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
