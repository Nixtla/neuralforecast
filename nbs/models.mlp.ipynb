{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77bc74d-58a1-4c14-b477-f52d28f2a869",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp models.mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15392f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "12fa25a4",
   "metadata": {},
   "source": [
    "# MLP\n",
    "> One of the simplest neural architectures are Multi Layer Perceptrons (`MLP`) composed of stacked Fully Connected Neural Networks trained with backpropagation. Each node in the architecture is capable of modeling non-linear relationships granted by their activation functions. Novel activations like Rectified Linear Units (`ReLU`) have greatly improved the ability to fit deeper networks overcoming gradient vanishing problems that were associated with `Sigmoid` and `TanH` activations. For the forecasting task the last layer is changed to follow a auto-regression problem.<br><br>**References**<br>-[Rosenblatt, F. (1958). \"The perceptron: A probabilistic model for information storage and organization in the brain.\"](https://psycnet.apa.org/record/1959-09865-001)<br>-[Fukushima, K. (1975). \"Cognitron: A self-organizing multilayered neural network.\"](https://pascal-francis.inist.fr/vibad/index.php?action=getRecordDetail&idt=PASCAL7750396723)<br>-[Vinod Nair, Geoffrey E. Hinton (2010). \"Rectified Linear Units Improve Restricted Boltzmann Machines\"](https://www.cs.toronto.edu/~fritz/absps/reluICML.pdf)<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e6036ce9",
   "metadata": {},
   "source": [
    "![Figure 1. Three layer MLP with autorregresive inputs.](imgs_models/mlp.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2508f7a9-1433-4ad8-8f2f-0078c6ed6c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from fastcore.test import test_eq\n",
    "from nbdev.showdoc import show_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44065066-e72a-431f-938f-1528adef9fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "from neuralforecast.losses.pytorch import MAE\n",
    "from neuralforecast.common._base_windows import BaseWindows\n",
    "from neuralforecast.common._modules import RevIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252e8738",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class CustomConv1d(nn.Module):\n",
    "    \"\"\"\n",
    "    Forward- and backward looking Conv1D\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, padding=0, dilation=1, mode='backward', groups=1):\n",
    "        super().__init__()\n",
    "        k = np.sqrt(1 / (in_channels * kernel_size))\n",
    "        weight_data = -k + 2 * k * torch.rand((out_channels, in_channels // groups, kernel_size))\n",
    "        bias_data = -k + 2 * k * torch.rand((out_channels))\n",
    "        self.weight = nn.Parameter(weight_data, requires_grad=True)\n",
    "        self.bias = nn.Parameter(bias_data, requires_grad=True)  \n",
    "        self.dilation = dilation\n",
    "        self.groups = groups\n",
    "        if mode == 'backward':\n",
    "            self.padding_left = padding\n",
    "            self.padding_right= 0\n",
    "        elif mode == 'forward':\n",
    "            self.padding_left = 0\n",
    "            self.padding_right= padding            \n",
    "\n",
    "    def forward(self, x):\n",
    "        xp = F.pad(x, (self.padding_left, self.padding_right))\n",
    "        return F.conv1d(xp, self.weight, self.bias, dilation=self.dilation, groups=self.groups)\n",
    "\n",
    "class TCNCell(nn.Module):\n",
    "    \"\"\"\n",
    "    Temporal Convolutional Network Cell, consisting of CustomConv1D modules.\n",
    "    \"\"\"    \n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, kernel_size, padding, dilation, mode, groups, dropout):\n",
    "        super().__init__()\n",
    "        self.conv1 = CustomConv1d(in_channels, hidden_channels, kernel_size, padding, dilation, mode, groups)\n",
    "        self.conv2 = CustomConv1d(hidden_channels, hidden_channels + out_channels, 1)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.hidden_channels = hidden_channels\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h_prev, out_prev = x\n",
    "        h = self.drop(F.gelu(self.conv1(h_prev)))\n",
    "        h = self.conv2(h)\n",
    "        h_next = h[:, :self.hidden_channels]\n",
    "        out_next = h[:, self.hidden_channels:]\n",
    "\n",
    "        return (h_prev + h_next, out_prev + out_next)\n",
    "    \n",
    "class MLPResidual(nn.Module):\n",
    "    \"\"\"\n",
    "    MLPResidual\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, hidden_size, output_dim, dropout, layernorm):\n",
    "        super().__init__()\n",
    "        self.layernorm = layernorm\n",
    "        if layernorm:\n",
    "            self.norm = nn.LayerNorm(output_dim)\n",
    "\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.lin1 = nn.Linear(input_dim, hidden_size)\n",
    "        self.lin2 = nn.Linear(hidden_size, output_dim)\n",
    "        self.skip = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # MLP dense\n",
    "        x = F.gelu(self.lin1(input))\n",
    "        x = self.drop(x)\n",
    "        x = self.lin2(x)\n",
    "\n",
    "        # Skip connection\n",
    "        x_skip = self.skip(input)\n",
    "\n",
    "        # Combine\n",
    "        x = x + x_skip\n",
    "\n",
    "        if self.layernorm:\n",
    "            return self.norm(x)\n",
    "\n",
    "        return x    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce70cd14-ecb1-4205-8511-fecbd26c8408",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class MLP(BaseWindows):\n",
    "    \"\"\" MLP\n",
    "\n",
    "    Modified MLP\n",
    "\n",
    "    **Parameters:**<br>\n",
    "    `h`: int, forecast horizon.<br>\n",
    "    `input_size`: int, considered autorregresive inputs (lags), y=[1,2,3,4] input_size=2 -> lags=[1,2].<br>\n",
    "    `hidden_size`: int=16, units for the hidden state size.<br>\n",
    "    `dropout`: float=0.1, dropout rate used for the dropout layers throughout the architecture.<br>\n",
    "    `futr_exog_list`: str list, future exogenous columns.<br>\n",
    "    `hist_exog_list`: str list, historic exogenous columns.<br>\n",
    "    `stat_exog_list`: str list, static exogenous columns.<br>\n",
    "    `exclude_insample_y`: bool=False, the model skips the autoregressive features y[t-input_size:t] if True.<br>\n",
    "    `loss`: PyTorch module, instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
    "    `valid_loss`: PyTorch module=`loss`, instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
    "    `max_steps`: int=1000, maximum number of training steps.<br>\n",
    "    `learning_rate`: float=1e-3, Learning rate between (0, 1).<br>\n",
    "    `num_lr_decays`: int=-1, Number of learning rate decays, evenly distributed across max_steps.<br>\n",
    "    `early_stop_patience_steps`: int=-1, Number of validation iterations before early stopping.<br>\n",
    "    `val_check_steps`: int=100, Number of training steps between every validation loss check.<br>\n",
    "    `batch_size`: int=32, number of different series in each batch.<br>\n",
    "    `valid_batch_size`: int=None, number of different series in each validation and test batch, if None uses batch_size.<br>\n",
    "    `windows_batch_size`: int=1024, number of windows to sample in each training batch, default uses all.<br>\n",
    "    `inference_windows_batch_size`: int=-1, number of windows to sample in each inference batch, -1 uses all.<br>\n",
    "    `start_padding_enabled`: bool=False, if True, the model will pad the time series with zeros at the beginning, by input size.<br>\n",
    "    `step_size`: int=1, step size between each window of temporal data.<br>\n",
    "    `scaler_type`: str='identity', type of scaler for temporal inputs normalization see [temporal scalers](https://nixtla.github.io/neuralforecast/common.scalers.html).<br>\n",
    "    `random_seed`: int=1, random_seed for pytorch initializer and numpy generators.<br>\n",
    "    `drop_last_loader`: bool=False, if True `TimeSeriesDataLoader` drops last non-full batch.<br>\n",
    "    `alias`: str, optional,  Custom name of the model.<br>\n",
    "    `optimizer`: Subclass of 'torch.optim.Optimizer', optional, user specified optimizer instead of the default choice (Adam).<br>\n",
    "    `optimizer_kwargs`: dict, optional, list of parameters used by the user specified `optimizer`.<br>\n",
    "    `lr_scheduler`: Subclass of 'torch.optim.lr_scheduler.LRScheduler', optional, user specified lr_scheduler instead of the default choice (StepLR).<br>\n",
    "    `lr_scheduler_kwargs`: dict, optional, list of parameters used by the user specified `lr_scheduler`.<br>\n",
    "    `dataloader_kwargs`: dict, optional, list of parameters passed into the PyTorch Lightning dataloader by the `TimeSeriesDataLoader`. <br>\n",
    "    `**trainer_kwargs`: int,  keyword trainer arguments inherited from [PyTorch Lighning's trainer](https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.trainer.trainer.Trainer.html?highlight=trainer).<br>    \n",
    "\n",
    "    **References**<br>  \n",
    "\n",
    "    \"\"\"\n",
    "    # Class attributes\n",
    "    SAMPLING_TYPE = 'windows'\n",
    "    EXOGENOUS_FUTR = False\n",
    "    EXOGENOUS_HIST = False\n",
    "    EXOGENOUS_STAT = False\n",
    "\n",
    "    def __init__(self,\n",
    "                 h: int,\n",
    "                 input_size: int,\n",
    "                 hidden_size: int = 128,\n",
    "                 dropout: float = 0.2,\n",
    "                 head_dropout: float = 0.0,\n",
    "                 patch_len: int = 16,\n",
    "                 stride: int = 8,\n",
    "                 revin: bool = False,\n",
    "                 encoder: str = \"transformer\",\n",
    "                 decoder: str = \"mlp\",\n",
    "                 n_heads_transformer: int = 16,\n",
    "                 num_encoder_layers: int = 3,\n",
    "                 num_decoder_layers: int = 1,\n",
    "                 kernel_size_tcn: int = 2,\n",
    "                 layernorm_mlp: bool = False,\n",
    "                 global_skip: bool = False,\n",
    "                 futr_exog_list = None,\n",
    "                 hist_exog_list = None,\n",
    "                 stat_exog_list = None,\n",
    "                 exclude_insample_y = False,\n",
    "                 loss = MAE(),\n",
    "                 valid_loss = None,\n",
    "                 max_steps: int = 1000,\n",
    "                 learning_rate: float = 1e-3,\n",
    "                 num_lr_decays: int = -1,\n",
    "                 early_stop_patience_steps: int =-1,\n",
    "                 val_check_steps: int = 100,\n",
    "                 batch_size: int = 32,\n",
    "                 valid_batch_size: Optional[int] = None,\n",
    "                 windows_batch_size = 1024,\n",
    "                 inference_windows_batch_size = 1024,\n",
    "                 start_padding_enabled = False,\n",
    "                 step_size: int = 1,\n",
    "                 scaler_type: str = 'identity',\n",
    "                 random_seed: int = 1,\n",
    "                 drop_last_loader: bool = False,\n",
    "                 optimizer = None,\n",
    "                 optimizer_kwargs = None,\n",
    "                 lr_scheduler = None,\n",
    "                 lr_scheduler_kwargs = None,\n",
    "                 dataloader_kwargs=None,\n",
    "                 **trainer_kwargs):\n",
    "        super(MLP, self).__init__(\n",
    "            h=h,\n",
    "            input_size=input_size,\n",
    "            futr_exog_list=futr_exog_list,\n",
    "            hist_exog_list=hist_exog_list,\n",
    "            stat_exog_list=stat_exog_list,\n",
    "            exclude_insample_y = exclude_insample_y,\n",
    "            loss=loss,\n",
    "            valid_loss=valid_loss,\n",
    "            max_steps=max_steps,\n",
    "            learning_rate=learning_rate,\n",
    "            num_lr_decays=num_lr_decays,\n",
    "            early_stop_patience_steps=early_stop_patience_steps,\n",
    "            val_check_steps=val_check_steps,\n",
    "            batch_size=batch_size,\n",
    "            valid_batch_size=valid_batch_size,\n",
    "            windows_batch_size=windows_batch_size,\n",
    "            inference_windows_batch_size=inference_windows_batch_size,\n",
    "            start_padding_enabled=start_padding_enabled,\n",
    "            step_size=step_size,\n",
    "            scaler_type=scaler_type,\n",
    "            random_seed=random_seed,\n",
    "            drop_last_loader=drop_last_loader,\n",
    "            optimizer=optimizer,\n",
    "            optimizer_kwargs=optimizer_kwargs,\n",
    "            lr_scheduler=lr_scheduler,\n",
    "            lr_scheduler_kwargs=lr_scheduler_kwargs,\n",
    "            dataloader_kwargs=dataloader_kwargs,\n",
    "            **trainer_kwargs\n",
    "        )\n",
    "\n",
    "        #----------------------------------- Parse dimensions -----------------------------------#\n",
    "        self.hidden_size = hidden_size\n",
    "        self.h = h\n",
    "        self.input_size = input_size\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Global skip\n",
    "        self.global_skip = global_skip\n",
    "        if global_skip:\n",
    "            self.lin_global_skip = nn.Linear(input_size, h * self.loss.outputsize_multiplier)\n",
    "\n",
    "        # RevIN\n",
    "        self.revin = revin\n",
    "        if self.revin:\n",
    "          self.revin_layer = RevIN(1, affine=False, subtract_last=True)\n",
    "\n",
    "        # Patches\n",
    "        n_patches = int((input_size - patch_len) / stride + 1)\n",
    "        self.patch_len = min(input_size + stride, patch_len)\n",
    "        self.stride = stride\n",
    "        self.linear_projection = nn.Linear(patch_len, hidden_size)\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = encoder\n",
    "        if self.encoder == \"tcn\":\n",
    "            self.kernel_size = kernel_size_tcn\n",
    "            n_layers_tcn_enc = int(np.ceil(np.log2(((n_patches - 1) / (self.kernel_size - 1)) + 1)))     \n",
    "            layers_tcn_backward = [TCNCell(\n",
    "                            hidden_size, \n",
    "                            hidden_size, \n",
    "                            hidden_size,\n",
    "                            self.kernel_size, \n",
    "                            padding = (self.kernel_size-1)*2**i, \n",
    "                            dilation = 2**i, \n",
    "                            mode = 'backward', \n",
    "                            groups = 1, \n",
    "                            dropout = dropout) for i in range(n_layers_tcn_enc)]\n",
    "            layers_tcn_forward = [TCNCell(\n",
    "                            hidden_size, \n",
    "                            hidden_size, \n",
    "                            hidden_size,\n",
    "                            self.kernel_size, \n",
    "                            padding = (self.kernel_size-1)*2**i, \n",
    "                            dilation = 2**i, \n",
    "                            mode = 'forward', \n",
    "                            groups = 1, \n",
    "                            dropout = dropout) for i in range(n_layers_tcn_enc)]                   \n",
    "            self.tcn_encoder = nn.Sequential(*layers_tcn_backward, *layers_tcn_forward)\n",
    "        elif self.encoder == \"transformer\":\n",
    "            # Positional encoding, learned\n",
    "            pos_enc = torch.empty((n_patches, hidden_size))\n",
    "            nn.init.uniform_(pos_enc, -0.02, 0.02)\n",
    "            self.pos_enc = nn.Parameter(pos_enc, requires_grad=True)\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "            # Transformer encoder\n",
    "            encoder_layer = nn.TransformerEncoderLayer(d_model = hidden_size, nhead = n_heads_transformer, dim_feedforward = 4 * hidden_size, dropout=dropout, activation=F.gelu, batch_first=True)\n",
    "            encoder_norm = nn.LayerNorm(hidden_size)\n",
    "            self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_encoder_layers, encoder_norm)\n",
    "        elif self.encoder == \"lstm\":\n",
    "            self.lstm_encoder = nn.LSTM(hidden_size, hidden_size, num_layers=num_encoder_layers, batch_first=True, dropout=dropout)\n",
    "        elif self.encoder == \"mlp\":\n",
    "            mlp_encoder_layers = [\n",
    "                MLPResidual(\n",
    "                    input_dim=hidden_size * n_patches if i == 0 else hidden_size,\n",
    "                    hidden_size=hidden_size,\n",
    "                    output_dim=hidden_size,\n",
    "                    dropout=dropout,\n",
    "                    layernorm=layernorm_mlp,\n",
    "                )\n",
    "                for i in range(num_encoder_layers)\n",
    "            ]\n",
    "            mlp_encoder_layers.insert(0, nn.Flatten(start_dim=-2))\n",
    "            mlp_encoder_layers.append(nn.Linear(hidden_size, hidden_size * n_patches))\n",
    "            self.mlp_encoder = nn.Sequential(*mlp_encoder_layers)\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = decoder\n",
    "        if self.decoder == \"mlp\":\n",
    "            decoder_output_size = h * self.loss.outputsize_multiplier\n",
    "            dense_decoder_layers = [\n",
    "                MLPResidual(\n",
    "                    input_dim=hidden_size * n_patches if i == 0 else hidden_size,\n",
    "                    hidden_size=hidden_size,\n",
    "                    output_dim=(\n",
    "                        decoder_output_size if i == num_decoder_layers - 1 else hidden_size\n",
    "                    ),\n",
    "                    dropout=dropout,\n",
    "                    layernorm=layernorm_mlp,\n",
    "                )\n",
    "                for i in range(num_decoder_layers)\n",
    "            ]\n",
    "            dense_decoder_layers.insert(0, nn.Flatten(start_dim=-2))\n",
    "            self.mlp_decoder = nn.Sequential(*dense_decoder_layers)\n",
    "        elif self.decoder == \"tcn\":\n",
    "            self.tcn_projection = nn.Linear(n_patches, h)\n",
    "            self.kernel_size = kernel_size_tcn\n",
    "            n_layers_tcn_dec = int(np.ceil(np.log2(((self.h - 1) / (self.kernel_size - 1)) + 1)))     \n",
    "            layers_tcn_backward = [TCNCell(\n",
    "                            hidden_size, \n",
    "                            hidden_size, \n",
    "                            self.loss.outputsize_multiplier,\n",
    "                            self.kernel_size, \n",
    "                            padding = (self.kernel_size-1)*2**i, \n",
    "                            dilation = 2**i, \n",
    "                            mode = 'backward', \n",
    "                            groups = 1, \n",
    "                            dropout = dropout) for i in range(n_layers_tcn_dec)]\n",
    "            layers_tcn_forward = [TCNCell(\n",
    "                            hidden_size, \n",
    "                            hidden_size, \n",
    "                            self.loss.outputsize_multiplier,\n",
    "                            self.kernel_size, \n",
    "                            padding = (self.kernel_size-1)*2**i, \n",
    "                            dilation = 2**i, \n",
    "                            mode = 'forward', \n",
    "                            groups = 1, \n",
    "                            dropout = dropout) for i in range(n_layers_tcn_dec)]                   \n",
    "            self.tcn_decoder = nn.Sequential(*layers_tcn_backward, *layers_tcn_forward)\n",
    "        elif self.decoder == \"transformer\":\n",
    "            self.transformer_projection = nn.Linear(n_patches, h)\n",
    "            # Positional encoding, learned\n",
    "            pos_dec = torch.empty((h, hidden_size))\n",
    "            nn.init.uniform_(pos_dec, -0.02, 0.02)\n",
    "            self.pos_dec = nn.Parameter(pos_dec, requires_grad=True)\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "            # Transformer decoder\n",
    "            decoder_layer = nn.TransformerEncoderLayer(d_model = hidden_size, nhead = n_heads_transformer, dim_feedforward = 4 * hidden_size, dropout=dropout, activation=F.gelu, batch_first=True)\n",
    "            decoder_norm = nn.LayerNorm(hidden_size)\n",
    "            self.transformer_decoder = nn.TransformerEncoder(decoder_layer, num_decoder_layers, decoder_norm)\n",
    "            # Output projection\n",
    "            self.transformer_output_projection = nn.Linear(hidden_size, self.loss.outputsize_multiplier)\n",
    "\n",
    "    def forward(self, windows_batch):\n",
    "        # Parse windows_batch\n",
    "        x    = windows_batch['insample_y']                          #   [B, L] \n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # RevIN\n",
    "        if self.revin:\n",
    "            x = self.revin_layer(x.unsqueeze(-1), mode=\"norm\").squeeze(-1)\n",
    "\n",
    "        # Global skip\n",
    "        if self.global_skip:\n",
    "            x_skip = self.lin_global_skip(x)\n",
    "\n",
    "        # Unfold patches\n",
    "        x = x.unfold(-1, self.patch_len, self.stride)               #   [B, L] -> [B, n_patches, patch_len]\n",
    "\n",
    "        # Linear projection\n",
    "        x = self.linear_projection(x)                               #   [B, n_patches, patch_len] -> [B, n_patches, hidden_size]\n",
    "        \n",
    "        # Encoder\n",
    "        if self.encoder == \"tcn\":\n",
    "            x = x.permute(0, 2, 1)                                      #   [B, n_patches, hidden_size] -> [B, hidden_size, n_patches]\n",
    "            _, x = self.tcn_encoder((x, 0))                                 #   [B, hidden_size, n_patches] -> [B, hidden_size, n_patches]\n",
    "            x = x.permute(0, 2, 1)                                      #   [B, hidden_size, n_patches] -> [B, n_patches, hidden_size]\n",
    "        elif self.encoder == \"transformer\":\n",
    "            x = self.dropout(x + self.pos_enc)\n",
    "            x = self.transformer_encoder(x)                                     #   [B, n_patches, hidden_size] -> [B, n_patches, hidden_size]        \n",
    "        elif self.encoder == \"lstm\":\n",
    "            x, _ = self.lstm_encoder(x)                                     #   [B, n_patches, hidden_size] -> [B, n_patches, hidden_size]                                \n",
    "        elif self.encoder == \"mlp\":\n",
    "            x = self.mlp_encoder(x)                                     #   [B, n_patches, hidden_size] -> [B, n_patches * hidden_size]\n",
    "            x = x.reshape(batch_size, -1, self.hidden_size)         #   [B, n_patches * hidden_size] -> [B, n_patches, hidden_size]\n",
    "\n",
    "        # Decoder\n",
    "        if self.decoder == \"mlp\":\n",
    "            x = self.mlp_decoder(x)                                     #   [B, n_patches, hidden_size] -> [B, h * n_outputs]\n",
    "        elif self.decoder == \"tcn\":\n",
    "            x = x.permute(0, 2, 1)                                      #   [B, n_patches, hidden_size] -> [B, hidden_size, n_patches]\n",
    "            x = self.tcn_projection(x)                                  #   [B, hidden_size, n_patches] -> [B, hidden_size, h]\n",
    "            _, x = self.tcn_decoder((x, 0))                                 #   [B, hidden_size, h] -> [B, n_outputs, h]\n",
    "            x = x.reshape(batch_size, -1)                               #   [B, h, n_outputs] -> [B, h * n_outputs]\n",
    "        elif self.decoder == \"transformer\":\n",
    "            x = x.permute(0, 2, 1)                                      #   [B, n_patches, hidden_size] -> [B, hidden_size, n_patches]\n",
    "            x = self.transformer_projection(x)                                  #   [B, hidden_size, n_patches] -> [B, hidden_size, h]\n",
    "            x = x.permute(0, 2, 1)                                      #   [B, hidden_size, h] -> [B, h, hidden_size]\n",
    "            x = self.dropout(x + self.pos_dec)                          #   [B, h, hidden_size] -> [B, h, hidden_size]\n",
    "            x = self.transformer_decoder(x)                                     #   [B, h, hidden_size] -> [B, h, hidden_size]\n",
    "            x = self.transformer_output_projection(x)                             #   [B, h, hidden_size] -> [B, h, n_outputs]\n",
    "            x = x.reshape(batch_size, -1)                               #   [B, h, n_outputs] -> [B, h * n_outputs]\n",
    "\n",
    "        # Global skip\n",
    "        if self.global_skip:\n",
    "            x = x + x_skip                                                  #   [B, h * n_outputs] -> [B, h * n_outputs] \n",
    "        \n",
    "        x = x.reshape(batch_size, self.h, -1)                           #   [B, h * n_outputs] -> [B, h, n_outputs]\n",
    "\n",
    "        if self.revin:\n",
    "            x = self.revin_layer(x, mode=\"denorm\")\n",
    "\n",
    "        # Map to output domain\n",
    "        forecast = self.loss.domain_map(x)\n",
    "        \n",
    "        return forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc06a06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/models/mlp.py#L127){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### MLP\n",
       "\n",
       ">      MLP (h:int, input_size:int, hidden_size:int=128, dropout:float=0.2,\n",
       ">           head_dropout:float=0.0, patch_len:int=16, stride:int=8,\n",
       ">           revin:bool=False, encoder:str='transformer', decoder:str='mlp',\n",
       ">           n_heads_transformer:int=16, num_encoder_layers:int=3,\n",
       ">           num_decoder_layers:int=1, kernel_size_tcn:int=2,\n",
       ">           layernorm_mlp:bool=False, global_skip:bool=False,\n",
       ">           futr_exog_list=None, hist_exog_list=None, stat_exog_list=None,\n",
       ">           exclude_insample_y=False, loss=MAE(), valid_loss=None,\n",
       ">           max_steps:int=1000, learning_rate:float=0.001, num_lr_decays:int=-1,\n",
       ">           early_stop_patience_steps:int=-1, val_check_steps:int=100,\n",
       ">           batch_size:int=32, valid_batch_size:Optional[int]=None,\n",
       ">           windows_batch_size=1024, inference_windows_batch_size=1024,\n",
       ">           start_padding_enabled=False, step_size:int=1,\n",
       ">           scaler_type:str='identity', random_seed:int=1,\n",
       ">           drop_last_loader:bool=False, optimizer=None, optimizer_kwargs=None,\n",
       ">           lr_scheduler=None, lr_scheduler_kwargs=None, dataloader_kwargs=None,\n",
       ">           **trainer_kwargs)\n",
       "\n",
       "*MLP\n",
       "\n",
       "Modified MLP\n",
       "\n",
       "**Parameters:**<br>\n",
       "`h`: int, forecast horizon.<br>\n",
       "`input_size`: int, considered autorregresive inputs (lags), y=[1,2,3,4] input_size=2 -> lags=[1,2].<br>\n",
       "`hidden_size`: int=16, units for the hidden state size.<br>\n",
       "`dropout`: float=0.1, dropout rate used for the dropout layers throughout the architecture.<br>\n",
       "`futr_exog_list`: str list, future exogenous columns.<br>\n",
       "`hist_exog_list`: str list, historic exogenous columns.<br>\n",
       "`stat_exog_list`: str list, static exogenous columns.<br>\n",
       "`exclude_insample_y`: bool=False, the model skips the autoregressive features y[t-input_size:t] if True.<br>\n",
       "`loss`: PyTorch module, instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
       "`valid_loss`: PyTorch module=`loss`, instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
       "`max_steps`: int=1000, maximum number of training steps.<br>\n",
       "`learning_rate`: float=1e-3, Learning rate between (0, 1).<br>\n",
       "`num_lr_decays`: int=-1, Number of learning rate decays, evenly distributed across max_steps.<br>\n",
       "`early_stop_patience_steps`: int=-1, Number of validation iterations before early stopping.<br>\n",
       "`val_check_steps`: int=100, Number of training steps between every validation loss check.<br>\n",
       "`batch_size`: int=32, number of different series in each batch.<br>\n",
       "`valid_batch_size`: int=None, number of different series in each validation and test batch, if None uses batch_size.<br>\n",
       "`windows_batch_size`: int=1024, number of windows to sample in each training batch, default uses all.<br>\n",
       "`inference_windows_batch_size`: int=-1, number of windows to sample in each inference batch, -1 uses all.<br>\n",
       "`start_padding_enabled`: bool=False, if True, the model will pad the time series with zeros at the beginning, by input size.<br>\n",
       "`step_size`: int=1, step size between each window of temporal data.<br>\n",
       "`scaler_type`: str='identity', type of scaler for temporal inputs normalization see [temporal scalers](https://nixtla.github.io/neuralforecast/common.scalers.html).<br>\n",
       "`random_seed`: int=1, random_seed for pytorch initializer and numpy generators.<br>\n",
       "`drop_last_loader`: bool=False, if True `TimeSeriesDataLoader` drops last non-full batch.<br>\n",
       "`alias`: str, optional,  Custom name of the model.<br>\n",
       "`optimizer`: Subclass of 'torch.optim.Optimizer', optional, user specified optimizer instead of the default choice (Adam).<br>\n",
       "`optimizer_kwargs`: dict, optional, list of parameters used by the user specified `optimizer`.<br>\n",
       "`lr_scheduler`: Subclass of 'torch.optim.lr_scheduler.LRScheduler', optional, user specified lr_scheduler instead of the default choice (StepLR).<br>\n",
       "`lr_scheduler_kwargs`: dict, optional, list of parameters used by the user specified `lr_scheduler`.<br>\n",
       "`dataloader_kwargs`: dict, optional, list of parameters passed into the PyTorch Lightning dataloader by the `TimeSeriesDataLoader`. <br>\n",
       "`**trainer_kwargs`: int,  keyword trainer arguments inherited from [PyTorch Lighning's trainer](https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.trainer.trainer.Trainer.html?highlight=trainer).<br>    \n",
       "\n",
       "**References**<br>*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/models/mlp.py#L127){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### MLP\n",
       "\n",
       ">      MLP (h:int, input_size:int, hidden_size:int=128, dropout:float=0.2,\n",
       ">           head_dropout:float=0.0, patch_len:int=16, stride:int=8,\n",
       ">           revin:bool=False, encoder:str='transformer', decoder:str='mlp',\n",
       ">           n_heads_transformer:int=16, num_encoder_layers:int=3,\n",
       ">           num_decoder_layers:int=1, kernel_size_tcn:int=2,\n",
       ">           layernorm_mlp:bool=False, global_skip:bool=False,\n",
       ">           futr_exog_list=None, hist_exog_list=None, stat_exog_list=None,\n",
       ">           exclude_insample_y=False, loss=MAE(), valid_loss=None,\n",
       ">           max_steps:int=1000, learning_rate:float=0.001, num_lr_decays:int=-1,\n",
       ">           early_stop_patience_steps:int=-1, val_check_steps:int=100,\n",
       ">           batch_size:int=32, valid_batch_size:Optional[int]=None,\n",
       ">           windows_batch_size=1024, inference_windows_batch_size=1024,\n",
       ">           start_padding_enabled=False, step_size:int=1,\n",
       ">           scaler_type:str='identity', random_seed:int=1,\n",
       ">           drop_last_loader:bool=False, optimizer=None, optimizer_kwargs=None,\n",
       ">           lr_scheduler=None, lr_scheduler_kwargs=None, dataloader_kwargs=None,\n",
       ">           **trainer_kwargs)\n",
       "\n",
       "*MLP\n",
       "\n",
       "Modified MLP\n",
       "\n",
       "**Parameters:**<br>\n",
       "`h`: int, forecast horizon.<br>\n",
       "`input_size`: int, considered autorregresive inputs (lags), y=[1,2,3,4] input_size=2 -> lags=[1,2].<br>\n",
       "`hidden_size`: int=16, units for the hidden state size.<br>\n",
       "`dropout`: float=0.1, dropout rate used for the dropout layers throughout the architecture.<br>\n",
       "`futr_exog_list`: str list, future exogenous columns.<br>\n",
       "`hist_exog_list`: str list, historic exogenous columns.<br>\n",
       "`stat_exog_list`: str list, static exogenous columns.<br>\n",
       "`exclude_insample_y`: bool=False, the model skips the autoregressive features y[t-input_size:t] if True.<br>\n",
       "`loss`: PyTorch module, instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
       "`valid_loss`: PyTorch module=`loss`, instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
       "`max_steps`: int=1000, maximum number of training steps.<br>\n",
       "`learning_rate`: float=1e-3, Learning rate between (0, 1).<br>\n",
       "`num_lr_decays`: int=-1, Number of learning rate decays, evenly distributed across max_steps.<br>\n",
       "`early_stop_patience_steps`: int=-1, Number of validation iterations before early stopping.<br>\n",
       "`val_check_steps`: int=100, Number of training steps between every validation loss check.<br>\n",
       "`batch_size`: int=32, number of different series in each batch.<br>\n",
       "`valid_batch_size`: int=None, number of different series in each validation and test batch, if None uses batch_size.<br>\n",
       "`windows_batch_size`: int=1024, number of windows to sample in each training batch, default uses all.<br>\n",
       "`inference_windows_batch_size`: int=-1, number of windows to sample in each inference batch, -1 uses all.<br>\n",
       "`start_padding_enabled`: bool=False, if True, the model will pad the time series with zeros at the beginning, by input size.<br>\n",
       "`step_size`: int=1, step size between each window of temporal data.<br>\n",
       "`scaler_type`: str='identity', type of scaler for temporal inputs normalization see [temporal scalers](https://nixtla.github.io/neuralforecast/common.scalers.html).<br>\n",
       "`random_seed`: int=1, random_seed for pytorch initializer and numpy generators.<br>\n",
       "`drop_last_loader`: bool=False, if True `TimeSeriesDataLoader` drops last non-full batch.<br>\n",
       "`alias`: str, optional,  Custom name of the model.<br>\n",
       "`optimizer`: Subclass of 'torch.optim.Optimizer', optional, user specified optimizer instead of the default choice (Adam).<br>\n",
       "`optimizer_kwargs`: dict, optional, list of parameters used by the user specified `optimizer`.<br>\n",
       "`lr_scheduler`: Subclass of 'torch.optim.lr_scheduler.LRScheduler', optional, user specified lr_scheduler instead of the default choice (StepLR).<br>\n",
       "`lr_scheduler_kwargs`: dict, optional, list of parameters used by the user specified `lr_scheduler`.<br>\n",
       "`dataloader_kwargs`: dict, optional, list of parameters passed into the PyTorch Lightning dataloader by the `TimeSeriesDataLoader`. <br>\n",
       "`**trainer_kwargs`: int,  keyword trainer arguments inherited from [PyTorch Lighning's trainer](https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.trainer.trainer.Trainer.html?highlight=trainer).<br>    \n",
       "\n",
       "**References**<br>*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a23696b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### MLP.fit\n",
       "\n",
       ">      MLP.fit (dataset, val_size=0, test_size=0, random_seed=None,\n",
       ">               distributed_config=None)\n",
       "\n",
       "*Fit.\n",
       "\n",
       "The `fit` method, optimizes the neural network's weights using the\n",
       "initialization parameters (`learning_rate`, `windows_batch_size`, ...)\n",
       "and the `loss` function as defined during the initialization.\n",
       "Within `fit` we use a PyTorch Lightning `Trainer` that\n",
       "inherits the initialization's `self.trainer_kwargs`, to customize\n",
       "its inputs, see [PL's trainer arguments](https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.trainer.trainer.Trainer.html?highlight=trainer).\n",
       "\n",
       "The method is designed to be compatible with SKLearn-like classes\n",
       "and in particular to be compatible with the StatsForecast library.\n",
       "\n",
       "By default the `model` is not saving training checkpoints to protect\n",
       "disk memory, to get them change `enable_checkpointing=True` in `__init__`.\n",
       "\n",
       "**Parameters:**<br>\n",
       "`dataset`: NeuralForecast's `TimeSeriesDataset`, see [documentation](https://nixtla.github.io/neuralforecast/tsdataset.html).<br>\n",
       "`val_size`: int, validation size for temporal cross-validation.<br>\n",
       "`random_seed`: int=None, random_seed for pytorch initializer and numpy generators, overwrites model.__init__'s.<br>\n",
       "`test_size`: int, test size for temporal cross-validation.<br>*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### MLP.fit\n",
       "\n",
       ">      MLP.fit (dataset, val_size=0, test_size=0, random_seed=None,\n",
       ">               distributed_config=None)\n",
       "\n",
       "*Fit.\n",
       "\n",
       "The `fit` method, optimizes the neural network's weights using the\n",
       "initialization parameters (`learning_rate`, `windows_batch_size`, ...)\n",
       "and the `loss` function as defined during the initialization.\n",
       "Within `fit` we use a PyTorch Lightning `Trainer` that\n",
       "inherits the initialization's `self.trainer_kwargs`, to customize\n",
       "its inputs, see [PL's trainer arguments](https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.trainer.trainer.Trainer.html?highlight=trainer).\n",
       "\n",
       "The method is designed to be compatible with SKLearn-like classes\n",
       "and in particular to be compatible with the StatsForecast library.\n",
       "\n",
       "By default the `model` is not saving training checkpoints to protect\n",
       "disk memory, to get them change `enable_checkpointing=True` in `__init__`.\n",
       "\n",
       "**Parameters:**<br>\n",
       "`dataset`: NeuralForecast's `TimeSeriesDataset`, see [documentation](https://nixtla.github.io/neuralforecast/tsdataset.html).<br>\n",
       "`val_size`: int, validation size for temporal cross-validation.<br>\n",
       "`random_seed`: int=None, random_seed for pytorch initializer and numpy generators, overwrites model.__init__'s.<br>\n",
       "`test_size`: int, test size for temporal cross-validation.<br>*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(MLP.fit, name='MLP.fit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8475d33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### MLP.predict\n",
       "\n",
       ">      MLP.predict (dataset, test_size=None, step_size=1, random_seed=None,\n",
       ">                   **data_module_kwargs)\n",
       "\n",
       "*Predict.\n",
       "\n",
       "Neural network prediction with PL's `Trainer` execution of `predict_step`.\n",
       "\n",
       "**Parameters:**<br>\n",
       "`dataset`: NeuralForecast's `TimeSeriesDataset`, see [documentation](https://nixtla.github.io/neuralforecast/tsdataset.html).<br>\n",
       "`test_size`: int=None, test size for temporal cross-validation.<br>\n",
       "`step_size`: int=1, Step size between each window.<br>\n",
       "`random_seed`: int=None, random_seed for pytorch initializer and numpy generators, overwrites model.__init__'s.<br>\n",
       "`**data_module_kwargs`: PL's TimeSeriesDataModule args, see [documentation](https://pytorch-lightning.readthedocs.io/en/1.6.1/extensions/datamodules.html#using-a-datamodule).*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### MLP.predict\n",
       "\n",
       ">      MLP.predict (dataset, test_size=None, step_size=1, random_seed=None,\n",
       ">                   **data_module_kwargs)\n",
       "\n",
       "*Predict.\n",
       "\n",
       "Neural network prediction with PL's `Trainer` execution of `predict_step`.\n",
       "\n",
       "**Parameters:**<br>\n",
       "`dataset`: NeuralForecast's `TimeSeriesDataset`, see [documentation](https://nixtla.github.io/neuralforecast/tsdataset.html).<br>\n",
       "`test_size`: int=None, test size for temporal cross-validation.<br>\n",
       "`step_size`: int=1, Step size between each window.<br>\n",
       "`random_seed`: int=None, random_seed for pytorch initializer and numpy generators, overwrites model.__init__'s.<br>\n",
       "`**data_module_kwargs`: PL's TimeSeriesDataModule args, see [documentation](https://pytorch-lightning.readthedocs.io/en/1.6.1/extensions/datamodules.html#using-a-datamodule).*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(MLP.predict, name='MLP.predict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac34472d-5670-45b5-a7c5-1dba54f8e782",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from neuralforecast.utils import AirPassengersDF as Y_df\n",
    "from neuralforecast.tsdataset import TimeSeriesDataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9c61645f",
   "metadata": {},
   "source": [
    "## Usage Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b60ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 1\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name              | Type             | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0 | loss              | DistributionLoss | 5      | train\n",
      "1 | padder_train      | ConstantPad1d    | 0      | train\n",
      "2 | scaler            | TemporalNorm     | 0      | train\n",
      "3 | revin_layer       | RevIN            | 0      | train\n",
      "4 | linear_projection | Linear           | 2.2 K  | train\n",
      "5 | tcn_encoder       | Sequential       | 131 K  | train\n",
      "6 | tcn_projection    | Linear           | 36     | train\n",
      "7 | tcn_decoder       | Sequential       | 397 K  | train\n",
      "---------------------------------------------------------------\n",
      "531 K     Trainable params\n",
      "5         Non-trainable params\n",
      "531 K     Total params\n",
      "2.126     Total estimated model params size (MB)\n",
      "48        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 399: 100%|██████████| 1/1 [00:00<00:00, 24.32it/s, v_num=9722, train_loss_step=3.51e+3, train_loss_epoch=3.51e+3, valid_loss=15.00]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ospra\\miniconda3\\envs\\neuralforecast\\lib\\site-packages\\utilsforecast\\processing.py:384: FutureWarning: 'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n",
      "  freq = pd.tseries.frequencies.to_offset(freq)\n",
      "c:\\Users\\ospra\\miniconda3\\envs\\neuralforecast\\lib\\site-packages\\utilsforecast\\processing.py:438: FutureWarning: 'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n",
      "  freq = pd.tseries.frequencies.to_offset(freq)\n",
      "Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 77.77it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| eval: false\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from neuralforecast import NeuralForecast\n",
    "# from neuralforecast.models import MLP\n",
    "from neuralforecast.losses.pytorch import DistributionLoss, GMM\n",
    "from neuralforecast.utils import AirPassengersPanel, AirPassengersStatic\n",
    "\n",
    "Y_train_df = AirPassengersPanel[AirPassengersPanel.ds<AirPassengersPanel['ds'].values[-12]] # 132 train\n",
    "Y_test_df = AirPassengersPanel[AirPassengersPanel.ds>=AirPassengersPanel['ds'].values[-12]].reset_index(drop=True) # 12 test\n",
    "\n",
    "model = MLP(h=12, \n",
    "            input_size=24,\n",
    "            # loss=GMM(n_components=7, return_params=True, level=[80,90]),\n",
    "            loss=DistributionLoss(distribution='Normal', level=[80, 90]),\n",
    "            scaler_type='robust',\n",
    "            max_steps=1000,\n",
    "            early_stop_patience_steps=3,\n",
    "            encoder=\"tcn\",\n",
    "            decoder=\"tcn\",\n",
    "            num_encoder_layers=3,\n",
    "            global_skip=False,\n",
    "            revin=True,\n",
    "            )\n",
    "\n",
    "fcst = NeuralForecast(\n",
    "    models=[model],\n",
    "    freq='M'\n",
    ")\n",
    "fcst.fit(df=Y_train_df, static_df=AirPassengersStatic, val_size=12)\n",
    "forecasts = fcst.predict(futr_df=Y_test_df)\n",
    "\n",
    "Y_hat_df = forecasts.reset_index(drop=False).drop(columns=['unique_id','ds'])\n",
    "plot_df = pd.concat([Y_test_df, Y_hat_df], axis=1)\n",
    "plot_df = pd.concat([Y_train_df, plot_df])\n",
    "\n",
    "plot_df = plot_df[plot_df.unique_id=='Airline1'].drop('unique_id', axis=1)\n",
    "plt.plot(plot_df['ds'], plot_df['y'], c='black', label='True')\n",
    "plt.plot(plot_df['ds'], plot_df['MLP-median'], c='blue', label='median')\n",
    "plt.fill_between(x=plot_df['ds'][-12:], \n",
    "                 y1=plot_df['MLP-lo-90'][-12:].values, \n",
    "                 y2=plot_df['MLP-hi-90'][-12:].values,\n",
    "                 alpha=0.4, label='level 90')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
