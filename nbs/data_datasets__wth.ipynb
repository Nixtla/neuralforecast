{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp data.datasets.wth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weather (WTH) dataset\n",
    "\n",
    "> Download the WTH dataset: https://www.ncei.noaa.gov/data/local-climatological-data/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Tuple, Union\n",
    "\n",
    "import gdown\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nixtlats.data.datasets.utils import Info, time_features_from_frequency_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETT meta information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@dataclass\n",
    "class WTH:\n",
    "    freq: str = 'H'\n",
    "    name: str = 'WTH'\n",
    "    n_ts: int = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "WTHInfo = Info(groups=('WTH',),\n",
    "              class_groups=(WTH,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@dataclass\n",
    "class WTH:\n",
    "    \n",
    "    source_url: str = 'https://drive.google.com/uc?id=1UBRz-aM_57i_KCC-iaSWoKDPTGGv6EaG'\n",
    "\n",
    "    @staticmethod\n",
    "    def load(directory: str,\n",
    "             cache: bool = True) -> Tuple[pd.DataFrame, \n",
    "                                          Optional[pd.DataFrame], \n",
    "                                          Optional[pd.DataFrame]]:\n",
    "        \"\"\"Downloads and loads ETT data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        directory: str\n",
    "            Directory where data will be downloaded.\n",
    "        cache: bool\n",
    "            If `True` saves and loads \n",
    "            \n",
    "        Notes\n",
    "        -----\n",
    "        [1] Returns train+val+test sets.\n",
    "        \"\"\"\n",
    "        path = f'{directory}/wth/datasets'\n",
    "        file_cache = f'{path}/WTH.p'\n",
    "        \n",
    "        if os.path.exists(file_cache) and cache:\n",
    "            df, X_df, S_df = pd.read_pickle(file_cache)\n",
    "            \n",
    "            return df, X_df, S_df\n",
    "        \n",
    "        \n",
    "        WTH.download(directory)\n",
    "        path = f'{directory}/wth/datasets'\n",
    "        \n",
    "        y_df = pd.read_csv(f'{path}/WTH.csv')\n",
    "        y_df['date'] = pd.to_datetime(y_df['date'])\n",
    "        y_df.rename(columns={'date': 'ds'}, inplace=True)\n",
    "        u_ids = y_df.columns.to_list()\n",
    "        u_ids.remove('ds')\n",
    "        \n",
    "        time_cls = time_features_from_frequency_str('h')\n",
    "        for cls_ in time_cls:\n",
    "            cls_name = cls_.__class__.__name__\n",
    "            y_df[cls_name] = cls_(y_df['ds'].dt)\n",
    "\n",
    "        X_df = y_df.drop(u_ids, axis=1)\n",
    "        y_df = y_df.filter(items=['ds'] + u_ids)\n",
    "        y_df = y_df.set_index('ds').stack()\n",
    "        y_df = y_df.rename('y').rename_axis(['ds', 'unique_id']).reset_index()\n",
    "        y_df['unique_id'] = pd.Categorical(y_df['unique_id'], u_ids)\n",
    "        y_df = y_df[['unique_id', 'ds', 'y']].sort_values(['unique_id', 'ds'])\n",
    "        \n",
    "        X_df = y_df[['unique_id', 'ds']].merge(X_df, how='left', on=['ds'])\n",
    "       \n",
    "        S_df = None\n",
    "        if cache:\n",
    "            pd.to_pickle((y_df, X_df, S_df), file_cache)\n",
    "            \n",
    "        return y_df, X_df, S_df\n",
    "\n",
    "    @staticmethod\n",
    "    def download(directory: str) -> None:\n",
    "        \"\"\"Download WTH Dataset.\"\"\"\n",
    "        path = f'{directory}/wth/datasets/'\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "            gdown.download(WTH.source_url, f'{path}/WTH.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_df, x_df, _ = WTH.load('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group: WTH n_series: 12 ex_vars: HourOfDay, DayOfWeek, DayOfMonth, DayOfYear\n"
     ]
    }
   ],
   "source": [
    "for group, meta in WTHInfo:\n",
    "    y_df, x_df, s_df = WTH.load(directory='data', cache=False)\n",
    "    n_series = len(np.unique(y_df.unique_id.values))\n",
    "    ex_vars = x_df.columns.to_list()\n",
    "    ex_vars.remove('unique_id')\n",
    "    ex_vars.remove('ds')\n",
    "\n",
    "    display_str  = f'Group: {group} '\n",
    "    display_str += f'n_series: {n_series} '\n",
    "    display_str += f'ex_vars: {\", \".join(ex_vars)}'\n",
    "\n",
    "    print(display_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_wth(expected_first_ds_y: np.ndarray,\n",
    "               expected_first_ds_x: np.ndarray = None) -> None:\n",
    "    y_df, x_df, _ = WTH.load(directory='data', cache=False)\n",
    "    first_ds_y = y_df.groupby('unique_id').head(1)['y'].values\n",
    "    first_ds_x = x_df.groupby('unique_id').head(1).drop(['unique_id', 'ds'], axis=1).values\n",
    "    expected_x = np.repeat(expected_first_ds_x.reshape(1, -1), first_ds_x.shape[0], axis=0)\n",
    "    \n",
    "    np.testing.assert_array_almost_equal(first_ds_y, expected_first_ds_y)\n",
    "    np.testing.assert_array_almost_equal(first_ds_x, expected_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_wth(expected_first_ds_y=np.array([ 10.,  16.,  -9.,  13.,   7., -14.,  67.,   7.,  130.,  21.65,  30.35, -10.3]),\n",
    "         expected_first_ds_x=np.array([-0.5       ,  0.16666667, -0.5       , -0.5       ]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nixtla",
   "language": "python",
   "name": "nixtla"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
