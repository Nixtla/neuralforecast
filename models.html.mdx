---
description: >-
  NeuralForecast contains user-friendly implementations of neural forecasting
  models that allow for easy transition of computing capabilities (GPU/CPU),
  computation parallelization, and hyperparameter tuning.
output-file: models.html
title: AutoModels
---


All the NeuralForecast models are “global” because we train them with
all the series from the input pd.DataFrame data `Y_df`, yet the
optimization objective is, momentarily, “univariate” as it does not
consider the interaction between the output predictions across time
series. Like the StatsForecast library, `core.NeuralForecast` allows you
to explore collections of models efficiently and contains functions for
convenient wrangling of input and output pd.DataFrames predictions.

First we load the AirPassengers dataset such that you can run all the
examples.

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from neuralforecast.tsdataset import TimeSeriesDataset
from neuralforecast.utils import AirPassengersDF as Y_df
```


```python
# Split train/test and declare time series dataset
Y_train_df = Y_df[Y_df.ds<='1959-12-31'] # 132 train
Y_test_df = Y_df[Y_df.ds>'1959-12-31']   # 12 test
dataset, *_ = TimeSeriesDataset.from_df(Y_train_df)
```

# 1. Automatic Forecasting

## A. RNN-Based

------------------------------------------------------------------------

<a
href="https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/auto.py#L61"
target="_blank" style={{ float: "right", fontSize: "smaller" }}>source</a>

### AutoRNN

> ``` text
>  AutoRNN (h, loss=MAE(), valid_loss=None, config=None,
>           search_alg=<ray.tune.search.basic_variant.BasicVariantGenerator
>           object at 0x7fc4241c59f0>, num_samples=10, refit_with_val=False,
>           cpus=4, gpus=0, verbose=False, alias=None, backend='ray',
>           callbacks=None)
> ```

\*Class for Automatic Hyperparameter Optimization, it builds on top of
`ray` to give access to a wide variety of hyperparameter optimization
tools ranging from classic grid search, to Bayesian optimization and
HyperBand algorithm.

The validation loss to be optimized is defined by the `config['loss']`
dictionary value, the config also contains the rest of the
hyperparameter search space.

It is important to note that the success of this hyperparameter
optimization heavily relies on a strong correlation between the
validation and test periods.\*

|  | **Type** | **Default** | **Details** |
|------|------------------|-------------------------|-------------------------|
| h | int |  | Forecast horizon |
| loss | MAE | MAE() | Instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| valid_loss | NoneType | None | Instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| config | NoneType | None | Dictionary with ray.tune defined search space or function that takes an optuna trial and returns a configuration dict. |
| search_alg | BasicVariantGenerator | \<ray.tune.search.basic_variant.BasicVariantGenerator object at 0x7fc4241c59f0\> | For ray see https://docs.ray.io/en/latest/tune/api_docs/suggestion.html<br/>For optuna see https://optuna.readthedocs.io/en/stable/reference/samplers/index.html. |
| num_samples | int | 10 | Number of hyperparameter optimization steps/samples. |
| refit_with_val | bool | False | Refit of best model should preserve val_size. |
| cpus | int | 4 | Number of cpus to use during optimization. Only used with ray tune. |
| gpus | int | 0 | Number of gpus to use during optimization, default all available. Only used with ray tune. |
| verbose | bool | False | Track progress. |
| alias | NoneType | None | Custom name of the model. |
| backend | str | ray | Backend to use for searching the hyperparameter space, can be either ‘ray’ or ‘optuna’. |
| callbacks | NoneType | None | List of functions to call during the optimization process.<br/>ray reference: https://docs.ray.io/en/latest/tune/tutorials/tune-metrics.html<br/>optuna reference: https://optuna.readthedocs.io/en/stable/tutorial/20_recipes/007_optuna_callback.html |

```python
# Use your own config or AutoRNN.default_config
config = dict(max_steps=1, val_check_steps=1, input_size=-1, encoder_hidden_size=8)
model = AutoRNN(h=12, config=config, num_samples=1, cpus=1)

model.fit(dataset=dataset)
y_hat = model.predict(dataset=dataset)

# Optuna
model = AutoRNN(h=12, config=None, num_samples=1, cpus=1, backend='optuna')
```

------------------------------------------------------------------------

<a
href="https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/auto.py#L136"
target="_blank" style={{ float: "right", fontSize: "smaller" }}>source</a>

### AutoLSTM

> ``` text
>  AutoLSTM (h, loss=MAE(), valid_loss=None, config=None,
>            search_alg=<ray.tune.search.basic_variant.BasicVariantGenerator
>            object at 0x7fc41ed29660>, num_samples=10,
>            refit_with_val=False, cpus=4, gpus=0, verbose=False,
>            alias=None, backend='ray', callbacks=None)
> ```

\*Class for Automatic Hyperparameter Optimization, it builds on top of
`ray` to give access to a wide variety of hyperparameter optimization
tools ranging from classic grid search, to Bayesian optimization and
HyperBand algorithm.

The validation loss to be optimized is defined by the `config['loss']`
dictionary value, the config also contains the rest of the
hyperparameter search space.

It is important to note that the success of this hyperparameter
optimization heavily relies on a strong correlation between the
validation and test periods.\*

|  | **Type** | **Default** | **Details** |
|------|------------------|-------------------------|-------------------------|
| h | int |  | Forecast horizon |
| loss | MAE | MAE() | Instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| valid_loss | NoneType | None | Instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| config | NoneType | None | Dictionary with ray.tune defined search space or function that takes an optuna trial and returns a configuration dict. |
| search_alg | BasicVariantGenerator | \<ray.tune.search.basic_variant.BasicVariantGenerator object at 0x7fc41ed29660\> | For ray see https://docs.ray.io/en/latest/tune/api_docs/suggestion.html<br/>For optuna see https://optuna.readthedocs.io/en/stable/reference/samplers/index.html. |
| num_samples | int | 10 | Number of hyperparameter optimization steps/samples. |
| refit_with_val | bool | False | Refit of best model should preserve val_size. |
| cpus | int | 4 | Number of cpus to use during optimization. Only used with ray tune. |
| gpus | int | 0 | Number of gpus to use during optimization, default all available. Only used with ray tune. |
| verbose | bool | False | Track progress. |
| alias | NoneType | None | Custom name of the model. |
| backend | str | ray | Backend to use for searching the hyperparameter space, can be either ‘ray’ or ‘optuna’. |
| callbacks | NoneType | None | List of functions to call during the optimization process.<br/>ray reference: https://docs.ray.io/en/latest/tune/tutorials/tune-metrics.html<br/>optuna reference: https://optuna.readthedocs.io/en/stable/tutorial/20_recipes/007_optuna_callback.html |

```python
# Use your own config or AutoLSTM.default_config
config = dict(max_steps=1, val_check_steps=1, input_size=-1, encoder_hidden_size=8)
model = AutoLSTM(h=12, config=config, num_samples=1, cpus=1)

# Fit and predict
model.fit(dataset=dataset)
y_hat = model.predict(dataset=dataset)

# Optuna
model = AutoLSTM(h=12, config=None, backend='optuna')
```

------------------------------------------------------------------------

<a
href="https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/auto.py#L207"
target="_blank" style={{ float: "right", fontSize: "smaller" }}>source</a>

### AutoGRU

> ``` text
>  AutoGRU (h, loss=MAE(), valid_loss=None, config=None,
>           search_alg=<ray.tune.search.basic_variant.BasicVariantGenerator
>           object at 0x7fc41ed3abc0>, num_samples=10, refit_with_val=False,
>           cpus=4, gpus=0, verbose=False, alias=None, backend='ray',
>           callbacks=None)
> ```

\*Class for Automatic Hyperparameter Optimization, it builds on top of
`ray` to give access to a wide variety of hyperparameter optimization
tools ranging from classic grid search, to Bayesian optimization and
HyperBand algorithm.

The validation loss to be optimized is defined by the `config['loss']`
dictionary value, the config also contains the rest of the
hyperparameter search space.

It is important to note that the success of this hyperparameter
optimization heavily relies on a strong correlation between the
validation and test periods.\*

|  | **Type** | **Default** | **Details** |
|------|------------------|-------------------------|-------------------------|
| h | int |  | Forecast horizon |
| loss | MAE | MAE() | Instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| valid_loss | NoneType | None | Instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| config | NoneType | None | Dictionary with ray.tune defined search space or function that takes an optuna trial and returns a configuration dict. |
| search_alg | BasicVariantGenerator | \<ray.tune.search.basic_variant.BasicVariantGenerator object at 0x7fc41ed3abc0\> | For ray see https://docs.ray.io/en/latest/tune/api_docs/suggestion.html<br/>For optuna see https://optuna.readthedocs.io/en/stable/reference/samplers/index.html. |
| num_samples | int | 10 | Number of hyperparameter optimization steps/samples. |
| refit_with_val | bool | False | Refit of best model should preserve val_size. |
| cpus | int | 4 | Number of cpus to use during optimization. Only used with ray tune. |
| gpus | int | 0 | Number of gpus to use during optimization, default all available. Only used with ray tune. |
| verbose | bool | False | Track progress. |
| alias | NoneType | None | Custom name of the model. |
| backend | str | ray | Backend to use for searching the hyperparameter space, can be either ‘ray’ or ‘optuna’. |
| callbacks | NoneType | None | List of functions to call during the optimization process.<br/>ray reference: https://docs.ray.io/en/latest/tune/tutorials/tune-metrics.html<br/>optuna reference: https://optuna.readthedocs.io/en/stable/tutorial/20_recipes/007_optuna_callback.html |

```python
# Use your own config or AutoGRU.default_config
config = dict(max_steps=1, val_check_steps=1, input_size=-1, encoder_hidden_size=8)
model = AutoGRU(h=12, config=config, num_samples=1, cpus=1)

# Fit and predict
model.fit(dataset=dataset)
y_hat = model.predict(dataset=dataset)

# Optuna
model = AutoGRU(h=12, config=None, backend='optuna')
```

------------------------------------------------------------------------

<a
href="https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/auto.py#L278"
target="_blank" style={{ float: "right", fontSize: "smaller" }}>source</a>

### AutoTCN

> ``` text
>  AutoTCN (h, loss=MAE(), valid_loss=None, config=None,
>           search_alg=<ray.tune.search.basic_variant.BasicVariantGenerator
>           object at 0x7fc41ed08e80>, num_samples=10, refit_with_val=False,
>           cpus=4, gpus=0, verbose=False, alias=None, backend='ray',
>           callbacks=None)
> ```

\*Class for Automatic Hyperparameter Optimization, it builds on top of
`ray` to give access to a wide variety of hyperparameter optimization
tools ranging from classic grid search, to Bayesian optimization and
HyperBand algorithm.

The validation loss to be optimized is defined by the `config['loss']`
dictionary value, the config also contains the rest of the
hyperparameter search space.

It is important to note that the success of this hyperparameter
optimization heavily relies on a strong correlation between the
validation and test periods.\*

|  | **Type** | **Default** | **Details** |
|------|------------------|-------------------------|-------------------------|
| h | int |  | Forecast horizon |
| loss | MAE | MAE() | Instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| valid_loss | NoneType | None | Instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| config | NoneType | None | Dictionary with ray.tune defined search space or function that takes an optuna trial and returns a configuration dict. |
| search_alg | BasicVariantGenerator | \<ray.tune.search.basic_variant.BasicVariantGenerator object at 0x7fc41ed08e80\> | For ray see https://docs.ray.io/en/latest/tune/api_docs/suggestion.html<br/>For optuna see https://optuna.readthedocs.io/en/stable/reference/samplers/index.html. |
| num_samples | int | 10 | Number of hyperparameter optimization steps/samples. |
| refit_with_val | bool | False | Refit of best model should preserve val_size. |
| cpus | int | 4 | Number of cpus to use during optimization. Only used with ray tune. |
| gpus | int | 0 | Number of gpus to use during optimization, default all available. Only used with ray tune. |
| verbose | bool | False | Track progress. |
| alias | NoneType | None | Custom name of the model. |
| backend | str | ray | Backend to use for searching the hyperparameter space, can be either ‘ray’ or ‘optuna’. |
| callbacks | NoneType | None | List of functions to call during the optimization process.<br/>ray reference: https://docs.ray.io/en/latest/tune/tutorials/tune-metrics.html<br/>optuna reference: https://optuna.readthedocs.io/en/stable/tutorial/20_recipes/007_optuna_callback.html |

```python
# Use your own config or AutoTCN.default_config
config = dict(max_steps=1, val_check_steps=1, input_size=-1, encoder_hidden_size=8)
model = AutoTCN(h=12, config=config, num_samples=1, cpus=1)

# Fit and predict
model.fit(dataset=dataset)
y_hat = model.predict(dataset=dataset)

# Optuna
model = AutoTCN(h=12, config=None, backend='optuna')
```

------------------------------------------------------------------------

<a
href="https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/auto.py#L348"
target="_blank" style={{ float: "right", fontSize: "smaller" }}>source</a>

### AutoDeepAR

> ``` text
>  AutoDeepAR (h, loss=DistributionLoss(), valid_loss=MQLoss(), config=None,
>              search_alg=<ray.tune.search.basic_variant.BasicVariantGenerat
>              or object at 0x7fc41ed38af0>, num_samples=10,
>              refit_with_val=False, cpus=4, gpus=0, verbose=False,
>              alias=None, backend='ray', callbacks=None)
> ```

\*Class for Automatic Hyperparameter Optimization, it builds on top of
`ray` to give access to a wide variety of hyperparameter optimization
tools ranging from classic grid search, to Bayesian optimization and
HyperBand algorithm.

The validation loss to be optimized is defined by the `config['loss']`
dictionary value, the config also contains the rest of the
hyperparameter search space.

It is important to note that the success of this hyperparameter
optimization heavily relies on a strong correlation between the
validation and test periods.\*

|  | **Type** | **Default** | **Details** |
|------|------------------|-------------------------|-------------------------|
| h | int |  | Forecast horizon |
| loss | DistributionLoss | DistributionLoss() | Instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| valid_loss | MQLoss | MQLoss() | Instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| config | NoneType | None | Dictionary with ray.tune defined search space or function that takes an optuna trial and returns a configuration dict. |
| search_alg | BasicVariantGenerator | \<ray.tune.search.basic_variant.BasicVariantGenerator object at 0x7fc41ed38af0\> | For ray see https://docs.ray.io/en/latest/tune/api_docs/suggestion.html<br/>For optuna see https://optuna.readthedocs.io/en/stable/reference/samplers/index.html. |
| num_samples | int | 10 | Number of hyperparameter optimization steps/samples. |
| refit_with_val | bool | False | Refit of best model should preserve val_size. |
| cpus | int | 4 | Number of cpus to use during optimization. Only used with ray tune. |
| gpus | int | 0 | Number of gpus to use during optimization, default all available. Only used with ray tune. |
| verbose | bool | False | Track progress. |
| alias | NoneType | None | Custom name of the model. |
| backend | str | ray | Backend to use for searching the hyperparameter space, can be either ‘ray’ or ‘optuna’. |
| callbacks | NoneType | None | List of functions to call during the optimization process.<br/>ray reference: https://docs.ray.io/en/latest/tune/tutorials/tune-metrics.html<br/>optuna reference: https://optuna.readthedocs.io/en/stable/tutorial/20_recipes/007_optuna_callback.html |

```python
# Use your own config or AutoDeepAR.default_config
config = dict(max_steps=1, val_check_steps=1, input_size=12, lstm_hidden_size=8)
model = AutoDeepAR(h=12, config=config, num_samples=1, cpus=1)

# Fit and predict
model.fit(dataset=dataset)
y_hat = model.predict(dataset=dataset)

# Optuna
model = AutoDeepAR(h=12, config=None, backend='optuna')
```

------------------------------------------------------------------------

<a
href="https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/auto.py#L419"
target="_blank" style={{ float: "right", fontSize: "smaller" }}>source</a>

### AutoDilatedRNN

> ``` text
>  AutoDilatedRNN (h, loss=MAE(), valid_loss=None, config=None,
>                  search_alg=<ray.tune.search.basic_variant.BasicVariantGen
>                  erator object at 0x7fc41ffe5870>, num_samples=10,
>                  refit_with_val=False, cpus=4, gpus=0, verbose=False,
>                  alias=None, backend='ray', callbacks=None)
> ```

\*Class for Automatic Hyperparameter Optimization, it builds on top of
`ray` to give access to a wide variety of hyperparameter optimization
tools ranging from classic grid search, to Bayesian optimization and
HyperBand algorithm.

The validation loss to be optimized is defined by the `config['loss']`
dictionary value, the config also contains the rest of the
hyperparameter search space.

It is important to note that the success of this hyperparameter
optimization heavily relies on a strong correlation between the
validation and test periods.\*

|  | **Type** | **Default** | **Details** |
|------|------------------|-------------------------|-------------------------|
| h | int |  | Forecast horizon |
| loss | MAE | MAE() | Instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| valid_loss | NoneType | None | Instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| config | NoneType | None | Dictionary with ray.tune defined search space or function that takes an optuna trial and returns a configuration dict. |
| search_alg | BasicVariantGenerator | \<ray.tune.search.basic_variant.BasicVariantGenerator object at 0x7fc41ffe5870\> | For ray see https://docs.ray.io/en/latest/tune/api_docs/suggestion.html<br/>For optuna see https://optuna.readthedocs.io/en/stable/reference/samplers/index.html. |
| num_samples | int | 10 | Number of hyperparameter optimization steps/samples. |
| refit_with_val | bool | False | Refit of best model should preserve val_size. |
| cpus | int | 4 | Number of cpus to use during optimization. Only used with ray tune. |
| gpus | int | 0 | Number of gpus to use during optimization, default all available. Only used with ray tune. |
| verbose | bool | False | Track progress. |
| alias | NoneType | None | Custom name of the model. |
| backend | str | ray | Backend to use for searching the hyperparameter space, can be either ‘ray’ or ‘optuna’. |
| callbacks | NoneType | None | List of functions to call during the optimization process.<br/>ray reference: https://docs.ray.io/en/latest/tune/tutorials/tune-metrics.html<br/>optuna reference: https://optuna.readthedocs.io/en/stable/tutorial/20_recipes/007_optuna_callback.html |

```python
# Use your own config or AutoDilatedRNN.default_config
config = dict(max_steps=1, val_check_steps=1, input_size=-1, encoder_hidden_size=8)
model = AutoDilatedRNN(h=12, config=config, num_samples=1, cpus=1)

# Fit and predict
model.fit(dataset=dataset)
y_hat = model.predict(dataset=dataset)

# Optuna
model = AutoDilatedRNN(h=12, config=None, backend='optuna')
```

------------------------------------------------------------------------

<a
href="https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/auto.py#L491"
target="_blank" style={{ float: "right", fontSize: "smaller" }}>source</a>

### AutoBiTCN

> ``` text
>  AutoBiTCN (h, loss=MAE(), valid_loss=None, config=None,
>             search_alg=<ray.tune.search.basic_variant.BasicVariantGenerato
>             r object at 0x7fc41ffbd810>, num_samples=10,
>             refit_with_val=False, cpus=4, gpus=0, verbose=False,
>             alias=None, backend='ray', callbacks=None)
> ```

\*Class for Automatic Hyperparameter Optimization, it builds on top of
`ray` to give access to a wide variety of hyperparameter optimization
tools ranging from classic grid search, to Bayesian optimization and
HyperBand algorithm.

The validation loss to be optimized is defined by the `config['loss']`
dictionary value, the config also contains the rest of the
hyperparameter search space.

It is important to note that the success of this hyperparameter
optimization heavily relies on a strong correlation between the
validation and test periods.\*

|  | **Type** | **Default** | **Details** |
|------|------------------|-------------------------|-------------------------|
| h | int |  | Forecast horizon |
| loss | MAE | MAE() | Instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| valid_loss | NoneType | None | Instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| config | NoneType | None | Dictionary with ray.tune defined search space or function that takes an optuna trial and returns a configuration dict. |
| search_alg | BasicVariantGenerator | \<ray.tune.search.basic_variant.BasicVariantGenerator object at 0x7fc41ffbd810\> | For ray see https://docs.ray.io/en/latest/tune/api_docs/suggestion.html<br/>For optuna see https://optuna.readthedocs.io/en/stable/reference/samplers/index.html. |
| num_samples | int | 10 | Number of hyperparameter optimization steps/samples. |
| refit_with_val | bool | False | Refit of best model should preserve val_size. |
| cpus | int | 4 | Number of cpus to use during optimization. Only used with ray tune. |
| gpus | int | 0 | Number of gpus to use during optimization, default all available. Only used with ray tune. |
| verbose | bool | False | Track progress. |
| alias | NoneType | None | Custom name of the model. |
| backend | str | ray | Backend to use for searching the hyperparameter space, can be either ‘ray’ or ‘optuna’. |
| callbacks | NoneType | None | List of functions to call during the optimization process.<br/>ray reference: https://docs.ray.io/en/latest/tune/tutorials/tune-metrics.html<br/>optuna reference: https://optuna.readthedocs.io/en/stable/tutorial/20_recipes/007_optuna_callback.html |

```python
# Use your own config or AutoBiTCN.default_config
config = dict(max_steps=1, val_check_steps=1, input_size=12, hidden_size=8)
model = AutoBiTCN(h=12, config=config, num_samples=1, cpus=1)

# Fit and predict
model.fit(dataset=dataset)
y_hat = model.predict(dataset=dataset)

# Optuna
model = AutoBiTCN(h=12, config=None, backend='optuna')
```

## B. MLP-Based

------------------------------------------------------------------------

<a
href="https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/auto.py#L559"
target="_blank" style={{ float: "right", fontSize: "smaller" }}>source</a>

### AutoMLP

> ``` text
>  AutoMLP (h, loss=MAE(), valid_loss=None, config=None,
>           search_alg=<ray.tune.search.basic_variant.BasicVariantGenerator
>           object at 0x7fc51b76a2c0>, num_samples=10, refit_with_val=False,
>           cpus=4, gpus=0, verbose=False, alias=None, backend='ray',
>           callbacks=None)
> ```

\*Class for Automatic Hyperparameter Optimization, it builds on top of
`ray` to give access to a wide variety of hyperparameter optimization
tools ranging from classic grid search, to Bayesian optimization and
HyperBand algorithm.

The validation loss to be optimized is defined by the `config['loss']`
dictionary value, the config also contains the rest of the
hyperparameter search space.

It is important to note that the success of this hyperparameter
optimization heavily relies on a strong correlation between the
validation and test periods.\*

|  | **Type** | **Default** | **Details** |
|------|------------------|-------------------------|-------------------------|
| h | int |  | Forecast horizon |
| loss | MAE | MAE() | Instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| valid_loss | NoneType | None | Instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| config | NoneType | None | Dictionary with ray.tune defined search space or function that takes an optuna trial and returns a configuration dict. |
| search_alg | BasicVariantGenerator | \<ray.tune.search.basic_variant.BasicVariantGenerator object at 0x7fc51b76a2c0\> | For ray see https://docs.ray.io/en/latest/tune/api_docs/suggestion.html<br/>For optuna see https://optuna.readthedocs.io/en/stable/reference/samplers/index.html. |
| num_samples | int | 10 | Number of hyperparameter optimization steps/samples. |
| refit_with_val | bool | False | Refit of best model should preserve val_size. |
| cpus | int | 4 | Number of cpus to use during optimization. Only used with ray tune. |
| gpus | int | 0 | Number of gpus to use during optimization, default all available. Only used with ray tune. |
| verbose | bool | False | Track progress. |
| alias | NoneType | None | Custom name of the model. |
| backend | str | ray | Backend to use for searching the hyperparameter space, can be either ‘ray’ or ‘optuna’. |
| callbacks | NoneType | None | List of functions to call during the optimization process.<br/>ray reference: https://docs.ray.io/en/latest/tune/tutorials/tune-metrics.html<br/>optuna reference: https://optuna.readthedocs.io/en/stable/tutorial/20_recipes/007_optuna_callback.html |

```python
# Use your own config or AutoMLP.default_config
config = dict(max_steps=1, val_check_steps=1, input_size=12, hidden_size=8)
model = AutoMLP(h=12, config=config, num_samples=1, cpus=1)

# Fit and predict
model.fit(dataset=dataset)
y_hat = model.predict(dataset=dataset)

# Optuna
model = AutoMLP(h=12, config=None, backend='optuna')
```

------------------------------------------------------------------------

<a
href="https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/auto.py#L627"
target="_blank" style={{ float: "right", fontSize: "smaller" }}>source</a>

### AutoNBEATS

> ``` text
>  AutoNBEATS (h, loss=MAE(), valid_loss=None, config=None,
>              search_alg=<ray.tune.search.basic_variant.BasicVariantGenerat
>              or object at 0x7fc41ffa0730>, num_samples=10,
>              refit_with_val=False, cpus=4, gpus=0, verbose=False,
>              alias=None, backend='ray', callbacks=None)
> ```

\*Class for Automatic Hyperparameter Optimization, it builds on top of
`ray` to give access to a wide variety of hyperparameter optimization
tools ranging from classic grid search, to Bayesian optimization and
HyperBand algorithm.

The validation loss to be optimized is defined by the `config['loss']`
dictionary value, the config also contains the rest of the
hyperparameter search space.

It is important to note that the success of this hyperparameter
optimization heavily relies on a strong correlation between the
validation and test periods.\*

|  | **Type** | **Default** | **Details** |
|------|------------------|-------------------------|-------------------------|
| h | int |  | Forecast horizon |
| loss | MAE | MAE() | Instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| valid_loss | NoneType | None | Instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| config | NoneType | None | Dictionary with ray.tune defined search space or function that takes an optuna trial and returns a configuration dict. |
| search_alg | BasicVariantGenerator | \<ray.tune.search.basic_variant.BasicVariantGenerator object at 0x7fc41ffa0730\> | For ray see https://docs.ray.io/en/latest/tune/api_docs/suggestion.html<br/>For optuna see https://optuna.readthedocs.io/en/stable/reference/samplers/index.html. |
| num_samples | int | 10 | Number of hyperparameter optimization steps/samples. |
| refit_with_val | bool | False | Refit of best model should preserve val_size. |
| cpus | int | 4 | Number of cpus to use during optimization. Only used with ray tune. |
| gpus | int | 0 | Number of gpus to use during optimization, default all available. Only used with ray tune. |
| verbose | bool | False | Track progress. |
| alias | NoneType | None | Custom name of the model. |
| backend | str | ray | Backend to use for searching the hyperparameter space, can be either ‘ray’ or ‘optuna’. |
| callbacks | NoneType | None | List of functions to call during the optimization process.<br/>ray reference: https://docs.ray.io/en/latest/tune/tutorials/tune-metrics.html<br/>optuna reference: https://optuna.readthedocs.io/en/stable/tutorial/20_recipes/007_optuna_callback.html |

```python
# Use your own config or AutoNBEATS.default_config
config = dict(max_steps=1, val_check_steps=1, input_size=12,
              mlp_units=3*[[8, 8]])
model = AutoNBEATS(h=12, config=config, num_samples=1, cpus=1)

# Fit and predict
model.fit(dataset=dataset)
y_hat = model.predict(dataset=dataset)

# Optuna
model = AutoNBEATS(h=12, config=None, backend='optuna')
```

------------------------------------------------------------------------

<a
href="https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/auto.py#L693"
target="_blank" style={{ float: "right", fontSize: "smaller" }}>source</a>

### AutoNBEATSx

> ``` text
>  AutoNBEATSx (h, loss=MAE(), valid_loss=None, config=None,
>               search_alg=<ray.tune.search.basic_variant.BasicVariantGenera
>               tor object at 0x7fc41ed2aa40>, num_samples=10,
>               refit_with_val=False, cpus=4, gpus=0, verbose=False,
>               alias=None, backend='ray', callbacks=None)
> ```

\*Class for Automatic Hyperparameter Optimization, it builds on top of
`ray` to give access to a wide variety of hyperparameter optimization
tools ranging from classic grid search, to Bayesian optimization and
HyperBand algorithm.

The validation loss to be optimized is defined by the `config['loss']`
dictionary value, the config also contains the rest of the
hyperparameter search space.

It is important to note that the success of this hyperparameter
optimization heavily relies on a strong correlation between the
validation and test periods.\*

|  | **Type** | **Default** | **Details** |
|------|------------------|-------------------------|-------------------------|
| h | int |  | Forecast horizon |
| loss | MAE | MAE() | Instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| valid_loss | NoneType | None | Instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| config | NoneType | None | Dictionary with ray.tune defined search space or function that takes an optuna trial and returns a configuration dict. |
| search_alg | BasicVariantGenerator | \<ray.tune.search.basic_variant.BasicVariantGenerator object at 0x7fc41ed2aa40\> | For ray see https://docs.ray.io/en/latest/tune/api_docs/suggestion.html<br/>For optuna see https://optuna.readthedocs.io/en/stable/reference/samplers/index.html. |
| num_samples | int | 10 | Number of hyperparameter optimization steps/samples. |
| refit_with_val | bool | False | Refit of best model should preserve val_size. |
| cpus | int | 4 | Number of cpus to use during optimization. Only used with ray tune. |
| gpus | int | 0 | Number of gpus to use during optimization, default all available. Only used with ray tune. |
| verbose | bool | False | Track progress. |
| alias | NoneType | None | Custom name of the model. |
| backend | str | ray | Backend to use for searching the hyperparameter space, can be either ‘ray’ or ‘optuna’. |
| callbacks | NoneType | None | List of functions to call during the optimization process.<br/>ray reference: https://docs.ray.io/en/latest/tune/tutorials/tune-metrics.html<br/>optuna reference: https://optuna.readthedocs.io/en/stable/tutorial/20_recipes/007_optuna_callback.html |

```python
# Use your own config or AutoNBEATSx.default_config
config = dict(max_steps=1, val_check_steps=1, input_size=12,
              mlp_units=3*[[8, 8]])
model = AutoNBEATSx(h=12, config=config, num_samples=1, cpus=1)

# Fit and predict
model.fit(dataset=dataset)
y_hat = model.predict(dataset=dataset)

# Optuna
model = AutoNBEATSx(h=12, config=None, backend='optuna')
```

------------------------------------------------------------------------

<a
href="https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/auto.py#L759"
target="_blank" style={{ float: "right", fontSize: "smaller" }}>source</a>

### AutoNHITS

> ``` text
>  AutoNHITS (h, loss=MAE(), valid_loss=None, config=None,
>             search_alg=<ray.tune.search.basic_variant.BasicVariantGenerato
>             r object at 0x7fc41ffdba90>, num_samples=10,
>             refit_with_val=False, cpus=4, gpus=0, verbose=False,
>             alias=None, backend='ray', callbacks=None)
> ```

\*Class for Automatic Hyperparameter Optimization, it builds on top of
`ray` to give access to a wide variety of hyperparameter optimization
tools ranging from classic grid search, to Bayesian optimization and
HyperBand algorithm.

The validation loss to be optimized is defined by the `config['loss']`
dictionary value, the config also contains the rest of the
hyperparameter search space.

It is important to note that the success of this hyperparameter
optimization heavily relies on a strong correlation between the
validation and test periods.\*

|  | **Type** | **Default** | **Details** |
|------|------------------|-------------------------|-------------------------|
| h | int |  | Forecast horizon |
| loss | MAE | MAE() | Instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| valid_loss | NoneType | None | Instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| config | NoneType | None | Dictionary with ray.tune defined search space or function that takes an optuna trial and returns a configuration dict. |
| search_alg | BasicVariantGenerator | \<ray.tune.search.basic_variant.BasicVariantGenerator object at 0x7fc41ffdba90\> | For ray see https://docs.ray.io/en/latest/tune/api_docs/suggestion.html<br/>For optuna see https://optuna.readthedocs.io/en/stable/reference/samplers/index.html. |
| num_samples | int | 10 | Number of hyperparameter optimization steps/samples. |
| refit_with_val | bool | False | Refit of best model should preserve val_size. |
| cpus | int | 4 | Number of cpus to use during optimization. Only used with ray tune. |
| gpus | int | 0 | Number of gpus to use during optimization, default all available. Only used with ray tune. |
| verbose | bool | False | Track progress. |
| alias | NoneType | None | Custom name of the model. |
| backend | str | ray | Backend to use for searching the hyperparameter space, can be either ‘ray’ or ‘optuna’. |
| callbacks | NoneType | None | List of functions to call during the optimization process.<br/>ray reference: https://docs.ray.io/en/latest/tune/tutorials/tune-metrics.html<br/>optuna reference: https://optuna.readthedocs.io/en/stable/tutorial/20_recipes/007_optuna_callback.html |

```python
# Use your own config or AutoNHITS.default_config
config = dict(max_steps=1, val_check_steps=1, input_size=12, 
              mlp_units=3 * [[8, 8]])
model = AutoNHITS(h=12, config=config, num_samples=1, cpus=1)

# Fit and predict
model.fit(dataset=dataset)
y_hat = model.predict(dataset=dataset)

# Optuna
model = AutoNHITS(h=12, config=None, backend='optuna')
```

------------------------------------------------------------------------

<a
href="https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/auto.py#L838"
target="_blank" style={{ float: "right", fontSize: "smaller" }}>source</a>

### AutoDLinear

> ``` text
>  AutoDLinear (h, loss=MAE(), valid_loss=None, config=None,
>               search_alg=<ray.tune.search.basic_variant.BasicVariantGenera
>               tor object at 0x7fc41ffa0ee0>, num_samples=10,
>               refit_with_val=False, cpus=4, gpus=0, verbose=False,
>               alias=None, backend='ray', callbacks=None)
> ```

\*Class for Automatic Hyperparameter Optimization, it builds on top of
`ray` to give access to a wide variety of hyperparameter optimization
tools ranging from classic grid search, to Bayesian optimization and
HyperBand algorithm.

The validation loss to be optimized is defined by the `config['loss']`
dictionary value, the config also contains the rest of the
hyperparameter search space.

It is important to note that the success of this hyperparameter
optimization heavily relies on a strong correlation between the
validation and test periods.\*

|  | **Type** | **Default** | **Details** |
|------|------------------|-------------------------|-------------------------|
| h | int |  | Forecast horizon |
| loss | MAE | MAE() | Instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| valid_loss | NoneType | None | Instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| config | NoneType | None | Dictionary with ray.tune defined search space or function that takes an optuna trial and returns a configuration dict. |
| search_alg | BasicVariantGenerator | \<ray.tune.search.basic_variant.BasicVariantGenerator object at 0x7fc41ffa0ee0\> | For ray see https://docs.ray.io/en/latest/tune/api_docs/suggestion.html<br/>For optuna see https://optuna.readthedocs.io/en/stable/reference/samplers/index.html. |
| num_samples | int | 10 | Number of hyperparameter optimization steps/samples. |
| refit_with_val | bool | False | Refit of best model should preserve val_size. |
| cpus | int | 4 | Number of cpus to use during optimization. Only used with ray tune. |
| gpus | int | 0 | Number of gpus to use during optimization, default all available. Only used with ray tune. |
| verbose | bool | False | Track progress. |
| alias | NoneType | None | Custom name of the model. |
| backend | str | ray | Backend to use for searching the hyperparameter space, can be either ‘ray’ or ‘optuna’. |
| callbacks | NoneType | None | List of functions to call during the optimization process.<br/>ray reference: https://docs.ray.io/en/latest/tune/tutorials/tune-metrics.html<br/>optuna reference: https://optuna.readthedocs.io/en/stable/tutorial/20_recipes/007_optuna_callback.html |

```python
# Use your own config or AutoDLinear.default_config
config = dict(max_steps=1, val_check_steps=1, input_size=12)
model = AutoDLinear(h=12, config=config, num_samples=1, cpus=1)

# Fit and predict
model.fit(dataset=dataset)
y_hat = model.predict(dataset=dataset)

# Optuna
model = AutoDLinear(h=12, config=None, backend='optuna')
```

------------------------------------------------------------------------

<a
href="https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/auto.py#L905"
target="_blank" style={{ float: "right", fontSize: "smaller" }}>source</a>

### AutoNLinear

> ``` text
>  AutoNLinear (h, loss=MAE(), valid_loss=None, config=None,
>               search_alg=<ray.tune.search.basic_variant.BasicVariantGenera
>               tor object at 0x7fc4243b94e0>, num_samples=10,
>               refit_with_val=False, cpus=4, gpus=0, verbose=False,
>               alias=None, backend='ray', callbacks=None)
> ```

\*Class for Automatic Hyperparameter Optimization, it builds on top of
`ray` to give access to a wide variety of hyperparameter optimization
tools ranging from classic grid search, to Bayesian optimization and
HyperBand algorithm.

The validation loss to be optimized is defined by the `config['loss']`
dictionary value, the config also contains the rest of the
hyperparameter search space.

It is important to note that the success of this hyperparameter
optimization heavily relies on a strong correlation between the
validation and test periods.\*

|  | **Type** | **Default** | **Details** |
|------|------------------|-------------------------|-------------------------|
| h | int |  | Forecast horizon |
| loss | MAE | MAE() | Instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| valid_loss | NoneType | None | Instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| config | NoneType | None | Dictionary with ray.tune defined search space or function that takes an optuna trial and returns a configuration dict. |
| search_alg | BasicVariantGenerator | \<ray.tune.search.basic_variant.BasicVariantGenerator object at 0x7fc4243b94e0\> | For ray see https://docs.ray.io/en/latest/tune/api_docs/suggestion.html<br/>For optuna see https://optuna.readthedocs.io/en/stable/reference/samplers/index.html. |
| num_samples | int | 10 | Number of hyperparameter optimization steps/samples. |
| refit_with_val | bool | False | Refit of best model should preserve val_size. |
| cpus | int | 4 | Number of cpus to use during optimization. Only used with ray tune. |
| gpus | int | 0 | Number of gpus to use during optimization, default all available. Only used with ray tune. |
| verbose | bool | False | Track progress. |
| alias | NoneType | None | Custom name of the model. |
| backend | str | ray | Backend to use for searching the hyperparameter space, can be either ‘ray’ or ‘optuna’. |
| callbacks | NoneType | None | List of functions to call during the optimization process.<br/>ray reference: https://docs.ray.io/en/latest/tune/tutorials/tune-metrics.html<br/>optuna reference: https://optuna.readthedocs.io/en/stable/tutorial/20_recipes/007_optuna_callback.html |

```python
# Use your own config or AutoNLinear.default_config
config = dict(max_steps=1, val_check_steps=1, input_size=12)
model = AutoNLinear(h=12, config=config, num_samples=1, cpus=1)

# Fit and predict
model.fit(dataset=dataset)
y_hat = model.predict(dataset=dataset)

# Optuna
model = AutoNLinear(h=12, config=None, backend='optuna')
```

------------------------------------------------------------------------

<a
href="https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/auto.py#L971"
target="_blank" style={{ float: "right", fontSize: "smaller" }}>source</a>

### AutoTiDE

> ``` text
>  AutoTiDE (h, loss=MAE(), valid_loss=None, config=None,
>            search_alg=<ray.tune.search.basic_variant.BasicVariantGenerator
>            object at 0x7fc4242500a0>, num_samples=10,
>            refit_with_val=False, cpus=4, gpus=0, verbose=False,
>            alias=None, backend='ray', callbacks=None)
> ```

\*Class for Automatic Hyperparameter Optimization, it builds on top of
`ray` to give access to a wide variety of hyperparameter optimization
tools ranging from classic grid search, to Bayesian optimization and
HyperBand algorithm.

The validation loss to be optimized is defined by the `config['loss']`
dictionary value, the config also contains the rest of the
hyperparameter search space.

It is important to note that the success of this hyperparameter
optimization heavily relies on a strong correlation between the
validation and test periods.\*

|  | **Type** | **Default** | **Details** |
|------|------------------|-------------------------|-------------------------|
| h | int |  | Forecast horizon |
| loss | MAE | MAE() | Instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| valid_loss | NoneType | None | Instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| config | NoneType | None | Dictionary with ray.tune defined search space or function that takes an optuna trial and returns a configuration dict. |
| search_alg | BasicVariantGenerator | \<ray.tune.search.basic_variant.BasicVariantGenerator object at 0x7fc4242500a0\> | For ray see https://docs.ray.io/en/latest/tune/api_docs/suggestion.html<br/>For optuna see https://optuna.readthedocs.io/en/stable/reference/samplers/index.html. |
| num_samples | int | 10 | Number of hyperparameter optimization steps/samples. |
| refit_with_val | bool | False | Refit of best model should preserve val_size. |
| cpus | int | 4 | Number of cpus to use during optimization. Only used with ray tune. |
| gpus | int | 0 | Number of gpus to use during optimization, default all available. Only used with ray tune. |
| verbose | bool | False | Track progress. |
| alias | NoneType | None | Custom name of the model. |
| backend | str | ray | Backend to use for searching the hyperparameter space, can be either ‘ray’ or ‘optuna’. |
| callbacks | NoneType | None | List of functions to call during the optimization process.<br/>ray reference: https://docs.ray.io/en/latest/tune/tutorials/tune-metrics.html<br/>optuna reference: https://optuna.readthedocs.io/en/stable/tutorial/20_recipes/007_optuna_callback.html |

```python
# Use your own config or AutoTiDE.default_config
config = dict(max_steps=1, val_check_steps=1, input_size=12)
model = AutoTiDE(h=12, config=config, num_samples=1, cpus=1)

# Fit and predict
model.fit(dataset=dataset)
y_hat = model.predict(dataset=dataset)

# Optuna
model = AutoTiDE(h=12, config=None, backend='optuna')
```

------------------------------------------------------------------------

<a
href="https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/auto.py#L1045"
target="_blank" style={{ float: "right", fontSize: "smaller" }}>source</a>

### AutoDeepNPTS

> ``` text
>  AutoDeepNPTS (h, loss=MAE(), valid_loss=None, config=None,
>                search_alg=<ray.tune.search.basic_variant.BasicVariantGener
>                ator object at 0x7fc41ff70df0>, num_samples=10,
>                refit_with_val=False, cpus=4, gpus=0, verbose=False,
>                alias=None, backend='ray', callbacks=None)
> ```

\*Class for Automatic Hyperparameter Optimization, it builds on top of
`ray` to give access to a wide variety of hyperparameter optimization
tools ranging from classic grid search, to Bayesian optimization and
HyperBand algorithm.

The validation loss to be optimized is defined by the `config['loss']`
dictionary value, the config also contains the rest of the
hyperparameter search space.

It is important to note that the success of this hyperparameter
optimization heavily relies on a strong correlation between the
validation and test periods.\*

|  | **Type** | **Default** | **Details** |
|------|------------------|-------------------------|-------------------------|
| h | int |  | Forecast horizon |
| loss | MAE | MAE() | Instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| valid_loss | NoneType | None | Instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| config | NoneType | None | Dictionary with ray.tune defined search space or function that takes an optuna trial and returns a configuration dict. |
| search_alg | BasicVariantGenerator | \<ray.tune.search.basic_variant.BasicVariantGenerator object at 0x7fc41ff70df0\> | For ray see https://docs.ray.io/en/latest/tune/api_docs/suggestion.html<br/>For optuna see https://optuna.readthedocs.io/en/stable/reference/samplers/index.html. |
| num_samples | int | 10 | Number of hyperparameter optimization steps/samples. |
| refit_with_val | bool | False | Refit of best model should preserve val_size. |
| cpus | int | 4 | Number of cpus to use during optimization. Only used with ray tune. |
| gpus | int | 0 | Number of gpus to use during optimization, default all available. Only used with ray tune. |
| verbose | bool | False | Track progress. |
| alias | NoneType | None | Custom name of the model. |
| backend | str | ray | Backend to use for searching the hyperparameter space, can be either ‘ray’ or ‘optuna’. |
| callbacks | NoneType | None | List of functions to call during the optimization process.<br/>ray reference: https://docs.ray.io/en/latest/tune/tutorials/tune-metrics.html<br/>optuna reference: https://optuna.readthedocs.io/en/stable/tutorial/20_recipes/007_optuna_callback.html |

```python
# Use your own config or AutoDeepNPTS.default_config
config = dict(max_steps=1, val_check_steps=1, input_size=12)
model = AutoDeepNPTS(h=12, config=config, num_samples=1, cpus=1)

# Fit and predict
model.fit(dataset=dataset)
y_hat = model.predict(dataset=dataset)

# Optuna
model = AutoDeepNPTS(h=12, config=None, backend='optuna')
```

## C. KAN-Based

------------------------------------------------------------------------

<a
href="https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/auto.py#L1114"
target="_blank" style={{ float: "right", fontSize: "smaller" }}>source</a>

### AutoKAN

> ``` text
>  AutoKAN (h, loss=MAE(), valid_loss=None, config=None,
>           search_alg=<ray.tune.search.basic_variant.BasicVariantGenerator
>           object at 0x7fc41ff82800>, num_samples=10, refit_with_val=False,
>           cpus=4, gpus=0, verbose=False, alias=None, backend='ray',
>           callbacks=None)
> ```

\*Class for Automatic Hyperparameter Optimization, it builds on top of
`ray` to give access to a wide variety of hyperparameter optimization
tools ranging from classic grid search, to Bayesian optimization and
HyperBand algorithm.

The validation loss to be optimized is defined by the `config['loss']`
dictionary value, the config also contains the rest of the
hyperparameter search space.

It is important to note that the success of this hyperparameter
optimization heavily relies on a strong correlation between the
validation and test periods.\*

|  | **Type** | **Default** | **Details** |
|------|------------------|-------------------------|-------------------------|
| h | int |  | Forecast horizon |
| loss | MAE | MAE() | Instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| valid_loss | NoneType | None | Instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| config | NoneType | None | Dictionary with ray.tune defined search space or function that takes an optuna trial and returns a configuration dict. |
| search_alg | BasicVariantGenerator | \<ray.tune.search.basic_variant.BasicVariantGenerator object at 0x7fc41ff82800\> | For ray see https://docs.ray.io/en/latest/tune/api_docs/suggestion.html<br/>For optuna see https://optuna.readthedocs.io/en/stable/reference/samplers/index.html. |
| num_samples | int | 10 | Number of hyperparameter optimization steps/samples. |
| refit_with_val | bool | False | Refit of best model should preserve val_size. |
| cpus | int | 4 | Number of cpus to use during optimization. Only used with ray tune. |
| gpus | int | 0 | Number of gpus to use during optimization, default all available. Only used with ray tune. |
| verbose | bool | False | Track progress. |
| alias | NoneType | None | Custom name of the model. |
| backend | str | ray | Backend to use for searching the hyperparameter space, can be either ‘ray’ or ‘optuna’. |
| callbacks | NoneType | None | List of functions to call during the optimization process.<br/>ray reference: https://docs.ray.io/en/latest/tune/tutorials/tune-metrics.html<br/>optuna reference: https://optuna.readthedocs.io/en/stable/tutorial/20_recipes/007_optuna_callback.html |

```python
# Use your own config or AutoKAN.default_config
config = dict(max_steps=1, val_check_steps=1, input_size=12)
model = AutoKAN(h=12, config=config, num_samples=1, cpus=1)

# Fit and predict
model.fit(dataset=dataset)
y_hat = model.predict(dataset=dataset)

# Optuna
model = AutoKAN(h=12, config=None, backend='optuna')
```

## D. Transformer-Based

------------------------------------------------------------------------

<a
href="https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/auto.py#L1183"
target="_blank" style={{ float: "right", fontSize: "smaller" }}>source</a>

### AutoTFT

> ``` text
>  AutoTFT (h, loss=MAE(), valid_loss=None, config=None,
>           search_alg=<ray.tune.search.basic_variant.BasicVariantGenerator
>           object at 0x7fc4241587c0>, num_samples=10, refit_with_val=False,
>           cpus=4, gpus=0, verbose=False, alias=None, backend='ray',
>           callbacks=None)
> ```

\*Class for Automatic Hyperparameter Optimization, it builds on top of
`ray` to give access to a wide variety of hyperparameter optimization
tools ranging from classic grid search, to Bayesian optimization and
HyperBand algorithm.

The validation loss to be optimized is defined by the `config['loss']`
dictionary value, the config also contains the rest of the
hyperparameter search space.

It is important to note that the success of this hyperparameter
optimization heavily relies on a strong correlation between the
validation and test periods.\*

|  | **Type** | **Default** | **Details** |
|------|------------------|-------------------------|-------------------------|
| h | int |  | Forecast horizon |
| loss | MAE | MAE() | Instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| valid_loss | NoneType | None | Instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| config | NoneType | None | Dictionary with ray.tune defined search space or function that takes an optuna trial and returns a configuration dict. |
| search_alg | BasicVariantGenerator | \<ray.tune.search.basic_variant.BasicVariantGenerator object at 0x7fc4241587c0\> | For ray see https://docs.ray.io/en/latest/tune/api_docs/suggestion.html<br/>For optuna see https://optuna.readthedocs.io/en/stable/reference/samplers/index.html. |
| num_samples | int | 10 | Number of hyperparameter optimization steps/samples. |
| refit_with_val | bool | False | Refit of best model should preserve val_size. |
| cpus | int | 4 | Number of cpus to use during optimization. Only used with ray tune. |
| gpus | int | 0 | Number of gpus to use during optimization, default all available. Only used with ray tune. |
| verbose | bool | False | Track progress. |
| alias | NoneType | None | Custom name of the model. |
| backend | str | ray | Backend to use for searching the hyperparameter space, can be either ‘ray’ or ‘optuna’. |
| callbacks | NoneType | None | List of functions to call during the optimization process.<br/>ray reference: https://docs.ray.io/en/latest/tune/tutorials/tune-metrics.html<br/>optuna reference: https://optuna.readthedocs.io/en/stable/tutorial/20_recipes/007_optuna_callback.html |

```python
# Use your own config or AutoTFT.default_config
config = dict(max_steps=1, val_check_steps=1, input_size=12, hidden_size=8)
model = AutoTFT(h=12, config=config, num_samples=1, cpus=1)

# Fit and predict
model.fit(dataset=dataset)
y_hat = model.predict(dataset=dataset)

# Optuna
model = AutoTFT(h=12, config=None, backend='optuna')
```

------------------------------------------------------------------------

<a
href="https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/auto.py#L1251"
target="_blank" style={{ float: "right", fontSize: "smaller" }}>source</a>

### AutoVanillaTransformer

> ``` text
>  AutoVanillaTransformer (h, loss=MAE(), valid_loss=None, config=None,
>                          search_alg=<ray.tune.search.basic_variant.BasicVa
>                          riantGenerator object at 0x7fc41ff81000>,
>                          num_samples=10, refit_with_val=False, cpus=4,
>                          gpus=0, verbose=False, alias=None, backend='ray',
>                          callbacks=None)
> ```

\*Class for Automatic Hyperparameter Optimization, it builds on top of
`ray` to give access to a wide variety of hyperparameter optimization
tools ranging from classic grid search, to Bayesian optimization and
HyperBand algorithm.

The validation loss to be optimized is defined by the `config['loss']`
dictionary value, the config also contains the rest of the
hyperparameter search space.

It is important to note that the success of this hyperparameter
optimization heavily relies on a strong correlation between the
validation and test periods.\*

|  | **Type** | **Default** | **Details** |
|------|------------------|-------------------------|-------------------------|
| h | int |  | Forecast horizon |
| loss | MAE | MAE() | Instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| valid_loss | NoneType | None | Instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| config | NoneType | None | Dictionary with ray.tune defined search space or function that takes an optuna trial and returns a configuration dict. |
| search_alg | BasicVariantGenerator | \<ray.tune.search.basic_variant.BasicVariantGenerator object at 0x7fc41ff81000\> | For ray see https://docs.ray.io/en/latest/tune/api_docs/suggestion.html<br/>For optuna see https://optuna.readthedocs.io/en/stable/reference/samplers/index.html. |
| num_samples | int | 10 | Number of hyperparameter optimization steps/samples. |
| refit_with_val | bool | False | Refit of best model should preserve val_size. |
| cpus | int | 4 | Number of cpus to use during optimization. Only used with ray tune. |
| gpus | int | 0 | Number of gpus to use during optimization, default all available. Only used with ray tune. |
| verbose | bool | False | Track progress. |
| alias | NoneType | None | Custom name of the model. |
| backend | str | ray | Backend to use for searching the hyperparameter space, can be either ‘ray’ or ‘optuna’. |
| callbacks | NoneType | None | List of functions to call during the optimization process.<br/>ray reference: https://docs.ray.io/en/latest/tune/tutorials/tune-metrics.html<br/>optuna reference: https://optuna.readthedocs.io/en/stable/tutorial/20_recipes/007_optuna_callback.html |

```python
# Use your own config or AutoVanillaTransformer.default_config
config = dict(max_steps=1, val_check_steps=1, input_size=12, hidden_size=8)
model = AutoVanillaTransformer(h=12, config=config, num_samples=1, cpus=1)

# Fit and predict
model.fit(dataset=dataset)
y_hat = model.predict(dataset=dataset)

# Optuna
model = AutoVanillaTransformer(h=12, config=None, backend='optuna')
```

------------------------------------------------------------------------

<a
href="https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/auto.py#L1319"
target="_blank" style={{ float: "right", fontSize: "smaller" }}>source</a>

### AutoInformer

> ``` text
>  AutoInformer (h, loss=MAE(), valid_loss=None, config=None,
>                search_alg=<ray.tune.search.basic_variant.BasicVariantGener
>                ator object at 0x7fc41ff96200>, num_samples=10,
>                refit_with_val=False, cpus=4, gpus=0, verbose=False,
>                alias=None, backend='ray', callbacks=None)
> ```

\*Class for Automatic Hyperparameter Optimization, it builds on top of
`ray` to give access to a wide variety of hyperparameter optimization
tools ranging from classic grid search, to Bayesian optimization and
HyperBand algorithm.

The validation loss to be optimized is defined by the `config['loss']`
dictionary value, the config also contains the rest of the
hyperparameter search space.

It is important to note that the success of this hyperparameter
optimization heavily relies on a strong correlation between the
validation and test periods.\*

|  | **Type** | **Default** | **Details** |
|------|------------------|-------------------------|-------------------------|
| h | int |  | Forecast horizon |
| loss | MAE | MAE() | Instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| valid_loss | NoneType | None | Instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| config | NoneType | None | Dictionary with ray.tune defined search space or function that takes an optuna trial and returns a configuration dict. |
| search_alg | BasicVariantGenerator | \<ray.tune.search.basic_variant.BasicVariantGenerator object at 0x7fc41ff96200\> | For ray see https://docs.ray.io/en/latest/tune/api_docs/suggestion.html<br/>For optuna see https://optuna.readthedocs.io/en/stable/reference/samplers/index.html. |
| num_samples | int | 10 | Number of hyperparameter optimization steps/samples. |
| refit_with_val | bool | False | Refit of best model should preserve val_size. |
| cpus | int | 4 | Number of cpus to use during optimization. Only used with ray tune. |
| gpus | int | 0 | Number of gpus to use during optimization, default all available. Only used with ray tune. |
| verbose | bool | False | Track progress. |
| alias | NoneType | None | Custom name of the model. |
| backend | str | ray | Backend to use for searching the hyperparameter space, can be either ‘ray’ or ‘optuna’. |
| callbacks | NoneType | None | List of functions to call during the optimization process.<br/>ray reference: https://docs.ray.io/en/latest/tune/tutorials/tune-metrics.html<br/>optuna reference: https://optuna.readthedocs.io/en/stable/tutorial/20_recipes/007_optuna_callback.html |

```python
# Use your own config or AutoInformer.default_config
config = dict(max_steps=1, val_check_steps=1, input_size=12, hidden_size=8)
model = AutoInformer(h=12, config=config, num_samples=1, cpus=1)

# Fit and predict
model.fit(dataset=dataset)
y_hat = model.predict(dataset=dataset)

# Optuna
model = AutoInformer(h=12, config=None, backend='optuna')
```

------------------------------------------------------------------------

<a
href="https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/auto.py#L1387"
target="_blank" style={{ float: "right", fontSize: "smaller" }}>source</a>

### AutoAutoformer

> ``` text
>  AutoAutoformer (h, loss=MAE(), valid_loss=None, config=None,
>                  search_alg=<ray.tune.search.basic_variant.BasicVariantGen
>                  erator object at 0x7fc41ffbfee0>, num_samples=10,
>                  refit_with_val=False, cpus=4, gpus=0, verbose=False,
>                  alias=None, backend='ray', callbacks=None)
> ```

\*Class for Automatic Hyperparameter Optimization, it builds on top of
`ray` to give access to a wide variety of hyperparameter optimization
tools ranging from classic grid search, to Bayesian optimization and
HyperBand algorithm.

The validation loss to be optimized is defined by the `config['loss']`
dictionary value, the config also contains the rest of the
hyperparameter search space.

It is important to note that the success of this hyperparameter
optimization heavily relies on a strong correlation between the
validation and test periods.\*

|  | **Type** | **Default** | **Details** |
|------|------------------|-------------------------|-------------------------|
| h | int |  | Forecast horizon |
| loss | MAE | MAE() | Instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| valid_loss | NoneType | None | Instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| config | NoneType | None | Dictionary with ray.tune defined search space or function that takes an optuna trial and returns a configuration dict. |
| search_alg | BasicVariantGenerator | \<ray.tune.search.basic_variant.BasicVariantGenerator object at 0x7fc41ffbfee0\> | For ray see https://docs.ray.io/en/latest/tune/api_docs/suggestion.html<br/>For optuna see https://optuna.readthedocs.io/en/stable/reference/samplers/index.html. |
| num_samples | int | 10 | Number of hyperparameter optimization steps/samples. |
| refit_with_val | bool | False | Refit of best model should preserve val_size. |
| cpus | int | 4 | Number of cpus to use during optimization. Only used with ray tune. |
| gpus | int | 0 | Number of gpus to use during optimization, default all available. Only used with ray tune. |
| verbose | bool | False | Track progress. |
| alias | NoneType | None | Custom name of the model. |
| backend | str | ray | Backend to use for searching the hyperparameter space, can be either ‘ray’ or ‘optuna’. |
| callbacks | NoneType | None | List of functions to call during the optimization process.<br/>ray reference: https://docs.ray.io/en/latest/tune/tutorials/tune-metrics.html<br/>optuna reference: https://optuna.readthedocs.io/en/stable/tutorial/20_recipes/007_optuna_callback.html |

```python
# Use your own config or AutoAutoformer.default_config
config = dict(max_steps=1, val_check_steps=1, input_size=12, hidden_size=8)
model = AutoAutoformer(h=12, config=config, num_samples=1, cpus=1)

# Fit and predict
model.fit(dataset=dataset)
y_hat = model.predict(dataset=dataset)

# Optuna
model = AutoAutoformer(h=12, config=None, backend='optuna')
```

------------------------------------------------------------------------

<a
href="https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/auto.py#L1455"
target="_blank" style={{ float: "right", fontSize: "smaller" }}>source</a>

### AutoFEDformer

> ``` text
>  AutoFEDformer (h, loss=MAE(), valid_loss=None, config=None,
>                 search_alg=<ray.tune.search.basic_variant.BasicVariantGene
>                 rator object at 0x7fc41ff383a0>, num_samples=10,
>                 refit_with_val=False, cpus=4, gpus=0, verbose=False,
>                 alias=None, backend='ray', callbacks=None)
> ```

\*Class for Automatic Hyperparameter Optimization, it builds on top of
`ray` to give access to a wide variety of hyperparameter optimization
tools ranging from classic grid search, to Bayesian optimization and
HyperBand algorithm.

The validation loss to be optimized is defined by the `config['loss']`
dictionary value, the config also contains the rest of the
hyperparameter search space.

It is important to note that the success of this hyperparameter
optimization heavily relies on a strong correlation between the
validation and test periods.\*

|  | **Type** | **Default** | **Details** |
|------|------------------|-------------------------|-------------------------|
| h | int |  | Forecast horizon |
| loss | MAE | MAE() | Instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| valid_loss | NoneType | None | Instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| config | NoneType | None | Dictionary with ray.tune defined search space or function that takes an optuna trial and returns a configuration dict. |
| search_alg | BasicVariantGenerator | \<ray.tune.search.basic_variant.BasicVariantGenerator object at 0x7fc41ff383a0\> | For ray see https://docs.ray.io/en/latest/tune/api_docs/suggestion.html<br/>For optuna see https://optuna.readthedocs.io/en/stable/reference/samplers/index.html. |
| num_samples | int | 10 | Number of hyperparameter optimization steps/samples. |
| refit_with_val | bool | False | Refit of best model should preserve val_size. |
| cpus | int | 4 | Number of cpus to use during optimization. Only used with ray tune. |
| gpus | int | 0 | Number of gpus to use during optimization, default all available. Only used with ray tune. |
| verbose | bool | False | Track progress. |
| alias | NoneType | None | Custom name of the model. |
| backend | str | ray | Backend to use for searching the hyperparameter space, can be either ‘ray’ or ‘optuna’. |
| callbacks | NoneType | None | List of functions to call during the optimization process.<br/>ray reference: https://docs.ray.io/en/latest/tune/tutorials/tune-metrics.html<br/>optuna reference: https://optuna.readthedocs.io/en/stable/tutorial/20_recipes/007_optuna_callback.html |

```python
# Use your own config or AutoFEDFormer.default_config
config = dict(max_steps=1, val_check_steps=1, input_size=12, hidden_size=64)
model = AutoFEDformer(h=12, config=config, num_samples=1, cpus=1)

# Fit and predict
model.fit(dataset=dataset)
y_hat = model.predict(dataset=dataset)

# Optuna
model = AutoFEDformer(h=12, config=None, backend='optuna')
```

------------------------------------------------------------------------

<a
href="https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/auto.py#L1522"
target="_blank" style={{ float: "right", fontSize: "smaller" }}>source</a>

### AutoPatchTST

> ``` text
>  AutoPatchTST (h, loss=MAE(), valid_loss=None, config=None,
>                search_alg=<ray.tune.search.basic_variant.BasicVariantGener
>                ator object at 0x7fc41ff20100>, num_samples=10,
>                refit_with_val=False, cpus=4, gpus=0, verbose=False,
>                alias=None, backend='ray', callbacks=None)
> ```

\*Class for Automatic Hyperparameter Optimization, it builds on top of
`ray` to give access to a wide variety of hyperparameter optimization
tools ranging from classic grid search, to Bayesian optimization and
HyperBand algorithm.

The validation loss to be optimized is defined by the `config['loss']`
dictionary value, the config also contains the rest of the
hyperparameter search space.

It is important to note that the success of this hyperparameter
optimization heavily relies on a strong correlation between the
validation and test periods.\*

|  | **Type** | **Default** | **Details** |
|------|------------------|-------------------------|-------------------------|
| h | int |  | Forecast horizon |
| loss | MAE | MAE() | Instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| valid_loss | NoneType | None | Instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| config | NoneType | None | Dictionary with ray.tune defined search space or function that takes an optuna trial and returns a configuration dict. |
| search_alg | BasicVariantGenerator | \<ray.tune.search.basic_variant.BasicVariantGenerator object at 0x7fc41ff20100\> | For ray see https://docs.ray.io/en/latest/tune/api_docs/suggestion.html<br/>For optuna see https://optuna.readthedocs.io/en/stable/reference/samplers/index.html. |
| num_samples | int | 10 | Number of hyperparameter optimization steps/samples. |
| refit_with_val | bool | False | Refit of best model should preserve val_size. |
| cpus | int | 4 | Number of cpus to use during optimization. Only used with ray tune. |
| gpus | int | 0 | Number of gpus to use during optimization, default all available. Only used with ray tune. |
| verbose | bool | False | Track progress. |
| alias | NoneType | None | Custom name of the model. |
| backend | str | ray | Backend to use for searching the hyperparameter space, can be either ‘ray’ or ‘optuna’. |
| callbacks | NoneType | None | List of functions to call during the optimization process.<br/>ray reference: https://docs.ray.io/en/latest/tune/tutorials/tune-metrics.html<br/>optuna reference: https://optuna.readthedocs.io/en/stable/tutorial/20_recipes/007_optuna_callback.html |

```python
# Use your own config or AutoPatchTST.default_config
config = dict(max_steps=1, val_check_steps=1, input_size=12, hidden_size=16)
model = AutoPatchTST(h=12, config=config, num_samples=1, cpus=1)

# Fit and predict
model.fit(dataset=dataset)
y_hat = model.predict(dataset=dataset)

# Optuna
model = AutoPatchTST(h=12, config=None, backend='optuna')
```

------------------------------------------------------------------------

<a
href="https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/auto.py#L1592"
target="_blank" style={{ float: "right", fontSize: "smaller" }}>source</a>

### AutoiTransformer

> ``` text
>  AutoiTransformer (h, n_series, loss=MAE(), valid_loss=None, config=None,
>                    search_alg=<ray.tune.search.basic_variant.BasicVariantG
>                    enerator object at 0x7fc4240596c0>, num_samples=10,
>                    refit_with_val=False, cpus=4, gpus=0, verbose=False,
>                    alias=None, backend='ray', callbacks=None)
> ```

\*Class for Automatic Hyperparameter Optimization, it builds on top of
`ray` to give access to a wide variety of hyperparameter optimization
tools ranging from classic grid search, to Bayesian optimization and
HyperBand algorithm.

The validation loss to be optimized is defined by the `config['loss']`
dictionary value, the config also contains the rest of the
hyperparameter search space.

It is important to note that the success of this hyperparameter
optimization heavily relies on a strong correlation between the
validation and test periods.\*

|  | **Type** | **Default** | **Details** |
|------|------------------|-------------------------|-------------------------|
| h | int |  | Forecast horizon |
| n_series |  |  |  |
| loss | MAE | MAE() | Instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| valid_loss | NoneType | None | Instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| config | NoneType | None | Dictionary with ray.tune defined search space or function that takes an optuna trial and returns a configuration dict. |
| search_alg | BasicVariantGenerator | \<ray.tune.search.basic_variant.BasicVariantGenerator object at 0x7fc4240596c0\> | For ray see https://docs.ray.io/en/latest/tune/api_docs/suggestion.html<br/>For optuna see https://optuna.readthedocs.io/en/stable/reference/samplers/index.html. |
| num_samples | int | 10 | Number of hyperparameter optimization steps/samples. |
| refit_with_val | bool | False | Refit of best model should preserve val_size. |
| cpus | int | 4 | Number of cpus to use during optimization. Only used with ray tune. |
| gpus | int | 0 | Number of gpus to use during optimization, default all available. Only used with ray tune. |
| verbose | bool | False | Track progress. |
| alias | NoneType | None | Custom name of the model. |
| backend | str | ray | Backend to use for searching the hyperparameter space, can be either ‘ray’ or ‘optuna’. |
| callbacks | NoneType | None | List of functions to call during the optimization process.<br/>ray reference: https://docs.ray.io/en/latest/tune/tutorials/tune-metrics.html<br/>optuna reference: https://optuna.readthedocs.io/en/stable/tutorial/20_recipes/007_optuna_callback.html |

```python
# Use your own config or AutoiTransformer.default_config
config = dict(max_steps=1, val_check_steps=1, input_size=12, hidden_size=16)
model = AutoiTransformer(h=12, n_series=1, config=config, num_samples=1, cpus=1)

# Fit and predict
model.fit(dataset=dataset)
y_hat = model.predict(dataset=dataset)

# Optuna
model = AutoiTransformer(h=12, n_series=1, config=None, backend='optuna')
```

------------------------------------------------------------------------

<a
href="https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/auto.py#L1677"
target="_blank" style={{ float: "right", fontSize: "smaller" }}>source</a>

### AutoTimeXer

> ``` text
>  AutoTimeXer (h, n_series, loss=MAE(), valid_loss=None, config=None,
>               search_alg=<ray.tune.search.basic_variant.BasicVariantGenera
>               tor object at 0x7fc41ff0e6b0>, num_samples=10,
>               refit_with_val=False, cpus=4, gpus=0, verbose=False,
>               alias=None, backend='ray', callbacks=None)
> ```

\*Class for Automatic Hyperparameter Optimization, it builds on top of
`ray` to give access to a wide variety of hyperparameter optimization
tools ranging from classic grid search, to Bayesian optimization and
HyperBand algorithm.

The validation loss to be optimized is defined by the `config['loss']`
dictionary value, the config also contains the rest of the
hyperparameter search space.

It is important to note that the success of this hyperparameter
optimization heavily relies on a strong correlation between the
validation and test periods.\*

|  | **Type** | **Default** | **Details** |
|------|------------------|-------------------------|-------------------------|
| h | int |  | Forecast horizon |
| n_series |  |  |  |
| loss | MAE | MAE() | Instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| valid_loss | NoneType | None | Instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| config | NoneType | None | Dictionary with ray.tune defined search space or function that takes an optuna trial and returns a configuration dict. |
| search_alg | BasicVariantGenerator | \<ray.tune.search.basic_variant.BasicVariantGenerator object at 0x7fc41ff0e6b0\> | For ray see https://docs.ray.io/en/latest/tune/api_docs/suggestion.html<br/>For optuna see https://optuna.readthedocs.io/en/stable/reference/samplers/index.html. |
| num_samples | int | 10 | Number of hyperparameter optimization steps/samples. |
| refit_with_val | bool | False | Refit of best model should preserve val_size. |
| cpus | int | 4 | Number of cpus to use during optimization. Only used with ray tune. |
| gpus | int | 0 | Number of gpus to use during optimization, default all available. Only used with ray tune. |
| verbose | bool | False | Track progress. |
| alias | NoneType | None | Custom name of the model. |
| backend | str | ray | Backend to use for searching the hyperparameter space, can be either ‘ray’ or ‘optuna’. |
| callbacks | NoneType | None | List of functions to call during the optimization process.<br/>ray reference: https://docs.ray.io/en/latest/tune/tutorials/tune-metrics.html<br/>optuna reference: https://optuna.readthedocs.io/en/stable/tutorial/20_recipes/007_optuna_callback.html |

```python
# Use your own config or AutoTimeXer.default_config
config = dict(max_steps=1, val_check_steps=1, input_size=12, patch_len=12)
model = AutoTimeXer(h=12, n_series=1, config=config, num_samples=1, cpus=1)

# Fit and predict
model.fit(dataset=dataset)
y_hat = model.predict(dataset=dataset)

# Optuna
model = AutoTimeXer(h=12, n_series=1, config=None, backend='optuna')
```

## E. CNN Based

------------------------------------------------------------------------

<a
href="https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/auto.py#L1762"
target="_blank" style={{ float: "right", fontSize: "smaller" }}>source</a>

### AutoTimesNet

> ``` text
>  AutoTimesNet (h, loss=MAE(), valid_loss=None, config=None,
>                search_alg=<ray.tune.search.basic_variant.BasicVariantGener
>                ator object at 0x7fc424059000>, num_samples=10,
>                refit_with_val=False, cpus=4, gpus=0, verbose=False,
>                alias=None, backend='ray', callbacks=None)
> ```

\*Class for Automatic Hyperparameter Optimization, it builds on top of
`ray` to give access to a wide variety of hyperparameter optimization
tools ranging from classic grid search, to Bayesian optimization and
HyperBand algorithm.

The validation loss to be optimized is defined by the `config['loss']`
dictionary value, the config also contains the rest of the
hyperparameter search space.

It is important to note that the success of this hyperparameter
optimization heavily relies on a strong correlation between the
validation and test periods.\*

|  | **Type** | **Default** | **Details** |
|------|------------------|-------------------------|-------------------------|
| h | int |  | Forecast horizon |
| loss | MAE | MAE() | Instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| valid_loss | NoneType | None | Instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| config | NoneType | None | Dictionary with ray.tune defined search space or function that takes an optuna trial and returns a configuration dict. |
| search_alg | BasicVariantGenerator | \<ray.tune.search.basic_variant.BasicVariantGenerator object at 0x7fc424059000\> | For ray see https://docs.ray.io/en/latest/tune/api_docs/suggestion.html<br/>For optuna see https://optuna.readthedocs.io/en/stable/reference/samplers/index.html. |
| num_samples | int | 10 | Number of hyperparameter optimization steps/samples. |
| refit_with_val | bool | False | Refit of best model should preserve val_size. |
| cpus | int | 4 | Number of cpus to use during optimization. Only used with ray tune. |
| gpus | int | 0 | Number of gpus to use during optimization, default all available. Only used with ray tune. |
| verbose | bool | False | Track progress. |
| alias | NoneType | None | Custom name of the model. |
| backend | str | ray | Backend to use for searching the hyperparameter space, can be either ‘ray’ or ‘optuna’. |
| callbacks | NoneType | None | List of functions to call during the optimization process.<br/>ray reference: https://docs.ray.io/en/latest/tune/tutorials/tune-metrics.html<br/>optuna reference: https://optuna.readthedocs.io/en/stable/tutorial/20_recipes/007_optuna_callback.html |

```python
# Use your own config or AutoTimesNet.default_config
config = dict(max_steps=1, val_check_steps=1, input_size=12, hidden_size=32)
model = AutoTimesNet(h=12, config=config, num_samples=1, cpus=1)

# Fit and predict
model.fit(dataset=dataset)
y_hat = model.predict(dataset=dataset)

# Optuna
model = AutoTimesNet(h=12, config=None, backend='optuna')
```

## F. Multivariate

------------------------------------------------------------------------

<a
href="https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/auto.py#L1830"
target="_blank" style={{ float: "right", fontSize: "smaller" }}>source</a>

### AutoStemGNN

> ``` text
>  AutoStemGNN (h, n_series, loss=MAE(), valid_loss=None, config=None,
>               search_alg=<ray.tune.search.basic_variant.BasicVariantGenera
>               tor object at 0x7fc424358be0>, num_samples=10,
>               refit_with_val=False, cpus=4, gpus=0, verbose=False,
>               alias=None, backend='ray', callbacks=None)
> ```

\*Class for Automatic Hyperparameter Optimization, it builds on top of
`ray` to give access to a wide variety of hyperparameter optimization
tools ranging from classic grid search, to Bayesian optimization and
HyperBand algorithm.

The validation loss to be optimized is defined by the `config['loss']`
dictionary value, the config also contains the rest of the
hyperparameter search space.

It is important to note that the success of this hyperparameter
optimization heavily relies on a strong correlation between the
validation and test periods.\*

|  | **Type** | **Default** | **Details** |
|------|------------------|-------------------------|-------------------------|
| h | int |  | Forecast horizon |
| n_series |  |  |  |
| loss | MAE | MAE() | Instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| valid_loss | NoneType | None | Instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| config | NoneType | None | Dictionary with ray.tune defined search space or function that takes an optuna trial and returns a configuration dict. |
| search_alg | BasicVariantGenerator | \<ray.tune.search.basic_variant.BasicVariantGenerator object at 0x7fc424358be0\> | For ray see https://docs.ray.io/en/latest/tune/api_docs/suggestion.html<br/>For optuna see https://optuna.readthedocs.io/en/stable/reference/samplers/index.html. |
| num_samples | int | 10 | Number of hyperparameter optimization steps/samples. |
| refit_with_val | bool | False | Refit of best model should preserve val_size. |
| cpus | int | 4 | Number of cpus to use during optimization. Only used with ray tune. |
| gpus | int | 0 | Number of gpus to use during optimization, default all available. Only used with ray tune. |
| verbose | bool | False | Track progress. |
| alias | NoneType | None | Custom name of the model. |
| backend | str | ray | Backend to use for searching the hyperparameter space, can be either ‘ray’ or ‘optuna’. |
| callbacks | NoneType | None | List of functions to call during the optimization process.<br/>ray reference: https://docs.ray.io/en/latest/tune/tutorials/tune-metrics.html<br/>optuna reference: https://optuna.readthedocs.io/en/stable/tutorial/20_recipes/007_optuna_callback.html |

```python
# Use your own config or AutoStemGNN.default_config
config = dict(max_steps=1, val_check_steps=1, input_size=12)
model = AutoStemGNN(h=12, n_series=1, config=config, num_samples=1, cpus=1)

# Fit and predict
model.fit(dataset=dataset)
y_hat = model.predict(dataset=dataset)

# Optuna
model = AutoStemGNN(h=12, n_series=1, config=None, backend='optuna')
```

------------------------------------------------------------------------

<a
href="https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/auto.py#L1915"
target="_blank" style={{ float: "right", fontSize: "smaller" }}>source</a>

### AutoHINT

> ``` text
>  AutoHINT (cls_model, h, loss, valid_loss, S, config,
>            search_alg=<ray.tune.search.basic_variant.BasicVariantGenerator
>            object at 0x7fc4240483d0>, num_samples=10, cpus=4, gpus=0,
>            refit_with_val=False, verbose=False, alias=None, backend='ray',
>            callbacks=None)
> ```

\*Class for Automatic Hyperparameter Optimization, it builds on top of
`ray` to give access to a wide variety of hyperparameter optimization
tools ranging from classic grid search, to Bayesian optimization and
HyperBand algorithm.

The validation loss to be optimized is defined by the `config['loss']`
dictionary value, the config also contains the rest of the
hyperparameter search space.

It is important to note that the success of this hyperparameter
optimization heavily relies on a strong correlation between the
validation and test periods.\*

|  | **Type** | **Default** | **Details** |
|------|------------------|-------------------------|-------------------------|
| cls_model | PyTorch/PyTorchLightning model |  | See `neuralforecast.models` [collection here](https://nixtla.github.io/neuralforecast/models.html). |
| h | int |  | Forecast horizon |
| loss | PyTorch module |  | Instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| valid_loss | PyTorch module |  | Instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| S |  |  |  |
| config | dict or callable |  | Dictionary with ray.tune defined search space or function that takes an optuna trial and returns a configuration dict. |
| search_alg | BasicVariantGenerator | \<ray.tune.search.basic_variant.BasicVariantGenerator object at 0x7fc4240483d0\> | For ray see https://docs.ray.io/en/latest/tune/api_docs/suggestion.html<br/>For optuna see https://optuna.readthedocs.io/en/stable/reference/samplers/index.html. |
| num_samples | int | 10 | Number of hyperparameter optimization steps/samples. |
| cpus | int | 4 | Number of cpus to use during optimization. Only used with ray tune. |
| gpus | int | 0 | Number of gpus to use during optimization, default all available. Only used with ray tune. |
| refit_with_val | bool | False | Refit of best model should preserve val_size. |
| verbose | bool | False | Track progress. |
| alias | NoneType | None | Custom name of the model. |
| backend | str | ray | Backend to use for searching the hyperparameter space, can be either ‘ray’ or ‘optuna’. |
| callbacks | NoneType | None | List of functions to call during the optimization process.<br/>ray reference: https://docs.ray.io/en/latest/tune/tutorials/tune-metrics.html<br/>optuna reference: https://optuna.readthedocs.io/en/stable/tutorial/20_recipes/007_optuna_callback.html |

```python
# Perform a simple hyperparameter optimization with 
# NHITS and then reconcile with HINT
from neuralforecast.losses.pytorch import GMM, sCRPS

base_config = dict(max_steps=1, val_check_steps=1, input_size=8)
base_model = AutoNHITS(h=4, loss=GMM(n_components=2, quantiles=quantiles), 
                       config=base_config, num_samples=1, cpus=1)
model = HINT(h=4, S=S_df.values,
             model=base_model,  reconciliation='MinTraceOLS')

model.fit(dataset=dataset)
y_hat = model.predict(dataset=hint_dataset)

# Perform a conjunct hyperparameter optimization with 
# NHITS + HINT reconciliation configurations
nhits_config = {
       "learning_rate": tune.choice([1e-3]),                                     # Initial Learning rate
       "max_steps": tune.choice([1]),                                            # Number of SGD steps
       "val_check_steps": tune.choice([1]),                                      # Number of steps between validation
       "input_size": tune.choice([5 * 12]),                                      # input_size = multiplier * horizon
       "batch_size": tune.choice([7]),                                           # Number of series in windows
       "windows_batch_size": tune.choice([256]),                                 # Number of windows in batch
       "n_pool_kernel_size": tune.choice([[2, 2, 2], [16, 8, 1]]),               # MaxPool's Kernelsize
       "n_freq_downsample": tune.choice([[168, 24, 1], [24, 12, 1], [1, 1, 1]]), # Interpolation expressivity ratios
       "activation": tune.choice(['ReLU']),                                      # Type of non-linear activation
       "n_blocks":  tune.choice([[1, 1, 1]]),                                    # Blocks per each 3 stacks
       "mlp_units":  tune.choice([[[512, 512], [512, 512], [512, 512]]]),        # 2 512-Layers per block for each stack
       "interpolation_mode": tune.choice(['linear']),                            # Type of multi-step interpolation
       "random_seed": tune.randint(1, 10),
       "reconciliation": tune.choice(['BottomUp', 'MinTraceOLS', 'MinTraceWLS'])
    }
model = AutoHINT(h=4, S=S_df.values,
                 cls_model=NHITS,
                 config=nhits_config,
                 loss=GMM(n_components=2, level=[80, 90]),
                 valid_loss=sCRPS(level=[80, 90]),
                 num_samples=1, cpus=1)
model.fit(dataset=dataset)
y_hat = model.predict(dataset=hint_dataset)
```

------------------------------------------------------------------------

<a
href="https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/auto.py#L1987"
target="_blank" style={{ float: "right", fontSize: "smaller" }}>source</a>

### AutoTSMixer

> ``` text
>  AutoTSMixer (h, n_series, loss=MAE(), valid_loss=None, config=None,
>               search_alg=<ray.tune.search.basic_variant.BasicVariantGenera
>               tor object at 0x7fc4240a9360>, num_samples=10,
>               refit_with_val=False, cpus=4, gpus=0, verbose=False,
>               alias=None, backend='ray', callbacks=None)
> ```

\*Class for Automatic Hyperparameter Optimization, it builds on top of
`ray` to give access to a wide variety of hyperparameter optimization
tools ranging from classic grid search, to Bayesian optimization and
HyperBand algorithm.

The validation loss to be optimized is defined by the `config['loss']`
dictionary value, the config also contains the rest of the
hyperparameter search space.

It is important to note that the success of this hyperparameter
optimization heavily relies on a strong correlation between the
validation and test periods.\*

|  | **Type** | **Default** | **Details** |
|------|------------------|-------------------------|-------------------------|
| h | int |  | Forecast horizon |
| n_series |  |  |  |
| loss | MAE | MAE() | Instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| valid_loss | NoneType | None | Instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| config | NoneType | None | Dictionary with ray.tune defined search space or function that takes an optuna trial and returns a configuration dict. |
| search_alg | BasicVariantGenerator | \<ray.tune.search.basic_variant.BasicVariantGenerator object at 0x7fc4240a9360\> | For ray see https://docs.ray.io/en/latest/tune/api_docs/suggestion.html<br/>For optuna see https://optuna.readthedocs.io/en/stable/reference/samplers/index.html. |
| num_samples | int | 10 | Number of hyperparameter optimization steps/samples. |
| refit_with_val | bool | False | Refit of best model should preserve val_size. |
| cpus | int | 4 | Number of cpus to use during optimization. Only used with ray tune. |
| gpus | int | 0 | Number of gpus to use during optimization, default all available. Only used with ray tune. |
| verbose | bool | False | Track progress. |
| alias | NoneType | None | Custom name of the model. |
| backend | str | ray | Backend to use for searching the hyperparameter space, can be either ‘ray’ or ‘optuna’. |
| callbacks | NoneType | None | List of functions to call during the optimization process.<br/>ray reference: https://docs.ray.io/en/latest/tune/tutorials/tune-metrics.html<br/>optuna reference: https://optuna.readthedocs.io/en/stable/tutorial/20_recipes/007_optuna_callback.html |

```python
# Use your own config or AutoTSMixer.default_config
config = dict(max_steps=1, val_check_steps=1, input_size=12)
model = AutoTSMixer(h=12, n_series=1, config=config, num_samples=1, cpus=1)

# Fit and predict
model.fit(dataset=dataset)
y_hat = model.predict(dataset=dataset)

# Optuna
model = AutoTSMixer(h=12, n_series=1, config=None, backend='optuna')
```

------------------------------------------------------------------------

<a
href="https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/auto.py#L2073"
target="_blank" style={{ float: "right", fontSize: "smaller" }}>source</a>

### AutoTSMixerx

> ``` text
>  AutoTSMixerx (h, n_series, loss=MAE(), valid_loss=None, config=None,
>                search_alg=<ray.tune.search.basic_variant.BasicVariantGener
>                ator object at 0x7fc4240c9720>, num_samples=10,
>                refit_with_val=False, cpus=4, gpus=0, verbose=False,
>                alias=None, backend='ray', callbacks=None)
> ```

\*Class for Automatic Hyperparameter Optimization, it builds on top of
`ray` to give access to a wide variety of hyperparameter optimization
tools ranging from classic grid search, to Bayesian optimization and
HyperBand algorithm.

The validation loss to be optimized is defined by the `config['loss']`
dictionary value, the config also contains the rest of the
hyperparameter search space.

It is important to note that the success of this hyperparameter
optimization heavily relies on a strong correlation between the
validation and test periods.\*

|  | **Type** | **Default** | **Details** |
|------|------------------|-------------------------|-------------------------|
| h | int |  | Forecast horizon |
| n_series |  |  |  |
| loss | MAE | MAE() | Instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| valid_loss | NoneType | None | Instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| config | NoneType | None | Dictionary with ray.tune defined search space or function that takes an optuna trial and returns a configuration dict. |
| search_alg | BasicVariantGenerator | \<ray.tune.search.basic_variant.BasicVariantGenerator object at 0x7fc4240c9720\> | For ray see https://docs.ray.io/en/latest/tune/api_docs/suggestion.html<br/>For optuna see https://optuna.readthedocs.io/en/stable/reference/samplers/index.html. |
| num_samples | int | 10 | Number of hyperparameter optimization steps/samples. |
| refit_with_val | bool | False | Refit of best model should preserve val_size. |
| cpus | int | 4 | Number of cpus to use during optimization. Only used with ray tune. |
| gpus | int | 0 | Number of gpus to use during optimization, default all available. Only used with ray tune. |
| verbose | bool | False | Track progress. |
| alias | NoneType | None | Custom name of the model. |
| backend | str | ray | Backend to use for searching the hyperparameter space, can be either ‘ray’ or ‘optuna’. |
| callbacks | NoneType | None | List of functions to call during the optimization process.<br/>ray reference: https://docs.ray.io/en/latest/tune/tutorials/tune-metrics.html<br/>optuna reference: https://optuna.readthedocs.io/en/stable/tutorial/20_recipes/007_optuna_callback.html |

```python
# Use your own config or AutoTSMixerx.default_config
config = dict(max_steps=1, val_check_steps=1, input_size=12)
model = AutoTSMixerx(h=12, n_series=1, config=config, num_samples=1, cpus=1)

# Fit and predict
model.fit(dataset=dataset)
y_hat = model.predict(dataset=dataset)

# Optuna
model = AutoTSMixerx(h=12, n_series=1, config=None, backend='optuna')
```

------------------------------------------------------------------------

<a
href="https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/auto.py#L2159"
target="_blank" style={{ float: "right", fontSize: "smaller" }}>source</a>

### AutoMLPMultivariate

> ``` text
>  AutoMLPMultivariate (h, n_series, loss=MAE(), valid_loss=None,
>                       config=None, search_alg=<ray.tune.search.basic_varia
>                       nt.BasicVariantGenerator object at 0x7fc4240c5300>,
>                       num_samples=10, refit_with_val=False, cpus=4,
>                       gpus=0, verbose=False, alias=None, backend='ray',
>                       callbacks=None)
> ```

\*Class for Automatic Hyperparameter Optimization, it builds on top of
`ray` to give access to a wide variety of hyperparameter optimization
tools ranging from classic grid search, to Bayesian optimization and
HyperBand algorithm.

The validation loss to be optimized is defined by the `config['loss']`
dictionary value, the config also contains the rest of the
hyperparameter search space.

It is important to note that the success of this hyperparameter
optimization heavily relies on a strong correlation between the
validation and test periods.\*

|  | **Type** | **Default** | **Details** |
|------|------------------|-------------------------|-------------------------|
| h | int |  | Forecast horizon |
| n_series |  |  |  |
| loss | MAE | MAE() | Instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| valid_loss | NoneType | None | Instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| config | NoneType | None | Dictionary with ray.tune defined search space or function that takes an optuna trial and returns a configuration dict. |
| search_alg | BasicVariantGenerator | \<ray.tune.search.basic_variant.BasicVariantGenerator object at 0x7fc4240c5300\> | For ray see https://docs.ray.io/en/latest/tune/api_docs/suggestion.html<br/>For optuna see https://optuna.readthedocs.io/en/stable/reference/samplers/index.html. |
| num_samples | int | 10 | Number of hyperparameter optimization steps/samples. |
| refit_with_val | bool | False | Refit of best model should preserve val_size. |
| cpus | int | 4 | Number of cpus to use during optimization. Only used with ray tune. |
| gpus | int | 0 | Number of gpus to use during optimization, default all available. Only used with ray tune. |
| verbose | bool | False | Track progress. |
| alias | NoneType | None | Custom name of the model. |
| backend | str | ray | Backend to use for searching the hyperparameter space, can be either ‘ray’ or ‘optuna’. |
| callbacks | NoneType | None | List of functions to call during the optimization process.<br/>ray reference: https://docs.ray.io/en/latest/tune/tutorials/tune-metrics.html<br/>optuna reference: https://optuna.readthedocs.io/en/stable/tutorial/20_recipes/007_optuna_callback.html |

```python
# Use your own config or AutoMLPMultivariate.default_config
config = dict(max_steps=1, val_check_steps=1, input_size=12)
model = AutoMLPMultivariate(h=12, n_series=1, config=config, num_samples=1, cpus=1)

# Fit and predict
model.fit(dataset=dataset)
y_hat = model.predict(dataset=dataset)

# Optuna
model = AutoMLPMultivariate(h=12, n_series=1, config=None, backend='optuna')
```

------------------------------------------------------------------------

<a
href="https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/auto.py#L2244"
target="_blank" style={{ float: "right", fontSize: "smaller" }}>source</a>

### AutoSOFTS

> ``` text
>  AutoSOFTS (h, n_series, loss=MAE(), valid_loss=None, config=None,
>             search_alg=<ray.tune.search.basic_variant.BasicVariantGenerato
>             r object at 0x7fc4242c5c60>, num_samples=10,
>             refit_with_val=False, cpus=4, gpus=0, verbose=False,
>             alias=None, backend='ray', callbacks=None)
> ```

\*Class for Automatic Hyperparameter Optimization, it builds on top of
`ray` to give access to a wide variety of hyperparameter optimization
tools ranging from classic grid search, to Bayesian optimization and
HyperBand algorithm.

The validation loss to be optimized is defined by the `config['loss']`
dictionary value, the config also contains the rest of the
hyperparameter search space.

It is important to note that the success of this hyperparameter
optimization heavily relies on a strong correlation between the
validation and test periods.\*

|  | **Type** | **Default** | **Details** |
|------|------------------|-------------------------|-------------------------|
| h | int |  | Forecast horizon |
| n_series |  |  |  |
| loss | MAE | MAE() | Instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| valid_loss | NoneType | None | Instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| config | NoneType | None | Dictionary with ray.tune defined search space or function that takes an optuna trial and returns a configuration dict. |
| search_alg | BasicVariantGenerator | \<ray.tune.search.basic_variant.BasicVariantGenerator object at 0x7fc4242c5c60\> | For ray see https://docs.ray.io/en/latest/tune/api_docs/suggestion.html<br/>For optuna see https://optuna.readthedocs.io/en/stable/reference/samplers/index.html. |
| num_samples | int | 10 | Number of hyperparameter optimization steps/samples. |
| refit_with_val | bool | False | Refit of best model should preserve val_size. |
| cpus | int | 4 | Number of cpus to use during optimization. Only used with ray tune. |
| gpus | int | 0 | Number of gpus to use during optimization, default all available. Only used with ray tune. |
| verbose | bool | False | Track progress. |
| alias | NoneType | None | Custom name of the model. |
| backend | str | ray | Backend to use for searching the hyperparameter space, can be either ‘ray’ or ‘optuna’. |
| callbacks | NoneType | None | List of functions to call during the optimization process.<br/>ray reference: https://docs.ray.io/en/latest/tune/tutorials/tune-metrics.html<br/>optuna reference: https://optuna.readthedocs.io/en/stable/tutorial/20_recipes/007_optuna_callback.html |

```python
# Use your own config or AutoSOFTS.default_config
config = dict(max_steps=1, val_check_steps=1, input_size=12, hidden_size=16)
model = AutoSOFTS(h=12, n_series=1, config=config, num_samples=1, cpus=1)

# Fit and predict
model.fit(dataset=dataset)
y_hat = model.predict(dataset=dataset)

# Optuna
model = AutoSOFTS(h=12, n_series=1, config=None, backend='optuna')
```

------------------------------------------------------------------------

<a
href="https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/auto.py#L2329"
target="_blank" style={{ float: "right", fontSize: "smaller" }}>source</a>

### AutoTimeMixer

> ``` text
>  AutoTimeMixer (h, n_series, loss=MAE(), valid_loss=None, config=None,
>                 search_alg=<ray.tune.search.basic_variant.BasicVariantGene
>                 rator object at 0x7fc4240ab040>, num_samples=10,
>                 refit_with_val=False, cpus=4, gpus=0, verbose=False,
>                 alias=None, backend='ray', callbacks=None)
> ```

\*Class for Automatic Hyperparameter Optimization, it builds on top of
`ray` to give access to a wide variety of hyperparameter optimization
tools ranging from classic grid search, to Bayesian optimization and
HyperBand algorithm.

The validation loss to be optimized is defined by the `config['loss']`
dictionary value, the config also contains the rest of the
hyperparameter search space.

It is important to note that the success of this hyperparameter
optimization heavily relies on a strong correlation between the
validation and test periods.\*

|  | **Type** | **Default** | **Details** |
|------|------------------|-------------------------|-------------------------|
| h | int |  | Forecast horizon |
| n_series |  |  |  |
| loss | MAE | MAE() | Instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| valid_loss | NoneType | None | Instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| config | NoneType | None | Dictionary with ray.tune defined search space or function that takes an optuna trial and returns a configuration dict. |
| search_alg | BasicVariantGenerator | \<ray.tune.search.basic_variant.BasicVariantGenerator object at 0x7fc4240ab040\> | For ray see https://docs.ray.io/en/latest/tune/api_docs/suggestion.html<br/>For optuna see https://optuna.readthedocs.io/en/stable/reference/samplers/index.html. |
| num_samples | int | 10 | Number of hyperparameter optimization steps/samples. |
| refit_with_val | bool | False | Refit of best model should preserve val_size. |
| cpus | int | 4 | Number of cpus to use during optimization. Only used with ray tune. |
| gpus | int | 0 | Number of gpus to use during optimization, default all available. Only used with ray tune. |
| verbose | bool | False | Track progress. |
| alias | NoneType | None | Custom name of the model. |
| backend | str | ray | Backend to use for searching the hyperparameter space, can be either ‘ray’ or ‘optuna’. |
| callbacks | NoneType | None | List of functions to call during the optimization process.<br/>ray reference: https://docs.ray.io/en/latest/tune/tutorials/tune-metrics.html<br/>optuna reference: https://optuna.readthedocs.io/en/stable/tutorial/20_recipes/007_optuna_callback.html |

```python
# Use your own config or AutoTimeMixer.default_config
config = dict(max_steps=1, val_check_steps=1, input_size=12, d_model=16)
model = AutoTimeMixer(h=12, n_series=1, config=config, num_samples=1, cpus=1)

# Fit and predict
model.fit(dataset=dataset)
y_hat = model.predict(dataset=dataset)

# Optuna
model = AutoTimeMixer(h=12, n_series=1, config=None, backend='optuna')
```

------------------------------------------------------------------------

<a
href="https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/auto.py#L2415"
target="_blank" style={{ float: "right", fontSize: "smaller" }}>source</a>

### AutoRMoK

> ``` text
>  AutoRMoK (h, n_series, loss=MAE(), valid_loss=None, config=None,
>            search_alg=<ray.tune.search.basic_variant.BasicVariantGenerator
>            object at 0x7fc424065060>, num_samples=10,
>            refit_with_val=False, cpus=4, gpus=0, verbose=False,
>            alias=None, backend='ray', callbacks=None)
> ```

\*Class for Automatic Hyperparameter Optimization, it builds on top of
`ray` to give access to a wide variety of hyperparameter optimization
tools ranging from classic grid search, to Bayesian optimization and
HyperBand algorithm.

The validation loss to be optimized is defined by the `config['loss']`
dictionary value, the config also contains the rest of the
hyperparameter search space.

It is important to note that the success of this hyperparameter
optimization heavily relies on a strong correlation between the
validation and test periods.\*

|  | **Type** | **Default** | **Details** |
|------|------------------|-------------------------|-------------------------|
| h | int |  | Forecast horizon |
| n_series |  |  |  |
| loss | MAE | MAE() | Instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| valid_loss | NoneType | None | Instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| config | NoneType | None | Dictionary with ray.tune defined search space or function that takes an optuna trial and returns a configuration dict. |
| search_alg | BasicVariantGenerator | \<ray.tune.search.basic_variant.BasicVariantGenerator object at 0x7fc424065060\> | For ray see https://docs.ray.io/en/latest/tune/api_docs/suggestion.html<br/>For optuna see https://optuna.readthedocs.io/en/stable/reference/samplers/index.html. |
| num_samples | int | 10 | Number of hyperparameter optimization steps/samples. |
| refit_with_val | bool | False | Refit of best model should preserve val_size. |
| cpus | int | 4 | Number of cpus to use during optimization. Only used with ray tune. |
| gpus | int | 0 | Number of gpus to use during optimization, default all available. Only used with ray tune. |
| verbose | bool | False | Track progress. |
| alias | NoneType | None | Custom name of the model. |
| backend | str | ray | Backend to use for searching the hyperparameter space, can be either ‘ray’ or ‘optuna’. |
| callbacks | NoneType | None | List of functions to call during the optimization process.<br/>ray reference: https://docs.ray.io/en/latest/tune/tutorials/tune-metrics.html<br/>optuna reference: https://optuna.readthedocs.io/en/stable/tutorial/20_recipes/007_optuna_callback.html |

```python
# Use your own config or AutoRMoK.default_config
config = dict(max_steps=1, val_check_steps=1, input_size=12, learning_rate=1e-2)
model = AutoRMoK(h=12, n_series=1, config=config, num_samples=1, cpus=1)

# Fit and predict
model.fit(dataset=dataset)
y_hat = model.predict(dataset=dataset)

# Optuna
model = AutoRMoK(h=12, n_series=1, config=None, backend='optuna')
```

# TESTS

