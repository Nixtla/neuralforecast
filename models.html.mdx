---
description: >-
  NeuralForecast contains user-friendly implementations of neural forecasting
  models that allow for easy transition of computing capabilities (GPU/CPU),
  computation parallelization, and hyperparameter tuning.
output-file: models.html
title: Models
---


All the NeuralForecast models are “global” because we train them with
all the series from the input pd.DataFrame data `Y_df`, yet the
optimization objective is, momentarily, “univariate” as it does not
consider the interaction between the output predictions across time
series. Like the StatsForecast library, `core.NeuralForecast` allows you
to explore collections of models efficiently and contains functions for
convenient wrangling of input and output pd.DataFrames predictions.

First we load the AirPassengers dataset such that you can run all the
examples.

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from neuralforecast.tsdataset import TimeSeriesDataset
from neuralforecast.utils import AirPassengersDF as Y_df
```


```python
# Split train/test and declare time series dataset
Y_train_df = Y_df[Y_df.ds<='1959-12-31'] # 132 train
Y_test_df = Y_df[Y_df.ds>'1959-12-31']   # 12 test
dataset, *_ = TimeSeriesDataset.from_df(Y_train_df)
```

# 1. Automatic Forecasting

## A. RNN-Based

------------------------------------------------------------------------

<a
href="https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/auto.py#L56"
target="_blank" style={{ float: "right", fontSize: "smaller" }}>source</a>

### AutoRNN

> ``` text
>  AutoRNN (h, loss=MAE(), valid_loss=None, config=None,
>           search_alg=<ray.tune.search.basic_variant.BasicVariantGenerator
>           object at 0x7f27d1b626e0>, num_samples=10, refit_with_val=False,
>           cpus=4, gpus=0, verbose=False, alias=None, backend='ray',
>           callbacks=None)
> ```

\*Class for Automatic Hyperparameter Optimization, it builds on top of
`ray` to give access to a wide variety of hyperparameter optimization
tools ranging from classic grid search, to Bayesian optimization and
HyperBand algorithm.

The validation loss to be optimized is defined by the `config['loss']`
dictionary value, the config also contains the rest of the
hyperparameter search space.

It is important to note that the success of this hyperparameter
optimization heavily relies on a strong correlation between the
validation and test periods.\*

|  | **Type** | **Default** | **Details** |
|------|------------------|-------------------------|-------------------------|
| h | int |  | Forecast horizon |
| loss | MAE | MAE() | Instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| valid_loss | NoneType | None | Instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| config | NoneType | None | Dictionary with ray.tune defined search space or function that takes an optuna trial and returns a configuration dict. |
| search_alg | BasicVariantGenerator | \<ray.tune.search.basic_variant.BasicVariantGenerator object at 0x7f27d1b626e0\> | For ray see https://docs.ray.io/en/latest/tune/api_docs/suggestion.html<br/>For optuna see https://optuna.readthedocs.io/en/stable/reference/samplers/index.html. |
| num_samples | int | 10 | Number of hyperparameter optimization steps/samples. |
| refit_with_val | bool | False | Refit of best model should preserve val_size. |
| cpus | int | 4 | Number of cpus to use during optimization. Only used with ray tune. |
| gpus | int | 0 | Number of gpus to use during optimization, default all available. Only used with ray tune. |
| verbose | bool | False | Track progress. |
| alias | NoneType | None | Custom name of the model. |
| backend | str | ray | Backend to use for searching the hyperparameter space, can be either ‘ray’ or ‘optuna’. |
| callbacks | NoneType | None | List of functions to call during the optimization process.<br/>ray reference: https://docs.ray.io/en/latest/tune/tutorials/tune-metrics.html<br/>optuna reference: https://optuna.readthedocs.io/en/stable/tutorial/20_recipes/007_optuna_callback.html |

```python
# Use your own config or AutoRNN.default_config
config = dict(max_steps=2, val_check_steps=1, input_size=-1, encoder_hidden_size=8)
model = AutoRNN(h=12, config=config, num_samples=1, cpus=1)

model.fit(dataset=dataset)
y_hat = model.predict(dataset=dataset)

# Optuna
model = AutoRNN(h=12, config=None, num_samples=1, cpus=1, backend='optuna')
```

------------------------------------------------------------------------

<a
href="https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/auto.py#L131"
target="_blank" style={{ float: "right", fontSize: "smaller" }}>source</a>

### AutoLSTM

> ``` text
>  AutoLSTM (h, loss=MAE(), valid_loss=None, config=None,
>            search_alg=<ray.tune.search.basic_variant.BasicVariantGenerator
>            object at 0x7f27d3dc59c0>, num_samples=10,
>            refit_with_val=False, cpus=4, gpus=0, verbose=False,
>            alias=None, backend='ray', callbacks=None)
> ```

\*Class for Automatic Hyperparameter Optimization, it builds on top of
`ray` to give access to a wide variety of hyperparameter optimization
tools ranging from classic grid search, to Bayesian optimization and
HyperBand algorithm.

The validation loss to be optimized is defined by the `config['loss']`
dictionary value, the config also contains the rest of the
hyperparameter search space.

It is important to note that the success of this hyperparameter
optimization heavily relies on a strong correlation between the
validation and test periods.\*

|  | **Type** | **Default** | **Details** |
|------|------------------|-------------------------|-------------------------|
| h | int |  | Forecast horizon |
| loss | MAE | MAE() | Instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| valid_loss | NoneType | None | Instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| config | NoneType | None | Dictionary with ray.tune defined search space or function that takes an optuna trial and returns a configuration dict. |
| search_alg | BasicVariantGenerator | \<ray.tune.search.basic_variant.BasicVariantGenerator object at 0x7f27d3dc59c0\> | For ray see https://docs.ray.io/en/latest/tune/api_docs/suggestion.html<br/>For optuna see https://optuna.readthedocs.io/en/stable/reference/samplers/index.html. |
| num_samples | int | 10 | Number of hyperparameter optimization steps/samples. |
| refit_with_val | bool | False | Refit of best model should preserve val_size. |
| cpus | int | 4 | Number of cpus to use during optimization. Only used with ray tune. |
| gpus | int | 0 | Number of gpus to use during optimization, default all available. Only used with ray tune. |
| verbose | bool | False | Track progress. |
| alias | NoneType | None | Custom name of the model. |
| backend | str | ray | Backend to use for searching the hyperparameter space, can be either ‘ray’ or ‘optuna’. |
| callbacks | NoneType | None | List of functions to call during the optimization process.<br/>ray reference: https://docs.ray.io/en/latest/tune/tutorials/tune-metrics.html<br/>optuna reference: https://optuna.readthedocs.io/en/stable/tutorial/20_recipes/007_optuna_callback.html |

```python
# Use your own config or AutoLSTM.default_config
config = dict(max_steps=2, val_check_steps=1, input_size=-1, encoder_hidden_size=8)
model = AutoLSTM(h=12, config=config, num_samples=1, cpus=1)

# Fit and predict
model.fit(dataset=dataset)
y_hat = model.predict(dataset=dataset)

# Optuna
model = AutoLSTM(h=12, config=None, backend='optuna')
```

------------------------------------------------------------------------

<a
href="https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/auto.py#L202"
target="_blank" style={{ float: "right", fontSize: "smaller" }}>source</a>

### AutoGRU

> ``` text
>  AutoGRU (h, loss=MAE(), valid_loss=None, config=None,
>           search_alg=<ray.tune.search.basic_variant.BasicVariantGenerator
>           object at 0x7f27da8438e0>, num_samples=10, refit_with_val=False,
>           cpus=4, gpus=0, verbose=False, alias=None, backend='ray',
>           callbacks=None)
> ```

\*Class for Automatic Hyperparameter Optimization, it builds on top of
`ray` to give access to a wide variety of hyperparameter optimization
tools ranging from classic grid search, to Bayesian optimization and
HyperBand algorithm.

The validation loss to be optimized is defined by the `config['loss']`
dictionary value, the config also contains the rest of the
hyperparameter search space.

It is important to note that the success of this hyperparameter
optimization heavily relies on a strong correlation between the
validation and test periods.\*

|  | **Type** | **Default** | **Details** |
|------|------------------|-------------------------|-------------------------|
| h | int |  | Forecast horizon |
| loss | MAE | MAE() | Instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| valid_loss | NoneType | None | Instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| config | NoneType | None | Dictionary with ray.tune defined search space or function that takes an optuna trial and returns a configuration dict. |
| search_alg | BasicVariantGenerator | \<ray.tune.search.basic_variant.BasicVariantGenerator object at 0x7f27da8438e0\> | For ray see https://docs.ray.io/en/latest/tune/api_docs/suggestion.html<br/>For optuna see https://optuna.readthedocs.io/en/stable/reference/samplers/index.html. |
| num_samples | int | 10 | Number of hyperparameter optimization steps/samples. |
| refit_with_val | bool | False | Refit of best model should preserve val_size. |
| cpus | int | 4 | Number of cpus to use during optimization. Only used with ray tune. |
| gpus | int | 0 | Number of gpus to use during optimization, default all available. Only used with ray tune. |
| verbose | bool | False | Track progress. |
| alias | NoneType | None | Custom name of the model. |
| backend | str | ray | Backend to use for searching the hyperparameter space, can be either ‘ray’ or ‘optuna’. |
| callbacks | NoneType | None | List of functions to call during the optimization process.<br/>ray reference: https://docs.ray.io/en/latest/tune/tutorials/tune-metrics.html<br/>optuna reference: https://optuna.readthedocs.io/en/stable/tutorial/20_recipes/007_optuna_callback.html |

```python
# Use your own config or AutoGRU.default_config
config = dict(max_steps=2, val_check_steps=1, input_size=-1, encoder_hidden_size=8)
model = AutoGRU(h=12, config=config, num_samples=1, cpus=1)

# Fit and predict
model.fit(dataset=dataset)
y_hat = model.predict(dataset=dataset)

# Optuna
model = AutoGRU(h=12, config=None, backend='optuna')
```

------------------------------------------------------------------------

<a
href="https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/auto.py#L273"
target="_blank" style={{ float: "right", fontSize: "smaller" }}>source</a>

### AutoTCN

> ``` text
>  AutoTCN (h, loss=MAE(), valid_loss=None, config=None,
>           search_alg=<ray.tune.search.basic_variant.BasicVariantGenerator
>           object at 0x7f27d3db9600>, num_samples=10, refit_with_val=False,
>           cpus=4, gpus=0, verbose=False, alias=None, backend='ray',
>           callbacks=None)
> ```

\*Class for Automatic Hyperparameter Optimization, it builds on top of
`ray` to give access to a wide variety of hyperparameter optimization
tools ranging from classic grid search, to Bayesian optimization and
HyperBand algorithm.

The validation loss to be optimized is defined by the `config['loss']`
dictionary value, the config also contains the rest of the
hyperparameter search space.

It is important to note that the success of this hyperparameter
optimization heavily relies on a strong correlation between the
validation and test periods.\*

|  | **Type** | **Default** | **Details** |
|------|------------------|-------------------------|-------------------------|
| h | int |  | Forecast horizon |
| loss | MAE | MAE() | Instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| valid_loss | NoneType | None | Instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| config | NoneType | None | Dictionary with ray.tune defined search space or function that takes an optuna trial and returns a configuration dict. |
| search_alg | BasicVariantGenerator | \<ray.tune.search.basic_variant.BasicVariantGenerator object at 0x7f27d3db9600\> | For ray see https://docs.ray.io/en/latest/tune/api_docs/suggestion.html<br/>For optuna see https://optuna.readthedocs.io/en/stable/reference/samplers/index.html. |
| num_samples | int | 10 | Number of hyperparameter optimization steps/samples. |
| refit_with_val | bool | False | Refit of best model should preserve val_size. |
| cpus | int | 4 | Number of cpus to use during optimization. Only used with ray tune. |
| gpus | int | 0 | Number of gpus to use during optimization, default all available. Only used with ray tune. |
| verbose | bool | False | Track progress. |
| alias | NoneType | None | Custom name of the model. |
| backend | str | ray | Backend to use for searching the hyperparameter space, can be either ‘ray’ or ‘optuna’. |
| callbacks | NoneType | None | List of functions to call during the optimization process.<br/>ray reference: https://docs.ray.io/en/latest/tune/tutorials/tune-metrics.html<br/>optuna reference: https://optuna.readthedocs.io/en/stable/tutorial/20_recipes/007_optuna_callback.html |

```python
# Use your own config or AutoTCN.default_config
config = dict(max_steps=2, val_check_steps=1, input_size=-1, encoder_hidden_size=8)
model = AutoTCN(h=12, config=config, num_samples=1, cpus=1)

# Fit and predict
model.fit(dataset=dataset)
y_hat = model.predict(dataset=dataset)

# Optuna
model = AutoTCN(h=12, config=None, backend='optuna')
```

------------------------------------------------------------------------

<a
href="https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/auto.py#L343"
target="_blank" style={{ float: "right", fontSize: "smaller" }}>source</a>

### AutoDeepAR

> ``` text
>  AutoDeepAR (h, loss=DistributionLoss(), valid_loss=MQLoss(), config=None,
>              search_alg=<ray.tune.search.basic_variant.BasicVariantGenerat
>              or object at 0x7f27d3dc7580>, num_samples=10,
>              refit_with_val=False, cpus=4, gpus=0, verbose=False,
>              alias=None, backend='ray', callbacks=None)
> ```

\*Class for Automatic Hyperparameter Optimization, it builds on top of
`ray` to give access to a wide variety of hyperparameter optimization
tools ranging from classic grid search, to Bayesian optimization and
HyperBand algorithm.

The validation loss to be optimized is defined by the `config['loss']`
dictionary value, the config also contains the rest of the
hyperparameter search space.

It is important to note that the success of this hyperparameter
optimization heavily relies on a strong correlation between the
validation and test periods.\*

|  | **Type** | **Default** | **Details** |
|------|------------------|-------------------------|-------------------------|
| h | int |  | Forecast horizon |
| loss | DistributionLoss | DistributionLoss() | Instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| valid_loss | MQLoss | MQLoss() | Instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| config | NoneType | None | Dictionary with ray.tune defined search space or function that takes an optuna trial and returns a configuration dict. |
| search_alg | BasicVariantGenerator | \<ray.tune.search.basic_variant.BasicVariantGenerator object at 0x7f27d3dc7580\> | For ray see https://docs.ray.io/en/latest/tune/api_docs/suggestion.html<br/>For optuna see https://optuna.readthedocs.io/en/stable/reference/samplers/index.html. |
| num_samples | int | 10 | Number of hyperparameter optimization steps/samples. |
| refit_with_val | bool | False | Refit of best model should preserve val_size. |
| cpus | int | 4 | Number of cpus to use during optimization. Only used with ray tune. |
| gpus | int | 0 | Number of gpus to use during optimization, default all available. Only used with ray tune. |
| verbose | bool | False | Track progress. |
| alias | NoneType | None | Custom name of the model. |
| backend | str | ray | Backend to use for searching the hyperparameter space, can be either ‘ray’ or ‘optuna’. |
| callbacks | NoneType | None | List of functions to call during the optimization process.<br/>ray reference: https://docs.ray.io/en/latest/tune/tutorials/tune-metrics.html<br/>optuna reference: https://optuna.readthedocs.io/en/stable/tutorial/20_recipes/007_optuna_callback.html |

```python
# Use your own config or AutoNHITS.default_config
config = dict(max_steps=1, val_check_steps=1, input_size=12, lstm_hidden_size=8)
model = AutoDeepAR(h=12, config=config, num_samples=1, cpus=1)

# Fit and predict
model.fit(dataset=dataset)
y_hat = model.predict(dataset=dataset)

# Optuna
model = AutoDeepAR(h=12, config=None, backend='optuna')
```

------------------------------------------------------------------------

<a
href="https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/auto.py#L414"
target="_blank" style={{ float: "right", fontSize: "smaller" }}>source</a>

### AutoDilatedRNN

> ``` text
>  AutoDilatedRNN (h, loss=MAE(), valid_loss=None, config=None,
>                  search_alg=<ray.tune.search.basic_variant.BasicVariantGen
>                  erator object at 0x7f27d3ddb6a0>, num_samples=10,
>                  refit_with_val=False, cpus=4, gpus=0, verbose=False,
>                  alias=None, backend='ray', callbacks=None)
> ```

\*Class for Automatic Hyperparameter Optimization, it builds on top of
`ray` to give access to a wide variety of hyperparameter optimization
tools ranging from classic grid search, to Bayesian optimization and
HyperBand algorithm.

The validation loss to be optimized is defined by the `config['loss']`
dictionary value, the config also contains the rest of the
hyperparameter search space.

It is important to note that the success of this hyperparameter
optimization heavily relies on a strong correlation between the
validation and test periods.\*

|  | **Type** | **Default** | **Details** |
|------|------------------|-------------------------|-------------------------|
| h | int |  | Forecast horizon |
| loss | MAE | MAE() | Instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| valid_loss | NoneType | None | Instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| config | NoneType | None | Dictionary with ray.tune defined search space or function that takes an optuna trial and returns a configuration dict. |
| search_alg | BasicVariantGenerator | \<ray.tune.search.basic_variant.BasicVariantGenerator object at 0x7f27d3ddb6a0\> | For ray see https://docs.ray.io/en/latest/tune/api_docs/suggestion.html<br/>For optuna see https://optuna.readthedocs.io/en/stable/reference/samplers/index.html. |
| num_samples | int | 10 | Number of hyperparameter optimization steps/samples. |
| refit_with_val | bool | False | Refit of best model should preserve val_size. |
| cpus | int | 4 | Number of cpus to use during optimization. Only used with ray tune. |
| gpus | int | 0 | Number of gpus to use during optimization, default all available. Only used with ray tune. |
| verbose | bool | False | Track progress. |
| alias | NoneType | None | Custom name of the model. |
| backend | str | ray | Backend to use for searching the hyperparameter space, can be either ‘ray’ or ‘optuna’. |
| callbacks | NoneType | None | List of functions to call during the optimization process.<br/>ray reference: https://docs.ray.io/en/latest/tune/tutorials/tune-metrics.html<br/>optuna reference: https://optuna.readthedocs.io/en/stable/tutorial/20_recipes/007_optuna_callback.html |

```python
# Use your own config or AutoDilatedRNN.default_config
config = dict(max_steps=2, val_check_steps=1, input_size=-1, encoder_hidden_size=8)
model = AutoDilatedRNN(h=12, config=config, num_samples=1, cpus=1)

# Fit and predict
model.fit(dataset=dataset)
y_hat = model.predict(dataset=dataset)

# Optuna
model = AutoDilatedRNN(h=12, config=None, backend='optuna')
```

------------------------------------------------------------------------

<a
href="https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/auto.py#L486"
target="_blank" style={{ float: "right", fontSize: "smaller" }}>source</a>

### AutoBiTCN

> ``` text
>  AutoBiTCN (h, loss=MAE(), valid_loss=None, config=None,
>             search_alg=<ray.tune.search.basic_variant.BasicVariantGenerato
>             r object at 0x7f27dab14fa0>, num_samples=10,
>             refit_with_val=False, cpus=4, gpus=0, verbose=False,
>             alias=None, backend='ray', callbacks=None)
> ```

\*Class for Automatic Hyperparameter Optimization, it builds on top of
`ray` to give access to a wide variety of hyperparameter optimization
tools ranging from classic grid search, to Bayesian optimization and
HyperBand algorithm.

The validation loss to be optimized is defined by the `config['loss']`
dictionary value, the config also contains the rest of the
hyperparameter search space.

It is important to note that the success of this hyperparameter
optimization heavily relies on a strong correlation between the
validation and test periods.\*

|  | **Type** | **Default** | **Details** |
|------|------------------|-------------------------|-------------------------|
| h | int |  | Forecast horizon |
| loss | MAE | MAE() | Instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| valid_loss | NoneType | None | Instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| config | NoneType | None | Dictionary with ray.tune defined search space or function that takes an optuna trial and returns a configuration dict. |
| search_alg | BasicVariantGenerator | \<ray.tune.search.basic_variant.BasicVariantGenerator object at 0x7f27dab14fa0\> | For ray see https://docs.ray.io/en/latest/tune/api_docs/suggestion.html<br/>For optuna see https://optuna.readthedocs.io/en/stable/reference/samplers/index.html. |
| num_samples | int | 10 | Number of hyperparameter optimization steps/samples. |
| refit_with_val | bool | False | Refit of best model should preserve val_size. |
| cpus | int | 4 | Number of cpus to use during optimization. Only used with ray tune. |
| gpus | int | 0 | Number of gpus to use during optimization, default all available. Only used with ray tune. |
| verbose | bool | False | Track progress. |
| alias | NoneType | None | Custom name of the model. |
| backend | str | ray | Backend to use for searching the hyperparameter space, can be either ‘ray’ or ‘optuna’. |
| callbacks | NoneType | None | List of functions to call during the optimization process.<br/>ray reference: https://docs.ray.io/en/latest/tune/tutorials/tune-metrics.html<br/>optuna reference: https://optuna.readthedocs.io/en/stable/tutorial/20_recipes/007_optuna_callback.html |

```python
# Use your own config or AutoNHITS.default_config
config = dict(max_steps=1, val_check_steps=1, input_size=12, hidden_size=8)
model = AutoBiTCN(h=12, config=config, num_samples=1, cpus=1)

# Fit and predict
model.fit(dataset=dataset)
y_hat = model.predict(dataset=dataset)

# Optuna
model = AutoBiTCN(h=12, config=None, backend='optuna')
```

## B. MLP-Based

------------------------------------------------------------------------

<a
href="https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/auto.py#L554"
target="_blank" style={{ float: "right", fontSize: "smaller" }}>source</a>

### AutoMLP

> ``` text
>  AutoMLP (h, loss=MAE(), valid_loss=None, config=None,
>           search_alg=<ray.tune.search.basic_variant.BasicVariantGenerator
>           object at 0x7f27d3dd81c0>, num_samples=10, refit_with_val=False,
>           cpus=4, gpus=0, verbose=False, alias=None, backend='ray',
>           callbacks=None)
> ```

\*Class for Automatic Hyperparameter Optimization, it builds on top of
`ray` to give access to a wide variety of hyperparameter optimization
tools ranging from classic grid search, to Bayesian optimization and
HyperBand algorithm.

The validation loss to be optimized is defined by the `config['loss']`
dictionary value, the config also contains the rest of the
hyperparameter search space.

It is important to note that the success of this hyperparameter
optimization heavily relies on a strong correlation between the
validation and test periods.\*

|  | **Type** | **Default** | **Details** |
|------|------------------|-------------------------|-------------------------|
| h | int |  | Forecast horizon |
| loss | MAE | MAE() | Instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| valid_loss | NoneType | None | Instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| config | NoneType | None | Dictionary with ray.tune defined search space or function that takes an optuna trial and returns a configuration dict. |
| search_alg | BasicVariantGenerator | \<ray.tune.search.basic_variant.BasicVariantGenerator object at 0x7f27d3dd81c0\> | For ray see https://docs.ray.io/en/latest/tune/api_docs/suggestion.html<br/>For optuna see https://optuna.readthedocs.io/en/stable/reference/samplers/index.html. |
| num_samples | int | 10 | Number of hyperparameter optimization steps/samples. |
| refit_with_val | bool | False | Refit of best model should preserve val_size. |
| cpus | int | 4 | Number of cpus to use during optimization. Only used with ray tune. |
| gpus | int | 0 | Number of gpus to use during optimization, default all available. Only used with ray tune. |
| verbose | bool | False | Track progress. |
| alias | NoneType | None | Custom name of the model. |
| backend | str | ray | Backend to use for searching the hyperparameter space, can be either ‘ray’ or ‘optuna’. |
| callbacks | NoneType | None | List of functions to call during the optimization process.<br/>ray reference: https://docs.ray.io/en/latest/tune/tutorials/tune-metrics.html<br/>optuna reference: https://optuna.readthedocs.io/en/stable/tutorial/20_recipes/007_optuna_callback.html |

```python
# Use your own config or AutoMLP.default_config
config = dict(max_steps=2, val_check_steps=1, input_size=12, hidden_size=8)
model = AutoMLP(h=12, config=config, num_samples=1, cpus=1)

# Fit and predict
model.fit(dataset=dataset)
y_hat = model.predict(dataset=dataset)

# Optuna
model = AutoMLP(h=12, config=None, backend='optuna')
```

------------------------------------------------------------------------

<a
href="https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/auto.py#L622"
target="_blank" style={{ float: "right", fontSize: "smaller" }}>source</a>

### AutoNBEATS

> ``` text
>  AutoNBEATS (h, loss=MAE(), valid_loss=None, config=None,
>              search_alg=<ray.tune.search.basic_variant.BasicVariantGenerat
>              or object at 0x7f27d3dbbdf0>, num_samples=10,
>              refit_with_val=False, cpus=4, gpus=0, verbose=False,
>              alias=None, backend='ray', callbacks=None)
> ```

\*Class for Automatic Hyperparameter Optimization, it builds on top of
`ray` to give access to a wide variety of hyperparameter optimization
tools ranging from classic grid search, to Bayesian optimization and
HyperBand algorithm.

The validation loss to be optimized is defined by the `config['loss']`
dictionary value, the config also contains the rest of the
hyperparameter search space.

It is important to note that the success of this hyperparameter
optimization heavily relies on a strong correlation between the
validation and test periods.\*

|  | **Type** | **Default** | **Details** |
|------|------------------|-------------------------|-------------------------|
| h | int |  | Forecast horizon |
| loss | MAE | MAE() | Instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| valid_loss | NoneType | None | Instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| config | NoneType | None | Dictionary with ray.tune defined search space or function that takes an optuna trial and returns a configuration dict. |
| search_alg | BasicVariantGenerator | \<ray.tune.search.basic_variant.BasicVariantGenerator object at 0x7f27d3dbbdf0\> | For ray see https://docs.ray.io/en/latest/tune/api_docs/suggestion.html<br/>For optuna see https://optuna.readthedocs.io/en/stable/reference/samplers/index.html. |
| num_samples | int | 10 | Number of hyperparameter optimization steps/samples. |
| refit_with_val | bool | False | Refit of best model should preserve val_size. |
| cpus | int | 4 | Number of cpus to use during optimization. Only used with ray tune. |
| gpus | int | 0 | Number of gpus to use during optimization, default all available. Only used with ray tune. |
| verbose | bool | False | Track progress. |
| alias | NoneType | None | Custom name of the model. |
| backend | str | ray | Backend to use for searching the hyperparameter space, can be either ‘ray’ or ‘optuna’. |
| callbacks | NoneType | None | List of functions to call during the optimization process.<br/>ray reference: https://docs.ray.io/en/latest/tune/tutorials/tune-metrics.html<br/>optuna reference: https://optuna.readthedocs.io/en/stable/tutorial/20_recipes/007_optuna_callback.html |

```python
# Use your own config or AutoNBEATS.default_config
config = dict(max_steps=2, val_check_steps=1, input_size=12,
              mlp_units=3*[[8, 8]])
model = AutoNBEATS(h=12, config=config, num_samples=1, cpus=1)

# Fit and predict
model.fit(dataset=dataset)
y_hat = model.predict(dataset=dataset)

# Optuna
model = AutoNBEATS(h=12, config=None, backend='optuna')
```

------------------------------------------------------------------------

<a
href="https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/auto.py#L688"
target="_blank" style={{ float: "right", fontSize: "smaller" }}>source</a>

### AutoNBEATSx

> ``` text
>  AutoNBEATSx (h, loss=MAE(), valid_loss=None, config=None,
>               search_alg=<ray.tune.search.basic_variant.BasicVariantGenera
>               tor object at 0x7f27d18b01c0>, num_samples=10,
>               refit_with_val=False, cpus=4, gpus=0, verbose=False,
>               alias=None, backend='ray', callbacks=None)
> ```

\*Class for Automatic Hyperparameter Optimization, it builds on top of
`ray` to give access to a wide variety of hyperparameter optimization
tools ranging from classic grid search, to Bayesian optimization and
HyperBand algorithm.

The validation loss to be optimized is defined by the `config['loss']`
dictionary value, the config also contains the rest of the
hyperparameter search space.

It is important to note that the success of this hyperparameter
optimization heavily relies on a strong correlation between the
validation and test periods.\*

|  | **Type** | **Default** | **Details** |
|------|------------------|-------------------------|-------------------------|
| h | int |  | Forecast horizon |
| loss | MAE | MAE() | Instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| valid_loss | NoneType | None | Instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| config | NoneType | None | Dictionary with ray.tune defined search space or function that takes an optuna trial and returns a configuration dict. |
| search_alg | BasicVariantGenerator | \<ray.tune.search.basic_variant.BasicVariantGenerator object at 0x7f27d18b01c0\> | For ray see https://docs.ray.io/en/latest/tune/api_docs/suggestion.html<br/>For optuna see https://optuna.readthedocs.io/en/stable/reference/samplers/index.html. |
| num_samples | int | 10 | Number of hyperparameter optimization steps/samples. |
| refit_with_val | bool | False | Refit of best model should preserve val_size. |
| cpus | int | 4 | Number of cpus to use during optimization. Only used with ray tune. |
| gpus | int | 0 | Number of gpus to use during optimization, default all available. Only used with ray tune. |
| verbose | bool | False | Track progress. |
| alias | NoneType | None | Custom name of the model. |
| backend | str | ray | Backend to use for searching the hyperparameter space, can be either ‘ray’ or ‘optuna’. |
| callbacks | NoneType | None | List of functions to call during the optimization process.<br/>ray reference: https://docs.ray.io/en/latest/tune/tutorials/tune-metrics.html<br/>optuna reference: https://optuna.readthedocs.io/en/stable/tutorial/20_recipes/007_optuna_callback.html |

```python
# Use your own config or AutoNBEATS.default_config
config = dict(max_steps=2, val_check_steps=1, input_size=12,
              mlp_units=3*[[8, 8]])
model = AutoNBEATSx(h=12, config=config, num_samples=1, cpus=1)

# Fit and predict
model.fit(dataset=dataset)
y_hat = model.predict(dataset=dataset)

# Optuna
model = AutoNBEATSx(h=12, config=None, backend='optuna')
```

------------------------------------------------------------------------

<a
href="https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/auto.py#L754"
target="_blank" style={{ float: "right", fontSize: "smaller" }}>source</a>

### AutoNHITS

> ``` text
>  AutoNHITS (h, loss=MAE(), valid_loss=None, config=None,
>             search_alg=<ray.tune.search.basic_variant.BasicVariantGenerato
>             r object at 0x7f27d3d56890>, num_samples=10,
>             refit_with_val=False, cpus=4, gpus=0, verbose=False,
>             alias=None, backend='ray', callbacks=None)
> ```

\*Class for Automatic Hyperparameter Optimization, it builds on top of
`ray` to give access to a wide variety of hyperparameter optimization
tools ranging from classic grid search, to Bayesian optimization and
HyperBand algorithm.

The validation loss to be optimized is defined by the `config['loss']`
dictionary value, the config also contains the rest of the
hyperparameter search space.

It is important to note that the success of this hyperparameter
optimization heavily relies on a strong correlation between the
validation and test periods.\*

|  | **Type** | **Default** | **Details** |
|------|------------------|-------------------------|-------------------------|
| h | int |  | Forecast horizon |
| loss | MAE | MAE() | Instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| valid_loss | NoneType | None | Instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| config | NoneType | None | Dictionary with ray.tune defined search space or function that takes an optuna trial and returns a configuration dict. |
| search_alg | BasicVariantGenerator | \<ray.tune.search.basic_variant.BasicVariantGenerator object at 0x7f27d3d56890\> | For ray see https://docs.ray.io/en/latest/tune/api_docs/suggestion.html<br/>For optuna see https://optuna.readthedocs.io/en/stable/reference/samplers/index.html. |
| num_samples | int | 10 | Number of hyperparameter optimization steps/samples. |
| refit_with_val | bool | False | Refit of best model should preserve val_size. |
| cpus | int | 4 | Number of cpus to use during optimization. Only used with ray tune. |
| gpus | int | 0 | Number of gpus to use during optimization, default all available. Only used with ray tune. |
| verbose | bool | False | Track progress. |
| alias | NoneType | None | Custom name of the model. |
| backend | str | ray | Backend to use for searching the hyperparameter space, can be either ‘ray’ or ‘optuna’. |
| callbacks | NoneType | None | List of functions to call during the optimization process.<br/>ray reference: https://docs.ray.io/en/latest/tune/tutorials/tune-metrics.html<br/>optuna reference: https://optuna.readthedocs.io/en/stable/tutorial/20_recipes/007_optuna_callback.html |

```python
# Use your own config or AutoNHITS.default_config
config = dict(max_steps=2, val_check_steps=1, input_size=12, 
              mlp_units=3 * [[8, 8]])
model = AutoNHITS(h=12, config=config, num_samples=1, cpus=1)

# Fit and predict
model.fit(dataset=dataset)
y_hat = model.predict(dataset=dataset)

# Optuna
model = AutoNHITS(h=12, config=None, backend='optuna')
```

------------------------------------------------------------------------

<a
href="https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/auto.py#L833"
target="_blank" style={{ float: "right", fontSize: "smaller" }}>source</a>

### AutoDLinear

> ``` text
>  AutoDLinear (h, loss=MAE(), valid_loss=None, config=None,
>               search_alg=<ray.tune.search.basic_variant.BasicVariantGenera
>               tor object at 0x7f27d3d56bf0>, num_samples=10,
>               refit_with_val=False, cpus=4, gpus=0, verbose=False,
>               alias=None, backend='ray', callbacks=None)
> ```

\*Class for Automatic Hyperparameter Optimization, it builds on top of
`ray` to give access to a wide variety of hyperparameter optimization
tools ranging from classic grid search, to Bayesian optimization and
HyperBand algorithm.

The validation loss to be optimized is defined by the `config['loss']`
dictionary value, the config also contains the rest of the
hyperparameter search space.

It is important to note that the success of this hyperparameter
optimization heavily relies on a strong correlation between the
validation and test periods.\*

|  | **Type** | **Default** | **Details** |
|------|------------------|-------------------------|-------------------------|
| h | int |  | Forecast horizon |
| loss | MAE | MAE() | Instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| valid_loss | NoneType | None | Instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| config | NoneType | None | Dictionary with ray.tune defined search space or function that takes an optuna trial and returns a configuration dict. |
| search_alg | BasicVariantGenerator | \<ray.tune.search.basic_variant.BasicVariantGenerator object at 0x7f27d3d56bf0\> | For ray see https://docs.ray.io/en/latest/tune/api_docs/suggestion.html<br/>For optuna see https://optuna.readthedocs.io/en/stable/reference/samplers/index.html. |
| num_samples | int | 10 | Number of hyperparameter optimization steps/samples. |
| refit_with_val | bool | False | Refit of best model should preserve val_size. |
| cpus | int | 4 | Number of cpus to use during optimization. Only used with ray tune. |
| gpus | int | 0 | Number of gpus to use during optimization, default all available. Only used with ray tune. |
| verbose | bool | False | Track progress. |
| alias | NoneType | None | Custom name of the model. |
| backend | str | ray | Backend to use for searching the hyperparameter space, can be either ‘ray’ or ‘optuna’. |
| callbacks | NoneType | None | List of functions to call during the optimization process.<br/>ray reference: https://docs.ray.io/en/latest/tune/tutorials/tune-metrics.html<br/>optuna reference: https://optuna.readthedocs.io/en/stable/tutorial/20_recipes/007_optuna_callback.html |

```python
# Use your own config or AutoDLinear.default_config
config = dict(max_steps=2, val_check_steps=1, input_size=12)
model = AutoDLinear(h=12, config=config, num_samples=1, cpus=1)

# Fit and predict
model.fit(dataset=dataset)
y_hat = model.predict(dataset=dataset)

# Optuna
model = AutoDLinear(h=12, config=None, backend='optuna')
```

------------------------------------------------------------------------

<a
href="https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/auto.py#L900"
target="_blank" style={{ float: "right", fontSize: "smaller" }}>source</a>

### AutoNLinear

> ``` text
>  AutoNLinear (h, loss=MAE(), valid_loss=None, config=None,
>               search_alg=<ray.tune.search.basic_variant.BasicVariantGenera
>               tor object at 0x7f27d3d32da0>, num_samples=10,
>               refit_with_val=False, cpus=4, gpus=0, verbose=False,
>               alias=None, backend='ray', callbacks=None)
> ```

\*Class for Automatic Hyperparameter Optimization, it builds on top of
`ray` to give access to a wide variety of hyperparameter optimization
tools ranging from classic grid search, to Bayesian optimization and
HyperBand algorithm.

The validation loss to be optimized is defined by the `config['loss']`
dictionary value, the config also contains the rest of the
hyperparameter search space.

It is important to note that the success of this hyperparameter
optimization heavily relies on a strong correlation between the
validation and test periods.\*

|  | **Type** | **Default** | **Details** |
|------|------------------|-------------------------|-------------------------|
| h | int |  | Forecast horizon |
| loss | MAE | MAE() | Instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| valid_loss | NoneType | None | Instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| config | NoneType | None | Dictionary with ray.tune defined search space or function that takes an optuna trial and returns a configuration dict. |
| search_alg | BasicVariantGenerator | \<ray.tune.search.basic_variant.BasicVariantGenerator object at 0x7f27d3d32da0\> | For ray see https://docs.ray.io/en/latest/tune/api_docs/suggestion.html<br/>For optuna see https://optuna.readthedocs.io/en/stable/reference/samplers/index.html. |
| num_samples | int | 10 | Number of hyperparameter optimization steps/samples. |
| refit_with_val | bool | False | Refit of best model should preserve val_size. |
| cpus | int | 4 | Number of cpus to use during optimization. Only used with ray tune. |
| gpus | int | 0 | Number of gpus to use during optimization, default all available. Only used with ray tune. |
| verbose | bool | False | Track progress. |
| alias | NoneType | None | Custom name of the model. |
| backend | str | ray | Backend to use for searching the hyperparameter space, can be either ‘ray’ or ‘optuna’. |
| callbacks | NoneType | None | List of functions to call during the optimization process.<br/>ray reference: https://docs.ray.io/en/latest/tune/tutorials/tune-metrics.html<br/>optuna reference: https://optuna.readthedocs.io/en/stable/tutorial/20_recipes/007_optuna_callback.html |

```python
# Use your own config or AutoNLinear.default_config
config = dict(max_steps=2, val_check_steps=1, input_size=12)
model = AutoNLinear(h=12, config=config, num_samples=1, cpus=1)

# Fit and predict
model.fit(dataset=dataset)
y_hat = model.predict(dataset=dataset)

# Optuna
model = AutoNLinear(h=12, config=None, backend='optuna')
```

------------------------------------------------------------------------

<a
href="https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/auto.py#L966"
target="_blank" style={{ float: "right", fontSize: "smaller" }}>source</a>

### AutoTiDE

> ``` text
>  AutoTiDE (h, loss=MAE(), valid_loss=None, config=None,
>            search_alg=<ray.tune.search.basic_variant.BasicVariantGenerator
>            object at 0x7f27d3d55540>, num_samples=10,
>            refit_with_val=False, cpus=4, gpus=0, verbose=False,
>            alias=None, backend='ray', callbacks=None)
> ```

\*Class for Automatic Hyperparameter Optimization, it builds on top of
`ray` to give access to a wide variety of hyperparameter optimization
tools ranging from classic grid search, to Bayesian optimization and
HyperBand algorithm.

The validation loss to be optimized is defined by the `config['loss']`
dictionary value, the config also contains the rest of the
hyperparameter search space.

It is important to note that the success of this hyperparameter
optimization heavily relies on a strong correlation between the
validation and test periods.\*

|  | **Type** | **Default** | **Details** |
|------|------------------|-------------------------|-------------------------|
| h | int |  | Forecast horizon |
| loss | MAE | MAE() | Instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| valid_loss | NoneType | None | Instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| config | NoneType | None | Dictionary with ray.tune defined search space or function that takes an optuna trial and returns a configuration dict. |
| search_alg | BasicVariantGenerator | \<ray.tune.search.basic_variant.BasicVariantGenerator object at 0x7f27d3d55540\> | For ray see https://docs.ray.io/en/latest/tune/api_docs/suggestion.html<br/>For optuna see https://optuna.readthedocs.io/en/stable/reference/samplers/index.html. |
| num_samples | int | 10 | Number of hyperparameter optimization steps/samples. |
| refit_with_val | bool | False | Refit of best model should preserve val_size. |
| cpus | int | 4 | Number of cpus to use during optimization. Only used with ray tune. |
| gpus | int | 0 | Number of gpus to use during optimization, default all available. Only used with ray tune. |
| verbose | bool | False | Track progress. |
| alias | NoneType | None | Custom name of the model. |
| backend | str | ray | Backend to use for searching the hyperparameter space, can be either ‘ray’ or ‘optuna’. |
| callbacks | NoneType | None | List of functions to call during the optimization process.<br/>ray reference: https://docs.ray.io/en/latest/tune/tutorials/tune-metrics.html<br/>optuna reference: https://optuna.readthedocs.io/en/stable/tutorial/20_recipes/007_optuna_callback.html |

```python
# Use your own config or AutoTiDE.default_config
config = dict(max_steps=2, val_check_steps=1, input_size=12)
model = AutoTiDE(h=12, config=config, num_samples=1, cpus=1)

# Fit and predict
model.fit(dataset=dataset)
y_hat = model.predict(dataset=dataset)

# Optuna
model = AutoTiDE(h=12, config=None, backend='optuna')
```

------------------------------------------------------------------------

<a
href="https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/auto.py#L1040"
target="_blank" style={{ float: "right", fontSize: "smaller" }}>source</a>

### AutoDeepNPTS

> ``` text
>  AutoDeepNPTS (h, loss=MAE(), valid_loss=None, config=None,
>                search_alg=<ray.tune.search.basic_variant.BasicVariantGener
>                ator object at 0x7f27d3d6ba60>, num_samples=10,
>                refit_with_val=False, cpus=4, gpus=0, verbose=False,
>                alias=None, backend='ray', callbacks=None)
> ```

\*Class for Automatic Hyperparameter Optimization, it builds on top of
`ray` to give access to a wide variety of hyperparameter optimization
tools ranging from classic grid search, to Bayesian optimization and
HyperBand algorithm.

The validation loss to be optimized is defined by the `config['loss']`
dictionary value, the config also contains the rest of the
hyperparameter search space.

It is important to note that the success of this hyperparameter
optimization heavily relies on a strong correlation between the
validation and test periods.\*

|  | **Type** | **Default** | **Details** |
|------|------------------|-------------------------|-------------------------|
| h | int |  | Forecast horizon |
| loss | MAE | MAE() | Instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| valid_loss | NoneType | None | Instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| config | NoneType | None | Dictionary with ray.tune defined search space or function that takes an optuna trial and returns a configuration dict. |
| search_alg | BasicVariantGenerator | \<ray.tune.search.basic_variant.BasicVariantGenerator object at 0x7f27d3d6ba60\> | For ray see https://docs.ray.io/en/latest/tune/api_docs/suggestion.html<br/>For optuna see https://optuna.readthedocs.io/en/stable/reference/samplers/index.html. |
| num_samples | int | 10 | Number of hyperparameter optimization steps/samples. |
| refit_with_val | bool | False | Refit of best model should preserve val_size. |
| cpus | int | 4 | Number of cpus to use during optimization. Only used with ray tune. |
| gpus | int | 0 | Number of gpus to use during optimization, default all available. Only used with ray tune. |
| verbose | bool | False | Track progress. |
| alias | NoneType | None | Custom name of the model. |
| backend | str | ray | Backend to use for searching the hyperparameter space, can be either ‘ray’ or ‘optuna’. |
| callbacks | NoneType | None | List of functions to call during the optimization process.<br/>ray reference: https://docs.ray.io/en/latest/tune/tutorials/tune-metrics.html<br/>optuna reference: https://optuna.readthedocs.io/en/stable/tutorial/20_recipes/007_optuna_callback.html |

```python
# Use your own config or AutoDeepNPTS.default_config
config = dict(max_steps=2, val_check_steps=1, input_size=12)
model = AutoDeepNPTS(h=12, config=config, num_samples=1, cpus=1)

# Fit and predict
model.fit(dataset=dataset)
y_hat = model.predict(dataset=dataset)

# Optuna
model = AutoDeepNPTS(h=12, config=None, backend='optuna')
```

## C. Transformer-Based

------------------------------------------------------------------------

<a
href="https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/auto.py#L1109"
target="_blank" style={{ float: "right", fontSize: "smaller" }}>source</a>

### AutoTFT

> ``` text
>  AutoTFT (h, loss=MAE(), valid_loss=None, config=None,
>           search_alg=<ray.tune.search.basic_variant.BasicVariantGenerator
>           object at 0x7f27d3d849a0>, num_samples=10, refit_with_val=False,
>           cpus=4, gpus=0, verbose=False, alias=None, backend='ray',
>           callbacks=None)
> ```

\*Class for Automatic Hyperparameter Optimization, it builds on top of
`ray` to give access to a wide variety of hyperparameter optimization
tools ranging from classic grid search, to Bayesian optimization and
HyperBand algorithm.

The validation loss to be optimized is defined by the `config['loss']`
dictionary value, the config also contains the rest of the
hyperparameter search space.

It is important to note that the success of this hyperparameter
optimization heavily relies on a strong correlation between the
validation and test periods.\*

|  | **Type** | **Default** | **Details** |
|------|------------------|-------------------------|-------------------------|
| h | int |  | Forecast horizon |
| loss | MAE | MAE() | Instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| valid_loss | NoneType | None | Instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| config | NoneType | None | Dictionary with ray.tune defined search space or function that takes an optuna trial and returns a configuration dict. |
| search_alg | BasicVariantGenerator | \<ray.tune.search.basic_variant.BasicVariantGenerator object at 0x7f27d3d849a0\> | For ray see https://docs.ray.io/en/latest/tune/api_docs/suggestion.html<br/>For optuna see https://optuna.readthedocs.io/en/stable/reference/samplers/index.html. |
| num_samples | int | 10 | Number of hyperparameter optimization steps/samples. |
| refit_with_val | bool | False | Refit of best model should preserve val_size. |
| cpus | int | 4 | Number of cpus to use during optimization. Only used with ray tune. |
| gpus | int | 0 | Number of gpus to use during optimization, default all available. Only used with ray tune. |
| verbose | bool | False | Track progress. |
| alias | NoneType | None | Custom name of the model. |
| backend | str | ray | Backend to use for searching the hyperparameter space, can be either ‘ray’ or ‘optuna’. |
| callbacks | NoneType | None | List of functions to call during the optimization process.<br/>ray reference: https://docs.ray.io/en/latest/tune/tutorials/tune-metrics.html<br/>optuna reference: https://optuna.readthedocs.io/en/stable/tutorial/20_recipes/007_optuna_callback.html |

```python
# Use your own config or AutoNHITS.default_config
config = dict(max_steps=1, val_check_steps=1, input_size=12, hidden_size=8)
model = AutoTFT(h=12, config=config, num_samples=1, cpus=1)

# Fit and predict
model.fit(dataset=dataset)
y_hat = model.predict(dataset=dataset)

# Optuna
model = AutoTFT(h=12, config=None, backend='optuna')
```

------------------------------------------------------------------------

<a
href="https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/auto.py#L1177"
target="_blank" style={{ float: "right", fontSize: "smaller" }}>source</a>

### AutoVanillaTransformer

> ``` text
>  AutoVanillaTransformer (h, loss=MAE(), valid_loss=None, config=None,
>                          search_alg=<ray.tune.search.basic_variant.BasicVa
>                          riantGenerator object at 0x7f27d3d21ba0>,
>                          num_samples=10, refit_with_val=False, cpus=4,
>                          gpus=0, verbose=False, alias=None, backend='ray',
>                          callbacks=None)
> ```

\*Class for Automatic Hyperparameter Optimization, it builds on top of
`ray` to give access to a wide variety of hyperparameter optimization
tools ranging from classic grid search, to Bayesian optimization and
HyperBand algorithm.

The validation loss to be optimized is defined by the `config['loss']`
dictionary value, the config also contains the rest of the
hyperparameter search space.

It is important to note that the success of this hyperparameter
optimization heavily relies on a strong correlation between the
validation and test periods.\*

|  | **Type** | **Default** | **Details** |
|------|------------------|-------------------------|-------------------------|
| h | int |  | Forecast horizon |
| loss | MAE | MAE() | Instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| valid_loss | NoneType | None | Instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| config | NoneType | None | Dictionary with ray.tune defined search space or function that takes an optuna trial and returns a configuration dict. |
| search_alg | BasicVariantGenerator | \<ray.tune.search.basic_variant.BasicVariantGenerator object at 0x7f27d3d21ba0\> | For ray see https://docs.ray.io/en/latest/tune/api_docs/suggestion.html<br/>For optuna see https://optuna.readthedocs.io/en/stable/reference/samplers/index.html. |
| num_samples | int | 10 | Number of hyperparameter optimization steps/samples. |
| refit_with_val | bool | False | Refit of best model should preserve val_size. |
| cpus | int | 4 | Number of cpus to use during optimization. Only used with ray tune. |
| gpus | int | 0 | Number of gpus to use during optimization, default all available. Only used with ray tune. |
| verbose | bool | False | Track progress. |
| alias | NoneType | None | Custom name of the model. |
| backend | str | ray | Backend to use for searching the hyperparameter space, can be either ‘ray’ or ‘optuna’. |
| callbacks | NoneType | None | List of functions to call during the optimization process.<br/>ray reference: https://docs.ray.io/en/latest/tune/tutorials/tune-metrics.html<br/>optuna reference: https://optuna.readthedocs.io/en/stable/tutorial/20_recipes/007_optuna_callback.html |

```python
# Use your own config or AutoNHITS.default_config
config = dict(max_steps=1, val_check_steps=1, input_size=12, hidden_size=8)
model = AutoVanillaTransformer(h=12, config=config, num_samples=1, cpus=1)

# Fit and predict
model.fit(dataset=dataset)
y_hat = model.predict(dataset=dataset)

# Optuna
model = AutoVanillaTransformer(h=12, config=None, backend='optuna')
```

------------------------------------------------------------------------

<a
href="https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/auto.py#L1245"
target="_blank" style={{ float: "right", fontSize: "smaller" }}>source</a>

### AutoInformer

> ``` text
>  AutoInformer (h, loss=MAE(), valid_loss=None, config=None,
>                search_alg=<ray.tune.search.basic_variant.BasicVariantGener
>                ator object at 0x7f27d3dbabf0>, num_samples=10,
>                refit_with_val=False, cpus=4, gpus=0, verbose=False,
>                alias=None, backend='ray', callbacks=None)
> ```

\*Class for Automatic Hyperparameter Optimization, it builds on top of
`ray` to give access to a wide variety of hyperparameter optimization
tools ranging from classic grid search, to Bayesian optimization and
HyperBand algorithm.

The validation loss to be optimized is defined by the `config['loss']`
dictionary value, the config also contains the rest of the
hyperparameter search space.

It is important to note that the success of this hyperparameter
optimization heavily relies on a strong correlation between the
validation and test periods.\*

|  | **Type** | **Default** | **Details** |
|------|------------------|-------------------------|-------------------------|
| h | int |  | Forecast horizon |
| loss | MAE | MAE() | Instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| valid_loss | NoneType | None | Instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| config | NoneType | None | Dictionary with ray.tune defined search space or function that takes an optuna trial and returns a configuration dict. |
| search_alg | BasicVariantGenerator | \<ray.tune.search.basic_variant.BasicVariantGenerator object at 0x7f27d3dbabf0\> | For ray see https://docs.ray.io/en/latest/tune/api_docs/suggestion.html<br/>For optuna see https://optuna.readthedocs.io/en/stable/reference/samplers/index.html. |
| num_samples | int | 10 | Number of hyperparameter optimization steps/samples. |
| refit_with_val | bool | False | Refit of best model should preserve val_size. |
| cpus | int | 4 | Number of cpus to use during optimization. Only used with ray tune. |
| gpus | int | 0 | Number of gpus to use during optimization, default all available. Only used with ray tune. |
| verbose | bool | False | Track progress. |
| alias | NoneType | None | Custom name of the model. |
| backend | str | ray | Backend to use for searching the hyperparameter space, can be either ‘ray’ or ‘optuna’. |
| callbacks | NoneType | None | List of functions to call during the optimization process.<br/>ray reference: https://docs.ray.io/en/latest/tune/tutorials/tune-metrics.html<br/>optuna reference: https://optuna.readthedocs.io/en/stable/tutorial/20_recipes/007_optuna_callback.html |

```python
# Use your own config or AutoNHITS.default_config
config = dict(max_steps=1, val_check_steps=1, input_size=12, hidden_size=8)
model = AutoInformer(h=12, config=config, num_samples=1, cpus=1)

# Fit and predict
model.fit(dataset=dataset)
y_hat = model.predict(dataset=dataset)

# Optuna
model = AutoInformer(h=12, config=None, backend='optuna')
```

------------------------------------------------------------------------

<a
href="https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/auto.py#L1313"
target="_blank" style={{ float: "right", fontSize: "smaller" }}>source</a>

### AutoAutoformer

> ``` text
>  AutoAutoformer (h, loss=MAE(), valid_loss=None, config=None,
>                  search_alg=<ray.tune.search.basic_variant.BasicVariantGen
>                  erator object at 0x7f27d3d33bb0>, num_samples=10,
>                  refit_with_val=False, cpus=4, gpus=0, verbose=False,
>                  alias=None, backend='ray', callbacks=None)
> ```

\*Class for Automatic Hyperparameter Optimization, it builds on top of
`ray` to give access to a wide variety of hyperparameter optimization
tools ranging from classic grid search, to Bayesian optimization and
HyperBand algorithm.

The validation loss to be optimized is defined by the `config['loss']`
dictionary value, the config also contains the rest of the
hyperparameter search space.

It is important to note that the success of this hyperparameter
optimization heavily relies on a strong correlation between the
validation and test periods.\*

|  | **Type** | **Default** | **Details** |
|------|------------------|-------------------------|-------------------------|
| h | int |  | Forecast horizon |
| loss | MAE | MAE() | Instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| valid_loss | NoneType | None | Instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| config | NoneType | None | Dictionary with ray.tune defined search space or function that takes an optuna trial and returns a configuration dict. |
| search_alg | BasicVariantGenerator | \<ray.tune.search.basic_variant.BasicVariantGenerator object at 0x7f27d3d33bb0\> | For ray see https://docs.ray.io/en/latest/tune/api_docs/suggestion.html<br/>For optuna see https://optuna.readthedocs.io/en/stable/reference/samplers/index.html. |
| num_samples | int | 10 | Number of hyperparameter optimization steps/samples. |
| refit_with_val | bool | False | Refit of best model should preserve val_size. |
| cpus | int | 4 | Number of cpus to use during optimization. Only used with ray tune. |
| gpus | int | 0 | Number of gpus to use during optimization, default all available. Only used with ray tune. |
| verbose | bool | False | Track progress. |
| alias | NoneType | None | Custom name of the model. |
| backend | str | ray | Backend to use for searching the hyperparameter space, can be either ‘ray’ or ‘optuna’. |
| callbacks | NoneType | None | List of functions to call during the optimization process.<br/>ray reference: https://docs.ray.io/en/latest/tune/tutorials/tune-metrics.html<br/>optuna reference: https://optuna.readthedocs.io/en/stable/tutorial/20_recipes/007_optuna_callback.html |

```python
# Use your own config or AutoNHITS.default_config
config = dict(max_steps=1, val_check_steps=1, input_size=12, hidden_size=8)
model = AutoAutoformer(h=12, config=config, num_samples=1, cpus=1)

# Fit and predict
model.fit(dataset=dataset)
y_hat = model.predict(dataset=dataset)

# Optuna
model = AutoAutoformer(h=12, config=None, backend='optuna')
```

------------------------------------------------------------------------

<a
href="https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/auto.py#L1381"
target="_blank" style={{ float: "right", fontSize: "smaller" }}>source</a>

### AutoFEDformer

> ``` text
>  AutoFEDformer (h, loss=MAE(), valid_loss=None, config=None,
>                 search_alg=<ray.tune.search.basic_variant.BasicVariantGene
>                 rator object at 0x7f27d3f74370>, num_samples=10,
>                 refit_with_val=False, cpus=4, gpus=0, verbose=False,
>                 alias=None, backend='ray', callbacks=None)
> ```

\*Class for Automatic Hyperparameter Optimization, it builds on top of
`ray` to give access to a wide variety of hyperparameter optimization
tools ranging from classic grid search, to Bayesian optimization and
HyperBand algorithm.

The validation loss to be optimized is defined by the `config['loss']`
dictionary value, the config also contains the rest of the
hyperparameter search space.

It is important to note that the success of this hyperparameter
optimization heavily relies on a strong correlation between the
validation and test periods.\*

|  | **Type** | **Default** | **Details** |
|------|------------------|-------------------------|-------------------------|
| h | int |  | Forecast horizon |
| loss | MAE | MAE() | Instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| valid_loss | NoneType | None | Instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| config | NoneType | None | Dictionary with ray.tune defined search space or function that takes an optuna trial and returns a configuration dict. |
| search_alg | BasicVariantGenerator | \<ray.tune.search.basic_variant.BasicVariantGenerator object at 0x7f27d3f74370\> | For ray see https://docs.ray.io/en/latest/tune/api_docs/suggestion.html<br/>For optuna see https://optuna.readthedocs.io/en/stable/reference/samplers/index.html. |
| num_samples | int | 10 | Number of hyperparameter optimization steps/samples. |
| refit_with_val | bool | False | Refit of best model should preserve val_size. |
| cpus | int | 4 | Number of cpus to use during optimization. Only used with ray tune. |
| gpus | int | 0 | Number of gpus to use during optimization, default all available. Only used with ray tune. |
| verbose | bool | False | Track progress. |
| alias | NoneType | None | Custom name of the model. |
| backend | str | ray | Backend to use for searching the hyperparameter space, can be either ‘ray’ or ‘optuna’. |
| callbacks | NoneType | None | List of functions to call during the optimization process.<br/>ray reference: https://docs.ray.io/en/latest/tune/tutorials/tune-metrics.html<br/>optuna reference: https://optuna.readthedocs.io/en/stable/tutorial/20_recipes/007_optuna_callback.html |

```python
# Use your own config or AutoNHITS.default_config
config = dict(max_steps=1, val_check_steps=1, input_size=12, hidden_size=64)
model = AutoFEDformer(h=12, config=config, num_samples=1, cpus=1)

# Fit and predict
model.fit(dataset=dataset)
y_hat = model.predict(dataset=dataset)

# Optuna
model = AutoFEDformer(h=12, config=None, backend='optuna')
```

------------------------------------------------------------------------

<a
href="https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/auto.py#L1448"
target="_blank" style={{ float: "right", fontSize: "smaller" }}>source</a>

### AutoPatchTST

> ``` text
>  AutoPatchTST (h, loss=MAE(), valid_loss=None, config=None,
>                search_alg=<ray.tune.search.basic_variant.BasicVariantGener
>                ator object at 0x7f27d3fc1a80>, num_samples=10,
>                refit_with_val=False, cpus=4, gpus=0, verbose=False,
>                alias=None, backend='ray', callbacks=None)
> ```

\*Class for Automatic Hyperparameter Optimization, it builds on top of
`ray` to give access to a wide variety of hyperparameter optimization
tools ranging from classic grid search, to Bayesian optimization and
HyperBand algorithm.

The validation loss to be optimized is defined by the `config['loss']`
dictionary value, the config also contains the rest of the
hyperparameter search space.

It is important to note that the success of this hyperparameter
optimization heavily relies on a strong correlation between the
validation and test periods.\*

|  | **Type** | **Default** | **Details** |
|------|------------------|-------------------------|-------------------------|
| h | int |  | Forecast horizon |
| loss | MAE | MAE() | Instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| valid_loss | NoneType | None | Instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| config | NoneType | None | Dictionary with ray.tune defined search space or function that takes an optuna trial and returns a configuration dict. |
| search_alg | BasicVariantGenerator | \<ray.tune.search.basic_variant.BasicVariantGenerator object at 0x7f27d3fc1a80\> | For ray see https://docs.ray.io/en/latest/tune/api_docs/suggestion.html<br/>For optuna see https://optuna.readthedocs.io/en/stable/reference/samplers/index.html. |
| num_samples | int | 10 | Number of hyperparameter optimization steps/samples. |
| refit_with_val | bool | False | Refit of best model should preserve val_size. |
| cpus | int | 4 | Number of cpus to use during optimization. Only used with ray tune. |
| gpus | int | 0 | Number of gpus to use during optimization, default all available. Only used with ray tune. |
| verbose | bool | False | Track progress. |
| alias | NoneType | None | Custom name of the model. |
| backend | str | ray | Backend to use for searching the hyperparameter space, can be either ‘ray’ or ‘optuna’. |
| callbacks | NoneType | None | List of functions to call during the optimization process.<br/>ray reference: https://docs.ray.io/en/latest/tune/tutorials/tune-metrics.html<br/>optuna reference: https://optuna.readthedocs.io/en/stable/tutorial/20_recipes/007_optuna_callback.html |

```python
# Use your own config or AutoNHITS.default_config
config = dict(max_steps=1, val_check_steps=1, input_size=12, hidden_size=16)
model = AutoPatchTST(h=12, config=config, num_samples=1, cpus=1)

# Fit and predict
model.fit(dataset=dataset)
y_hat = model.predict(dataset=dataset)

# Optuna
model = AutoPatchTST(h=12, config=None, backend='optuna')
```

------------------------------------------------------------------------

<a
href="https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/auto.py#L1518"
target="_blank" style={{ float: "right", fontSize: "smaller" }}>source</a>

### AutoiTransformer

> ``` text
>  AutoiTransformer (h, n_series, loss=MAE(), valid_loss=None, config=None,
>                    search_alg=<ray.tune.search.basic_variant.BasicVariantG
>                    enerator object at 0x7f27d3d06a40>, num_samples=10,
>                    refit_with_val=False, cpus=4, gpus=0, verbose=False,
>                    alias=None, backend='ray', callbacks=None)
> ```

\*Class for Automatic Hyperparameter Optimization, it builds on top of
`ray` to give access to a wide variety of hyperparameter optimization
tools ranging from classic grid search, to Bayesian optimization and
HyperBand algorithm.

The validation loss to be optimized is defined by the `config['loss']`
dictionary value, the config also contains the rest of the
hyperparameter search space.

It is important to note that the success of this hyperparameter
optimization heavily relies on a strong correlation between the
validation and test periods.\*

|  | **Type** | **Default** | **Details** |
|------|------------------|-------------------------|-------------------------|
| h | int |  | Forecast horizon |
| n_series |  |  |  |
| loss | MAE | MAE() | Instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| valid_loss | NoneType | None | Instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| config | NoneType | None | Dictionary with ray.tune defined search space or function that takes an optuna trial and returns a configuration dict. |
| search_alg | BasicVariantGenerator | \<ray.tune.search.basic_variant.BasicVariantGenerator object at 0x7f27d3d06a40\> | For ray see https://docs.ray.io/en/latest/tune/api_docs/suggestion.html<br/>For optuna see https://optuna.readthedocs.io/en/stable/reference/samplers/index.html. |
| num_samples | int | 10 | Number of hyperparameter optimization steps/samples. |
| refit_with_val | bool | False | Refit of best model should preserve val_size. |
| cpus | int | 4 | Number of cpus to use during optimization. Only used with ray tune. |
| gpus | int | 0 | Number of gpus to use during optimization, default all available. Only used with ray tune. |
| verbose | bool | False | Track progress. |
| alias | NoneType | None | Custom name of the model. |
| backend | str | ray | Backend to use for searching the hyperparameter space, can be either ‘ray’ or ‘optuna’. |
| callbacks | NoneType | None | List of functions to call during the optimization process.<br/>ray reference: https://docs.ray.io/en/latest/tune/tutorials/tune-metrics.html<br/>optuna reference: https://optuna.readthedocs.io/en/stable/tutorial/20_recipes/007_optuna_callback.html |

```python
# Use your own config or AutoiTransformer.default_config
config = dict(max_steps=1, val_check_steps=1, input_size=12, hidden_size=16)
model = AutoiTransformer(h=12, n_series=1, config=config, num_samples=1, cpus=1)

# Fit and predict
model.fit(dataset=dataset)
y_hat = model.predict(dataset=dataset)

# Optuna
model = AutoiTransformer(h=12, n_series=1, config=None, backend='optuna')
```

## D. CNN Based

------------------------------------------------------------------------

<a
href="https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/auto.py#L1603"
target="_blank" style={{ float: "right", fontSize: "smaller" }}>source</a>

### AutoTimesNet

> ``` text
>  AutoTimesNet (h, loss=MAE(), valid_loss=None, config=None,
>                search_alg=<ray.tune.search.basic_variant.BasicVariantGener
>                ator object at 0x7f27d3d20730>, num_samples=10,
>                refit_with_val=False, cpus=4, gpus=0, verbose=False,
>                alias=None, backend='ray', callbacks=None)
> ```

\*Class for Automatic Hyperparameter Optimization, it builds on top of
`ray` to give access to a wide variety of hyperparameter optimization
tools ranging from classic grid search, to Bayesian optimization and
HyperBand algorithm.

The validation loss to be optimized is defined by the `config['loss']`
dictionary value, the config also contains the rest of the
hyperparameter search space.

It is important to note that the success of this hyperparameter
optimization heavily relies on a strong correlation between the
validation and test periods.\*

|  | **Type** | **Default** | **Details** |
|------|------------------|-------------------------|-------------------------|
| h | int |  | Forecast horizon |
| loss | MAE | MAE() | Instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| valid_loss | NoneType | None | Instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| config | NoneType | None | Dictionary with ray.tune defined search space or function that takes an optuna trial and returns a configuration dict. |
| search_alg | BasicVariantGenerator | \<ray.tune.search.basic_variant.BasicVariantGenerator object at 0x7f27d3d20730\> | For ray see https://docs.ray.io/en/latest/tune/api_docs/suggestion.html<br/>For optuna see https://optuna.readthedocs.io/en/stable/reference/samplers/index.html. |
| num_samples | int | 10 | Number of hyperparameter optimization steps/samples. |
| refit_with_val | bool | False | Refit of best model should preserve val_size. |
| cpus | int | 4 | Number of cpus to use during optimization. Only used with ray tune. |
| gpus | int | 0 | Number of gpus to use during optimization, default all available. Only used with ray tune. |
| verbose | bool | False | Track progress. |
| alias | NoneType | None | Custom name of the model. |
| backend | str | ray | Backend to use for searching the hyperparameter space, can be either ‘ray’ or ‘optuna’. |
| callbacks | NoneType | None | List of functions to call during the optimization process.<br/>ray reference: https://docs.ray.io/en/latest/tune/tutorials/tune-metrics.html<br/>optuna reference: https://optuna.readthedocs.io/en/stable/tutorial/20_recipes/007_optuna_callback.html |

```python
# Use your own config or AutoTimesNet.default_config
config = dict(max_steps=1, val_check_steps=1, input_size=12, hidden_size=32)
model = AutoTimesNet(h=12, config=config, num_samples=1, cpus=1)

# Fit and predict
model.fit(dataset=dataset)
y_hat = model.predict(dataset=dataset)

# Optuna
model = AutoTimesNet(h=12, config=None, backend='optuna')
```

## E. Multivariate

------------------------------------------------------------------------

<a
href="https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/auto.py#L1671"
target="_blank" style={{ float: "right", fontSize: "smaller" }}>source</a>

### AutoStemGNN

> ``` text
>  AutoStemGNN (h, n_series, loss=MAE(), valid_loss=None, config=None,
>               search_alg=<ray.tune.search.basic_variant.BasicVariantGenera
>               tor object at 0x7f27d3ffb310>, num_samples=10,
>               refit_with_val=False, cpus=4, gpus=0, verbose=False,
>               alias=None, backend='ray', callbacks=None)
> ```

\*Class for Automatic Hyperparameter Optimization, it builds on top of
`ray` to give access to a wide variety of hyperparameter optimization
tools ranging from classic grid search, to Bayesian optimization and
HyperBand algorithm.

The validation loss to be optimized is defined by the `config['loss']`
dictionary value, the config also contains the rest of the
hyperparameter search space.

It is important to note that the success of this hyperparameter
optimization heavily relies on a strong correlation between the
validation and test periods.\*

|  | **Type** | **Default** | **Details** |
|------|------------------|-------------------------|-------------------------|
| h | int |  | Forecast horizon |
| n_series |  |  |  |
| loss | MAE | MAE() | Instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| valid_loss | NoneType | None | Instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| config | NoneType | None | Dictionary with ray.tune defined search space or function that takes an optuna trial and returns a configuration dict. |
| search_alg | BasicVariantGenerator | \<ray.tune.search.basic_variant.BasicVariantGenerator object at 0x7f27d3ffb310\> | For ray see https://docs.ray.io/en/latest/tune/api_docs/suggestion.html<br/>For optuna see https://optuna.readthedocs.io/en/stable/reference/samplers/index.html. |
| num_samples | int | 10 | Number of hyperparameter optimization steps/samples. |
| refit_with_val | bool | False | Refit of best model should preserve val_size. |
| cpus | int | 4 | Number of cpus to use during optimization. Only used with ray tune. |
| gpus | int | 0 | Number of gpus to use during optimization, default all available. Only used with ray tune. |
| verbose | bool | False | Track progress. |
| alias | NoneType | None | Custom name of the model. |
| backend | str | ray | Backend to use for searching the hyperparameter space, can be either ‘ray’ or ‘optuna’. |
| callbacks | NoneType | None | List of functions to call during the optimization process.<br/>ray reference: https://docs.ray.io/en/latest/tune/tutorials/tune-metrics.html<br/>optuna reference: https://optuna.readthedocs.io/en/stable/tutorial/20_recipes/007_optuna_callback.html |

```python
# Use your own config or AutoStemGNN.default_config
config = dict(max_steps=1, val_check_steps=1, input_size=12)
model = AutoStemGNN(h=12, n_series=1, config=config, num_samples=1, cpus=1)

# Fit and predict
model.fit(dataset=dataset)
y_hat = model.predict(dataset=dataset)

# Optuna
model = AutoStemGNN(h=12, n_series=1, config=None, backend='optuna')
```

------------------------------------------------------------------------

<a
href="https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/auto.py#L1756"
target="_blank" style={{ float: "right", fontSize: "smaller" }}>source</a>

### AutoHINT

> ``` text
>  AutoHINT (cls_model, h, loss, valid_loss, S, config,
>            search_alg=<ray.tune.search.basic_variant.BasicVariantGenerator
>            object at 0x7f27d3d23940>, num_samples=10, cpus=4, gpus=0,
>            refit_with_val=False, verbose=False, alias=None, backend='ray',
>            callbacks=None)
> ```

\*Class for Automatic Hyperparameter Optimization, it builds on top of
`ray` to give access to a wide variety of hyperparameter optimization
tools ranging from classic grid search, to Bayesian optimization and
HyperBand algorithm.

The validation loss to be optimized is defined by the `config['loss']`
dictionary value, the config also contains the rest of the
hyperparameter search space.

It is important to note that the success of this hyperparameter
optimization heavily relies on a strong correlation between the
validation and test periods.\*

|  | **Type** | **Default** | **Details** |
|------|------------------|-------------------------|-------------------------|
| cls_model | PyTorch/PyTorchLightning model |  | See `neuralforecast.models` [collection here](https://nixtla.github.io/neuralforecast/models.html). |
| h | int |  | Forecast horizon |
| loss | PyTorch module |  | Instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| valid_loss | PyTorch module |  | Instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| S |  |  |  |
| config | dict or callable |  | Dictionary with ray.tune defined search space or function that takes an optuna trial and returns a configuration dict. |
| search_alg | BasicVariantGenerator | \<ray.tune.search.basic_variant.BasicVariantGenerator object at 0x7f27d3d23940\> | For ray see https://docs.ray.io/en/latest/tune/api_docs/suggestion.html<br/>For optuna see https://optuna.readthedocs.io/en/stable/reference/samplers/index.html. |
| num_samples | int | 10 | Number of hyperparameter optimization steps/samples. |
| cpus | int | 4 | Number of cpus to use during optimization. Only used with ray tune. |
| gpus | int | 0 | Number of gpus to use during optimization, default all available. Only used with ray tune. |
| refit_with_val | bool | False | Refit of best model should preserve val_size. |
| verbose | bool | False | Track progress. |
| alias | NoneType | None | Custom name of the model. |
| backend | str | ray | Backend to use for searching the hyperparameter space, can be either ‘ray’ or ‘optuna’. |
| callbacks | NoneType | None | List of functions to call during the optimization process.<br/>ray reference: https://docs.ray.io/en/latest/tune/tutorials/tune-metrics.html<br/>optuna reference: https://optuna.readthedocs.io/en/stable/tutorial/20_recipes/007_optuna_callback.html |

```python
# Perform a simple hyperparameter optimization with 
# NHITS and then reconcile with HINT
from neuralforecast.losses.pytorch import GMM, sCRPS

base_config = dict(max_steps=1, val_check_steps=1, input_size=8)
base_model = AutoNHITS(h=4, loss=GMM(n_components=2, quantiles=quantiles), 
                       config=base_config, num_samples=1, cpus=1)
model = HINT(h=4, S=S_df.values,
             model=base_model,  reconciliation='MinTraceOLS')

model.fit(dataset=dataset)
y_hat = model.predict(dataset=hint_dataset)

# Perform a conjunct hyperparameter optimization with 
# NHITS + HINT reconciliation configurations
nhits_config = {
       "learning_rate": tune.choice([1e-3]),                                     # Initial Learning rate
       "max_steps": tune.choice([1]),                                            # Number of SGD steps
       "val_check_steps": tune.choice([1]),                                      # Number of steps between validation
       "input_size": tune.choice([5 * 12]),                                      # input_size = multiplier * horizon
       "batch_size": tune.choice([7]),                                           # Number of series in windows
       "windows_batch_size": tune.choice([256]),                                 # Number of windows in batch
       "n_pool_kernel_size": tune.choice([[2, 2, 2], [16, 8, 1]]),               # MaxPool's Kernelsize
       "n_freq_downsample": tune.choice([[168, 24, 1], [24, 12, 1], [1, 1, 1]]), # Interpolation expressivity ratios
       "activation": tune.choice(['ReLU']),                                      # Type of non-linear activation
       "n_blocks":  tune.choice([[1, 1, 1]]),                                    # Blocks per each 3 stacks
       "mlp_units":  tune.choice([[[512, 512], [512, 512], [512, 512]]]),        # 2 512-Layers per block for each stack
       "interpolation_mode": tune.choice(['linear']),                            # Type of multi-step interpolation
       "random_seed": tune.randint(1, 10),
       "reconciliation": tune.choice(['BottomUp', 'MinTraceOLS', 'MinTraceWLS'])
    }
model = AutoHINT(h=4, S=S_df.values,
                 cls_model=NHITS,
                 config=nhits_config,
                 loss=GMM(n_components=2, level=[80, 90]),
                 valid_loss=sCRPS(level=[80, 90]),
                 num_samples=1, cpus=1)
model.fit(dataset=dataset)
y_hat = model.predict(dataset=hint_dataset)
```

------------------------------------------------------------------------

<a
href="https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/auto.py#L1828"
target="_blank" style={{ float: "right", fontSize: "smaller" }}>source</a>

### AutoTSMixer

> ``` text
>  AutoTSMixer (h, n_series, loss=MAE(), valid_loss=None, config=None,
>               search_alg=<ray.tune.search.basic_variant.BasicVariantGenera
>               tor object at 0x7f27d3ffb190>, num_samples=10,
>               refit_with_val=False, cpus=4, gpus=0, verbose=False,
>               alias=None, backend='ray', callbacks=None)
> ```

\*Class for Automatic Hyperparameter Optimization, it builds on top of
`ray` to give access to a wide variety of hyperparameter optimization
tools ranging from classic grid search, to Bayesian optimization and
HyperBand algorithm.

The validation loss to be optimized is defined by the `config['loss']`
dictionary value, the config also contains the rest of the
hyperparameter search space.

It is important to note that the success of this hyperparameter
optimization heavily relies on a strong correlation between the
validation and test periods.\*

|  | **Type** | **Default** | **Details** |
|------|------------------|-------------------------|-------------------------|
| h | int |  | Forecast horizon |
| n_series |  |  |  |
| loss | MAE | MAE() | Instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| valid_loss | NoneType | None | Instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| config | NoneType | None | Dictionary with ray.tune defined search space or function that takes an optuna trial and returns a configuration dict. |
| search_alg | BasicVariantGenerator | \<ray.tune.search.basic_variant.BasicVariantGenerator object at 0x7f27d3ffb190\> | For ray see https://docs.ray.io/en/latest/tune/api_docs/suggestion.html<br/>For optuna see https://optuna.readthedocs.io/en/stable/reference/samplers/index.html. |
| num_samples | int | 10 | Number of hyperparameter optimization steps/samples. |
| refit_with_val | bool | False | Refit of best model should preserve val_size. |
| cpus | int | 4 | Number of cpus to use during optimization. Only used with ray tune. |
| gpus | int | 0 | Number of gpus to use during optimization, default all available. Only used with ray tune. |
| verbose | bool | False | Track progress. |
| alias | NoneType | None | Custom name of the model. |
| backend | str | ray | Backend to use for searching the hyperparameter space, can be either ‘ray’ or ‘optuna’. |
| callbacks | NoneType | None | List of functions to call during the optimization process.<br/>ray reference: https://docs.ray.io/en/latest/tune/tutorials/tune-metrics.html<br/>optuna reference: https://optuna.readthedocs.io/en/stable/tutorial/20_recipes/007_optuna_callback.html |

```python
# Use your own config or AutoTSMixer.default_config
config = dict(max_steps=1, val_check_steps=1, input_size=12)
model = AutoTSMixer(h=12, n_series=1, config=config, num_samples=1, cpus=1)

# Fit and predict
model.fit(dataset=dataset)
y_hat = model.predict(dataset=dataset)

# Optuna
model = AutoTSMixer(h=12, n_series=1, config=None, backend='optuna')
```

------------------------------------------------------------------------

<a
href="https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/auto.py#L1914"
target="_blank" style={{ float: "right", fontSize: "smaller" }}>source</a>

### AutoTSMixerx

> ``` text
>  AutoTSMixerx (h, n_series, loss=MAE(), valid_loss=None, config=None,
>                search_alg=<ray.tune.search.basic_variant.BasicVariantGener
>                ator object at 0x7f27d19476a0>, num_samples=10,
>                refit_with_val=False, cpus=4, gpus=0, verbose=False,
>                alias=None, backend='ray', callbacks=None)
> ```

\*Class for Automatic Hyperparameter Optimization, it builds on top of
`ray` to give access to a wide variety of hyperparameter optimization
tools ranging from classic grid search, to Bayesian optimization and
HyperBand algorithm.

The validation loss to be optimized is defined by the `config['loss']`
dictionary value, the config also contains the rest of the
hyperparameter search space.

It is important to note that the success of this hyperparameter
optimization heavily relies on a strong correlation between the
validation and test periods.\*

|  | **Type** | **Default** | **Details** |
|------|------------------|-------------------------|-------------------------|
| h | int |  | Forecast horizon |
| n_series |  |  |  |
| loss | MAE | MAE() | Instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| valid_loss | NoneType | None | Instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| config | NoneType | None | Dictionary with ray.tune defined search space or function that takes an optuna trial and returns a configuration dict. |
| search_alg | BasicVariantGenerator | \<ray.tune.search.basic_variant.BasicVariantGenerator object at 0x7f27d19476a0\> | For ray see https://docs.ray.io/en/latest/tune/api_docs/suggestion.html<br/>For optuna see https://optuna.readthedocs.io/en/stable/reference/samplers/index.html. |
| num_samples | int | 10 | Number of hyperparameter optimization steps/samples. |
| refit_with_val | bool | False | Refit of best model should preserve val_size. |
| cpus | int | 4 | Number of cpus to use during optimization. Only used with ray tune. |
| gpus | int | 0 | Number of gpus to use during optimization, default all available. Only used with ray tune. |
| verbose | bool | False | Track progress. |
| alias | NoneType | None | Custom name of the model. |
| backend | str | ray | Backend to use for searching the hyperparameter space, can be either ‘ray’ or ‘optuna’. |
| callbacks | NoneType | None | List of functions to call during the optimization process.<br/>ray reference: https://docs.ray.io/en/latest/tune/tutorials/tune-metrics.html<br/>optuna reference: https://optuna.readthedocs.io/en/stable/tutorial/20_recipes/007_optuna_callback.html |

```python
# Use your own config or AutoTSMixerx.default_config
config = dict(max_steps=1, val_check_steps=1, input_size=12)
model = AutoTSMixerx(h=12, n_series=1, config=config, num_samples=1, cpus=1)

# Fit and predict
model.fit(dataset=dataset)
y_hat = model.predict(dataset=dataset)

# Optuna
model = AutoTSMixerx(h=12, n_series=1, config=None, backend='optuna')
```

------------------------------------------------------------------------

<a
href="https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/auto.py#L2000"
target="_blank" style={{ float: "right", fontSize: "smaller" }}>source</a>

### AutoMLPMultivariate

> ``` text
>  AutoMLPMultivariate (h, n_series, loss=MAE(), valid_loss=None,
>                       config=None, search_alg=<ray.tune.search.basic_varia
>                       nt.BasicVariantGenerator object at 0x7f27d3e230d0>,
>                       num_samples=10, refit_with_val=False, cpus=4,
>                       gpus=0, verbose=False, alias=None, backend='ray',
>                       callbacks=None)
> ```

\*Class for Automatic Hyperparameter Optimization, it builds on top of
`ray` to give access to a wide variety of hyperparameter optimization
tools ranging from classic grid search, to Bayesian optimization and
HyperBand algorithm.

The validation loss to be optimized is defined by the `config['loss']`
dictionary value, the config also contains the rest of the
hyperparameter search space.

It is important to note that the success of this hyperparameter
optimization heavily relies on a strong correlation between the
validation and test periods.\*

|  | **Type** | **Default** | **Details** |
|------|------------------|-------------------------|-------------------------|
| h | int |  | Forecast horizon |
| n_series |  |  |  |
| loss | MAE | MAE() | Instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| valid_loss | NoneType | None | Instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| config | NoneType | None | Dictionary with ray.tune defined search space or function that takes an optuna trial and returns a configuration dict. |
| search_alg | BasicVariantGenerator | \<ray.tune.search.basic_variant.BasicVariantGenerator object at 0x7f27d3e230d0\> | For ray see https://docs.ray.io/en/latest/tune/api_docs/suggestion.html<br/>For optuna see https://optuna.readthedocs.io/en/stable/reference/samplers/index.html. |
| num_samples | int | 10 | Number of hyperparameter optimization steps/samples. |
| refit_with_val | bool | False | Refit of best model should preserve val_size. |
| cpus | int | 4 | Number of cpus to use during optimization. Only used with ray tune. |
| gpus | int | 0 | Number of gpus to use during optimization, default all available. Only used with ray tune. |
| verbose | bool | False | Track progress. |
| alias | NoneType | None | Custom name of the model. |
| backend | str | ray | Backend to use for searching the hyperparameter space, can be either ‘ray’ or ‘optuna’. |
| callbacks | NoneType | None | List of functions to call during the optimization process.<br/>ray reference: https://docs.ray.io/en/latest/tune/tutorials/tune-metrics.html<br/>optuna reference: https://optuna.readthedocs.io/en/stable/tutorial/20_recipes/007_optuna_callback.html |

```python
# Use your own config or AutoTSMixerx.default_config
config = dict(max_steps=1, val_check_steps=1, input_size=12)
model = AutoMLPMultivariate(h=12, n_series=1, config=config, num_samples=1, cpus=1)

# Fit and predict
model.fit(dataset=dataset)
y_hat = model.predict(dataset=dataset)

# Optuna
model = AutoMLPMultivariate(h=12, n_series=1, config=None, backend='optuna')
```

------------------------------------------------------------------------

<a
href="https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/auto.py#L2085"
target="_blank" style={{ float: "right", fontSize: "smaller" }}>source</a>

### AutoSOFTS

> ``` text
>  AutoSOFTS (h, n_series, loss=MAE(), valid_loss=None, config=None,
>             search_alg=<ray.tune.search.basic_variant.BasicVariantGenerato
>             r object at 0x7f27d3ffa7d0>, num_samples=10,
>             refit_with_val=False, cpus=4, gpus=0, verbose=False,
>             alias=None, backend='ray', callbacks=None)
> ```

\*Class for Automatic Hyperparameter Optimization, it builds on top of
`ray` to give access to a wide variety of hyperparameter optimization
tools ranging from classic grid search, to Bayesian optimization and
HyperBand algorithm.

The validation loss to be optimized is defined by the `config['loss']`
dictionary value, the config also contains the rest of the
hyperparameter search space.

It is important to note that the success of this hyperparameter
optimization heavily relies on a strong correlation between the
validation and test periods.\*

|  | **Type** | **Default** | **Details** |
|------|------------------|-------------------------|-------------------------|
| h | int |  | Forecast horizon |
| n_series |  |  |  |
| loss | MAE | MAE() | Instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| valid_loss | NoneType | None | Instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html). |
| config | NoneType | None | Dictionary with ray.tune defined search space or function that takes an optuna trial and returns a configuration dict. |
| search_alg | BasicVariantGenerator | \<ray.tune.search.basic_variant.BasicVariantGenerator object at 0x7f27d3ffa7d0\> | For ray see https://docs.ray.io/en/latest/tune/api_docs/suggestion.html<br/>For optuna see https://optuna.readthedocs.io/en/stable/reference/samplers/index.html. |
| num_samples | int | 10 | Number of hyperparameter optimization steps/samples. |
| refit_with_val | bool | False | Refit of best model should preserve val_size. |
| cpus | int | 4 | Number of cpus to use during optimization. Only used with ray tune. |
| gpus | int | 0 | Number of gpus to use during optimization, default all available. Only used with ray tune. |
| verbose | bool | False | Track progress. |
| alias | NoneType | None | Custom name of the model. |
| backend | str | ray | Backend to use for searching the hyperparameter space, can be either ‘ray’ or ‘optuna’. |
| callbacks | NoneType | None | List of functions to call during the optimization process.<br/>ray reference: https://docs.ray.io/en/latest/tune/tutorials/tune-metrics.html<br/>optuna reference: https://optuna.readthedocs.io/en/stable/tutorial/20_recipes/007_optuna_callback.html |

```python
# Use your own config or AutoSOFTS.default_config
config = dict(max_steps=1, val_check_steps=1, input_size=12, hidden_size=16)
model = AutoSOFTS(h=12, n_series=1, config=config, num_samples=1, cpus=1)

# Fit and predict
model.fit(dataset=dataset)
y_hat = model.predict(dataset=dataset)

# Optuna
model = AutoSOFTS(h=12, n_series=1, config=None, backend='optuna')
```

``` text
2024-05-31 15:06:51,623 INFO worker.py:1749 -- Started a local Ray instance.
2024-05-31 15:06:52,217 INFO tune.py:263 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.
2024-05-31 15:06:52,219 INFO tune.py:624 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949
2024-05-31 15:06:54,633 ERROR tune_controller.py:1332 -- Trial task failed for trial _train_tune_eb814_00000
Traceback (most recent call last):
  File "/Users/marcopeix/miniconda3/envs/neuralforecast/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py", line 110, in resolve_future
    result = ray.get(future)
  File "/Users/marcopeix/miniconda3/envs/neuralforecast/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/Users/marcopeix/miniconda3/envs/neuralforecast/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/Users/marcopeix/miniconda3/envs/neuralforecast/lib/python3.10/site-packages/ray/_private/worker.py", line 2623, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/Users/marcopeix/miniconda3/envs/neuralforecast/lib/python3.10/site-packages/ray/_private/worker.py", line 861, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ValueError): ray::ImplicitFunc.train() (pid=3951, ip=127.0.0.1, actor_id=1641e0248328a9ca364b098901000000, repr=_train_tune)
  File "/Users/marcopeix/miniconda3/envs/neuralforecast/lib/python3.10/site-packages/ray/tune/trainable/trainable.py", line 334, in train
    raise skipped from exception_cause(skipped)
  File "/Users/marcopeix/miniconda3/envs/neuralforecast/lib/python3.10/site-packages/ray/air/_internal/util.py", line 98, in run
    self._ret = self._target(*self._args, **self._kwargs)
  File "/Users/marcopeix/miniconda3/envs/neuralforecast/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py", line 53, in <lambda>
    training_func=lambda: self._trainable_func(self.config),
  File "/Users/marcopeix/miniconda3/envs/neuralforecast/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py", line 261, in _trainable_func
    output = fn()
  File "/Users/marcopeix/miniconda3/envs/neuralforecast/lib/python3.10/site-packages/ray/tune/trainable/util.py", line 130, in inner
    return trainable(config, **fn_kwargs)
  File "/Users/marcopeix/miniconda3/envs/neuralforecast/lib/python3.10/site-packages/neuralforecast/common/_base_auto.py", line 209, in _train_tune
    _ = self._fit_model(
  File "/Users/marcopeix/miniconda3/envs/neuralforecast/lib/python3.10/site-packages/neuralforecast/common/_base_auto.py", line 357, in _fit_model
    model = model.fit(
  File "/Users/marcopeix/miniconda3/envs/neuralforecast/lib/python3.10/site-packages/neuralforecast/common/_base_multivariate.py", line 541, in fit
    return self._fit(
  File "/Users/marcopeix/miniconda3/envs/neuralforecast/lib/python3.10/site-packages/neuralforecast/common/_base_model.py", line 281, in _fit
    trainer = pl.Trainer(**model.trainer_kwargs)
  File "/Users/marcopeix/miniconda3/envs/neuralforecast/lib/python3.10/site-packages/pytorch_lightning/utilities/argparse.py", line 70, in insert_env_defaults
    return fn(self, **kwargs)
  File "/Users/marcopeix/miniconda3/envs/neuralforecast/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 431, in __init__
    self._callback_connector.on_trainer_init(
  File "/Users/marcopeix/miniconda3/envs/neuralforecast/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py", line 79, in on_trainer_init
    _validate_callbacks_list(self.trainer.callbacks)
  File "/Users/marcopeix/miniconda3/envs/neuralforecast/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py", line 227, in _validate_callbacks_list
    stateful_callbacks = [cb for cb in callbacks if is_overridden("state_dict", instance=cb)]
  File "/Users/marcopeix/miniconda3/envs/neuralforecast/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py", line 227, in <listcomp>
    stateful_callbacks = [cb for cb in callbacks if is_overridden("state_dict", instance=cb)]
  File "/Users/marcopeix/miniconda3/envs/neuralforecast/lib/python3.10/site-packages/pytorch_lightning/utilities/model_helpers.py", line 39, in is_overridden
    raise ValueError("Expected a parent")
ValueError: Expected a parent
2024-05-31 15:06:54,638 WARNING experiment_state.py:205 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this error by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0).
2024-05-31 15:06:54,640 INFO tune.py:1021 -- Wrote the latest version of all result files and experiment state to '/Users/marcopeix/ray_results/_train_tune_2024-05-31_15-06-49' in 0.0031s.
2024-05-31 15:06:54,641 ERROR tune.py:1049 -- Trials did not complete: [_train_tune_eb814_00000]
Seed set to 1
GPU available: True (mps), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1)` was configured so validation will run after every batch.
`Trainer.fit` stopped: `max_steps=1` reached.
GPU available: True (mps), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1)` was configured so validation will run after every batch.
```

# TESTS

