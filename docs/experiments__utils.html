---

title: Experiment Utils


keywords: fastai
sidebar: home_sidebar

summary: "Set of functions to easily perform experiments."
description: "Set of functions to easily perform experiments."
nb_path: "nbs/experiments__utils.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/experiments__utils.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="get_mask_dfs" class="doc_header"><code>get_mask_dfs</code><a href="https://github.com/Nixtla/nixtlats/tree/master/nixtlats/experiments/utils.py#L36" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>get_mask_dfs</code>(<strong><code>Y_df</code></strong>, <strong><code>ds_in_val</code></strong>, <strong><code>ds_in_test</code></strong>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="get_random_mask_dfs" class="doc_header"><code>get_random_mask_dfs</code><a href="https://github.com/Nixtla/nixtlats/tree/master/nixtlats/experiments/utils.py#L66" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>get_random_mask_dfs</code>(<strong><code>Y_df</code></strong>, <strong><code>ds_in_test</code></strong>, <strong><code>n_val_windows</code></strong>, <strong><code>n_ds_val_window</code></strong>, <strong><code>n_uids</code></strong>, <strong><code>freq</code></strong>)</p>
</blockquote>
<p>Generates train, test and random validation mask.
Train mask begins by avoiding ds_in_test</p>
<p>Validation mask: 1) samples n_uids unique ids
                 2) creates windows of size n_ds_val_window</p>
<h2 id="Parameters">Parameters<a class="anchor-link" href="#Parameters"> </a></h2><p>ds_in_test: int
    Number of ds in test.
n_uids: int
    Number of unique ids in validation.
n_val_windows: int
    Number of windows for validation.
n_ds_val_window: int
    Number of ds in each validation window.
periods: int
    ds_in_test multiplier.
freq: str
    string that determines datestamp frequency, used in
    random windows creation.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="scale_data" class="doc_header"><code>scale_data</code><a href="https://github.com/Nixtla/nixtlats/tree/master/nixtlats/experiments/utils.py#L122" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>scale_data</code>(<strong><code>Y_df</code></strong>, <strong><code>X_df</code></strong>, <strong><code>mask_df</code></strong>, <strong><code>normalizer_y</code></strong>, <strong><code>normalizer_x</code></strong>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="create_datasets" class="doc_header"><code>create_datasets</code><a href="https://github.com/Nixtla/nixtlats/tree/master/nixtlats/experiments/utils.py#L140" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>create_datasets</code>(<strong><code>mc</code></strong>, <strong><code>S_df</code></strong>, <strong><code>Y_df</code></strong>, <strong><code>X_df</code></strong>, <strong><code>f_cols</code></strong>, <strong><code>ds_in_test</code></strong>, <strong><code>ds_in_val</code></strong>, <strong><code>n_uids</code></strong>, <strong><code>n_val_windows</code></strong>, <strong><code>freq</code></strong>, <strong><code>is_val_random</code></strong>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="instantiate_loaders" class="doc_header"><code>instantiate_loaders</code><a href="https://github.com/Nixtla/nixtlats/tree/master/nixtlats/experiments/utils.py#L199" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>instantiate_loaders</code>(<strong><code>mc</code></strong>, <strong><code>train_dataset</code></strong>, <strong><code>val_dataset</code></strong>, <strong><code>test_dataset</code></strong>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="instantiate_nbeats" class="doc_header"><code>instantiate_nbeats</code><a href="https://github.com/Nixtla/nixtlats/tree/master/nixtlats/experiments/utils.py#L222" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>instantiate_nbeats</code>(<strong><code>mc</code></strong>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="instantiate_esrnn" class="doc_header"><code>instantiate_esrnn</code><a href="https://github.com/Nixtla/nixtlats/tree/master/nixtlats/experiments/utils.py#L254" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>instantiate_esrnn</code>(<strong><code>mc</code></strong>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="instantiate_mqesrnn" class="doc_header"><code>instantiate_mqesrnn</code><a href="https://github.com/Nixtla/nixtlats/tree/master/nixtlats/experiments/utils.py#L285" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>instantiate_mqesrnn</code>(<strong><code>mc</code></strong>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="instantiate_model" class="doc_header"><code>instantiate_model</code><a href="https://github.com/Nixtla/nixtlats/tree/master/nixtlats/experiments/utils.py#L313" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>instantiate_model</code>(<strong><code>mc</code></strong>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="model_fit_predict" class="doc_header"><code>model_fit_predict</code><a href="https://github.com/Nixtla/nixtlats/tree/master/nixtlats/experiments/utils.py#L320" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>model_fit_predict</code>(<strong><code>mc</code></strong>, <strong><code>S_df</code></strong>, <strong><code>Y_df</code></strong>, <strong><code>X_df</code></strong>, <strong><code>f_cols</code></strong>, <strong><code>ds_in_test</code></strong>, <strong><code>ds_in_val</code></strong>, <strong><code>n_uids</code></strong>, <strong><code>n_val_windows</code></strong>, <strong><code>freq</code></strong>, <strong><code>is_val_random</code></strong>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="evaluate_model" class="doc_header"><code>evaluate_model</code><a href="https://github.com/Nixtla/nixtlats/tree/master/nixtlats/experiments/utils.py#L389" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>evaluate_model</code>(<strong><code>mc</code></strong>, <strong><code>loss_function</code></strong>, <strong><code>S_df</code></strong>, <strong><code>Y_df</code></strong>, <strong><code>X_df</code></strong>, <strong><code>f_cols</code></strong>, <strong><code>ds_in_test</code></strong>, <strong><code>ds_in_val</code></strong>, <strong><code>n_uids</code></strong>, <strong><code>n_val_windows</code></strong>, <strong><code>freq</code></strong>, <strong><code>is_val_random</code></strong>, <strong><code>loss_kwargs</code></strong>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="hyperopt_tunning" class="doc_header"><code>hyperopt_tunning</code><a href="https://github.com/Nixtla/nixtlats/tree/master/nixtlats/experiments/utils.py#L435" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>hyperopt_tunning</code>(<strong><code>space</code></strong>, <strong><code>hyperopt_max_evals</code></strong>, <strong><code>loss_function</code></strong>, <strong><code>S_df</code></strong>, <strong><code>Y_df</code></strong>, <strong><code>X_df</code></strong>, <strong><code>f_cols</code></strong>, <strong><code>ds_in_val</code></strong>, <strong><code>n_uids</code></strong>, <strong><code>n_val_windows</code></strong>, <strong><code>freq</code></strong>, <strong><code>is_val_random</code></strong>, <strong><code>save_trials</code></strong>=<em><code>False</code></em>, <strong><code>loss_kwargs</code></strong>=<em><code>None</code></em>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Experiment-Utils-Examples">Experiment Utils Examples<a class="anchor-link" href="#Experiment-Utils-Examples"> </a></h1>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span> <span class="k">as</span> <span class="nn">t</span>
<span class="kn">from</span> <span class="nn">nixtlats.losses.numpy</span> <span class="kn">import</span> <span class="n">mae</span><span class="p">,</span> <span class="n">mape</span><span class="p">,</span> <span class="n">smape</span><span class="p">,</span> <span class="n">rmse</span><span class="p">,</span> <span class="n">pinball_loss</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">device</span> <span class="o">=</span> <span class="s1">&#39;cpu&#39;</span>
<span class="k">if</span> <span class="n">t</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span> <span class="n">device</span> <span class="o">=</span> <span class="s1">&#39;cuda&#39;</span>  

<span class="n">nbeats_space</span><span class="o">=</span> <span class="p">{</span><span class="c1"># Architecture parameters</span>
               <span class="s1">&#39;model&#39;</span><span class="p">:</span><span class="s1">&#39;nbeats&#39;</span><span class="p">,</span>
               <span class="s1">&#39;mode&#39;</span><span class="p">:</span> <span class="s1">&#39;simple&#39;</span><span class="p">,</span>
               <span class="s1">&#39;n_time_in&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;n_time_in&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">7</span><span class="o">*</span><span class="mi">24</span><span class="p">]),</span>
               <span class="s1">&#39;n_time_out&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;n_time_out&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">24</span><span class="p">]),</span>
               <span class="s1">&#39;n_x_hidden&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">quniform</span><span class="p">(</span><span class="s1">&#39;n_x_hidden&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
               <span class="s1">&#39;n_s_hidden&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;n_s_hidden&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">]),</span>
               <span class="s1">&#39;shared_weights&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;shared_weights&#39;</span><span class="p">,</span> <span class="p">[</span><span class="kc">False</span><span class="p">]),</span>
               <span class="s1">&#39;activation&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;activation&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;SELU&#39;</span><span class="p">]),</span>
               <span class="s1">&#39;initialization&#39;</span><span class="p">:</span>  <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;initialization&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;glorot_normal&#39;</span><span class="p">,</span><span class="s1">&#39;he_normal&#39;</span><span class="p">]),</span>
               <span class="s1">&#39;stack_types&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;stack_types&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">2</span><span class="o">*</span><span class="p">[</span><span class="s1">&#39;identity&#39;</span><span class="p">],</span>
                                                        <span class="mi">1</span><span class="o">*</span><span class="p">[</span><span class="s1">&#39;identity&#39;</span><span class="p">]</span><span class="o">+</span><span class="mi">1</span><span class="o">*</span><span class="p">[</span><span class="s1">&#39;exogenous_tcn&#39;</span><span class="p">],</span>
                                                        <span class="mi">1</span><span class="o">*</span><span class="p">[</span><span class="s1">&#39;exogenous_tcn&#39;</span><span class="p">]</span><span class="o">+</span><span class="mi">1</span><span class="o">*</span><span class="p">[</span><span class="s1">&#39;identity&#39;</span><span class="p">]</span> <span class="p">]),</span>
               <span class="s1">&#39;n_blocks&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;n_blocks&#39;</span><span class="p">,</span> <span class="p">[</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="p">]),</span>
               <span class="s1">&#39;n_layers&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;n_layers&#39;</span><span class="p">,</span> <span class="p">[</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span> <span class="p">]),</span>
               <span class="s1">&#39;n_hidden&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;n_hidden&#39;</span><span class="p">,</span> <span class="p">[</span> <span class="mi">256</span> <span class="p">]),</span>
               <span class="s1">&#39;n_harmonics&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;n_harmonics&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">]),</span>
               <span class="s1">&#39;n_polynomials&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;n_polynomials&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">2</span><span class="p">]),</span>
               <span class="c1"># Regularization and optimization parameters</span>
               <span class="s1">&#39;batch_normalization&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;batch_normalization&#39;</span><span class="p">,</span> <span class="p">[</span><span class="kc">False</span><span class="p">]),</span>
               <span class="s1">&#39;dropout_prob_theta&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="s1">&#39;dropout_prob_theta&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span>
               <span class="s1">&#39;dropout_prob_exogenous&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="s1">&#39;dropout_prob_exogenous&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span>
               <span class="s1">&#39;learning_rate&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">loguniform</span><span class="p">(</span><span class="s1">&#39;learning_rate&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mf">5e-4</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mf">0.001</span><span class="p">)),</span>
               <span class="s1">&#39;lr_decay&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="s1">&#39;lr_decay&#39;</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span>
               <span class="s1">&#39;lr_decay_step_size&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;lr_decay_step_size&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">100</span><span class="p">]),</span> 
               <span class="s1">&#39;weight_decay&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">loguniform</span><span class="p">(</span><span class="s1">&#39;weight_decay&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mf">5e-5</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mf">5e-3</span><span class="p">)),</span>
               <span class="s1">&#39;max_epochs&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;max_epochs&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">10</span><span class="p">]),</span> <span class="c1">#&#39;n_iterations&#39;: hp.choice(&#39;n_iterations&#39;, [10])</span>
               <span class="s1">&#39;max_steps&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;max_steps&#39;</span><span class="p">,</span> <span class="p">[</span><span class="kc">None</span><span class="p">]),</span>
               <span class="s1">&#39;early_stop_patience&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;early_stop_patience&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">16</span><span class="p">]),</span>
               <span class="s1">&#39;eval_freq&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;eval_freq&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">50</span><span class="p">]),</span>
               <span class="s1">&#39;n_val_weeks&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;n_val_weeks&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">52</span><span class="o">*</span><span class="mi">2</span><span class="p">]),</span>
               <span class="s1">&#39;loss_train&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;loss&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;MAE&#39;</span><span class="p">]),</span>
               <span class="s1">&#39;loss_hypar&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;loss_hypar&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">]),</span>                
               <span class="s1">&#39;loss_valid&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;loss_valid&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;MAE&#39;</span><span class="p">]),</span> <span class="c1">#[args.val_loss]),</span>
               <span class="s1">&#39;l1_theta&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;l1_theta&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">]),</span>
               <span class="c1"># Data parameters</span>
               <span class="s1">&#39;len_sample_chunks&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;len_sample_chunks&#39;</span><span class="p">,</span> <span class="p">[</span><span class="kc">None</span><span class="p">]),</span>
               <span class="s1">&#39;normalizer_y&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;normalizer_y&#39;</span><span class="p">,</span> <span class="p">[</span><span class="kc">None</span><span class="p">]),</span>
               <span class="s1">&#39;normalizer_x&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;normalizer_x&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;median&#39;</span><span class="p">]),</span>
               <span class="s1">&#39;window_sampling_limit&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;window_sampling_limit&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">100_000</span><span class="p">]),</span>
               <span class="s1">&#39;complete_inputs&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;complete_inputs&#39;</span><span class="p">,</span> <span class="p">[</span><span class="kc">False</span><span class="p">]),</span>
               <span class="s1">&#39;complete_sample&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;complete_sample&#39;</span><span class="p">,</span> <span class="p">[</span><span class="kc">False</span><span class="p">]),</span>                
               <span class="s1">&#39;frequency&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;frequency&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;H&#39;</span><span class="p">]),</span>
               <span class="s1">&#39;seasonality&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;seasonality&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">24</span><span class="p">]),</span>      
               <span class="s1">&#39;idx_to_sample_freq&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;idx_to_sample_freq&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">24</span><span class="p">]),</span>
               <span class="s1">&#39;val_idx_to_sample_freq&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;val_idx_to_sample_freq&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">24</span><span class="p">]),</span>
               <span class="s1">&#39;batch_size&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;batch_size&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">256</span><span class="p">]),</span>
               <span class="s1">&#39;n_series_per_batch&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;n_series_per_batch&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">]),</span>
               <span class="s1">&#39;random_seed&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">quniform</span><span class="p">(</span><span class="s1">&#39;random_seed&#39;</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
               <span class="s1">&#39;device&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;device&#39;</span><span class="p">,</span> <span class="p">[</span><span class="n">device</span><span class="p">])}</span>

<span class="n">mc</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;model&#39;</span><span class="p">:</span><span class="s1">&#39;nbeats&#39;</span><span class="p">,</span>
      <span class="c1"># Architecture parameters</span>
      <span class="s1">&#39;n_time_in&#39;</span><span class="p">:</span> <span class="mi">7</span><span class="o">*</span><span class="mi">24</span><span class="p">,</span>
      <span class="s1">&#39;n_time_out&#39;</span><span class="p">:</span> <span class="mi">24</span><span class="p">,</span>
      <span class="s1">&#39;n_x_hidden&#39;</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
      <span class="s1">&#39;n_s_hidden&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
      <span class="s1">&#39;shared_weights&#39;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
      <span class="s1">&#39;activation&#39;</span><span class="p">:</span> <span class="s1">&#39;SELU&#39;</span><span class="p">,</span>
      <span class="s1">&#39;initialization&#39;</span><span class="p">:</span> <span class="s1">&#39;he_normal&#39;</span><span class="p">,</span>
      <span class="s1">&#39;stack_types&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;exogenous_tcn&#39;</span><span class="p">]</span><span class="o">+</span><span class="mi">1</span><span class="o">*</span><span class="p">[</span><span class="s1">&#39;identity&#39;</span><span class="p">],</span>
      <span class="s1">&#39;n_blocks&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
      <span class="s1">&#39;n_layers&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
      <span class="s1">&#39;n_hidden&#39;</span><span class="p">:</span> <span class="mi">364</span><span class="p">,</span>
      <span class="s1">&#39;n_polynomials&#39;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
      <span class="s1">&#39;n_harmonics&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
      <span class="c1"># Regularization and optimization parameters</span>
      <span class="s1">&#39;max_epochs&#39;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span> <span class="c1">#&#39;n_iterations&#39;: 100,</span>
      <span class="s1">&#39;max_steps&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>      
      <span class="s1">&#39;early_stop_patience&#39;</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span>
      <span class="s1">&#39;batch_normalization&#39;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
      <span class="s1">&#39;dropout_prob_theta&#39;</span><span class="p">:</span> <span class="mf">0.2</span><span class="p">,</span>
      <span class="s1">&#39;learning_rate&#39;</span><span class="p">:</span> <span class="mf">0.0005</span><span class="p">,</span> <span class="c1">#0.002,</span>
      <span class="s1">&#39;lr_decay&#39;</span><span class="p">:</span> <span class="mf">0.64</span><span class="p">,</span>
      <span class="s1">&#39;lr_decay_step_size&#39;</span><span class="p">:</span> <span class="mi">100</span><span class="p">,</span>
      <span class="s1">&#39;weight_decay&#39;</span><span class="p">:</span> <span class="mf">0.00015</span><span class="p">,</span>
      <span class="s1">&#39;eval_freq&#39;</span><span class="p">:</span> <span class="mi">50</span><span class="p">,</span>
      <span class="s1">&#39;n_val_weeks&#39;</span><span class="p">:</span> <span class="mi">52</span><span class="o">*</span><span class="mi">2</span><span class="p">,</span>
      <span class="s1">&#39;loss_train&#39;</span><span class="p">:</span> <span class="s1">&#39;PINBALL&#39;</span><span class="p">,</span>
      <span class="s1">&#39;loss_hypar&#39;</span><span class="p">:</span> <span class="mf">0.5</span><span class="p">,</span> <span class="c1">#0.49,</span>
      <span class="s1">&#39;loss_valid&#39;</span><span class="p">:</span> <span class="s1">&#39;MAE&#39;</span><span class="p">,</span>
      <span class="s1">&#39;l1_theta&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
      <span class="c1"># Data parameters</span>
      <span class="s1">&#39;normalizer_y&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
      <span class="s1">&#39;normalizer_x&#39;</span><span class="p">:</span> <span class="s1">&#39;median&#39;</span><span class="p">,</span>
      <span class="s1">&#39;window_sampling_limit&#39;</span><span class="p">:</span> <span class="mi">100_000</span><span class="p">,</span>
      <span class="s1">&#39;complete_inputs&#39;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
      <span class="s1">&#39;frequency&#39;</span><span class="p">:</span><span class="s1">&#39;H&#39;</span><span class="p">,</span>
      <span class="s1">&#39;seasonality&#39;</span><span class="p">:</span> <span class="mi">24</span><span class="p">,</span>
      <span class="s1">&#39;idx_to_sample_freq&#39;</span><span class="p">:</span> <span class="mi">24</span><span class="p">,</span>
      <span class="s1">&#39;val_idx_to_sample_freq&#39;</span><span class="p">:</span> <span class="mi">24</span><span class="p">,</span>
      <span class="s1">&#39;batch_size&#39;</span><span class="p">:</span> <span class="mi">256</span><span class="p">,</span>
      <span class="s1">&#39;n_series_per_batch&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
      <span class="s1">&#39;random_seed&#39;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
      <span class="s1">&#39;device&#39;</span><span class="p">:</span> <span class="s1">&#39;cpu&#39;</span><span class="p">}</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">esrnn_space</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;model&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;model&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;esrnn&#39;</span><span class="p">]),</span>
               <span class="s1">&#39;mode&#39;</span><span class="p">:</span> <span class="s1">&#39;full&#39;</span><span class="p">,</span>
               <span class="c1"># Architecture parameters</span>
               <span class="s1">&#39;input_size_multiplier&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;input_size_multiplier&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">7</span><span class="p">]),</span>
               <span class="s1">&#39;output_size&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;output_size&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">24</span><span class="p">]),</span>
               <span class="s1">&#39;dilations&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;dilations&#39;</span><span class="p">,</span> <span class="p">[</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]],</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">14</span><span class="p">]]</span> <span class="p">]),</span>
               <span class="s1">&#39;es_component&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;es_component&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;multiplicative&#39;</span><span class="p">]),</span>
               <span class="s1">&#39;cell_type&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;cell_type&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;LSTM&#39;</span><span class="p">]),</span>
               <span class="s1">&#39;state_hsize&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">quniform</span><span class="p">(</span><span class="s1">&#39;state_hsize&#39;</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span>
               <span class="s1">&#39;add_nl_layer&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;add_nl_layer&#39;</span><span class="p">,</span> <span class="p">[</span><span class="kc">True</span><span class="p">,</span> <span class="kc">False</span><span class="p">]),</span>
               <span class="s1">&#39;seasonality&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;seasonality&#39;</span><span class="p">,</span> <span class="p">[</span> <span class="p">[</span><span class="mi">24</span><span class="p">]</span> <span class="p">]),</span>
               <span class="c1"># Regularization and optimization parameters</span>
               <span class="s1">&#39;max_epochs&#39;</span><span class="p">:</span><span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;max_epochs&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">10</span><span class="p">]),</span>
               <span class="s1">&#39;max_steps&#39;</span><span class="p">:</span><span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;max_steps&#39;</span><span class="p">,</span> <span class="p">[</span><span class="kc">None</span><span class="p">]),</span>
               <span class="s1">&#39;early_stop_patience&#39;</span><span class="p">:</span><span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;early_stop_patience&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">10</span><span class="p">]),</span>
               <span class="s1">&#39;eval_freq&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;eval_freq&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">10</span><span class="p">]),</span>
               <span class="s1">&#39;batch_size&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;batch_size&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">32</span><span class="p">]),</span>
               <span class="s1">&#39;learning_rate&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">loguniform</span><span class="p">(</span><span class="s1">&#39;learning_rate&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mf">5e-4</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mf">0.01</span><span class="p">)),</span>
               <span class="s1">&#39;lr_decay&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">quniform</span><span class="p">(</span><span class="s1">&#39;lr_decay&#39;</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">),</span>
               <span class="s1">&#39;lr_decay_step_size&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;lr_decay_step_size&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">100</span><span class="p">]),</span> 
               <span class="s1">&#39;per_series_lr_multip&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;per_series_lr_multip&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">]),</span>
               <span class="s1">&#39;gradient_eps&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;gradient_eps&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mf">1e-8</span><span class="p">]),</span>
               <span class="s1">&#39;gradient_clipping_threshold&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;gradient_clipping_threshold&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">50</span><span class="p">]),</span>
               <span class="s1">&#39;rnn_weight_decay&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;rnn_weight_decay&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.0005</span><span class="p">,</span> <span class="mf">0.005</span><span class="p">]),</span>
               <span class="s1">&#39;noise_std&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">loguniform</span><span class="p">(</span><span class="s1">&#39;noise_std&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mf">0.0001</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mf">0.001</span><span class="p">)),</span>
               <span class="s1">&#39;level_variability_penalty&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">quniform</span><span class="p">(</span><span class="s1">&#39;level_variability_penalty&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span>
               <span class="s1">&#39;testing_percentile&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;testing_percentile&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">50</span><span class="p">]),</span>
               <span class="s1">&#39;training_percentile&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;training_percentile&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">48</span><span class="p">,</span> <span class="mi">49</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">51</span><span class="p">]),</span>
               <span class="s1">&#39;random_seed&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">quniform</span><span class="p">(</span><span class="s1">&#39;random_seed&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
               <span class="s1">&#39;loss_train&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;loss_train&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;SMYL&#39;</span><span class="p">]),</span>
               <span class="s1">&#39;loss_valid&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;loss_valid&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;MAE&#39;</span><span class="p">]),</span>
               <span class="c1"># Data parameters</span>
               <span class="s1">&#39;len_sample_chunks&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;len_sample_chunks&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">7</span><span class="o">*</span><span class="mi">3</span><span class="o">*</span><span class="mi">24</span><span class="p">]),</span>
               <span class="s1">&#39;window_sampling_limit&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;window_sampling_limit&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">500_000</span><span class="p">]),</span>
               <span class="s1">&#39;complete_inputs&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;complete_inputs&#39;</span><span class="p">,</span> <span class="p">[</span><span class="kc">True</span><span class="p">]),</span>
               <span class="s1">&#39;complete_sample&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;complete_sample&#39;</span><span class="p">,</span> <span class="p">[</span><span class="kc">True</span><span class="p">]),</span>
               <span class="s1">&#39;idx_to_sample_freq&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;idx_to_sample_freq&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">24</span><span class="p">]),</span>
               <span class="s1">&#39;val_idx_to_sample_freq&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;val_idx_to_sample_freq&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">24</span><span class="p">]),</span>
               <span class="s1">&#39;n_series_per_batch&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;n_series_per_batch&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">]),</span>
               <span class="s1">&#39;normalizer_y&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;normalizer_y&#39;</span><span class="p">,</span> <span class="p">[</span><span class="kc">None</span><span class="p">]),</span>
               <span class="s1">&#39;normalizer_x&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;normalizer_x&#39;</span><span class="p">,</span>  <span class="p">[</span><span class="kc">None</span><span class="p">])}</span>

<span class="n">mc</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;model&#39;</span><span class="p">:</span><span class="s1">&#39;esrnn&#39;</span><span class="p">,</span>
      <span class="s1">&#39;mode&#39;</span><span class="p">:</span> <span class="s1">&#39;full&#39;</span><span class="p">,</span>
      <span class="c1"># Architecture parameters</span>
      <span class="s1">&#39;n_series&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
      <span class="s1">&#39;n_time_in&#39;</span><span class="p">:</span> <span class="mi">7</span><span class="o">*</span><span class="mi">24</span><span class="p">,</span>
      <span class="s1">&#39;n_time_out&#39;</span><span class="p">:</span> <span class="mi">24</span><span class="p">,</span>
      <span class="s1">&#39;n_x&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
      <span class="s1">&#39;n_s&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
      <span class="s1">&#39;dilations&#39;</span><span class="p">:</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">7</span><span class="p">]],</span>
      <span class="s1">&#39;es_component&#39;</span><span class="p">:</span> <span class="s1">&#39;multiplicative&#39;</span><span class="p">,</span>
      <span class="s1">&#39;cell_type&#39;</span><span class="p">:</span> <span class="s1">&#39;LSTM&#39;</span><span class="p">,</span>
      <span class="s1">&#39;state_hsize&#39;</span><span class="p">:</span> <span class="mi">50</span><span class="p">,</span>
      <span class="s1">&#39;add_nl_layer&#39;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
      <span class="s1">&#39;seasonality&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">24</span><span class="p">],</span>
      <span class="c1"># Regularization and optimization parameters</span>
      <span class="s1">&#39;max_epochs&#39;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span> <span class="c1">#&#39;n_iterations&#39;: 100,</span>
      <span class="s1">&#39;max_steps&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
      <span class="s1">&#39;early_stop_patience&#39;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
      <span class="s1">&#39;eval_freq&#39;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
      <span class="s1">&#39;batch_size&#39;</span><span class="p">:</span> <span class="mi">32</span><span class="p">,</span>
      <span class="s1">&#39;eq_batch_size&#39;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
      <span class="s1">&#39;learning_rate&#39;</span><span class="p">:</span> <span class="mf">0.0005</span><span class="p">,</span>
      <span class="s1">&#39;lr_decay&#39;</span><span class="p">:</span> <span class="mf">0.8</span><span class="p">,</span>
      <span class="s1">&#39;lr_decay_step_size&#39;</span><span class="p">:</span> <span class="mi">100</span><span class="p">,</span>
      <span class="s1">&#39;per_series_lr_multip&#39;</span><span class="p">:</span> <span class="mf">1.5</span><span class="p">,</span>
      <span class="s1">&#39;gradient_eps&#39;</span><span class="p">:</span> <span class="mf">1e-8</span><span class="p">,</span> 
      <span class="s1">&#39;gradient_clipping_threshold&#39;</span><span class="p">:</span> <span class="mi">20</span><span class="p">,</span>
      <span class="s1">&#39;rnn_weight_decay&#39;</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">,</span>
      <span class="s1">&#39;noise_std&#39;</span><span class="p">:</span> <span class="mf">0.0005</span><span class="p">,</span>
      <span class="s1">&#39;level_variability_penalty&#39;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
      <span class="s1">&#39;testing_percentile&#39;</span><span class="p">:</span> <span class="mi">50</span><span class="p">,</span>
      <span class="s1">&#39;training_percentile&#39;</span><span class="p">:</span> <span class="mi">50</span><span class="p">,</span>
      <span class="s1">&#39;random_seed&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
      <span class="s1">&#39;loss_train&#39;</span><span class="p">:</span> <span class="s1">&#39;SMYL&#39;</span><span class="p">,</span>
      <span class="s1">&#39;loss_valid&#39;</span><span class="p">:</span> <span class="s1">&#39;MAE&#39;</span><span class="p">,</span>
      <span class="c1"># Data parameters</span>
      <span class="s1">&#39;len_sample_chunks&#39;</span><span class="p">:</span> <span class="mi">7</span><span class="o">*</span><span class="mi">4</span><span class="o">*</span><span class="mi">24</span><span class="p">,</span>
      <span class="s1">&#39;window_sampling_limit&#39;</span><span class="p">:</span> <span class="mi">500_000</span><span class="p">,</span>
      <span class="s1">&#39;complete_inputs&#39;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
      <span class="s1">&#39;idx_to_sample_freq&#39;</span><span class="p">:</span> <span class="mi">24</span><span class="p">,</span>
      <span class="s1">&#39;val_idx_to_sample_freq&#39;</span><span class="p">:</span> <span class="mi">24</span><span class="p">,</span>
      <span class="s1">&#39;n_series_per_batch&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
      <span class="s1">&#39;normalizer_y&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
      <span class="s1">&#39;normalizer_x&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">}</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">instantiate_esrnn</span><span class="p">(</span><span class="n">mc</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">nixtlats.data.datasets.epf</span> <span class="kn">import</span> <span class="n">EPF</span><span class="p">,</span> <span class="n">EPFInfo</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;NP&#39;</span><span class="p">]</span>

<span class="n">Y_df</span><span class="p">,</span> <span class="n">X_df</span><span class="p">,</span> <span class="n">S_df</span> <span class="o">=</span> <span class="n">EPF</span><span class="o">.</span><span class="n">load_groups</span><span class="p">(</span><span class="n">directory</span><span class="o">=</span><span class="s1">&#39;data&#39;</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="n">dataset</span><span class="p">)</span>

<span class="n">X_df</span> <span class="o">=</span> <span class="n">X_df</span><span class="p">[[</span><span class="s1">&#39;unique_id&#39;</span><span class="p">,</span> <span class="s1">&#39;ds&#39;</span><span class="p">,</span> <span class="s1">&#39;week_day&#39;</span><span class="p">]]</span>
<span class="n">Y_min</span> <span class="o">=</span> <span class="n">Y_df</span><span class="o">.</span><span class="n">y</span><span class="o">.</span><span class="n">min</span><span class="p">()</span>
<span class="c1">#Y_df.y = Y_df.y - Y_min + 20</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">Y_df</span><span class="o">.</span><span class="n">y</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
"
>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">result</span> <span class="o">=</span> <span class="n">evaluate_model</span><span class="p">(</span><span class="n">loss_function</span><span class="o">=</span><span class="n">mae</span><span class="p">,</span> <span class="n">mc</span><span class="o">=</span><span class="n">mc</span><span class="p">,</span> 
                        <span class="n">S_df</span><span class="o">=</span><span class="n">S_df</span><span class="p">,</span> <span class="n">Y_df</span><span class="o">=</span><span class="n">Y_df</span><span class="p">,</span> <span class="n">X_df</span><span class="o">=</span><span class="n">X_df</span><span class="p">,</span> <span class="n">f_cols</span><span class="o">=</span><span class="p">[],</span>
                        <span class="n">ds_in_test</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">ds_in_val</span><span class="o">=</span><span class="mi">728</span><span class="o">*</span><span class="mi">24</span><span class="p">,</span>
                        <span class="n">n_uids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">n_val_windows</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">freq</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                        <span class="n">is_val_random</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">loss_kwargs</span><span class="o">=</span><span class="p">{})</span>
<span class="n">result</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>INFO:root:Train Validation splits

INFO:root:                              ds                    
                             min                 max
unique_id sample_mask                               
NP        0           2016-12-27 2018-12-24 23:00:00
          1           2013-01-01 2016-12-26 23:00:00
INFO:root:
Total data 			52416 time stamps 
Available percentage=100.0, 	52416 time stamps 
Insample  percentage=66.67, 	34944 time stamps 
Outsample percentage=33.33, 	17472 time stamps 

</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>===============================================

model                                   esrnn
mode                                     full
n_series                                    1
n_time_in                                 168
n_time_out                                 24
n_x                                         1
n_s                                         1
dilations                       [[1, 2], [7]]
es_component                   multiplicative
cell_type                                LSTM
state_hsize                                50
add_nl_layer                            False
seasonality                              [24]
max_epochs                                 10
max_steps                                None
early_stop_patience                        10
eval_freq                                  10
batch_size                                 32
eq_batch_size                           False
learning_rate                          0.0005
lr_decay                                  0.8
lr_decay_step_size                        100
per_series_lr_multip                      1.5
gradient_eps                              0.0
gradient_clipping_threshold                20
rnn_weight_decay                          0.0
noise_std                              0.0005
level_variability_penalty                  10
testing_percentile                         50
training_percentile                        50
random_seed                                 1
loss_train                               SMYL
loss_valid                                MAE
len_sample_chunks                         672
window_sampling_limit                  500000
complete_inputs                          True
idx_to_sample_freq                         24
val_idx_to_sample_freq                     24
n_series_per_batch                          1
normalizer_y                             None
normalizer_x                             None
dtype: object
===============================================

</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>INFO:root:Train Validation splits

INFO:root:                              ds                    
                             min                 max
unique_id sample_mask                               
NP        0           2013-01-01 2016-12-26 23:00:00
          1           2016-12-27 2018-12-24 23:00:00
INFO:root:
Total data 			52416 time stamps 
Available percentage=100.0, 	52416 time stamps 
Insample  percentage=33.33, 	17472 time stamps 
Outsample percentage=66.67, 	34944 time stamps 

INFO:root:Train Validation splits

INFO:root:                              ds                    
                             min                 max
unique_id sample_mask                               
NP        0           2013-01-01 2018-12-24 23:00:00
INFO:root:
Total data 			52416 time stamps 
Available percentage=100.0, 	52416 time stamps 
Insample  percentage=0.0, 	0 time stamps 
Outsample percentage=100.0, 	52416 time stamps 

GPU available: True, used: False
TPU available: False, using: 0 TPU cores
/home/ubuntu/anaconda3/envs/nixtla/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: GPU available but not used. Set the gpus flag in your trainer `Trainer(gpus=1)` or script `--gpus=1`.
  warnings.warn(*args, **kwargs)

  | Name  | Type   | Params
---------------------------------
0 | esrnn | _ESRNN | 124 K 
---------------------------------
124 K     Trainable params
0         Non-trainable params
124 K     Total params
0.499     Total estimated model params size (MB)
/home/ubuntu/anaconda3/envs/nixtla/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  warnings.warn(*args, **kwargs)
/home/ubuntu/anaconda3/envs/nixtla/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  warnings.warn(*args, **kwargs)
Metric val_loss improved. New best score: 4.635
Metric val_loss improved by 0.307 &gt;= min_delta = 0.0001. New best score: 4.328
Metric val_loss improved by 0.218 &gt;= min_delta = 0.0001. New best score: 4.110
Metric val_loss improved by 0.166 &gt;= min_delta = 0.0001. New best score: 3.944
Metric val_loss improved by 0.136 &gt;= min_delta = 0.0001. New best score: 3.808
Metric val_loss improved by 0.114 &gt;= min_delta = 0.0001. New best score: 3.694
Metric val_loss improved by 0.094 &gt;= min_delta = 0.0001. New best score: 3.600
Metric val_loss improved by 0.080 &gt;= min_delta = 0.0001. New best score: 3.520
Metric val_loss improved by 0.066 &gt;= min_delta = 0.0001. New best score: 3.454
Metric val_loss improved by 0.054 &gt;= min_delta = 0.0001. New best score: 3.399
/home/ubuntu/anaconda3/envs/nixtla/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, predict dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  warnings.warn(*args, **kwargs)
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>y_true.shape (#n_series, #n_fcds, #lt): (728, 24)
y_hat.shape (#n_series, #n_fcds, #lt): (728, 24)


</pre>
</div>
</div>

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>{&#39;loss&#39;: 3.4991977,
 &#39;mc&#39;: {&#39;model&#39;: &#39;esrnn&#39;,
  &#39;mode&#39;: &#39;full&#39;,
  &#39;n_series&#39;: 1,
  &#39;n_time_in&#39;: 168,
  &#39;n_time_out&#39;: 24,
  &#39;n_x&#39;: 1,
  &#39;n_s&#39;: 1,
  &#39;dilations&#39;: [[1, 2], [7]],
  &#39;es_component&#39;: &#39;multiplicative&#39;,
  &#39;cell_type&#39;: &#39;LSTM&#39;,
  &#39;state_hsize&#39;: 50,
  &#39;add_nl_layer&#39;: False,
  &#39;seasonality&#39;: [24],
  &#39;max_epochs&#39;: 10,
  &#39;max_steps&#39;: None,
  &#39;early_stop_patience&#39;: 10,
  &#39;eval_freq&#39;: 10,
  &#39;batch_size&#39;: 32,
  &#39;eq_batch_size&#39;: False,
  &#39;learning_rate&#39;: 0.0005,
  &#39;lr_decay&#39;: 0.8,
  &#39;lr_decay_step_size&#39;: 100,
  &#39;per_series_lr_multip&#39;: 1.5,
  &#39;gradient_eps&#39;: 1e-08,
  &#39;gradient_clipping_threshold&#39;: 20,
  &#39;rnn_weight_decay&#39;: 0.0,
  &#39;noise_std&#39;: 0.0005,
  &#39;level_variability_penalty&#39;: 10,
  &#39;testing_percentile&#39;: 50,
  &#39;training_percentile&#39;: 50,
  &#39;random_seed&#39;: 1,
  &#39;loss_train&#39;: &#39;SMYL&#39;,
  &#39;loss_valid&#39;: &#39;MAE&#39;,
  &#39;len_sample_chunks&#39;: 672,
  &#39;window_sampling_limit&#39;: 500000,
  &#39;complete_inputs&#39;: True,
  &#39;idx_to_sample_freq&#39;: 24,
  &#39;val_idx_to_sample_freq&#39;: 24,
  &#39;n_series_per_batch&#39;: 1,
  &#39;normalizer_y&#39;: None,
  &#39;normalizer_x&#39;: None},
 &#39;y_true&#39;: array([[24.08, 22.52, 20.13, ..., 28.37, 27.24, 25.73],
        [26.45, 26.26, 26.24, ..., 30.65, 30.02, 29.37],
        [29.26, 28.72, 28.29, ..., 30.01, 29.44, 28.76],
        ...,
        [48.39, 47.72, 47.23, ..., 52.05, 51.09, 50.47],
        [51.49, 50.83, 50.74, ..., 53.99, 53.86, 52.32],
        [51.09, 50.19, 48.98, ..., 49.09, 49.02, 48.1 ]], dtype=float32),
 &#39;y_hat&#39;: array([[24.33769 , 22.102844, 21.10142 , ..., 27.454004, 27.137604,
         26.435623],
        [24.343973, 22.348608, 21.180744, ..., 27.49965 , 26.88523 ,
         26.388895],
        [27.938967, 25.281277, 24.288834, ..., 31.5685  , 30.135736,
         30.171461],
        ...,
        [46.256874, 46.90347 , 46.982185, ..., 49.996956, 50.326725,
         51.179863],
        [48.246696, 47.63383 , 47.224636, ..., 52.28208 , 51.462975,
         52.726986],
        [51.020382, 50.916542, 50.6636  , ..., 52.560085, 52.79641 ,
         53.634846]], dtype=float32),
 &#39;run_time&#39;: 5.3648741245269775,
 &#39;status&#39;: &#39;ok&#39;}</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;y_hat&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">flatten</span><span class="p">())</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">Y_df</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">][</span><span class="o">-</span><span class="mi">728</span><span class="o">*</span><span class="mi">24</span><span class="p">:]</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>[&lt;matplotlib.lines.Line2D at 0x7fe707fccc10&gt;]</pre>
</div>

</div>

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
"
>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">trials</span> <span class="o">=</span> <span class="n">hyperopt_tunning</span><span class="p">(</span><span class="n">space</span><span class="o">=</span><span class="n">nbeats_space</span><span class="p">,</span> <span class="n">hyperopt_max_evals</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">loss_function</span><span class="o">=</span><span class="n">mae</span><span class="p">,</span>
                          <span class="n">S_df</span><span class="o">=</span><span class="n">S_df</span><span class="p">,</span> <span class="n">Y_df</span><span class="o">=</span><span class="n">Y_df</span><span class="p">,</span> <span class="n">X_df</span><span class="o">=</span><span class="n">X_df</span><span class="p">,</span> <span class="n">f_cols</span><span class="o">=</span><span class="p">[],</span>
                          <span class="n">ds_in_val</span><span class="o">=</span><span class="mi">728</span><span class="o">*</span><span class="mi">24</span><span class="p">,</span> <span class="n">n_uids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">n_val_windows</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">freq</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                          <span class="n">is_val_random</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">loss_kwargs</span><span class="o">=</span><span class="p">{})</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>  0%|          | 0/2 [00:00&lt;?, ?trial/s, best loss=?]</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>INFO:hyperopt.tpe:build_posterior_wrapper took 0.012679 seconds
INFO:hyperopt.tpe:TPE using 0 trials
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>===============================================      

activation                                     SELU  
batch_normalization                           False
batch_size                                      256
complete_inputs                               False
complete_sample                               False
device                                         cuda
dropout_prob_exogenous                     0.293461
dropout_prob_theta                         0.243891
early_stop_patience                              16
eval_freq                                        50
frequency                                         H
idx_to_sample_freq                               24
initialization                        glorot_normal
l1_theta                                          0
learning_rate                              0.000737
len_sample_chunks                              None
loss_hypar                                      0.5
loss_train                                      MAE
loss_valid                                      MAE
lr_decay                                   0.310092
lr_decay_step_size                              100
max_epochs                                       10
max_steps                                      None
mode                                         simple
model                                        nbeats
n_blocks                                     (1, 1)
n_harmonics                                       1
n_hidden                                        256
n_layers                                     (2, 2)
n_polynomials                                     2
n_s_hidden                                        0
n_series_per_batch                                1
n_time_in                                       168
n_time_out                                       24
n_val_weeks                                     104
n_x_hidden                                      9.0
normalizer_x                                 median
normalizer_y                                   None
random_seed                                    17.0
seasonality                                      24
shared_weights                                False
stack_types               (identity, exogenous_tcn)
val_idx_to_sample_freq                           24
weight_decay                               0.000063
window_sampling_limit                        100000
dtype: object
===============================================      

  0%|          | 0/2 [00:00&lt;?, ?trial/s, best loss=?]</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>INFO:root:Train Validation splits

INFO:root:                              ds                    
                             min                 max
unique_id sample_mask                               
NP        0           2016-12-27 2018-12-24 23:00:00
          1           2013-01-01 2016-12-26 23:00:00
INFO:root:
Total data 			52416 time stamps 
Available percentage=100.0, 	52416 time stamps 
Insample  percentage=66.67, 	34944 time stamps 
Outsample percentage=33.33, 	17472 time stamps 

INFO:root:Train Validation splits

INFO:root:                              ds                    
                             min                 max
unique_id sample_mask                               
NP        0           2013-01-01 2016-12-26 23:00:00
          1           2016-12-27 2018-12-24 23:00:00
INFO:root:
Total data 			52416 time stamps 
Available percentage=100.0, 	52416 time stamps 
Insample  percentage=33.33, 	17472 time stamps 
Outsample percentage=66.67, 	34944 time stamps 

INFO:root:Train Validation splits

INFO:root:                              ds                    
                             min                 max
unique_id sample_mask                               
NP        0           2013-01-01 2018-12-24 23:00:00
INFO:root:
Total data 			52416 time stamps 
Available percentage=100.0, 	52416 time stamps 
Insample  percentage=0.0, 	0 time stamps 
Outsample percentage=100.0, 	52416 time stamps 

GPU available: True, used: False
TPU available: False, using: 0 TPU cores
/home/ubuntu/anaconda3/envs/nixtla/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: GPU available but not used. Set the gpus flag in your trainer `Trainer(gpus=1)` or script `--gpus=1`.
  warnings.warn(*args, **kwargs)


  | Name  | Type    | Params
----------------------------------
0 | model | _NBEATS | 371 K 
----------------------------------
371 K     Trainable params
0         Non-trainable params
371 K     Total params
1.487     Total estimated model params size (MB)
/home/ubuntu/anaconda3/envs/nixtla/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  warnings.warn(*args, **kwargs)

/home/ubuntu/anaconda3/envs/nixtla/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  warnings.warn(*args, **kwargs)

Metric val_loss improved. New best score: 28.825
Metric val_loss improved by 13.843 &gt;= min_delta = 0.0001. New best score: 14.981
Metric val_loss improved by 11.207 &gt;= min_delta = 0.0001. New best score: 3.774
Metric val_loss improved by 0.530 &gt;= min_delta = 0.0001. New best score: 3.244
/home/ubuntu/anaconda3/envs/nixtla/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, predict dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  warnings.warn(*args, **kwargs)

</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>y_true.shape (#n_series, #n_fcds, #lt): (728,)       
y_hat.shape (#n_series, #n_fcds, #lt): (728,)        
 50%|     | 1/2 [00:02&lt;00:02,  2.79s/trial, best loss: 2.1017706394195557]</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>INFO:hyperopt.tpe:build_posterior_wrapper took 0.020423 seconds
INFO:hyperopt.tpe:TPE using 1/1 trials with best loss 2.101771
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>===============================================                                

activation                                SELU                                 
batch_normalization                      False
batch_size                                 256
complete_inputs                          False
complete_sample                          False
device                                    cuda
dropout_prob_exogenous                0.302431
dropout_prob_theta                    0.385168
early_stop_patience                         16
eval_freq                                   50
frequency                                    H
idx_to_sample_freq                          24
initialization                   glorot_normal
l1_theta                                     0
learning_rate                         0.000729
len_sample_chunks                         None
loss_hypar                                 0.5
loss_train                                 MAE
loss_valid                                 MAE
lr_decay                                0.3939
lr_decay_step_size                         100
max_epochs                                  10
max_steps                                 None
mode                                    simple
model                                   nbeats
n_blocks                                (1, 1)
n_harmonics                                  1
n_hidden                                   256
n_layers                                (2, 2)
n_polynomials                                2
n_s_hidden                                   0
n_series_per_batch                           1
n_time_in                                  168
n_time_out                                  24
n_val_weeks                                104
n_x_hidden                                 3.0
normalizer_x                            median
normalizer_y                              None
random_seed                               16.0
seasonality                                 24
shared_weights                           False
stack_types               (identity, identity)
val_idx_to_sample_freq                      24
weight_decay                          0.000083
window_sampling_limit                   100000
dtype: object
===============================================                                

 50%|     | 1/2 [00:02&lt;00:02,  2.79s/trial, best loss: 2.1017706394195557]</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>INFO:root:Train Validation splits

INFO:root:                              ds                    
                             min                 max
unique_id sample_mask                               
NP        0           2016-12-27 2018-12-24 23:00:00
          1           2013-01-01 2016-12-26 23:00:00
INFO:root:
Total data 			52416 time stamps 
Available percentage=100.0, 	52416 time stamps 
Insample  percentage=66.67, 	34944 time stamps 
Outsample percentage=33.33, 	17472 time stamps 

INFO:root:Train Validation splits

INFO:root:                              ds                    
                             min                 max
unique_id sample_mask                               
NP        0           2013-01-01 2016-12-26 23:00:00
          1           2016-12-27 2018-12-24 23:00:00
INFO:root:
Total data 			52416 time stamps 
Available percentage=100.0, 	52416 time stamps 
Insample  percentage=33.33, 	17472 time stamps 
Outsample percentage=66.67, 	34944 time stamps 

INFO:root:Train Validation splits

INFO:root:                              ds                    
                             min                 max
unique_id sample_mask                               
NP        0           2013-01-01 2018-12-24 23:00:00
INFO:root:
Total data 			52416 time stamps 
Available percentage=100.0, 	52416 time stamps 
Insample  percentage=0.0, 	0 time stamps 
Outsample percentage=100.0, 	52416 time stamps 

GPU available: True, used: False
TPU available: False, using: 0 TPU cores
/home/ubuntu/anaconda3/envs/nixtla/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: GPU available but not used. Set the gpus flag in your trainer `Trainer(gpus=1)` or script `--gpus=1`.
  warnings.warn(*args, **kwargs)


  | Name  | Type    | Params
----------------------------------
0 | model | _NBEATS | 415 K 
----------------------------------
415 K     Trainable params
0         Non-trainable params
415 K     Total params
1.660     Total estimated model params size (MB)
/home/ubuntu/anaconda3/envs/nixtla/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  warnings.warn(*args, **kwargs)

/home/ubuntu/anaconda3/envs/nixtla/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  warnings.warn(*args, **kwargs)

Metric val_loss improved. New best score: 3.649
Metric val_loss improved by 0.299 &gt;= min_delta = 0.0001. New best score: 3.350
Metric val_loss improved by 0.376 &gt;= min_delta = 0.0001. New best score: 2.974
Metric val_loss improved by 0.060 &gt;= min_delta = 0.0001. New best score: 2.914
Metric val_loss improved by 0.001 &gt;= min_delta = 0.0001. New best score: 2.913
Metric val_loss improved by 0.029 &gt;= min_delta = 0.0001. New best score: 2.884
Metric val_loss improved by 0.041 &gt;= min_delta = 0.0001. New best score: 2.843
Metric val_loss improved by 0.031 &gt;= min_delta = 0.0001. New best score: 2.812
Metric val_loss improved by 0.019 &gt;= min_delta = 0.0001. New best score: 2.793
/home/ubuntu/anaconda3/envs/nixtla/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, predict dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  warnings.warn(*args, **kwargs)

</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>y_true.shape (#n_series, #n_fcds, #lt): (728,)                                 
y_hat.shape (#n_series, #n_fcds, #lt): (728,)                                  
100%|| 2/2 [00:04&lt;00:00,  2.41s/trial, best loss: 1.9828330278396606]
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">trials</span><span class="o">.</span><span class="n">trials</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>[{&#39;state&#39;: 2,
  &#39;tid&#39;: 0,
  &#39;spec&#39;: None,
  &#39;result&#39;: {&#39;loss&#39;: 2.1017706394195557,
   &#39;mc&#39;: {&#39;activation&#39;: &#39;SELU&#39;,
    &#39;batch_normalization&#39;: False,
    &#39;batch_size&#39;: 256,
    &#39;complete_inputs&#39;: False,
    &#39;complete_sample&#39;: False,
    &#39;device&#39;: &#39;cuda&#39;,
    &#39;dropout_prob_exogenous&#39;: 0.2934610359635618,
    &#39;dropout_prob_theta&#39;: 0.24389096640506186,
    &#39;early_stop_patience&#39;: 16,
    &#39;eval_freq&#39;: 50,
    &#39;frequency&#39;: &#39;H&#39;,
    &#39;idx_to_sample_freq&#39;: 24,
    &#39;initialization&#39;: &#39;glorot_normal&#39;,
    &#39;l1_theta&#39;: 0,
    &#39;learning_rate&#39;: 0.0007367406100039542,
    &#39;len_sample_chunks&#39;: None,
    &#39;loss_hypar&#39;: 0.5,
    &#39;loss_train&#39;: &#39;MAE&#39;,
    &#39;loss_valid&#39;: &#39;MAE&#39;,
    &#39;lr_decay&#39;: 0.31009178056304154,
    &#39;lr_decay_step_size&#39;: 100,
    &#39;max_epochs&#39;: 10,
    &#39;max_steps&#39;: None,
    &#39;mode&#39;: &#39;simple&#39;,
    &#39;model&#39;: &#39;nbeats&#39;,
    &#39;n_blocks&#39;: (1, 1),
    &#39;n_harmonics&#39;: 1,
    &#39;n_hidden&#39;: 256,
    &#39;n_layers&#39;: (2, 2),
    &#39;n_polynomials&#39;: 2,
    &#39;n_s_hidden&#39;: 0,
    &#39;n_series_per_batch&#39;: 1,
    &#39;n_time_in&#39;: 168,
    &#39;n_time_out&#39;: 24,
    &#39;n_val_weeks&#39;: 104,
    &#39;n_x_hidden&#39;: 9.0,
    &#39;normalizer_x&#39;: &#39;median&#39;,
    &#39;normalizer_y&#39;: None,
    &#39;random_seed&#39;: 17.0,
    &#39;seasonality&#39;: 24,
    &#39;shared_weights&#39;: False,
    &#39;stack_types&#39;: (&#39;identity&#39;, &#39;exogenous_tcn&#39;),
    &#39;val_idx_to_sample_freq&#39;: 24,
    &#39;weight_decay&#39;: 6.317818377567524e-05,
    &#39;window_sampling_limit&#39;: 100000,
    &#39;n_x&#39;: 1,
    &#39;n_s&#39;: 1,
    &#39;n_theta_hidden&#39;: [[256, 256], [256, 256]]},
   &#39;y_true&#39;: array([25.73, 29.37, 28.76, 25.95, 26.71, 29.36, 30.93, 28.3 , 30.58,
          33.32, 28.86, 30.5 , 29.72, 28.69, 28.  , 25.04, 28.72, 29.79,
          29.41, 30.89, 30.99, 29.56, 27.84, 28.19, 27.36, 27.84, 28.13,
          28.42, 28.4 , 27.41, 28.66, 28.67, 28.51, 27.96, 29.31, 29.99,
          28.97, 29.96, 30.59, 30.8 , 30.62, 31.01, 32.  , 33.14, 32.63,
          32.11, 31.77, 31.04, 31.35, 31.96, 31.01, 29.98, 30.36, 28.15,
          28.87, 27.66, 29.85, 26.38, 27.82, 30.46, 28.49, 28.67, 27.58,
          29.55, 29.93, 29.03, 30.83, 29.08, 30.54, 30.97, 32.54, 30.1 ,
          29.72, 31.56, 29.66, 29.13, 29.94, 25.2 , 28.56, 27.86, 25.65,
          29.17, 27.05, 27.23, 28.04, 28.94, 29.22, 28.55, 28.05, 28.15,
          28.5 , 28.9 , 29.68, 27.87, 28.14, 27.49, 27.38, 28.06, 27.12,
          24.5 , 26.57, 27.03, 27.59, 25.54, 22.99, 25.42, 24.72, 27.11,
          28.73, 27.15, 29.23, 28.95, 29.76, 29.91, 27.27, 24.44, 25.9 ,
          29.06, 28.89, 31.36, 31.24, 31.39, 30.1 , 30.12, 27.03, 27.1 ,
          28.  , 27.99, 27.48, 29.84, 30.25, 27.39, 30.99, 31.73, 32.4 ,
          28.76, 30.05, 30.05, 30.26, 30.21, 26.95, 26.68, 26.74, 25.06,
          25.17, 24.04, 25.8 , 25.6 , 25.65, 25.42, 26.33, 24.14, 24.06,
          26.78, 26.11, 25.61, 26.57, 26.97, 26.21, 26.29, 26.2 , 13.97,
          19.62, 26.43, 24.68, 24.5 , 24.28, 20.64, 25.43, 24.76, 24.89,
          22.06, 23.07, 24.79, 24.81, 24.03, 26.5 , 26.34, 24.42, 23.49,
          21.8 , 23.48, 24.23, 23.69, 23.57, 22.99, 24.14, 24.19, 24.33,
          25.6 , 26.23, 26.55, 27.41, 26.73, 27.4 , 27.12, 27.44, 26.51,
          26.96, 26.51, 26.33, 25.51, 25.02, 25.63, 25.47, 26.65, 27.45,
          26.47, 26.65, 26.77, 27.02, 26.89, 27.46, 26.35, 26.75, 26.36,
          26.42, 26.7 , 27.04, 25.04, 23.65, 24.86, 25.4 , 26.11, 24.57,
          25.86, 25.87, 25.57, 21.97, 26.01, 26.3 , 26.17, 26.85, 25.74,
          25.99, 23.77, 25.79, 27.04, 27.77, 27.7 , 28.65, 29.16, 29.65,
          30.36, 29.78, 30.32, 30.15, 31.4 , 32.27, 31.37, 31.55, 31.47,
          31.56, 29.66, 30.83, 29.27, 29.87, 27.35, 27.67, 28.47, 26.5 ,
          28.19, 30.22, 31.05, 31.52, 31.57, 31.55, 31.55, 30.51, 30.65,
          30.42, 29.68, 29.51, 29.8 , 29.38, 29.81, 27.96, 27.85, 24.06,
          20.41, 24.3 , 17.11, 26.12, 26.91, 26.6 , 28.79, 30.26, 28.56,
          27.11, 27.63, 26.63, 26.03, 21.72, 26.06, 25.26, 28.37, 27.81,
          27.91, 26.  , 28.97, 29.57, 27.19, 27.1 , 27.64, 20.1 , 20.06,
          20.03, 28.5 , 28.95, 23.95, 28.85, 27.1 , 24.96, 28.03, 28.25,
          28.3 , 28.57, 26.03, 26.01, 28.29, 29.71, 30.37, 29.74, 29.66,
          27.57, 27.58, 27.88, 28.67, 31.55, 29.76, 27.01, 26.05, 29.13,
          28.24, 28.01, 28.62, 30.82, 32.18, 30.92, 30.97, 27.9 , 28.24,
          29.83, 27.07, 27.94, 24.64, 26.06, 26.79, 27.52, 28.21, 27.96,
          26.49, 28.26, 29.03, 29.02, 28.59, 30.43, 29.06, 28.  , 27.3 ,
          26.94, 20.08, 25.65, 25.22, 24.51, 28.22, 26.82, 26.68, 25.59,
          25.43, 25.02, 27.55, 27.31, 28.07, 29.66, 29.03, 27.75, 28.11,
          28.6 , 30.37, 30.05, 29.53, 28.79, 28.82, 25.69, 28.05, 29.31,
          30.45, 32.79, 31.64, 30.52, 33.06, 28.08, 25.5 , 28.98, 30.08,
          28.01, 27.57, 28.33, 30.14, 27.7 , 29.53, 31.9 , 32.12, 32.78,
          34.9 , 33.7 , 32.74, 30.91, 31.17, 29.96, 28.87, 31.02, 31.14,
          29.74, 32.22, 32.97, 34.89, 33.77, 35.77, 37.41, 37.34, 38.21,
          36.05, 36.93, 36.48, 38.98, 37.72, 37.73, 39.09, 39.81, 38.41,
          38.17, 40.4 , 38.6 , 37.29, 35.57, 37.9 , 36.03, 35.68, 36.62,
          37.64, 37.75, 37.68, 37.43, 36.79, 37.14, 37.48, 39.6 , 37.88,
          41.43, 39.59, 38.68, 39.54, 42.1 , 39.38, 39.26, 40.87, 39.52,
          39.56, 39.22, 39.22, 39.13, 38.39, 38.09, 37.42, 38.62, 38.68,
          38.01, 37.64, 38.96, 38.5 , 37.96, 40.01, 39.18, 41.55, 39.83,
          38.04, 34.93, 32.64, 35.34, 33.97, 34.21, 34.77, 34.35, 35.93,
          36.02, 35.91, 33.77, 32.13, 31.09, 36.03, 35.26, 34.15, 28.06,
          27.2 , 24.88, 12.42,  4.44,  9.48, 30.11, 19.5 , 20.07, 32.98,
          32.97, 24.98, 29.06, 33.81, 31.77, 15.27, 28.05, 36.81, 36.07,
          36.34, 39.04, 39.12, 37.69, 39.17, 38.85, 42.26, 41.36, 44.16,
          42.2 , 40.66, 43.07, 44.91, 45.17, 46.5 , 47.28, 45.29, 46.73,
          45.1 , 46.11, 46.91, 41.1 , 45.56, 44.46, 44.48, 41.49, 42.22,
          41.97, 38.05, 37.47, 41.01, 43.27, 43.92, 44.45, 45.29, 42.82,
          44.73, 45.48, 46.78, 48.94, 50.24, 50.84, 50.19, 47.56, 48.25,
          49.76, 49.68, 50.8 , 51.54, 51.2 , 51.94, 52.4 , 53.18, 51.73,
          51.33, 50.51, 51.9 , 52.56, 52.44, 53.29, 53.92, 54.39, 54.33,
          51.99, 50.87, 50.53, 53.  , 52.67, 52.72, 53.6 , 53.33, 53.44,
          49.6 , 51.51, 53.6 , 51.78, 50.74, 47.48, 49.08, 49.32, 49.43,
          49.87, 50.53, 50.59, 48.72, 48.83, 43.64, 48.23, 48.17, 49.92,
          48.26, 50.62, 49.63, 49.77, 50.14, 50.15, 55.8 , 52.37, 54.05,
          56.58, 55.2 , 53.87, 53.8 , 56.61, 56.84, 54.61, 53.44, 53.65,
          53.05, 54.19, 50.99, 49.92, 53.13, 50.19, 49.35, 48.58, 48.91,
          44.41, 39.04, 43.01,  3.27, 21.04, 30.61, 39.25, 34.23, 20.99,
          30.48, 41.79, 19.92, 42.05, 42.76, 42.51, 44.67, 43.34, 45.61,
          45.34, 47.03, 44.08, 41.38, 41.88, 42.4 , 39.36, 19.55,  4.91,
          40.59, 41.44, 41.01, 39.89, 41.37, 40.77, 35.08, 34.58, 30.02,
          41.19, 44.3 , 42.5 , 44.23, 40.99, 39.09, 39.04, 40.23, 41.93,
          42.54, 41.44, 42.1 , 42.99, 43.91, 43.49, 43.8 , 42.69, 39.58,
          38.47, 41.95, 43.68, 45.17, 44.97, 43.78, 45.38, 44.16, 43.48,
          44.65, 47.6 , 49.43, 47.59, 48.16, 47.88, 49.28, 49.5 , 42.4 ,
          42.66, 43.21, 43.23, 42.89, 44.65, 45.06, 46.69, 46.56, 43.81,
          44.25, 43.8 , 44.43, 48.58, 49.21, 49.65, 51.36, 46.47, 49.86,
          52.49, 48.69, 50.12, 48.12, 49.01, 50.47, 52.32, 48.1 ],
         dtype=float32),
   &#39;y_hat&#39;: array([24.274979 , 24.151375 , 27.62336  , 27.231997 , 25.00191  ,
          26.329071 , 28.709578 , 30.082626 , 26.985762 , 29.325583 ,
          32.78441  , 27.769854 , 30.630558 , 28.886063 , 27.736332 ,
          26.78309  , 23.714008 , 27.3941   , 28.75585  , 28.508244 ,
          30.063381 , 30.707876 , 27.902287 , 27.707556 , 27.12242  ,
          26.356369 , 26.811134 , 27.187178 , 27.148323 , 27.238522 ,
          26.103241 , 27.651236 , 27.76525  , 27.773495 , 27.093254 ,
          28.37559  , 28.825264 , 27.68026  , 28.978832 , 29.768927 ,
          30.237543 , 29.873684 , 30.234442 , 31.003107 , 32.38931  ,
          32.293568 , 31.792    , 32.105244 , 30.64681  , 30.267504 ,
          30.67257  , 29.61596  , 28.909952 , 29.630047 , 27.7068   ,
          28.00737  , 26.519136 , 28.464472 , 24.94335  , 26.585766 ,
          29.3663   , 27.784622 , 27.884521 , 26.677294 , 28.429745 ,
          28.780151 , 27.777325 , 29.862125 , 28.40974  , 29.857147 ,
          30.120184 , 31.823153 , 28.682964 , 29.227987 , 30.92515  ,
          29.00268  , 28.247536 , 28.932703 , 23.770943 , 27.362219 ,
          26.557596 , 24.510551 , 28.521505 , 25.977915 , 26.213808 ,
          26.900204 , 27.594507 , 28.094723 , 27.600344 , 27.374956 ,
          27.292204 , 27.61785  , 27.680994 , 28.263897 , 26.630545 ,
          27.272078 , 26.784348 , 26.37994  , 27.04722  , 25.686737 ,
          23.092642 , 25.494154 , 25.886713 , 26.814165 , 24.478006 ,
          21.898348 , 24.181335 , 23.21179  , 25.733816 , 27.503056 ,
          26.230324 , 28.450047 , 28.014227 , 28.837992 , 28.744743 ,
          26.134441 , 23.80624  , 25.405027 , 28.07377  , 27.380642 ,
          30.098505 , 30.157328 , 30.620106 , 29.950987 , 29.839226 ,
          25.939856 , 26.23904  , 26.601175 , 26.379705 , 25.993803 ,
          28.929214 , 29.504868 , 26.543722 , 30.083288 , 30.550014 ,
          31.273657 , 27.617188 , 29.35489  , 29.767118 , 29.548052 ,
          28.855413 , 25.46436  , 25.26842  , 25.496456 , 23.832712 ,
          24.282753 , 23.141321 , 24.55333  , 23.753923 , 24.289665 ,
          23.975307 , 25.374357 , 23.590536 , 22.965937 , 25.820562 ,
          24.58418  , 24.238613 , 25.53779  , 25.977158 , 25.66049  ,
          25.392864 , 24.960669 , 12.296654 , 18.35784  , 25.100418 ,
          22.745836 , 23.422836 , 23.124437 , 19.666288 , 24.485817 ,
          23.08405  , 23.27156  , 20.998053 , 22.356167 , 23.696114 ,
          23.844835 , 22.471884 , 25.06696  , 25.050526 , 23.484108 ,
          22.658344 , 20.722057 , 22.334675 , 22.33002  , 21.944422 ,
          22.227428 , 21.93957  , 23.449463 , 23.136076 , 23.174168 ,
          24.20129  , 24.852013 , 25.391117 , 26.40194  , 26.124254 ,
          26.661762 , 26.180788 , 26.161285 , 25.209972 , 25.670446 ,
          25.368677 , 25.506063 , 24.477448 , 23.949297 , 24.21766  ,
          24.03661  , 25.255653 , 26.20058  , 25.571434 , 25.765282 ,
          25.807896 , 25.69998  , 25.558922 , 26.103098 , 25.103502 ,
          26.030695 , 25.214296 , 24.885553 , 25.228767 , 25.544487 ,
          23.737814 , 22.759508 , 24.065638 , 24.222881 , 24.8203   ,
          22.806025 , 24.409908 , 24.577751 , 24.61347  , 21.433132 ,
          25.040352 , 25.034485 , 24.562162 , 25.354275 , 24.373697 ,
          25.22633  , 23.19918  , 24.86637  , 25.872313 , 26.35388  ,
          26.429806 , 27.478098 , 28.343151 , 29.337826 , 29.604403 ,
          29.08069  , 29.043783 , 29.124756 , 30.528587 , 31.42987  ,
          30.84937  , 30.65875  , 30.744358 , 30.432648 , 28.414162 ,
          29.867844 , 28.218534 , 29.292938 , 26.405739 , 26.724478 ,
          27.001707 , 25.043705 , 26.954588 , 29.09992  , 30.357775 ,
          30.820604 , 30.83753  , 30.465559 , 30.546125 , 29.483923 ,
          29.800129 , 29.913927 , 28.853573 , 28.517807 , 28.648684 ,
          28.051435 , 28.663448 , 27.036308 , 27.27805  , 23.14023  ,
          19.419209 , 22.910667 , 15.666532 , 24.701668 , 25.605722 ,
          25.90124  , 27.921314 , 29.45159  , 27.461905 , 26.27825  ,
          26.605997 , 25.877676 , 25.472223 , 20.675728 , 25.077793 ,
          23.725903 , 27.069338 , 26.402685 , 26.949566 , 25.617775 ,
          28.328047 , 28.784925 , 26.07539  , 25.935104 , 26.482718 ,
          18.957785 , 19.462414 , 19.115147 , 27.192125 , 27.138672 ,
          22.445704 , 27.87192  , 26.220161 , 24.64076  , 27.353558 ,
          27.015154 , 27.035896 , 27.499275 , 24.785042 , 25.37394  ,
          27.731283 , 28.789951 , 29.469065 , 28.36414  , 28.797054 ,
          26.566332 , 26.80316  , 27.122353 , 27.726164 , 30.948673 ,
          28.475277 , 26.136011 , 25.659744 , 28.588472 , 27.349361 ,
          27.06799  , 27.472652 , 29.795668 , 31.749004 , 30.171562 ,
          32.248634 , 27.329533 , 28.168154 , 28.50998  , 24.752846 ,
          26.331558 , 23.271944 , 25.032495 , 25.85082  , 26.513365 ,
          27.468914 , 26.620802 , 25.538994 , 27.841085 , 28.090399 ,
          28.461964 , 27.698467 , 30.041615 , 27.79973  , 27.23816  ,
          26.1297   , 26.418127 , 19.30274  , 24.76327  , 23.560438 ,
          22.748066 , 26.897594 , 25.224812 , 25.973627 , 25.126383 ,
          24.678543 , 23.866898 , 26.601555 , 25.516    , 27.469719 ,
          28.81172  , 28.606178 , 26.98822  , 27.447857 , 27.32585  ,
          29.640114 , 29.061127 , 28.882586 , 28.88016  , 28.296692 ,
          24.7521   , 26.733528 , 27.403772 , 29.495329 , 32.289867 ,
          31.28648  , 30.52792  , 32.823425 , 26.222988 , 24.347008 ,
          27.986612 , 29.593613 , 27.346851 , 26.633923 , 27.243177 ,
          28.986797 , 26.18184  , 28.779312 , 31.246065 , 31.613092 ,
          32.28903  , 35.1399   , 33.256237 , 32.65898  , 31.347315 ,
          31.082333 , 30.0033   , 28.060162 , 29.724709 , 29.515564 ,
          28.501434 , 31.609402 , 32.621387 , 34.78152  , 33.03356  ,
          35.804974 , 37.034332 , 36.963604 , 39.015263 , 36.18129  ,
          37.96429  , 36.42786  , 38.104424 , 36.338253 , 37.093544 ,
          39.945816 , 41.585228 , 39.710506 , 40.54038  , 39.302265 ,
          37.057457 , 37.535515 , 35.746304 , 38.074207 , 35.70967  ,
          35.43531  , 35.882576 , 36.692913 , 37.05887  , 37.18769  ,
          37.352528 , 37.022575 , 36.647392 , 36.742886 , 38.869072 ,
          36.8174   , 41.316082 , 39.27426  , 39.02798  , 38.977955 ,
          42.470024 , 38.482887 , 38.48348  , 40.184883 , 39.28535  ,
          39.536476 , 38.85411  , 38.56031  , 38.401722 , 37.466957 ,
          37.527256 , 37.09081  , 38.66069  , 38.309532 , 37.496693 ,
          36.74172  , 38.022903 , 37.814304 , 37.401108 , 39.83977  ,
          38.75593  , 41.228672 , 38.92672  , 37.271526 , 34.45977  ,
          32.263268 , 35.138527 , 33.183765 , 32.961357 , 33.508904 ,
          32.987484 , 34.736023 , 35.3529   , 35.465984 , 33.227238 ,
          31.445452 , 29.979563 , 34.60697  , 33.51411  , 33.394756 ,
          27.48575  , 26.777409 , 24.466917 , 10.688176 ,  3.0001388,
           8.662841 , 29.499388 , 18.443686 , 18.000277 , 32.886776 ,
          31.777882 , 25.187227 , 28.739923 , 33.19229  , 31.657385 ,
          14.504236 , 27.71342  , 35.067085 , 34.92782  , 35.588337 ,
          37.898148 , 39.390175 , 37.828823 , 38.66836  , 37.78954  ,
          41.334007 , 40.4862   , 43.64151  , 42.124756 , 40.458637 ,
          42.764885 , 44.078556 , 44.244164 , 45.806732 , 46.993446 ,
          45.48609  , 46.867256 , 44.887997 , 45.524452 , 46.130886 ,
          40.276268 , 45.254826 , 44.408367 , 44.147545 , 41.12059  ,
          41.239643 , 40.936375 , 37.194508 , 36.906967 , 40.94553  ,
          42.763367 , 43.296753 , 43.621094 , 44.627388 , 42.40095  ,
          44.56606  , 45.582287 , 46.59435  , 48.639282 , 49.66051  ,
          50.27722  , 49.83715  , 47.555386 , 48.67779  , 49.74229  ,
          49.42842  , 50.444084 , 50.925465 , 50.71973  , 51.98799  ,
          52.87847  , 53.431225 , 51.699234 , 51.006878 , 50.118435 ,
          51.45215  , 52.43405  , 52.777214 , 53.425617 , 53.91119  ,
          54.052254 , 53.880005 , 51.71592  , 50.937805 , 50.956272 ,
          53.16315  , 52.45288  , 52.185963 , 53.04005  , 52.807903 ,
          53.473328 , 50.160187 , 51.718536 , 53.587357 , 51.26681  ,
          50.268948 , 47.08365  , 48.806118 , 49.720894 , 49.136154 ,
          49.48593  , 49.800896 , 49.828156 , 48.30773  , 48.883785 ,
          44.157265 , 48.36637  , 47.896477 , 49.342163 , 47.677387 ,
          50.284645 , 49.36867  , 50.613262 , 50.388077 , 50.23631  ,
          55.647873 , 51.861374 , 54.03837  , 57.10276  , 56.167538 ,
          54.6243   , 54.11317  , 56.471893 , 56.860374 , 54.598686 ,
          53.828773 , 54.653194 , 53.49362  , 54.224224 , 50.485504 ,
          49.42242  , 52.72676  , 50.194237 , 49.99766  , 48.701073 ,
          48.73677  , 43.887745 , 38.5052   , 42.356293 ,  3.034913 ,
          21.50947  , 30.435131 , 37.99098  , 32.806175 , 19.179497 ,
          29.978582 , 41.616604 , 19.514305 , 42.182228 , 41.87714  ,
          41.90414  , 44.721016 , 42.670147 , 45.471348 , 46.45409  ,
          47.035778 , 43.69287  , 40.969944 , 41.039433 , 41.701286 ,
          38.986603 , 19.403566 ,  5.1350737, 40.52299  , 39.24665  ,
          40.126575 , 38.888515 , 40.3755   , 42.319897 , 35.32634  ,
          34.412773 , 29.079887 , 40.148098 , 43.23322  , 41.889507 ,
          43.779392 , 41.08276  , 39.26788  , 38.56762  , 39.423714 ,
          41.04775  , 42.07396  , 41.464153 , 41.828587 , 42.712788 ,
          43.4067   , 42.869583 , 43.455486 , 42.503803 , 39.876564 ,
          38.332375 , 41.497257 , 42.64832  , 44.554012 , 44.280907 ,
          43.73163  , 45.962032 , 44.1377   , 43.32402  , 44.10712  ,
          47.0243   , 49.19918  , 47.208725 , 49.311043 , 48.080334 ,
          49.475697 , 49.15904  , 41.81566  , 42.730614 , 43.27764  ,
          43.24905  , 42.644596 , 44.190956 , 44.136227 , 46.165054 ,
          46.12344  , 43.748993 , 44.7247   , 43.656147 , 44.223953 ,
          47.96081  , 48.645035 , 49.40003  , 52.21454  , 47.333664 ,
          50.418636 , 52.840508 , 48.045765 , 49.655754 , 47.882545 ,
          48.996212 , 51.196068 , 52.382473 ], dtype=float32),
   &#39;run_time&#39;: 2.7481112480163574,
   &#39;status&#39;: &#39;ok&#39;},
  &#39;misc&#39;: {&#39;tid&#39;: 0,
   &#39;cmd&#39;: (&#39;domain_attachment&#39;, &#39;FMinIter_Domain&#39;),
   &#39;workdir&#39;: None,
   &#39;idxs&#39;: {&#39;activation&#39;: [0],
    &#39;batch_normalization&#39;: [0],
    &#39;batch_size&#39;: [0],
    &#39;complete_inputs&#39;: [0],
    &#39;complete_sample&#39;: [0],
    &#39;device&#39;: [0],
    &#39;dropout_prob_exogenous&#39;: [0],
    &#39;dropout_prob_theta&#39;: [0],
    &#39;early_stop_patience&#39;: [0],
    &#39;eval_freq&#39;: [0],
    &#39;frequency&#39;: [0],
    &#39;idx_to_sample_freq&#39;: [0],
    &#39;initialization&#39;: [0],
    &#39;l1_theta&#39;: [0],
    &#39;learning_rate&#39;: [0],
    &#39;len_sample_chunks&#39;: [0],
    &#39;loss&#39;: [0],
    &#39;loss_hypar&#39;: [0],
    &#39;loss_valid&#39;: [0],
    &#39;lr_decay&#39;: [0],
    &#39;lr_decay_step_size&#39;: [0],
    &#39;max_epochs&#39;: [0],
    &#39;max_steps&#39;: [0],
    &#39;n_blocks&#39;: [0],
    &#39;n_harmonics&#39;: [0],
    &#39;n_hidden&#39;: [0],
    &#39;n_layers&#39;: [0],
    &#39;n_polynomials&#39;: [0],
    &#39;n_s_hidden&#39;: [0],
    &#39;n_series_per_batch&#39;: [0],
    &#39;n_time_in&#39;: [0],
    &#39;n_time_out&#39;: [0],
    &#39;n_val_weeks&#39;: [0],
    &#39;n_x_hidden&#39;: [0],
    &#39;normalizer_x&#39;: [0],
    &#39;normalizer_y&#39;: [0],
    &#39;random_seed&#39;: [0],
    &#39;seasonality&#39;: [0],
    &#39;shared_weights&#39;: [0],
    &#39;stack_types&#39;: [0],
    &#39;val_idx_to_sample_freq&#39;: [0],
    &#39;weight_decay&#39;: [0],
    &#39;window_sampling_limit&#39;: [0]},
   &#39;vals&#39;: {&#39;activation&#39;: [0],
    &#39;batch_normalization&#39;: [0],
    &#39;batch_size&#39;: [0],
    &#39;complete_inputs&#39;: [0],
    &#39;complete_sample&#39;: [0],
    &#39;device&#39;: [0],
    &#39;dropout_prob_exogenous&#39;: [0.2934610359635618],
    &#39;dropout_prob_theta&#39;: [0.24389096640506186],
    &#39;early_stop_patience&#39;: [0],
    &#39;eval_freq&#39;: [0],
    &#39;frequency&#39;: [0],
    &#39;idx_to_sample_freq&#39;: [0],
    &#39;initialization&#39;: [0],
    &#39;l1_theta&#39;: [0],
    &#39;learning_rate&#39;: [0.0007367406100039542],
    &#39;len_sample_chunks&#39;: [0],
    &#39;loss&#39;: [0],
    &#39;loss_hypar&#39;: [0],
    &#39;loss_valid&#39;: [0],
    &#39;lr_decay&#39;: [0.31009178056304154],
    &#39;lr_decay_step_size&#39;: [0],
    &#39;max_epochs&#39;: [0],
    &#39;max_steps&#39;: [0],
    &#39;n_blocks&#39;: [0],
    &#39;n_harmonics&#39;: [0],
    &#39;n_hidden&#39;: [0],
    &#39;n_layers&#39;: [0],
    &#39;n_polynomials&#39;: [0],
    &#39;n_s_hidden&#39;: [0],
    &#39;n_series_per_batch&#39;: [0],
    &#39;n_time_in&#39;: [0],
    &#39;n_time_out&#39;: [0],
    &#39;n_val_weeks&#39;: [0],
    &#39;n_x_hidden&#39;: [9.0],
    &#39;normalizer_x&#39;: [0],
    &#39;normalizer_y&#39;: [0],
    &#39;random_seed&#39;: [17.0],
    &#39;seasonality&#39;: [0],
    &#39;shared_weights&#39;: [0],
    &#39;stack_types&#39;: [1],
    &#39;val_idx_to_sample_freq&#39;: [0],
    &#39;weight_decay&#39;: [6.317818377567524e-05],
    &#39;window_sampling_limit&#39;: [0]}},
  &#39;exp_key&#39;: None,
  &#39;owner&#39;: None,
  &#39;version&#39;: 0,
  &#39;book_time&#39;: datetime.datetime(2021, 6, 6, 19, 34, 50, 389000),
  &#39;refresh_time&#39;: datetime.datetime(2021, 6, 6, 19, 34, 53, 156000)},
 {&#39;state&#39;: 2,
  &#39;tid&#39;: 1,
  &#39;spec&#39;: None,
  &#39;result&#39;: {&#39;loss&#39;: 1.9828330278396606,
   &#39;mc&#39;: {&#39;activation&#39;: &#39;SELU&#39;,
    &#39;batch_normalization&#39;: False,
    &#39;batch_size&#39;: 256,
    &#39;complete_inputs&#39;: False,
    &#39;complete_sample&#39;: False,
    &#39;device&#39;: &#39;cuda&#39;,
    &#39;dropout_prob_exogenous&#39;: 0.3024306449332244,
    &#39;dropout_prob_theta&#39;: 0.3851678624996854,
    &#39;early_stop_patience&#39;: 16,
    &#39;eval_freq&#39;: 50,
    &#39;frequency&#39;: &#39;H&#39;,
    &#39;idx_to_sample_freq&#39;: 24,
    &#39;initialization&#39;: &#39;glorot_normal&#39;,
    &#39;l1_theta&#39;: 0,
    &#39;learning_rate&#39;: 0.0007294826926374994,
    &#39;len_sample_chunks&#39;: None,
    &#39;loss_hypar&#39;: 0.5,
    &#39;loss_train&#39;: &#39;MAE&#39;,
    &#39;loss_valid&#39;: &#39;MAE&#39;,
    &#39;lr_decay&#39;: 0.3939003899063233,
    &#39;lr_decay_step_size&#39;: 100,
    &#39;max_epochs&#39;: 10,
    &#39;max_steps&#39;: None,
    &#39;mode&#39;: &#39;simple&#39;,
    &#39;model&#39;: &#39;nbeats&#39;,
    &#39;n_blocks&#39;: (1, 1),
    &#39;n_harmonics&#39;: 1,
    &#39;n_hidden&#39;: 256,
    &#39;n_layers&#39;: (2, 2),
    &#39;n_polynomials&#39;: 2,
    &#39;n_s_hidden&#39;: 0,
    &#39;n_series_per_batch&#39;: 1,
    &#39;n_time_in&#39;: 168,
    &#39;n_time_out&#39;: 24,
    &#39;n_val_weeks&#39;: 104,
    &#39;n_x_hidden&#39;: 3.0,
    &#39;normalizer_x&#39;: &#39;median&#39;,
    &#39;normalizer_y&#39;: None,
    &#39;random_seed&#39;: 16.0,
    &#39;seasonality&#39;: 24,
    &#39;shared_weights&#39;: False,
    &#39;stack_types&#39;: (&#39;identity&#39;, &#39;identity&#39;),
    &#39;val_idx_to_sample_freq&#39;: 24,
    &#39;weight_decay&#39;: 8.299088918068486e-05,
    &#39;window_sampling_limit&#39;: 100000,
    &#39;n_x&#39;: 1,
    &#39;n_s&#39;: 1,
    &#39;n_theta_hidden&#39;: [[256, 256], [256, 256]]},
   &#39;y_true&#39;: array([25.73, 29.37, 28.76, 25.95, 26.71, 29.36, 30.93, 28.3 , 30.58,
          33.32, 28.86, 30.5 , 29.72, 28.69, 28.  , 25.04, 28.72, 29.79,
          29.41, 30.89, 30.99, 29.56, 27.84, 28.19, 27.36, 27.84, 28.13,
          28.42, 28.4 , 27.41, 28.66, 28.67, 28.51, 27.96, 29.31, 29.99,
          28.97, 29.96, 30.59, 30.8 , 30.62, 31.01, 32.  , 33.14, 32.63,
          32.11, 31.77, 31.04, 31.35, 31.96, 31.01, 29.98, 30.36, 28.15,
          28.87, 27.66, 29.85, 26.38, 27.82, 30.46, 28.49, 28.67, 27.58,
          29.55, 29.93, 29.03, 30.83, 29.08, 30.54, 30.97, 32.54, 30.1 ,
          29.72, 31.56, 29.66, 29.13, 29.94, 25.2 , 28.56, 27.86, 25.65,
          29.17, 27.05, 27.23, 28.04, 28.94, 29.22, 28.55, 28.05, 28.15,
          28.5 , 28.9 , 29.68, 27.87, 28.14, 27.49, 27.38, 28.06, 27.12,
          24.5 , 26.57, 27.03, 27.59, 25.54, 22.99, 25.42, 24.72, 27.11,
          28.73, 27.15, 29.23, 28.95, 29.76, 29.91, 27.27, 24.44, 25.9 ,
          29.06, 28.89, 31.36, 31.24, 31.39, 30.1 , 30.12, 27.03, 27.1 ,
          28.  , 27.99, 27.48, 29.84, 30.25, 27.39, 30.99, 31.73, 32.4 ,
          28.76, 30.05, 30.05, 30.26, 30.21, 26.95, 26.68, 26.74, 25.06,
          25.17, 24.04, 25.8 , 25.6 , 25.65, 25.42, 26.33, 24.14, 24.06,
          26.78, 26.11, 25.61, 26.57, 26.97, 26.21, 26.29, 26.2 , 13.97,
          19.62, 26.43, 24.68, 24.5 , 24.28, 20.64, 25.43, 24.76, 24.89,
          22.06, 23.07, 24.79, 24.81, 24.03, 26.5 , 26.34, 24.42, 23.49,
          21.8 , 23.48, 24.23, 23.69, 23.57, 22.99, 24.14, 24.19, 24.33,
          25.6 , 26.23, 26.55, 27.41, 26.73, 27.4 , 27.12, 27.44, 26.51,
          26.96, 26.51, 26.33, 25.51, 25.02, 25.63, 25.47, 26.65, 27.45,
          26.47, 26.65, 26.77, 27.02, 26.89, 27.46, 26.35, 26.75, 26.36,
          26.42, 26.7 , 27.04, 25.04, 23.65, 24.86, 25.4 , 26.11, 24.57,
          25.86, 25.87, 25.57, 21.97, 26.01, 26.3 , 26.17, 26.85, 25.74,
          25.99, 23.77, 25.79, 27.04, 27.77, 27.7 , 28.65, 29.16, 29.65,
          30.36, 29.78, 30.32, 30.15, 31.4 , 32.27, 31.37, 31.55, 31.47,
          31.56, 29.66, 30.83, 29.27, 29.87, 27.35, 27.67, 28.47, 26.5 ,
          28.19, 30.22, 31.05, 31.52, 31.57, 31.55, 31.55, 30.51, 30.65,
          30.42, 29.68, 29.51, 29.8 , 29.38, 29.81, 27.96, 27.85, 24.06,
          20.41, 24.3 , 17.11, 26.12, 26.91, 26.6 , 28.79, 30.26, 28.56,
          27.11, 27.63, 26.63, 26.03, 21.72, 26.06, 25.26, 28.37, 27.81,
          27.91, 26.  , 28.97, 29.57, 27.19, 27.1 , 27.64, 20.1 , 20.06,
          20.03, 28.5 , 28.95, 23.95, 28.85, 27.1 , 24.96, 28.03, 28.25,
          28.3 , 28.57, 26.03, 26.01, 28.29, 29.71, 30.37, 29.74, 29.66,
          27.57, 27.58, 27.88, 28.67, 31.55, 29.76, 27.01, 26.05, 29.13,
          28.24, 28.01, 28.62, 30.82, 32.18, 30.92, 30.97, 27.9 , 28.24,
          29.83, 27.07, 27.94, 24.64, 26.06, 26.79, 27.52, 28.21, 27.96,
          26.49, 28.26, 29.03, 29.02, 28.59, 30.43, 29.06, 28.  , 27.3 ,
          26.94, 20.08, 25.65, 25.22, 24.51, 28.22, 26.82, 26.68, 25.59,
          25.43, 25.02, 27.55, 27.31, 28.07, 29.66, 29.03, 27.75, 28.11,
          28.6 , 30.37, 30.05, 29.53, 28.79, 28.82, 25.69, 28.05, 29.31,
          30.45, 32.79, 31.64, 30.52, 33.06, 28.08, 25.5 , 28.98, 30.08,
          28.01, 27.57, 28.33, 30.14, 27.7 , 29.53, 31.9 , 32.12, 32.78,
          34.9 , 33.7 , 32.74, 30.91, 31.17, 29.96, 28.87, 31.02, 31.14,
          29.74, 32.22, 32.97, 34.89, 33.77, 35.77, 37.41, 37.34, 38.21,
          36.05, 36.93, 36.48, 38.98, 37.72, 37.73, 39.09, 39.81, 38.41,
          38.17, 40.4 , 38.6 , 37.29, 35.57, 37.9 , 36.03, 35.68, 36.62,
          37.64, 37.75, 37.68, 37.43, 36.79, 37.14, 37.48, 39.6 , 37.88,
          41.43, 39.59, 38.68, 39.54, 42.1 , 39.38, 39.26, 40.87, 39.52,
          39.56, 39.22, 39.22, 39.13, 38.39, 38.09, 37.42, 38.62, 38.68,
          38.01, 37.64, 38.96, 38.5 , 37.96, 40.01, 39.18, 41.55, 39.83,
          38.04, 34.93, 32.64, 35.34, 33.97, 34.21, 34.77, 34.35, 35.93,
          36.02, 35.91, 33.77, 32.13, 31.09, 36.03, 35.26, 34.15, 28.06,
          27.2 , 24.88, 12.42,  4.44,  9.48, 30.11, 19.5 , 20.07, 32.98,
          32.97, 24.98, 29.06, 33.81, 31.77, 15.27, 28.05, 36.81, 36.07,
          36.34, 39.04, 39.12, 37.69, 39.17, 38.85, 42.26, 41.36, 44.16,
          42.2 , 40.66, 43.07, 44.91, 45.17, 46.5 , 47.28, 45.29, 46.73,
          45.1 , 46.11, 46.91, 41.1 , 45.56, 44.46, 44.48, 41.49, 42.22,
          41.97, 38.05, 37.47, 41.01, 43.27, 43.92, 44.45, 45.29, 42.82,
          44.73, 45.48, 46.78, 48.94, 50.24, 50.84, 50.19, 47.56, 48.25,
          49.76, 49.68, 50.8 , 51.54, 51.2 , 51.94, 52.4 , 53.18, 51.73,
          51.33, 50.51, 51.9 , 52.56, 52.44, 53.29, 53.92, 54.39, 54.33,
          51.99, 50.87, 50.53, 53.  , 52.67, 52.72, 53.6 , 53.33, 53.44,
          49.6 , 51.51, 53.6 , 51.78, 50.74, 47.48, 49.08, 49.32, 49.43,
          49.87, 50.53, 50.59, 48.72, 48.83, 43.64, 48.23, 48.17, 49.92,
          48.26, 50.62, 49.63, 49.77, 50.14, 50.15, 55.8 , 52.37, 54.05,
          56.58, 55.2 , 53.87, 53.8 , 56.61, 56.84, 54.61, 53.44, 53.65,
          53.05, 54.19, 50.99, 49.92, 53.13, 50.19, 49.35, 48.58, 48.91,
          44.41, 39.04, 43.01,  3.27, 21.04, 30.61, 39.25, 34.23, 20.99,
          30.48, 41.79, 19.92, 42.05, 42.76, 42.51, 44.67, 43.34, 45.61,
          45.34, 47.03, 44.08, 41.38, 41.88, 42.4 , 39.36, 19.55,  4.91,
          40.59, 41.44, 41.01, 39.89, 41.37, 40.77, 35.08, 34.58, 30.02,
          41.19, 44.3 , 42.5 , 44.23, 40.99, 39.09, 39.04, 40.23, 41.93,
          42.54, 41.44, 42.1 , 42.99, 43.91, 43.49, 43.8 , 42.69, 39.58,
          38.47, 41.95, 43.68, 45.17, 44.97, 43.78, 45.38, 44.16, 43.48,
          44.65, 47.6 , 49.43, 47.59, 48.16, 47.88, 49.28, 49.5 , 42.4 ,
          42.66, 43.21, 43.23, 42.89, 44.65, 45.06, 46.69, 46.56, 43.81,
          44.25, 43.8 , 44.43, 48.58, 49.21, 49.65, 51.36, 46.47, 49.86,
          52.49, 48.69, 50.12, 48.12, 49.01, 50.47, 52.32, 48.1 ],
         dtype=float32),
   &#39;y_hat&#39;: array([25.41661  , 25.362526 , 28.65475  , 28.06419  , 25.29368  ,
          26.086197 , 28.89031  , 30.366188 , 27.791208 , 29.853092 ,
          32.351826 , 28.317966 , 29.856863 , 29.229723 , 28.352434 ,
          27.72079  , 24.378376 , 28.183607 , 29.433    , 29.2034   ,
          30.270527 , 30.226616 , 28.824968 , 26.602867 , 27.249332 ,
          27.57016  , 28.087402 , 27.380291 , 27.882566 , 27.742237 ,
          26.677439 , 27.84697  , 28.025068 , 28.179861 , 27.50523  ,
          28.70795  , 29.431993 , 28.187832 , 29.14193  , 29.9163   ,
          30.371782 , 30.14372  , 30.316507 , 31.350912 , 32.44853  ,
          31.842918 , 31.303976 , 30.66082  , 30.520767 , 31.355097 ,
          31.890177 , 29.992426 , 29.169876 , 29.520784 , 27.613762 ,
          28.89928  , 27.321898 , 29.164005 , 25.747673 , 27.219221 ,
          29.97592  , 28.097654 , 28.232317 , 26.869125 , 28.99985  ,
          29.177443 , 28.437315 , 30.343084 , 28.602146 , 30.060677 ,
          30.354956 , 31.963589 , 29.452343 , 28.562216 , 30.817894 ,
          29.425291 , 29.15633  , 29.34681  , 24.681929 , 27.930264 ,
          27.222837 , 25.351742 , 28.809998 , 26.694473 , 26.763945 ,
          27.412798 , 28.085712 , 28.503677 , 28.072922 , 27.572243 ,
          27.530596 , 27.966782 , 28.52918  , 28.973478 , 27.053394 ,
          27.46535  , 27.05968  , 26.976376 , 27.62928  , 26.642979 ,
          23.596098 , 25.75708  , 26.683878 , 27.198196 , 25.101093 ,
          22.455273 , 24.938795 , 24.047499 , 26.607162 , 28.313858 ,
          26.617908 , 28.650928 , 28.212223 , 29.009615 , 29.244574 ,
          26.407375 , 23.541208 , 25.404324 , 28.972002 , 28.722761 ,
          30.549055 , 30.518562 , 30.428955 , 29.036015 , 29.344503 ,
          26.517332 , 26.760056 , 27.755133 , 27.487791 , 26.832731 ,
          29.076563 , 29.748938 , 26.9476   , 30.353174 , 30.952034 ,
          31.728981 , 27.982042 , 28.96253  , 29.208895 , 30.067701 ,
          30.03844  , 26.488958 , 25.96102  , 26.101326 , 24.746199 ,
          25.020163 , 23.806454 , 25.32413  , 25.383852 , 24.842157 ,
          24.67257  , 25.5861   , 23.837929 , 23.76212  , 26.106667 ,
          25.75076  , 25.062689 , 25.698368 , 26.212883 , 25.843622 ,
          25.854507 , 25.654089 , 13.838339 , 19.26919  , 26.008585 ,
          24.246696 , 24.35364  , 23.673676 , 19.784992 , 25.067299 ,
          23.961267 , 24.254189 , 21.515343 , 22.520176 , 24.270018 ,
          24.444834 , 23.64156  , 25.694836 , 25.702717 , 23.866346 ,
          23.005333 , 21.494696 , 23.158371 , 23.783371 , 22.999102 ,
          22.846483 , 22.350853 , 23.640795 , 23.766987 , 23.764082 ,
          25.06338  , 25.522629 , 25.896692 , 26.762669 , 26.114805 ,
          26.776403 , 26.46484  , 26.85195  , 25.753344 , 26.34628  ,
          26.046455 , 25.989113 , 25.067327 , 24.486258 , 25.125744 ,
          24.852137 , 26.0811   , 26.992306 , 26.022081 , 26.145105 ,
          26.090176 , 26.43768  , 26.20353  , 26.843088 , 25.86148  ,
          26.562094 , 26.016632 , 25.889532 , 26.144789 , 26.38907  ,
          24.208479 , 23.061052 , 24.460987 , 25.252546 , 25.57384  ,
          24.08729  , 25.135216 , 25.102814 , 24.932598 , 21.66578  ,
          25.679092 , 25.671812 , 25.718721 , 26.09059  , 24.928106 ,
          25.311045 , 23.340652 , 25.396933 , 26.511854 , 27.230978 ,
          27.022627 , 27.918346 , 28.24601  , 28.816359 , 29.790756 ,
          29.147406 , 29.75251  , 29.17994  , 30.633179 , 31.585651 ,
          30.988302 , 30.986292 , 30.785019 , 31.008081 , 28.845978 ,
          30.007729 , 28.703348 , 29.524035 , 27.047112 , 27.27489  ,
          28.016224 , 25.852041 , 27.589344 , 29.735994 , 30.50382  ,
          30.996056 , 30.840015 , 30.879679 , 30.718658 , 29.807081 ,
          30.083406 , 29.96691  , 29.284029 , 29.091217 , 29.275925 ,
          28.610643 , 29.13011  , 27.405283 , 27.509926 , 23.845757 ,
          20.249811 , 24.029701 , 16.85141  , 25.609386 , 26.582325 ,
          26.331724 , 28.174187 , 29.235594 , 28.057327 , 26.13892  ,
          26.794338 , 26.041788 , 25.977001 , 21.434864 , 25.674797 ,
          24.896008 , 27.851406 , 27.248234 , 27.467905 , 25.36056  ,
          28.313213 , 29.09295  , 26.757511 , 26.304047 , 26.88176  ,
          19.874664 , 19.966005 , 19.864897 , 28.248528 , 28.43006  ,
          23.346973 , 27.988401 , 26.213963 , 24.708984 , 27.678366 ,
          27.704075 , 27.753399 , 27.830973 , 25.152613 , 25.328667 ,
          27.85526  , 29.503685 , 29.929497 , 28.921373 , 28.747734 ,
          26.668062 , 27.264751 , 27.60403  , 28.404276 , 31.186258 ,
          29.088205 , 25.91056  , 24.779919 , 28.874279 , 28.31563  ,
          27.790146 , 27.783587 , 30.347786 , 31.165995 , 29.327497 ,
          30.117485 , 27.076523 , 28.93543  , 30.122555 , 26.113546 ,
          27.032402 , 23.601053 , 25.809748 , 26.660471 , 27.271805 ,
          27.805748 , 27.351368 , 25.024128 , 27.146177 , 28.728436 ,
          28.855135 , 27.269712 , 29.852549 , 28.486269 , 26.705875 ,
          25.934566 , 26.457287 , 20.424408 , 25.447147 , 24.646578 ,
          24.173613 , 27.622032 , 26.13592  , 26.062195 , 24.84364  ,
          25.000662 , 24.705992 , 27.194592 , 26.757635 , 27.251184 ,
          29.186628 , 28.531504 , 27.20889  , 27.471165 , 28.157984 ,
          29.703428 , 29.586231 , 28.893261 , 27.583488 , 28.074524 ,
          25.615677 , 27.874228 , 28.599905 , 29.936005 , 32.225086 ,
          30.936163 , 29.736628 , 32.518406 , 28.398268 , 24.731842 ,
          27.37051  , 29.722572 , 28.796026 , 27.122509 , 27.146317 ,
          29.983316 , 27.188934 , 28.666985 , 31.227135 , 31.85434  ,
          32.27162  , 33.93347  , 32.99524  , 31.913853 , 29.086285 ,
          30.406107 , 30.291332 , 29.00909  , 31.312963 , 30.64465  ,
          28.874569 , 31.286755 , 31.947662 , 34.761734 , 33.782654 ,
          34.76106  , 36.90637  , 36.51266  , 37.08396  , 35.284016 ,
          36.363495 , 36.029438 , 39.472034 , 38.120644 , 37.13321  ,
          38.238415 , 39.333878 , 37.37009  , 36.680515 , 40.971775 ,
          39.987133 , 36.862225 , 34.161045 , 37.24032  , 36.77732  ,
          35.55052  , 36.303562 , 37.276127 , 37.188595 , 36.928642 ,
          36.52015  , 36.21989  , 37.094433 , 37.42209  , 39.03246  ,
          36.92252  , 40.431503 , 38.95448  , 38.33959  , 39.05319  ,
          41.11738  , 39.350452 , 38.616917 , 39.250286 , 38.628815 ,
          39.4674   , 38.766212 , 38.546703 , 38.421696 , 37.563416 ,
          37.134296 , 36.507454 , 37.74684  , 38.294296 , 37.547855 ,
          36.976254 , 37.78562  , 37.4585   , 37.528954 , 39.546337 ,
          38.525444 , 40.785797 , 39.1792   , 37.031593 , 33.75205  ,
          31.94137  , 35.310326 , 33.80366  , 34.269726 , 34.336502 ,
          33.4627   , 35.263214 , 35.374363 , 35.3731   , 33.04885  ,
          31.447403 , 30.806961 , 35.283264 , 34.84679  , 33.56989  ,
          27.449963 , 26.416708 , 24.476103 , 12.330863 ,  4.1209407,
           9.051605 , 29.365496 , 19.449871 , 19.995571 , 30.928047 ,
          32.05147  , 23.665245 , 28.45046  , 32.696182 , 31.363104 ,
          15.023143 , 27.328934 , 35.850662 , 35.1831   , 35.93944  ,
          37.975    , 37.876446 , 37.08089  , 38.475056 , 38.144154 ,
          41.33968  , 40.510094 , 43.418358 , 41.596855 , 39.963245 ,
          42.365314 , 44.081745 , 44.30852  , 45.667717 , 46.484608 ,
          44.497356 , 45.905205 , 44.31375  , 45.499588 , 46.017353 ,
          40.37339  , 44.9212   , 43.76781  , 43.890938 , 40.933994 ,
          41.642155 , 41.03614  , 37.344902 , 37.03114  , 40.556843 ,
          42.787476 , 43.274426 , 43.544407 , 44.196194 , 41.86746  ,
          43.82056  , 44.779575 , 46.184322 , 48.07845  , 49.378708 ,
          49.795124 , 49.121067 , 46.67189  , 47.504036 , 49.06955  ,
          48.955296 , 50.021515 , 50.52259  , 50.219456 , 51.091743 ,
          51.58631  , 52.38862  , 50.819977 , 50.41538  , 49.545673 ,
          51.021694 , 51.840294 , 51.826065 , 52.50453  , 52.99625  ,
          53.444176 , 53.280357 , 51.04032  , 50.134396 , 49.952377 ,
          52.39417  , 51.882816 , 51.975883 , 52.708694 , 52.315014 ,
          52.575184 , 48.95003  , 50.874046 , 52.77778  , 50.93591  ,
          49.896336 , 46.571663 , 48.35065  , 48.72455  , 49.077736 ,
          49.117466 , 49.870434 , 49.62874  , 47.630196 , 47.900097 ,
          42.961536 , 47.657795 , 47.479446 , 49.2175   , 47.26467  ,
          49.627872 , 48.738544 , 48.944397 , 49.388924 , 49.312645 ,
          54.839222 , 51.466644 , 53.010433 , 55.501152 , 54.283028 ,
          53.230408 , 53.059643 , 55.759247 , 55.851772 , 53.660656 ,
          52.54872  , 52.94801  , 52.607815 , 53.70941  , 50.485294 ,
          49.0341   , 52.212032 , 49.57907  , 48.922745 , 47.938866 ,
          48.29966  , 43.80809  , 38.32247  , 42.286427 ,  3.2506814,
          21.534088 , 30.253403 , 39.816998 , 33.607258 , 21.392477 ,
          28.7175   , 40.790447 , 19.364809 , 41.97953  , 41.1646   ,
          42.043068 , 43.736687 , 42.38166  , 44.741215 , 44.627235 ,
          46.401608 , 43.37106  , 40.775627 , 40.97582  , 41.871273 ,
          38.758442 , 19.613943 ,  5.1480775, 39.92442  , 41.060925 ,
          40.533623 , 39.132267 , 39.70695  , 39.698277 , 34.425804 ,
          34.31247  , 29.53926  , 40.3494   , 43.868008 , 42.3161   ,
          43.888165 , 39.595882 , 38.088852 , 38.475636 , 39.603607 ,
          41.145184 , 42.077213 , 40.90716  , 41.50854  , 42.16462  ,
          43.170803 , 42.5903   , 42.84438  , 41.777634 , 39.075123 ,
          37.99275  , 41.478737 , 43.29306  , 44.33116  , 44.035793 ,
          42.707157 , 44.848343 , 43.63185  , 43.012817 , 43.89084  ,
          46.677776 , 48.83101  , 46.763916 , 47.205807 , 47.040287 ,
          48.858963 , 49.15515  , 41.452824 , 41.16545  , 42.813118 ,
          43.188705 , 43.08741  , 43.85893  , 44.349174 , 45.684616 ,
          45.731953 , 42.950775 , 43.48398  , 43.15677  , 43.91449  ,
          48.145008 , 48.309155 , 48.68672  , 50.259872 , 45.408623 ,
          49.45479  , 51.9686   , 48.622566 , 48.939945 , 46.681744 ,
          48.386078 , 50.437023 , 51.582397 ], dtype=float32),
   &#39;run_time&#39;: 1.9847700595855713,
   &#39;status&#39;: &#39;ok&#39;},
  &#39;misc&#39;: {&#39;tid&#39;: 1,
   &#39;cmd&#39;: (&#39;domain_attachment&#39;, &#39;FMinIter_Domain&#39;),
   &#39;workdir&#39;: None,
   &#39;idxs&#39;: {&#39;activation&#39;: [1],
    &#39;batch_normalization&#39;: [1],
    &#39;batch_size&#39;: [1],
    &#39;complete_inputs&#39;: [1],
    &#39;complete_sample&#39;: [1],
    &#39;device&#39;: [1],
    &#39;dropout_prob_exogenous&#39;: [1],
    &#39;dropout_prob_theta&#39;: [1],
    &#39;early_stop_patience&#39;: [1],
    &#39;eval_freq&#39;: [1],
    &#39;frequency&#39;: [1],
    &#39;idx_to_sample_freq&#39;: [1],
    &#39;initialization&#39;: [1],
    &#39;l1_theta&#39;: [1],
    &#39;learning_rate&#39;: [1],
    &#39;len_sample_chunks&#39;: [1],
    &#39;loss&#39;: [1],
    &#39;loss_hypar&#39;: [1],
    &#39;loss_valid&#39;: [1],
    &#39;lr_decay&#39;: [1],
    &#39;lr_decay_step_size&#39;: [1],
    &#39;max_epochs&#39;: [1],
    &#39;max_steps&#39;: [1],
    &#39;n_blocks&#39;: [1],
    &#39;n_harmonics&#39;: [1],
    &#39;n_hidden&#39;: [1],
    &#39;n_layers&#39;: [1],
    &#39;n_polynomials&#39;: [1],
    &#39;n_s_hidden&#39;: [1],
    &#39;n_series_per_batch&#39;: [1],
    &#39;n_time_in&#39;: [1],
    &#39;n_time_out&#39;: [1],
    &#39;n_val_weeks&#39;: [1],
    &#39;n_x_hidden&#39;: [1],
    &#39;normalizer_x&#39;: [1],
    &#39;normalizer_y&#39;: [1],
    &#39;random_seed&#39;: [1],
    &#39;seasonality&#39;: [1],
    &#39;shared_weights&#39;: [1],
    &#39;stack_types&#39;: [1],
    &#39;val_idx_to_sample_freq&#39;: [1],
    &#39;weight_decay&#39;: [1],
    &#39;window_sampling_limit&#39;: [1]},
   &#39;vals&#39;: {&#39;activation&#39;: [0],
    &#39;batch_normalization&#39;: [0],
    &#39;batch_size&#39;: [0],
    &#39;complete_inputs&#39;: [0],
    &#39;complete_sample&#39;: [0],
    &#39;device&#39;: [0],
    &#39;dropout_prob_exogenous&#39;: [0.3024306449332244],
    &#39;dropout_prob_theta&#39;: [0.3851678624996854],
    &#39;early_stop_patience&#39;: [0],
    &#39;eval_freq&#39;: [0],
    &#39;frequency&#39;: [0],
    &#39;idx_to_sample_freq&#39;: [0],
    &#39;initialization&#39;: [0],
    &#39;l1_theta&#39;: [0],
    &#39;learning_rate&#39;: [0.0007294826926374994],
    &#39;len_sample_chunks&#39;: [0],
    &#39;loss&#39;: [0],
    &#39;loss_hypar&#39;: [0],
    &#39;loss_valid&#39;: [0],
    &#39;lr_decay&#39;: [0.3939003899063233],
    &#39;lr_decay_step_size&#39;: [0],
    &#39;max_epochs&#39;: [0],
    &#39;max_steps&#39;: [0],
    &#39;n_blocks&#39;: [0],
    &#39;n_harmonics&#39;: [0],
    &#39;n_hidden&#39;: [0],
    &#39;n_layers&#39;: [0],
    &#39;n_polynomials&#39;: [0],
    &#39;n_s_hidden&#39;: [0],
    &#39;n_series_per_batch&#39;: [0],
    &#39;n_time_in&#39;: [0],
    &#39;n_time_out&#39;: [0],
    &#39;n_val_weeks&#39;: [0],
    &#39;n_x_hidden&#39;: [3.0],
    &#39;normalizer_x&#39;: [0],
    &#39;normalizer_y&#39;: [0],
    &#39;random_seed&#39;: [16.0],
    &#39;seasonality&#39;: [0],
    &#39;shared_weights&#39;: [0],
    &#39;stack_types&#39;: [0],
    &#39;val_idx_to_sample_freq&#39;: [0],
    &#39;weight_decay&#39;: [8.299088918068486e-05],
    &#39;window_sampling_limit&#39;: [0]}},
  &#39;exp_key&#39;: None,
  &#39;owner&#39;: None,
  &#39;version&#39;: 0,
  &#39;book_time&#39;: datetime.datetime(2021, 6, 6, 19, 34, 53, 186000),
  &#39;refresh_time&#39;: datetime.datetime(2021, 6, 6, 19, 34, 55, 191000)}]</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

</div>
 

