---
description: Quantify uncertainty
output-file: uncertainty_quantification.html
title: Probabilistic Forecasting
---


Probabilistic forecasting is a natural answer to quantify the
uncertainty of target variable’s future. The task requires to model the
following conditional predictive distribution:

$$\mathbb{P}(\mathbf{y}_{t+1:t+H} \;|\; \mathbf{y}_{:t})$$

We will show you how to tackle the task with
[`NeuralForecast`](https://nixtlaverse.nixtla.io/neuralforecast/core.html#neuralforecast)
by combining a classic Long Short Term Memory Network
[(LSTM)](https://arxiv.org/abs/2201.12886) and the Neural Hierarchical
Interpolation [(NHITS)](https://arxiv.org/abs/2201.12886) with the multi
quantile loss function (MQLoss).

$$ \mathrm{MQLoss}(y_{\tau}, [\hat{y}^{(q1)}_{\tau},\hat{y}^{(q2)}_{\tau},\dots,\hat{y}^{(Q)}_{\tau}]) = \frac{1}{H} \sum_{q} \mathrm{QL}(y_{\tau}, \hat{y}^{(q)}_{\tau}) $$

In this notebook we will:<br/> 1. Install NeuralForecast Library<br/> 2.
Explore the M4-Hourly data.<br/> 3. Train the LSTM and NHITS<br/> 4.
Visualize the LSTM/NHITS prediction intervals.

You can run these experiments using GPU with Google Colab.

<a href="https://colab.research.google.com/github/Nixtla/neuralforecast/blob/main/nbs/examples/UncertaintyIntervals.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>

## 1. Installing NeuralForecast

```python
!pip install neuralforecast
```

#### Useful functions

The `plot_grid` auxiliary function defined below will be useful to plot
different time series, and different models’ forecasts.

```python
import logging
import random
import warnings
from itertools import product

import torch
from utilsforecast.plotting import plot_series
```


```python
warnings.filterwarnings("ignore")
```

## 2. Loading M4 Data

For testing purposes, we will use the Hourly dataset from the [M4
competition](https://www.researchgate.net/publication/325901666_The_M4_Competition_Results_findings_conclusion_and_way_forward).

```python
import pandas as pd
```


```python
Y_train_df = pd.read_csv('https://auto-arima-results.s3.amazonaws.com/M4-Hourly.csv')
Y_test_df = pd.read_csv(
    'https://auto-arima-results.s3.amazonaws.com/M4-Hourly-test.csv'
).rename(columns={'y': 'y_test'})
```

In this example we will use a subset of the data to avoid waiting too
long. You can modify the number of series if you want.

```python
n_series = 8
uids = Y_train_df['unique_id'].unique()[:n_series]
Y_train_df = Y_train_df.query('unique_id in @uids')
Y_test_df = Y_test_df.query('unique_id in @uids')
```


```python
plot_series(Y_train_df, Y_test_df)
```

![](/neuralforecast/docs/tutorials/03_uncertainty_quantification_files/figure-markdown_strict/cell-8-output-1.png)

## 3. Model Training

The `core.NeuralForecast` provides a high-level interface with our
collection of PyTorch models.
[`NeuralForecast`](https://nixtlaverse.nixtla.io/neuralforecast/core.html#neuralforecast)
is instantiated with a list of `models=[LSTM(...), NHITS(...)]`,
configured for the forecasting task.

-   The `horizon` parameter controls the number of steps ahead of the
    predictions, in this example 48 hours ahead (2 days).
-   The
    [`MQLoss`](https://nixtlaverse.nixtla.io/neuralforecast/losses.pytorch.html#mqloss)
    with `levels=[80,90]` specializes the network’s output into the 80%
    and 90% prediction intervals.
-   The `max_steps=2000`, controls the duration of the network’s
    training.

For more network’s instantiation details check their
[documentation](https://nixtla.github.io/neuralforecast/models.dilated_rnn.html).

```python
from neuralforecast import NeuralForecast
from neuralforecast.losses.pytorch import MQLoss
from neuralforecast.models import LSTM, NHITS
```


```python
logging.getLogger('pytorch_lightning').setLevel(logging.ERROR)
torch.set_float32_matmul_precision('high')
```


```python
horizon = 48
levels = [80, 90]
models = [LSTM(input_size=3*horizon, h=horizon,
               loss=MQLoss(level=levels), max_steps=1000),
          NHITS(input_size=7*horizon, h=horizon,
                n_freq_downsample=[24, 12, 1],
                loss=MQLoss(level=levels), max_steps=2000),]
nf = NeuralForecast(models=models, freq=1)
```

``` text
Seed set to 1
Seed set to 1
```

All the models of the library are global, meaning that all time series
in `Y_train_df` is used during a shared optimization to train a single
model with shared parameters. This is the most common practice in the
forecasting literature for deep learning models, and it is known as
“cross-learning”.

```python
nf.fit(df=Y_train_df)
```


```python
Y_hat_df = nf.predict()
Y_hat_df.head()
```

``` text
Predicting: |                                                                                                 …
```

``` text
Predicting: |                                                                                                 …
```

|  | unique_id | ds | LSTM-median | LSTM-lo-90 | LSTM-lo-80 | LSTM-hi-80 | LSTM-hi-90 | NHITS-median | NHITS-lo-90 | NHITS-lo-80 | NHITS-hi-80 | NHITS-hi-90 |
|----|----|----|----|----|----|----|----|----|----|----|----|----|
| 0 | H1 | 701 | 604.558350 | 522.655884 | 540.282959 | 637.053223 | 656.381653 | 630.159546 | 609.588196 | 610.796936 | 667.073975 | 669.051697 |
| 1 | H1 | 702 | 543.323303 | 437.307190 | 470.688477 | 592.671570 | 621.944153 | 579.778137 | 559.257080 | 557.859741 | 602.493164 | 610.493286 |
| 2 | H1 | 703 | 493.909180 | 379.216583 | 417.424408 | 551.339111 | 579.863464 | 529.128662 | 496.805786 | 509.799744 | 543.403442 | 554.367126 |
| 3 | H1 | 704 | 457.408600 | 347.231049 | 387.196930 | 524.955444 | 542.673462 | 502.934723 | 469.336945 | 476.858734 | 509.803619 | 517.505493 |
| 4 | H1 | 705 | 431.993744 | 324.040070 | 356.231689 | 487.194427 | 510.252930 | 481.218201 | 457.727081 | 465.636963 | 491.732178 | 508.069946 |

```python
Y_test_df = Y_test_df.merge(Y_hat_df, how='left', on=['unique_id', 'ds']).rename(columns=lambda x: x.replace('-median', ''))
```

## 4. Plotting Predictions

Here we finalize our analysis by plotting the prediction intervals and
verifying that both the
[`LSTM`](https://nixtlaverse.nixtla.io/neuralforecast/models.lstm.html#lstm)
and
[`NHITS`](https://nixtlaverse.nixtla.io/neuralforecast/models.nhits.html#nhits)
are giving excellent results.

Consider the output `[NHITS-lo-90.0`, `NHITS-hi-90.0]`, that represents
the 80% prediction interval of the
[`NHITS`](https://nixtlaverse.nixtla.io/neuralforecast/models.nhits.html#nhits)
network; its lower limit gives the 5th percentile (or 0.05 quantile)
while its upper limit gives the 95th percentile (or 0.95 quantile). For
well-trained models we expect that the target values lie within the
interval 90% of the time.

### LSTM

```python
plot_series(Y_train_df, Y_test_df, level=levels, models=['LSTM'])
```

![](/neuralforecast/docs/tutorials/03_uncertainty_quantification_files/figure-markdown_strict/cell-15-output-1.png)

### NHITS

```python
plot_series(Y_train_df, Y_test_df, level=levels, models=['NHITS'])
```

![](/neuralforecast/docs/tutorials/03_uncertainty_quantification_files/figure-markdown_strict/cell-16-output-1.png)

## References

-   [Roger Koenker and Gilbert Basset (1978). Regression Quantiles,
    Econometrica.](https://www.jstor.org/stable/1913643)<br/>
-   [Jeffrey L. Elman (1990). “Finding Structure in
    Time”.](https://onlinelibrary.wiley.com/doi/abs/10.1207/s15516709cog1402_1)<br/>
-   [Cristian Challu, Kin G. Olivares, Boris N. Oreshkin, Federico
    Garza, Max Mergenthaler-Canseco, Artur Dubrawski (2021). NHITS:
    Neural Hierarchical Interpolation for Time Series Forecasting.
    Accepted at AAAI 2023.](https://arxiv.org/abs/2201.12886)<br/>

