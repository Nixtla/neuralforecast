------------------------------------------------------------------------

### DistributedConfig

> ``` text
>  DistributedConfig (partitions_path:str, num_nodes:int, devices:int)
> ```

------------------------------------------------------------------------

### BaseModel

> ``` text
>  BaseModel (h:int, input_size:int, loss:Union[neuralforecast.losses.pytorc
>             h.BasePointLoss,neuralforecast.losses.pytorch.DistributionLoss
>             ,torch.nn.modules.module.Module], valid_loss:Union[neuralforec
>             ast.losses.pytorch.BasePointLoss,neuralforecast.losses.pytorch
>             .DistributionLoss,torch.nn.modules.module.Module],
>             learning_rate:float, max_steps:int, val_check_steps:int,
>             batch_size:int, valid_batch_size:Optional[int],
>             windows_batch_size:int,
>             inference_windows_batch_size:Optional[int],
>             start_padding_enabled:bool, n_series:Optional[int]=None,
>             n_samples:Optional[int]=100, h_train:int=1,
>             inference_input_size:Optional[int]=None, step_size:int=1,
>             num_lr_decays:int=0, early_stop_patience_steps:int=-1,
>             scaler_type:str='identity',
>             futr_exog_list:Optional[List]=None,
>             hist_exog_list:Optional[List]=None,
>             stat_exog_list:Optional[List]=None,
>             exclude_insample_y:Optional[bool]=False,
>             drop_last_loader:Optional[bool]=False,
>             random_seed:Optional[int]=1, alias:Optional[str]=None,
>             optimizer:Optional[torch.optim.optimizer.Optimizer]=None,
>             optimizer_kwargs:Optional[Dict]=None, lr_scheduler:Optional[to
>             rch.optim.lr_scheduler.LRScheduler]=None,
>             lr_scheduler_kwargs:Optional[Dict]=None,
>             dataloader_kwargs=None, **trainer_kwargs)
> ```

*Hooks to be used in LightningModule.*

