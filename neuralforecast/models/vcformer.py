# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/models.vcformer.ipynb.

# %% auto 0
__all__ = ['VCformer']

# %% ../../nbs/models.vcformer.ipynb 6
import torch
import torch.nn as nn
import torch.nn.functional as F

from math import sqrt

from einops import rearrange

from ..common._modules import MLP, TransEncoder

from ..losses.pytorch import MAE
from ..common._base_multivariate import BaseMultivariate

# %% ../../nbs/models.vcformer.ipynb 8
class DataEmbedding_inverted(nn.Module):
    def __init__(self, c_in, d_model, dropout=0.1):
        super(DataEmbedding_inverted, self).__init__()
        self.value_embedding = nn.Linear(c_in, d_model)
        self.dropout = nn.Dropout(p=dropout)

    def forward(self, x, x_mark):
        x = x.permute(0, 2, 1)
        # x: [Batch Variate Time]
        if x_mark is None:
            # if not time feature
            x = self.value_embedding(x)
        else:
            # if have time feature
            x = self.value_embedding(torch.cat([x, x_mark.permute(0, 2, 1)], 1))
        # x: [Batch Variate d_model]
        return self.dropout(x)

# %% ../../nbs/models.vcformer.ipynb 10
class KPLayerApprox(nn.Module):
    def __init__(self):
        super(KPLayerApprox, self).__init__()
        self.K = None
        self.K_step = None

    def forward(self, z):
        B, N, input_len, hidden_dim = z.shape
        pred_len = input_len
        z = rearrange(z, "b n pn m -> (b n) pn m")
        x, y = z[:, :-1], z[:, 1:]

        self.K = torch.linalg.lstsq(x, y).solution

        if torch.isnan(self.K).any():
            print("Encounter K with nan, replace K by identity matrix")
            self.K = (
                torch.eye(self.K.shape[1])
                .to(self.K.device)
                .unsqueeze(0)
                .repeat(B, 1, 1)
            )

        self.K_step = torch.linalg.matrix_power(self.K, pred_len)
        if torch.isnan(self.K_step).any():
            print("Encounter multistep K with nan, replace it by identity matrix")
            self.K_step = (
                torch.eye(self.K_step.shape[1])
                .to(self.K_step.device)
                .unsqueeze(0)
                .repeat(B, 1, 1)
            )
        z_pred = torch.bmm(z[:, -pred_len:, :], self.K_step)
        return z_pred


class KTDlayer(nn.Module):
    """
    Koopman Temporal Detector layer
    """

    def __init__(self, enc_in, snap_size, proj_dim, hidden_dim, hidden_layers):
        super(KTDlayer, self).__init__()
        self.enc_in = enc_in
        self.snap_size = snap_size
        self.dynamics = KPLayerApprox()
        self.encoder = MLP(
            in_features=snap_size,
            out_features=proj_dim,
            activation="Tanh",
            hidden_size=hidden_dim,
            num_layers=hidden_layers,
            dropout=0.05,
        )
        self.decoder = MLP(
            in_features=proj_dim,
            out_features=snap_size,
            activation="Tanh",
            hidden_size=hidden_dim,
            num_layers=hidden_layers,
            dropout=0.05,
        )
        self.padding_len = (
            snap_size - (enc_in % snap_size) if enc_in % snap_size != 0 else 0
        )

    def forward(self, x):
        # x: B L D
        B, N, D = x.shape

        res = torch.cat((x[:, :, D - self.padding_len :], x), dim=-1)

        res = rearrange(res, "b n (p_n p) -> b n p_n p", p=self.snap_size)

        res = self.encoder(res)  # b n p_n m, m means hidden dim

        # b*n f_n m, f_n means forecast patch num
        x_pred = self.dynamics(res)

        x_pred = self.decoder(x_pred)  # b*n f_n p

        x_pred = rearrange(x_pred, "(b n) f_n p -> b n (f_n p)", b=B)

        return x_pred

# %% ../../nbs/models.vcformer.ipynb 12
class EncoderLayer(nn.Module):
    def __init__(
        self, attention, ktd, d_model, d_ff=None, dropout=0.1, activation="relu"
    ):
        super(EncoderLayer, self).__init__()
        d_ff = d_ff or 4 * d_model
        self.attention = attention
        self.ktd = ktd
        self.lin1 = nn.Linear(d_model, d_ff)
        self.lin2 = nn.Linear(d_ff, d_model)
        self.conv1 = nn.Conv1d(in_channels=d_model, out_channels=d_ff, kernel_size=1)
        self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=d_model, kernel_size=1)
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.norm3 = nn.LayerNorm(d_model)
        self.dropout = nn.Dropout(dropout)
        self.activation = F.relu if activation == "relu" else F.gelu

    def forward(self, x, attn_mask=None, tau=None, delta=None):
        # VCA module
        new_x, attn = self.attention(x, x, x, attn_mask=attn_mask, tau=tau, delta=delta)
        x = x + self.dropout(new_x)
        y = x = self.norm1(x)

        # KTD Module
        x_ktd = y
        y = self.dropout(self.ktd(x_ktd))

        return self.norm2(x_ktd + y), attn

# %% ../../nbs/models.vcformer.ipynb 14
class TriangularCausalMask:
    def __init__(self, B, L, device="cpu"):
        mask_shape = [B, 1, L, L]
        with torch.no_grad():
            self._mask = torch.triu(
                torch.ones(mask_shape, dtype=torch.bool), diagonal=1
            ).to(device)

    @property
    def mask(self):
        return self._mask


class VarCorAttention(nn.Module):
    def __init__(
        self,
        mask_flag=True,
        factor=5,
        scale=None,
        attention_dropout=0.1,
        output_attention=False,
    ) -> None:
        super(VarCorAttention, self).__init__()

        self.scale = scale
        self.mask_flag = mask_flag
        self.output_attention = output_attention
        self.dropout = nn.Dropout(attention_dropout)

    def origin_compute_cross_cor(self, queries, keys):
        q_fft = torch.fft.rfft(queries, dim=-1)
        k_fft = torch.fft.rfft(keys, dim=-1)

        res = q_fft * k_fft
        corr = torch.fft.irfft(res, dim=-1)
        corr = corr.mean(dim=-1)
        return corr

    def compute_cross_cor(self, queries, keys):

        q_fft = torch.fft.rfft(queries, dim=-1)
        k_fft = torch.fft.rfft(keys, dim=-1)
        q_fft = q_fft.unsqueeze(1)  # [D,1,T/2+1]
        k_fft = torch.conj(k_fft.unsqueeze(0))  # [1,D,T/2+1]
        res = q_fft * k_fft  # [D,D,T/2+1]
        corr = torch.fft.irfft(res, dim=-1)
        corr = corr.mean(dim=-1)

        return corr

    def optimized_compute_cross_cor(self, queries, keys):
        # Perform batched FFT
        q_fft = torch.fft.rfft(queries, dim=-1)
        k_fft = torch.fft.rfft(keys, dim=-1)

        # Expand dimensions for broadcasting
        q_fft = q_fft.unsqueeze(2)  # [B, D, 1, T/2+1]
        k_fft = torch.conj(k_fft.unsqueeze(1))  # [B, 1, D, T/2+1]

        # Element-wise multiplication and batched inverse FFT
        res = q_fft * k_fft  # [B, D, D, T/2+1]
        corr = torch.fft.irfft(res, dim=-1)

        # Mean across the time dimension
        corr = corr.mean(dim=-1)

        return corr

    def forward(self, queries, keys, values, attn_mask, tau=None, delta=None):

        B, D, T = queries.shape
        _, S, _ = values.shape
        corr = torch.zeros(B, D, D).to(queries.device)
        scale = self.scale or 1.0 / sqrt(T)

        for i in range(B):
            for j in range(D):
                corr[[i], [j], :] = self.origin_compute_cross_cor(
                    queries[[i], [j], :], keys[[i], :, :]
                )

        for i in range(B):
            corr[[i], :, :] = self.compute_cross_cor(
                queries[[i], :, :], keys[[i], :, :]
            )

        if self.mask_flag:
            if attn_mask is None:
                attn_mask = TriangularCausalMask(B, T, device=queries.device)
        corr = torch.softmax(corr * scale, dim=-1)

        V = torch.einsum("bsd,bde->bse", corr, values)
        if self.output_attention:
            return (V.contiguous(), corr)
        else:
            return (V.contiguous(), None)


class VarCorAttentionLayer(nn.Module):
    def __init__(self, attention, d_model, n_heads, d_keys=None, d_values=None):
        super(VarCorAttentionLayer, self).__init__()

        d_keys = d_keys or (d_model // n_heads)
        d_values = d_values or (d_model // n_heads)

        self.inner_attention = attention
        self.query_projection = nn.Linear(d_model, d_keys * n_heads)
        self.key_projection = nn.Linear(d_model, d_keys * n_heads)
        self.value_projection = nn.Linear(d_model, d_values * n_heads)
        self.out_projection = nn.Linear(d_values * n_heads, d_model)
        self.n_heads = n_heads

    def forward(self, queries, keys, values, attn_mask, tau=None, delta=None):
        B, D, L = queries.shape
        _, _, S = keys.shape

        queries = self.query_projection(queries)
        keys = self.key_projection(keys)
        values = self.value_projection(values)
        out, attn = self.inner_attention(
            queries, keys, values, attn_mask, tau=tau, delta=delta
        )
        out = out.view(B, D, -1)

        return self.out_projection(out), attn

# %% ../../nbs/models.vcformer.ipynb 16
class VCformer(BaseMultivariate):
    """VCformer

    **Parameters:**<br>
    `h`: int, Forecast horizon. <br>
    `input_size`: int, autorregresive inputs size, y=[1,2,3,4] input_size=2 -> y_[t-2:t]=[1,2].<br>
    `n_series`: int, number of time-series.<br>
    `futr_exog_list`: str list, future exogenous columns.<br>
    `hist_exog_list`: str list, historic exogenous columns.<br>
    `stat_exog_list`: str list, static exogenous columns.<br>
    `hidden_size`: int, dimension of the model.<br>
    `n_heads`: int, number of heads.<br>
    `e_layers`: int, number of encoder layers.<br>
    `d_ff`: int, dimension of fully-connected layer.<br>
    `snap_size`: int, snapshot size for Koopman Temporal Detector<br>
    `projection_dim`: int, projection dimension of Koopman space<br>
    `hidden_enc_dim`: int, hidden dimension of Koopman encoder<br>
    `hidden_ktd_layers`: number of hidden layers of Koopman Temporal Detector<br>
    `factor`: int, attention factor.<br>
    `dropout`: float, dropout rate.<br>
    `loss`: PyTorch module, instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>
    `valid_loss`: PyTorch module=`loss`, instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>
    `max_steps`: int=1000, maximum number of training steps.<br>
    `learning_rate`: float=1e-3, Learning rate between (0, 1).<br>
    `num_lr_decays`: int=-1, Number of learning rate decays, evenly distributed across max_steps.<br>
    `early_stop_patience_steps`: int=-1, Number of validation iterations before early stopping.<br>
    `val_check_steps`: int=100, Number of training steps between every validation loss check.<br>
    `batch_size`: int=32, number of different series in each batch.<br>
    `step_size`: int=1, step size between each window of temporal data.<br>
    `scaler_type`: str='identity', type of scaler for temporal inputs normalization see [temporal scalers](https://nixtla.github.io/neuralforecast/common.scalers.html).<br>
    `random_seed`: int=1, random_seed for pytorch initializer and numpy generators.<br>
    `num_workers_loader`: int=os.cpu_count(), workers to be used by `TimeSeriesDataLoader`.<br>
    `drop_last_loader`: bool=False, if True `TimeSeriesDataLoader` drops last non-full batch.<br>
    `alias`: str, optional,  Custom name of the model.<br>
    `optimizer`: Subclass of 'torch.optim.Optimizer', optional, user specified optimizer instead of the default choice (Adam).<br>
    `optimizer_kwargs`: dict, optional, list of parameters used by the user specified `optimizer`.<br>
    `lr_scheduler`: Subclass of 'torch.optim.lr_scheduler.LRScheduler', optional, user specified lr_scheduler instead of the default choice (StepLR).<br>
    `lr_scheduler_kwargs`: dict, optional, list of parameters used by the user specified `lr_scheduler`.<br>
    `**trainer_kwargs`: int,  keyword trainer arguments inherited from [PyTorch Lighning's trainer](https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.trainer.trainer.Trainer.html?highlight=trainer).<br>
    **References**
    - [Yingnan Yang, Qingling Zhu, Jianyong Chen. "VCformer: Variable Correlation Transformer with Inherent Lagged Correlation for Multivariate Time Series Forecasting"](https://arxiv.org/abs/2405.11470)
    """

    # Class attributes
    SAMPLING_TYPE = "multivariate"
    EXOGENOUS_FUTR = False
    EXOGENOUS_HIST = False
    EXOGENOUS_STAT = False

    def __init__(
        self,
        h,
        input_size,
        n_series,
        futr_exog_list=None,
        hist_exog_list=None,
        stat_exog_list=None,
        # VCformer
        hidden_size: int = 512,
        n_heads: int = 8,
        e_layers: int = 2,
        d_ff: int = 2048,
        snap_size: int = 16,
        projection_dim: int = 128,
        hidden_enc_dim: int = 256,
        hidden_ktd_layers: int = 1,
        factor: int = 1,
        dropout: float = 0.1,
        loss=MAE(),
        valid_loss=None,
        max_steps: int = 1000,
        learning_rate: float = 1e-3,
        num_lr_decays: int = -1,
        early_stop_patience_steps: int = -1,
        val_check_steps: int = 100,
        batch_size: int = 32,
        step_size: int = 1,
        scaler_type: str = "identity",
        random_seed: int = 1,
        num_workers_loader: int = 0,
        drop_last_loader: bool = False,
        optimizer=None,
        optimizer_kwargs=None,
        lr_scheduler=None,
        lr_scheduler_kwargs=None,
        **trainer_kwargs
    ):

        super(VCformer, self).__init__(
            h=h,
            input_size=input_size,
            n_series=n_series,
            stat_exog_list=None,
            futr_exog_list=None,
            hist_exog_list=None,
            loss=loss,
            valid_loss=valid_loss,
            max_steps=max_steps,
            learning_rate=learning_rate,
            num_lr_decays=num_lr_decays,
            early_stop_patience_steps=early_stop_patience_steps,
            val_check_steps=val_check_steps,
            batch_size=batch_size,
            step_size=step_size,
            scaler_type=scaler_type,
            random_seed=random_seed,
            num_workers_loader=num_workers_loader,
            drop_last_loader=drop_last_loader,
            optimizer=optimizer,
            optimizer_kwargs=optimizer_kwargs,
            lr_scheduler=lr_scheduler,
            lr_scheduler_kwargs=lr_scheduler_kwargs,
            **trainer_kwargs
        )

        self.h = h
        self.enc_in = n_series
        self.hidden_size = hidden_size
        self.dropout = dropout
        self.factor = factor
        self.n_heads = n_heads
        self.snap_size = snap_size
        self.projection_dim = projection_dim
        self.hidden_enc_dim = hidden_enc_dim
        self.hidden_ktd_layers = hidden_ktd_layers
        self.d_ff = d_ff
        self.e_layers = e_layers

        # Architecture
        self.enc_embedding = DataEmbedding_inverted(
            input_size, hidden_size, self.dropout
        )

        self.encoder = TransEncoder(
            [
                EncoderLayer(
                    VarCorAttentionLayer(
                        VarCorAttention(
                            factor=self.factor, attention_dropout=self.dropout
                        ),
                        self.hidden_size,
                        self.n_heads,
                    ),
                    KTDlayer(
                        self.hidden_size,
                        self.snap_size,
                        self.projection_dim,
                        self.hidden_enc_dim,
                        self.hidden_ktd_layers,
                    ),
                    self.hidden_size,
                    self.d_ff,
                    self.dropout,
                    activation=F.gelu,
                )
                for l in range(self.e_layers)
            ],
            norm_layer=torch.nn.LayerNorm(self.hidden_size),
        )

        self.projection = nn.Linear(self.hidden_size, h, bias=True)

    def forecast(self, x_enc, x_mark_enc=None):
        # Normalization from Non-stationary Transformer
        means = x_enc.mean(1, keepdim=True).detach()
        x_enc = x_enc - means
        stdev = torch.sqrt(torch.var(x_enc, dim=1, keepdim=True, unbiased=False) + 1e-5)
        x_enc /= stdev

        _, _, N = x_enc.shape

        # Embedding
        enc_out = self.enc_embedding(x_enc, x_mark_enc)

        enc_out, attns = self.encoder(enc_out, attn_mask=None)
        # reshape enc_out[B,D,T] -> dec_out[B,T,D]
        dec_out = self.projection(enc_out).permute(0, 2, 1)[:, :, :N]
        # De-Normalization from Non-stationary Transformer
        dec_out = dec_out * (stdev[:, 0, :].unsqueeze(1).repeat(1, self.h, 1))
        dec_out = dec_out + (means[:, 0, :].unsqueeze(1).repeat(1, self.h, 1))
        return dec_out

    def forward(self, windows_batch):
        insample_y = windows_batch["insample_y"]

        y_pred = self.forecast(insample_y)
        y_pred = y_pred[:, -self.h :, :]
        y_pred = self.loss.domain_map(y_pred)

        # domain_map might have squeezed the last dimension in case n_series == 1
        if y_pred.ndim == 2:
            return y_pred.unsqueeze(-1)
        else:
            return y_pred
