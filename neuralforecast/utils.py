# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/utils.ipynb.

# %% auto 0
__all__ = ['AirPassengers', 'AirPassengersDF', 'unique_id', 'ds', 'y', 'AirPassengersPanel', 'snaive', 'airline1_dummy',
           'airline2_dummy', 'AirPassengersStatic', 'generate_series', 'TimeFeature', 'SecondOfMinute', 'MinuteOfHour',
           'HourOfDay', 'DayOfWeek', 'DayOfMonth', 'DayOfYear', 'MonthOfYear', 'WeekOfYear',
           'time_features_from_frequency_str', 'augment_calendar_df', 'get_indexer_raise_missing',
           'PredictionIntervals', 'add_conformal_distribution_intervals', 'add_conformal_error_intervals',
           'get_prediction_interval_method', 'level_to_quantiles', 'quantiles_to_level', 'ShapModelWrapper',
           'create_input_tensor_for_series', 'create_multi_series_background_data', 'model_predict',
           'create_multi_series_feature_names']

# %% ../nbs/utils.ipynb 3
import random
from itertools import chain
from typing import List, Union, Optional, Tuple
from utilsforecast.compat import DFType

import numpy as np
import pandas as pd
import torch

# %% ../nbs/utils.ipynb 6
def generate_series(
    n_series: int,
    freq: str = "D",
    min_length: int = 50,
    max_length: int = 500,
    n_temporal_features: int = 0,
    n_static_features: int = 0,
    equal_ends: bool = False,
    seed: int = 0,
) -> pd.DataFrame:
    """Generate Synthetic Panel Series.

    Generates `n_series` of frequency `freq` of different lengths in the interval [`min_length`, `max_length`].
    If `n_temporal_features > 0`, then each serie gets temporal features with random values.
    If `n_static_features > 0`, then a static dataframe is returned along the temporal dataframe.
    If `equal_ends == True` then all series end at the same date.

    **Parameters:**<br>
    `n_series`: int, number of series for synthetic panel.<br>
    `min_length`: int, minimal length of synthetic panel's series.<br>
    `max_length`: int, minimal length of synthetic panel's series.<br>
    `n_temporal_features`: int, default=0, number of temporal exogenous variables for synthetic panel's series.<br>
    `n_static_features`: int, default=0, number of static exogenous variables for synthetic panel's series.<br>
    `equal_ends`: bool, if True, series finish in the same date stamp `ds`.<br>
    `freq`: str, frequency of the data, [panda's available frequencies](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases).<br>

    **Returns:**<br>
    `freq`: pandas.DataFrame, synthetic panel with columns [`unique_id`, `ds`, `y`] and exogenous.
    """
    seasonalities = {"D": 7, "M": 12}
    season = seasonalities[freq]

    rng = np.random.RandomState(seed)
    series_lengths = rng.randint(min_length, max_length + 1, n_series)
    total_length = series_lengths.sum()

    dates = pd.date_range("2000-01-01", periods=max_length, freq=freq).values
    uids = [np.repeat(i, serie_length) for i, serie_length in enumerate(series_lengths)]
    if equal_ends:
        ds = [dates[-serie_length:] for serie_length in series_lengths]
    else:
        ds = [dates[:serie_length] for serie_length in series_lengths]

    y = np.arange(total_length) % season + rng.rand(total_length) * 0.5
    temporal_df = pd.DataFrame(
        dict(unique_id=chain.from_iterable(uids), ds=chain.from_iterable(ds), y=y)
    )

    random.seed(seed)
    for i in range(n_temporal_features):
        random.seed(seed)
        temporal_values = [
            [random.randint(0, 100)] * serie_length for serie_length in series_lengths
        ]
        temporal_df[f"temporal_{i}"] = np.hstack(temporal_values)
        temporal_df[f"temporal_{i}"] = temporal_df[f"temporal_{i}"].astype("category")
        if i == 0:
            temporal_df["y"] = temporal_df["y"] * (
                1 + temporal_df[f"temporal_{i}"].cat.codes
            )

    temporal_df["unique_id"] = temporal_df["unique_id"].astype("category")
    temporal_df["unique_id"] = temporal_df["unique_id"].cat.as_ordered()

    if n_static_features > 0:
        static_features = np.random.uniform(
            low=0.0, high=1.0, size=(n_series, n_static_features)
        )
        static_df = pd.DataFrame.from_records(
            static_features, columns=[f"static_{i}" for i in range(n_static_features)]
        )

        static_df["unique_id"] = np.arange(n_series)
        static_df["unique_id"] = static_df["unique_id"].astype("category")
        static_df["unique_id"] = static_df["unique_id"].cat.as_ordered()

        return temporal_df, static_df

    return temporal_df

# %% ../nbs/utils.ipynb 12
AirPassengers = np.array(
    [
        112.0,
        118.0,
        132.0,
        129.0,
        121.0,
        135.0,
        148.0,
        148.0,
        136.0,
        119.0,
        104.0,
        118.0,
        115.0,
        126.0,
        141.0,
        135.0,
        125.0,
        149.0,
        170.0,
        170.0,
        158.0,
        133.0,
        114.0,
        140.0,
        145.0,
        150.0,
        178.0,
        163.0,
        172.0,
        178.0,
        199.0,
        199.0,
        184.0,
        162.0,
        146.0,
        166.0,
        171.0,
        180.0,
        193.0,
        181.0,
        183.0,
        218.0,
        230.0,
        242.0,
        209.0,
        191.0,
        172.0,
        194.0,
        196.0,
        196.0,
        236.0,
        235.0,
        229.0,
        243.0,
        264.0,
        272.0,
        237.0,
        211.0,
        180.0,
        201.0,
        204.0,
        188.0,
        235.0,
        227.0,
        234.0,
        264.0,
        302.0,
        293.0,
        259.0,
        229.0,
        203.0,
        229.0,
        242.0,
        233.0,
        267.0,
        269.0,
        270.0,
        315.0,
        364.0,
        347.0,
        312.0,
        274.0,
        237.0,
        278.0,
        284.0,
        277.0,
        317.0,
        313.0,
        318.0,
        374.0,
        413.0,
        405.0,
        355.0,
        306.0,
        271.0,
        306.0,
        315.0,
        301.0,
        356.0,
        348.0,
        355.0,
        422.0,
        465.0,
        467.0,
        404.0,
        347.0,
        305.0,
        336.0,
        340.0,
        318.0,
        362.0,
        348.0,
        363.0,
        435.0,
        491.0,
        505.0,
        404.0,
        359.0,
        310.0,
        337.0,
        360.0,
        342.0,
        406.0,
        396.0,
        420.0,
        472.0,
        548.0,
        559.0,
        463.0,
        407.0,
        362.0,
        405.0,
        417.0,
        391.0,
        419.0,
        461.0,
        472.0,
        535.0,
        622.0,
        606.0,
        508.0,
        461.0,
        390.0,
        432.0,
    ],
    dtype=np.float32,
)

# %% ../nbs/utils.ipynb 13
AirPassengersDF = pd.DataFrame(
    {
        "unique_id": np.ones(len(AirPassengers)),
        "ds": pd.date_range(
            start="1949-01-01", periods=len(AirPassengers), freq=pd.offsets.MonthEnd()
        ),
        "y": AirPassengers,
    }
)

# %% ../nbs/utils.ipynb 20
# Declare Panel Data
unique_id = np.concatenate(
    [["Airline1"] * len(AirPassengers), ["Airline2"] * len(AirPassengers)]
)
ds = np.tile(
    pd.date_range(
        start="1949-01-01", periods=len(AirPassengers), freq=pd.offsets.MonthEnd()
    ).to_numpy(),
    2,
)
y = np.concatenate([AirPassengers, AirPassengers + 300])

AirPassengersPanel = pd.DataFrame({"unique_id": unique_id, "ds": ds, "y": y})

# For future exogenous variables
# Declare SeasonalNaive12 and fill first 12 values with y
snaive = (
    AirPassengersPanel.groupby("unique_id")["y"]
    .shift(periods=12)
    .reset_index(drop=True)
)
AirPassengersPanel["trend"] = range(len(AirPassengersPanel))
AirPassengersPanel["y_[lag12]"] = snaive.fillna(AirPassengersPanel["y"])

# Declare Static Data
unique_id = np.array(["Airline1", "Airline2"])
airline1_dummy = [0, 1]
airline2_dummy = [1, 0]
AirPassengersStatic = pd.DataFrame(
    {"unique_id": unique_id, "airline1": airline1_dummy, "airline2": airline2_dummy}
)

AirPassengersPanel.groupby("unique_id").tail(4)

# %% ../nbs/utils.ipynb 26
class TimeFeature:
    def __init__(self):
        pass

    def __call__(self, index: pd.DatetimeIndex):
        return print("Overwrite with corresponding feature")

    def __repr__(self):
        return self.__class__.__name__ + "()"


class SecondOfMinute(TimeFeature):
    """Minute of hour encoded as value between [-0.5, 0.5]"""

    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:
        return index.second / 59.0 - 0.5


class MinuteOfHour(TimeFeature):
    """Minute of hour encoded as value between [-0.5, 0.5]"""

    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:
        return index.minute / 59.0 - 0.5


class HourOfDay(TimeFeature):
    """Hour of day encoded as value between [-0.5, 0.5]"""

    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:
        return index.hour / 23.0 - 0.5


class DayOfWeek(TimeFeature):
    """Hour of day encoded as value between [-0.5, 0.5]"""

    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:
        return index.dayofweek / 6.0 - 0.5


class DayOfMonth(TimeFeature):
    """Day of month encoded as value between [-0.5, 0.5]"""

    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:
        return (index.day - 1) / 30.0 - 0.5


class DayOfYear(TimeFeature):
    """Day of year encoded as value between [-0.5, 0.5]"""

    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:
        return (index.dayofyear - 1) / 365.0 - 0.5


class MonthOfYear(TimeFeature):
    """Month of year encoded as value between [-0.5, 0.5]"""

    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:
        return (index.month - 1) / 11.0 - 0.5


class WeekOfYear(TimeFeature):
    """Week of year encoded as value between [-0.5, 0.5]"""

    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:
        return (index.week - 1) / 52.0 - 0.5


def time_features_from_frequency_str(freq_str: str) -> List[TimeFeature]:
    """
    Returns a list of time features that will be appropriate for the given frequency string.
    Parameters
    ----------
    freq_str
        Frequency string of the form [multiple][granularity] such as "12H", "5min", "1D" etc.
    """

    if freq_str not in ["Q", "M", "MS", "W", "D", "B", "H", "T", "S"]:
        raise Exception("Frequency not supported")

    if freq_str in ["Q", "M", "MS"]:
        return [cls() for cls in [MonthOfYear]]
    elif freq_str == "W":
        return [cls() for cls in [DayOfMonth, WeekOfYear]]
    elif freq_str in ["D", "B"]:
        return [cls() for cls in [DayOfWeek, DayOfMonth, DayOfYear]]
    elif freq_str == "H":
        return [cls() for cls in [HourOfDay, DayOfWeek, DayOfMonth, DayOfYear]]
    elif freq_str == "T":
        return [
            cls() for cls in [MinuteOfHour, HourOfDay, DayOfWeek, DayOfMonth, DayOfYear]
        ]
    else:
        return [
            cls()
            for cls in [
                SecondOfMinute,
                MinuteOfHour,
                HourOfDay,
                DayOfWeek,
                DayOfMonth,
                DayOfYear,
            ]
        ]


def augment_calendar_df(df, freq="H"):
    """
    > * Q - [month]
    > * M - [month]
    > * W - [Day of month, week of year]
    > * D - [Day of week, day of month, day of year]
    > * B - [Day of week, day of month, day of year]
    > * H - [Hour of day, day of week, day of month, day of year]
    > * T - [Minute of hour*, hour of day, day of week, day of month, day of year]
    > * S - [Second of minute, minute of hour, hour of day, day of week, day of month, day of year]
    *minute returns a number from 0-3 corresponding to the 15 minute period it falls into.
    """
    df = df.copy()

    freq_map = {
        "Q": ["month"],
        "M": ["month"],
        "MS": ["month"],
        "W": ["monthday", "yearweek"],
        "D": ["weekday", "monthday", "yearday"],
        "B": ["weekday", "monthday", "yearday"],
        "H": ["dayhour", "weekday", "monthday", "yearday"],
        "T": ["hourminute", "dayhour", "weekday", "monthday", "yearday"],
        "S": [
            "minutesecond",
            "hourminute",
            "dayhour",
            "weekday",
            "monthday",
            "yearday",
        ],
    }

    ds_col = pd.to_datetime(df.ds.values)
    ds_data = np.vstack(
        [feat(ds_col) for feat in time_features_from_frequency_str(freq)]
    ).transpose(1, 0)
    ds_data = pd.DataFrame(ds_data, columns=freq_map[freq])

    return pd.concat([df, ds_data], axis=1), freq_map[freq]

# %% ../nbs/utils.ipynb 29
def get_indexer_raise_missing(idx: pd.Index, vals: List[str]) -> List[int]:
    idxs = idx.get_indexer(vals)
    missing = [v for i, v in zip(idxs, vals) if i == -1]
    if missing:
        raise ValueError(f"The following values are missing from the index: {missing}")
    return idxs

# %% ../nbs/utils.ipynb 31
class PredictionIntervals:
    """Class for storing prediction intervals metadata information."""

    def __init__(
        self,
        n_windows: int = 2,
        method: str = "conformal_distribution",
    ):
        """
        n_windows : int
            Number of windows to evaluate.
        method : str, default is conformal_distribution
            One of the supported methods for the computation of prediction intervals:
            conformal_error or conformal_distribution
        """
        if n_windows < 2:
            raise ValueError(
                "You need at least two windows to compute conformal intervals"
            )
        allowed_methods = ["conformal_error", "conformal_distribution"]
        if method not in allowed_methods:
            raise ValueError(f"method must be one of {allowed_methods}")
        self.n_windows = n_windows
        self.method = method

    def __repr__(self):
        return (
            f"PredictionIntervals(n_windows={self.n_windows}, method='{self.method}')"
        )

# %% ../nbs/utils.ipynb 32
def add_conformal_distribution_intervals(
    model_fcsts: np.array,
    cs_df: DFType,
    model: str,
    cs_n_windows: int,
    n_series: int,
    horizon: int,
    level: Optional[List[Union[int, float]]] = None,
    quantiles: Optional[List[float]] = None,
) -> Tuple[np.array, List[str]]:
    """
    Adds conformal intervals to a `fcst_df` based on conformal scores `cs_df`.
    `level` should be already sorted. This strategy creates forecasts paths
    based on errors and calculate quantiles using those paths.
    """
    assert (
        level is not None or quantiles is not None
    ), "Either level or quantiles must be provided"

    if quantiles is None and level is not None:
        alphas = [100 - lv for lv in level]
        cuts = [alpha / 200 for alpha in reversed(alphas)]
        cuts.extend(1 - alpha / 200 for alpha in alphas)
    elif quantiles is not None:
        cuts = quantiles

    scores = cs_df[model].to_numpy().reshape(n_series, cs_n_windows, horizon)
    scores = scores.transpose(1, 0, 2)
    # restrict scores to horizon
    scores = scores[:, :, :horizon]
    mean = model_fcsts.reshape(1, n_series, -1)
    scores = np.vstack([mean - scores, mean + scores])
    scores_quantiles = np.quantile(
        scores,
        cuts,
        axis=0,
    )
    scores_quantiles = scores_quantiles.reshape(len(cuts), -1).T
    if quantiles is None and level is not None:
        lo_cols = [f"{model}-lo-{lv}" for lv in reversed(level)]
        hi_cols = [f"{model}-hi-{lv}" for lv in level]
        out_cols = lo_cols + hi_cols
    elif quantiles is not None:
        out_cols = [f"{model}-ql{q}" for q in quantiles]

    fcsts_with_intervals = np.hstack([model_fcsts, scores_quantiles])

    return fcsts_with_intervals, out_cols

# %% ../nbs/utils.ipynb 33
def add_conformal_error_intervals(
    model_fcsts: np.array,
    cs_df: DFType,
    model: str,
    cs_n_windows: int,
    n_series: int,
    horizon: int,
    level: Optional[List[Union[int, float]]] = None,
    quantiles: Optional[List[float]] = None,
) -> Tuple[np.array, List[str]]:
    """
    Adds conformal intervals to a `fcst_df` based on conformal scores `cs_df`.
    `level` should be already sorted. This startegy creates prediction intervals
    based on the absolute errors.
    """
    assert (
        level is not None or quantiles is not None
    ), "Either level or quantiles must be provided"

    if quantiles is None and level is not None:
        alphas = [100 - lv for lv in level]
        cuts = [alpha / 200 for alpha in reversed(alphas)]
        cuts.extend(1 - alpha / 200 for alpha in alphas)
    elif quantiles is not None:
        cuts = quantiles

    mean = model_fcsts.ravel()
    scores = cs_df[model].to_numpy().reshape(n_series, cs_n_windows, horizon)
    scores = scores.transpose(1, 0, 2)
    # restrict scores to horizon
    scores = scores[:, :, :horizon]
    scores_quantiles = np.quantile(
        scores,
        cuts,
        axis=0,
    )
    scores_quantiles = scores_quantiles.reshape(len(cuts), -1)

    if quantiles is None and level is not None:
        lo_cols = [f"{model}-lo-{lv}" for lv in reversed(level)]
        hi_cols = [f"{model}-hi-{lv}" for lv in level]
        out_cols = lo_cols + hi_cols
    else:
        out_cols = [f"{model}-ql{q}" for q in cuts]

    scores_quantiles_ls = []
    for i, q in enumerate(cuts):
        if q < 0.5:
            scores_quantiles_ls.append(mean - scores_quantiles[::-1][i])
        elif q > 0.5:
            scores_quantiles_ls.append(mean + scores_quantiles[i])
        else:
            scores_quantiles_ls.append(mean)
    scores_quantiles = np.vstack(scores_quantiles_ls).T

    fcsts_with_intervals = np.hstack([model_fcsts, scores_quantiles])

    return fcsts_with_intervals, out_cols

# %% ../nbs/utils.ipynb 34
def get_prediction_interval_method(method: str):
    available_methods = {
        "conformal_distribution": add_conformal_distribution_intervals,
        "conformal_error": add_conformal_error_intervals,
    }
    if method not in available_methods.keys():
        raise ValueError(
            f"prediction intervals method {method} not supported "
            f'please choose one of {", ".join(available_methods.keys())}'
        )
    return available_methods[method]

# %% ../nbs/utils.ipynb 35
def level_to_quantiles(level: List[Union[int, float]]) -> List[float]:
    """
    Converts a list of levels to a list of quantiles.
    """
    level_set = set(level)
    return sorted(
        list(
            set(sum([[(50 - l / 2) / 100, (50 + l / 2) / 100] for l in level_set], []))
        )
    )


def quantiles_to_level(quantiles: List[float]) -> List[Union[int, float]]:
    """
    Converts a list of quantiles to a list of levels.
    """
    quantiles_set = set(quantiles)
    return sorted(
        set(
            [
                int(round(100 - 200 * (q * (q < 0.5) + (1 - q) * (q >= 0.5)), 2))
                for q in quantiles_set
            ]
        )
    )

# %% ../nbs/utils.ipynb 38
class ShapModelWrapper(torch.nn.Module):
    """
    SHAP wrapper model that converts flattened tensor input to dictionary format
    and handles multiple series dynamically.
    """

    def __init__(self, original_model, train_df, static_df):
        super().__init__()
        self.original_model = original_model
        self.train_df = train_df
        self.static_df = static_df
        self.futr_exog_cols = original_model.futr_exog_list
        self.hist_exog_cols = original_model.hist_exog_list
        self.stat_exog_cols = original_model.stat_exog_list
        self.input_size = original_model.input_size
        self.h = original_model.h

        # Calculate input dimensions
        self.n_futr_features = (
            len(self.futr_exog_cols) * (self.input_size + self.h)
            if self.futr_exog_cols
            else 0
        )
        self.n_hist_exog_features = (
            len(self.hist_exog_cols) * self.input_size if self.hist_exog_cols else 0
        )
        self.n_hist_target_features = self.input_size
        self.n_series_features = 1  # unique_id encoded as integer

        # Create mapping for unique_ids to static features (only if static features exist)
        self.static_mapping = {}
        if self.stat_exog_cols and static_df is not None:
            for unique_id in static_df["unique_id"].unique():
                static_values = static_df[static_df["unique_id"] == unique_id][
                    self.stat_exog_cols
                ].values[0]
                self.static_mapping[unique_id] = torch.tensor(
                    static_values, dtype=torch.float32
                )

        # Create unique_id to integer mapping using training data
        available_unique_ids = sorted(self.train_df["unique_id"].unique())
        self.unique_id_to_int = {uid: i for i, uid in enumerate(available_unique_ids)}
        self.int_to_unique_id = {i: uid for uid, i in self.unique_id_to_int.items()}

    def forward(self, X_flat):
        """
        Convert flattened tensor input to dictionary format and call original model
        X_flat: [batch_size, n_total_features] where features include series_id + futr_exog + hist_exog + hist_target
        """
        batch_size = X_flat.shape[0]

        # Split the input tensor
        idx = 0

        # Series identifier (first feature)
        series_ids = X_flat[:, idx : idx + self.n_series_features].long().flatten()
        idx += self.n_series_features

        # Future exogenous features (varying or None)
        if self.futr_exog_cols:
            futr_flat = X_flat[:, idx : idx + self.n_futr_features]
            idx += self.n_futr_features
            futr_exog = futr_flat.reshape(
                batch_size, self.input_size + self.h, len(self.futr_exog_cols)
            )
        else:
            futr_exog = None

        # Historical exogenous features (varying or None)
        if self.hist_exog_cols:
            hist_exog_flat = X_flat[:, idx : idx + self.n_hist_exog_features]
            hist_exog = hist_exog_flat.reshape(
                batch_size, self.input_size, len(self.hist_exog_cols)
            )
            idx += self.n_hist_exog_features
        else:
            hist_exog = None

        # Historical target values (always present)
        hist_target_flat = X_flat[:, idx : idx + self.n_hist_target_features]
        insample_y = hist_target_flat.reshape(batch_size, self.input_size)

        # Static exogenous features (varies by series, None if no static features)
        if self.stat_exog_cols and self.static_mapping:
            stat_exog_batch = []
            for i in range(batch_size):
                series_int = series_ids[i].item()
                unique_id = self.int_to_unique_id[series_int]
                stat_exog_batch.append(self.static_mapping[unique_id])
            stat_exog = torch.stack(stat_exog_batch)
        else:
            stat_exog = None

        # Create windows_batch dictionary
        windows_batch = {
            "insample_y": insample_y.unsqueeze(-1),
            "futr_exog": futr_exog,
            "hist_exog": hist_exog,
            "stat_exog": stat_exog,
            "insample_mask": torch.ones(batch_size, self.input_size, dtype=torch.bool),
        }

        # Call original model
        return self.original_model(windows_batch)

# %% ../nbs/utils.ipynb 39
def create_input_tensor_for_series(train_df, unique_id, wrapper_model, futr_df=None):
    """Create input tensor for a specific series"""
    # Get series-specific data
    train_series = train_df[train_df["unique_id"] == unique_id]

    input_components = []
    futr_exog_cols = wrapper_model.futr_exog_cols
    hist_exog_cols = wrapper_model.hist_exog_cols
    input_size = wrapper_model.input_size

    # Series identifier (encoded as integer)
    series_int = wrapper_model.unique_id_to_int[unique_id]
    input_components.append(np.array([series_int]))

    # Future exogenous features (if they exist) - FULL LENGTH (historical + future)
    if futr_exog_cols:
        if futr_df is None:
            raise ValueError("You must pass a futr_df if futr_exog_list is specified.")
        # Prepare future exogenous data for this series
        futr_exog_series = futr_df[futr_df["unique_id"] == unique_id].reset_index(
            drop=True
        )
        # Get historical part
        futr_hist_data = train_series[futr_exog_cols].values[-input_size:]
        # Get future part
        futr_pred_data = futr_exog_series[futr_exog_cols].values
        # Combine and flatten
        full_futr_data = np.vstack([futr_hist_data, futr_pred_data]).flatten()
        input_components.append(full_futr_data)

    # Historical exogenous features (if they exist)
    if hist_exog_cols:
        hist_exog_data = train_series[hist_exog_cols].values[-input_size:].flatten()
        input_components.append(hist_exog_data)

    # Historical target values (always present)
    hist_target_data = train_series["y"].values[-input_size:]
    input_components.append(hist_target_data)

    # Combine all features
    complete_input = np.concatenate(input_components)
    return torch.tensor(complete_input, dtype=torch.float32).reshape(1, -1)

# %% ../nbs/utils.ipynb 40
def create_multi_series_background_data(train_df, wrapper_model):
    """Create background data including samples from all series"""
    background_samples = []

    futr_exog_cols = wrapper_model.futr_exog_cols
    hist_exog_cols = wrapper_model.hist_exog_cols
    input_size = wrapper_model.input_size
    horizon = wrapper_model.h

    for unique_id in train_df["unique_id"].unique():
        train_series = train_df[train_df["unique_id"] == unique_id]

        # Determine the range of valid indices
        start_idx = input_size
        if futr_exog_cols:
            end_idx = len(train_series) - horizon + 1
        else:
            end_idx = len(train_series)

        # Sample fewer points per series to keep total background size manageable
        for i in range(start_idx, end_idx, 6):  # Increased stride for efficiency
            sample_components = []

            # Series identifier
            series_int = wrapper_model.unique_id_to_int[unique_id]
            sample_components.append(np.array([series_int]))

            # Future exogenous features (if they exist) - FULL LENGTH
            if futr_exog_cols:
                futr_hist_data = (
                    train_series[futr_exog_cols]
                    .iloc[i - input_size : i]
                    .to_numpy()
                    .flatten()
                )
                futr_pred_data = (
                    train_series[futr_exog_cols]
                    .iloc[i : i + horizon]
                    .to_numpy()
                    .flatten()
                )
                full_futr_data = np.concatenate([futr_hist_data, futr_pred_data])
                sample_components.append(full_futr_data)

            # Historical exogenous features (if they exist)
            if hist_exog_cols:
                hist_exog_window = (
                    train_series[hist_exog_cols]
                    .iloc[i - input_size : i]
                    .to_numpy()
                    .flatten()
                )
                sample_components.append(hist_exog_window)

            # Historical target values
            hist_target_window = train_series["y"].iloc[i - input_size : i].to_numpy()
            sample_components.append(hist_target_window)

            # Combine all features
            complete_sample = np.concatenate(sample_components)
            background_samples.append(complete_sample)

    return np.array(background_samples)

# %% ../nbs/utils.ipynb 41
def model_predict(X, wrapper_model):
    """Prediction function that takes numpy array and returns predictions"""
    X_tensor = torch.tensor(X, dtype=torch.float32)
    with torch.no_grad():
        predictions = wrapper_model(X_tensor)
    return predictions.numpy()

# %% ../nbs/utils.ipynb 42
def create_multi_series_feature_names(wrapper_model):
    """Create feature names for multi-series input"""
    feature_names = ["series_id"]
    futr_exog_cols = wrapper_model.futr_exog_cols
    hist_exog_cols = wrapper_model.hist_exog_cols
    input_size = wrapper_model.input_size
    horizon = wrapper_model.h

    # Future exogenous features (if they exist) - FULL LENGTH
    if futr_exog_cols:
        for i in range(input_size):
            for col in futr_exog_cols:
                feature_names.append(f"{col}_hist_lag{i+1}")
        for i in range(horizon):
            for col in futr_exog_cols:
                feature_names.append(f"{col}_h{i+1}")

    # Historical exogenous features (if they exist)
    if hist_exog_cols:
        for i in range(input_size):
            for col in hist_exog_cols:
                feature_names.append(f"{col}_lag{i+1}")

    # Historical target values (always present)
    for i in range(input_size):
        feature_names.append(f"y_lag{i+1}")

    return feature_names
