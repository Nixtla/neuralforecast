



# <kbd>module</kbd> `neuralforecast.models.patchtst`





---



## <kbd>function</kbd> `get_activation_fn`

```python
get_activation_fn(activation)
```






---



## <kbd>function</kbd> `PositionalEncoding`

```python
PositionalEncoding(q_len, hidden_size, normalize=True)
```






---



## <kbd>function</kbd> `PositionalEncoding`

```python
PositionalEncoding(q_len, hidden_size, normalize=True)
```






---



## <kbd>function</kbd> `Coord2dPosEncoding`

```python
Coord2dPosEncoding(
    q_len,
    hidden_size,
    exponential=False,
    normalize=True,
    eps=0.001
)
```






---



## <kbd>function</kbd> `Coord1dPosEncoding`

```python
Coord1dPosEncoding(q_len, exponential=False, normalize=True)
```






---



## <kbd>function</kbd> `positional_encoding`

```python
positional_encoding(pe, learn_pe, q_len, hidden_size)
```






---



## <kbd>class</kbd> `Transpose`
Transpose 



### <kbd>method</kbd> `__init__`

```python
__init__(*dims, contiguous=False)
```








---



### <kbd>method</kbd> `forward`

```python
forward(x)
```






---



## <kbd>class</kbd> `PatchTST_backbone`
PatchTST_backbone 



### <kbd>method</kbd> `__init__`

```python
__init__(
    c_in: int,
    c_out: int,
    input_size: int,
    h: int,
    patch_len: int,
    stride: int,
    max_seq_len: Optional[int] = 1024,
    n_layers: int = 3,
    hidden_size=128,
    n_heads=16,
    d_k: Optional[int] = None,
    d_v: Optional[int] = None,
    linear_hidden_size: int = 256,
    norm: str = 'BatchNorm',
    attn_dropout: float = 0.0,
    dropout: float = 0.0,
    act: str = 'gelu',
    key_padding_mask: str = 'auto',
    padding_var: Optional[int] = None,
    attn_mask: Optional[Tensor] = None,
    res_attention: bool = True,
    pre_norm: bool = False,
    store_attn: bool = False,
    pe: str = 'zeros',
    learn_pe: bool = True,
    fc_dropout: float = 0.0,
    head_dropout=0,
    padding_patch=None,
    pretrain_head: bool = False,
    head_type='flatten',
    individual=False,
    revin=True,
    affine=True,
    subtract_last=False
)
```








---



### <kbd>method</kbd> `create_pretrain_head`

```python
create_pretrain_head(head_nf, vars, dropout)
```





---



### <kbd>method</kbd> `forward`

```python
forward(z)
```






---



## <kbd>class</kbd> `Flatten_Head`
Flatten_Head 



### <kbd>method</kbd> `__init__`

```python
__init__(individual, n_vars, nf, h, c_out, head_dropout=0)
```








---



### <kbd>method</kbd> `forward`

```python
forward(x)
```






---



## <kbd>class</kbd> `TSTiEncoder`
TSTiEncoder 



### <kbd>method</kbd> `__init__`

```python
__init__(
    c_in,
    patch_num,
    patch_len,
    max_seq_len=1024,
    n_layers=3,
    hidden_size=128,
    n_heads=16,
    d_k=None,
    d_v=None,
    linear_hidden_size=256,
    norm='BatchNorm',
    attn_dropout=0.0,
    dropout=0.0,
    act='gelu',
    store_attn=False,
    key_padding_mask='auto',
    padding_var=None,
    attn_mask=None,
    res_attention=True,
    pre_norm=False,
    pe='zeros',
    learn_pe=True
)
```








---



### <kbd>method</kbd> `forward`

```python
forward(x) â†’ Tensor
```






---



## <kbd>class</kbd> `TSTEncoder`
TSTEncoder 



### <kbd>method</kbd> `__init__`

```python
__init__(
    q_len,
    hidden_size,
    n_heads,
    d_k=None,
    d_v=None,
    linear_hidden_size=None,
    norm='BatchNorm',
    attn_dropout=0.0,
    dropout=0.0,
    activation='gelu',
    res_attention=False,
    n_layers=1,
    pre_norm=False,
    store_attn=False
)
```








---



### <kbd>method</kbd> `forward`

```python
forward(
    src: Tensor,
    key_padding_mask: Optional[Tensor] = None,
    attn_mask: Optional[Tensor] = None
)
```






---



## <kbd>class</kbd> `TSTEncoderLayer`
TSTEncoderLayer 



### <kbd>method</kbd> `__init__`

```python
__init__(
    q_len,
    hidden_size,
    n_heads,
    d_k=None,
    d_v=None,
    linear_hidden_size=256,
    store_attn=False,
    norm='BatchNorm',
    attn_dropout=0,
    dropout=0.0,
    bias=True,
    activation='gelu',
    res_attention=False,
    pre_norm=False
)
```








---



### <kbd>method</kbd> `forward`

```python
forward(
    src: Tensor,
    prev: Optional[Tensor] = None,
    key_padding_mask: Optional[Tensor] = None,
    attn_mask: Optional[Tensor] = None
)
```






---



## <kbd>class</kbd> `PatchTST`
PatchTST 

The PatchTST model is an efficient Transformer-based model for multivariate time series forecasting. 

It is based on two key components: 
- segmentation of time series into windows (patches) which are served as input tokens to Transformer 
- channel-independence, where each channel contains a single univariate time series. 



**Args:**
 
 - <b>`h`</b> (int):  forecast horizon. 
 - <b>`input_size`</b> (int):  autorregresive inputs size, y=[1,2,3,4] input_size=2 -> y_[t-2:t]=[1,2]. 
 - <b>`stat_exog_list`</b> (str list):  static exogenous columns. 
 - <b>`hist_exog_list`</b> (str list):  historic exogenous columns. 
 - <b>`futr_exog_list`</b> (str list):  future exogenous columns. 
 - <b>`exclude_insample_y`</b> (bool):  the model skips the autoregressive features y[t-input_size:t] if True. 
 - <b>`encoder_layers`</b> (int):  number of layers for encoder. 
 - <b>`n_heads`</b> (int):  number of multi-head's attention. 
 - <b>`hidden_size`</b> (int):  units of embeddings and encoders. 
 - <b>`linear_hidden_size`</b> (int):  units of linear layer. 
 - <b>`dropout`</b> (float):  dropout rate for residual connection. 
 - <b>`fc_dropout`</b> (float):  dropout rate for linear layer. 
 - <b>`head_dropout`</b> (float):  dropout rate for Flatten head layer. 
 - <b>`attn_dropout`</b> (float):  dropout rate for attention layer. 
 - <b>`patch_len`</b> (int):  length of patch. Note: patch_len = min(patch_len, input_size + stride). 
 - <b>`stride`</b> (int):  stride of patch. 
 - <b>`revin`</b> (bool):  bool to use RevIn. 
 - <b>`revin_affine`</b> (bool):  bool to use affine in RevIn. 
 - <b>`revin_subtract_last`</b> (bool):  bool to use substract last in RevIn. 
 - <b>`activation`</b> (str):  activation from ['gelu','relu']. 
 - <b>`res_attention`</b> (bool):  bool to use residual attention. 
 - <b>`batch_normalization`</b> (bool):  bool to use batch normalization. 
 - <b>`learn_pos_embed`</b> (bool):  bool to learn positional embedding. 
 - <b>`loss`</b> (PyTorch module):  instantiated train loss class from [losses collection](./losses.pytorch). 
 - <b>`valid_loss`</b> (PyTorch module):  instantiated valid loss class from [losses collection](./losses.pytorch). 
 - <b>`max_steps`</b> (int):  maximum number of training steps. 
 - <b>`learning_rate`</b> (float):  learning rate between (0, 1). 
 - <b>`num_lr_decays`</b> (int):  number of learning rate decays, evenly distributed across max_steps. 
 - <b>`early_stop_patience_steps`</b> (int):  number of validation iterations before early stopping. 
 - <b>`val_check_steps`</b> (int):  number of training steps between every validation loss check. 
 - <b>`batch_size`</b> (int):  number of different series in each batch. 
 - <b>`valid_batch_size`</b> (int):  number of different series in each validation and test batch, if None uses batch_size. 
 - <b>`windows_batch_size`</b> (int):  number of windows to sample in each training batch, default uses all. 
 - <b>`inference_windows_batch_size`</b> (int):  number of windows to sample in each inference batch. 
 - <b>`start_padding_enabled`</b> (bool):  if True, the model will pad the time series with zeros at the beginning, by input size. 
 - <b>`training_data_availability_threshold`</b> (Union[float, List[float]]):  minimum fraction of valid data points required for training windows. Single float applies to both insample and outsample; list of two floats specifies [insample_fraction, outsample_fraction]. Default 0.0 allows windows with only 1 valid data point (current behavior). 
 - <b>`step_size`</b> (int):  step size between each window of temporal data. 
 - <b>`scaler_type`</b> (str):  type of scaler for temporal inputs normalization see [temporal scalers](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/common/_scalers.py). 
 - <b>`random_seed`</b> (int):  random_seed for pytorch initializer and numpy generators. 
 - <b>`drop_last_loader`</b> (bool):  if True `TimeSeriesDataLoader` drops last non-full batch. 
 - <b>`alias`</b> (str):  optional,  Custom name of the model. 
 - <b>`optimizer`</b> (Subclass of 'torch.optim.Optimizer'):  optional, user specified optimizer instead of the default choice (Adam). 
 - <b>`optimizer_kwargs`</b> (dict):  optional, list of parameters used by the user specified `optimizer`. 
 - <b>`lr_scheduler`</b> (Subclass of 'torch.optim.lr_scheduler.LRScheduler'):  optional, user specified lr_scheduler instead of the default choice (StepLR). 
 - <b>`lr_scheduler_kwargs`</b> (dict):  optional, list of parameters used by the user specified `lr_scheduler`. 
 - <b>`dataloader_kwargs`</b> (dict):  optional, list of parameters passed into the PyTorch Lightning dataloader by the `TimeSeriesDataLoader`. 
 - <b>`**trainer_kwargs (int)`</b>:   keyword trainer arguments inherited from [PyTorch Lighning's trainer](https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.trainer.trainer.Trainer.html?highlight=trainer). 

References: 
    - [Nie, Y., Nguyen, N. H., Sinthong, P., & Kalagnanam, J. (2022). "A Time Series is Worth 64 Words: Long-term Forecasting with Transformers"](https://arxiv.org/pdf/2211.14730.pdf) 



### <kbd>method</kbd> `__init__`

```python
__init__(
    h,
    input_size,
    stat_exog_list=None,
    hist_exog_list=None,
    futr_exog_list=None,
    exclude_insample_y=False,
    encoder_layers: int = 3,
    n_heads: int = 16,
    hidden_size: int = 128,
    linear_hidden_size: int = 256,
    dropout: float = 0.2,
    fc_dropout: float = 0.2,
    head_dropout: float = 0.0,
    attn_dropout: float = 0.0,
    patch_len: int = 16,
    stride: int = 8,
    revin: bool = True,
    revin_affine: bool = False,
    revin_subtract_last: bool = True,
    activation: str = 'gelu',
    res_attention: bool = True,
    batch_normalization: bool = False,
    learn_pos_embed: bool = True,
    loss=MAE(),
    valid_loss=None,
    max_steps: int = 5000,
    learning_rate: float = 0.0001,
    num_lr_decays: int = -1,
    early_stop_patience_steps: int = -1,
    val_check_steps: int = 100,
    batch_size: int = 32,
    valid_batch_size: Optional[int] = None,
    windows_batch_size=1024,
    inference_windows_batch_size: int = 1024,
    start_padding_enabled=False,
    training_data_availability_threshold=0.0,
    step_size: int = 1,
    scaler_type: str = 'identity',
    random_seed: int = 1,
    drop_last_loader: bool = False,
    alias: Optional[str] = None,
    optimizer=None,
    optimizer_kwargs=None,
    lr_scheduler=None,
    lr_scheduler_kwargs=None,
    dataloader_kwargs=None,
    **trainer_kwargs
)
```






---

#### <kbd>property</kbd> automatic_optimization

If set to ``False`` you are responsible for calling ``.backward()``, ``.step()``, ``.zero_grad()``. 

---

#### <kbd>property</kbd> current_epoch

The current epoch in the ``Trainer``, or 0 if not attached. 

---

#### <kbd>property</kbd> device





---

#### <kbd>property</kbd> device_mesh

Strategies like ``ModelParallelStrategy`` will create a device mesh that can be accessed in the :meth:`~pytorch_lightning.core.hooks.ModelHooks.configure_model` hook to parallelize the LightningModule. 

---

#### <kbd>property</kbd> dtype





---

#### <kbd>property</kbd> example_input_array

The example input array is a specification of what the module can consume in the :meth:`forward` method. The return type is interpreted as follows: 


-   Single tensor: It is assumed the model takes a single argument, i.e.,  ``model.forward(model.example_input_array)`` 
-   Tuple: The input array should be interpreted as a sequence of positional arguments, i.e.,  ``model.forward(*model.example_input_array)`` 
-   Dict: The input array represents named keyword arguments, i.e.,  ``model.forward(**model.example_input_array)`` 

---

#### <kbd>property</kbd> fabric





---

#### <kbd>property</kbd> global_rank

The index of the current process across all nodes and devices. 

---

#### <kbd>property</kbd> global_step

Total training batches seen across all epochs. 

If no Trainer is attached, this property is 0. 

---

#### <kbd>property</kbd> hparams

The collection of hyperparameters saved with :meth:`save_hyperparameters`. It is mutable by the user. For the frozen set of initial hyperparameters, use :attr:`hparams_initial`. 



**Returns:**
  Mutable hyperparameters dictionary 

---

#### <kbd>property</kbd> hparams_initial

The collection of hyperparameters saved with :meth:`save_hyperparameters`. These contents are read-only. Manual updates to the saved hyperparameters can instead be performed through :attr:`hparams`. 



**Returns:**
 
 - <b>`AttributeDict`</b>:  immutable initial hyperparameters 

---

#### <kbd>property</kbd> local_rank

The index of the current process within a single node. 

---

#### <kbd>property</kbd> logger

Reference to the logger object in the Trainer. 

---

#### <kbd>property</kbd> loggers

Reference to the list of loggers in the Trainer. 

---

#### <kbd>property</kbd> on_gpu

Returns ``True`` if this model is currently located on a GPU. 

Useful to set flags around the LightningModule for different CPU vs GPU behavior. 

---

#### <kbd>property</kbd> strict_loading

Determines how Lightning loads this model using `.load_state_dict(..., strict=model.strict_loading)`. 

---

#### <kbd>property</kbd> trainer







---



### <kbd>method</kbd> `forward`

```python
forward(windows_batch)
```






